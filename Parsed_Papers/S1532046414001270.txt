@&#MAIN-TITLE@&#Multi-label classification of chronically ill patients with bag of words and supervised dimensionality reduction algorithms

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We study discrete medical time series datasets with multi-label classification.


                        
                        
                           
                           We focus on chronically ill patients with multiple illnesses.


                        
                        
                           
                           We use Bag of Words quantization on medical time series.


                        
                        
                           
                           We compare multi-label classification and dimensionality reduction algorithms.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multi-label classification

Complex patient

Diabetes type 2

Clinical data

Dimensionality reduction

Kernel methods

@&#ABSTRACT@&#


               
               
                  Objective
                  This research is motivated by the issue of classifying illnesses of chronically ill patients for decision support in clinical settings. Our main objective is to propose multi-label classification of multivariate time series contained in medical records of chronically ill patients, by means of quantization methods, such as bag of words (BoW), and multi-label classification algorithms. Our second objective is to compare supervised dimensionality reduction techniques to state-of-the-art multi-label classification algorithms. The hypothesis is that kernel methods and locality preserving projections make such algorithms good candidates to study multi-label medical time series.
               
               
                  Methods
                  We combine BoW and supervised dimensionality reduction algorithms to perform multi-label classification on health records of chronically ill patients. The considered algorithms are compared with state-of-the-art multi-label classifiers in two real world datasets. Portavita dataset contains 525 diabetes type 2 (DT2) patients, with co-morbidities of DT2 such as hypertension, dyslipidemia, and microvascular or macrovascular issues. MIMIC II dataset contains 2635 patients affected by thyroid disease, diabetes mellitus, lipoid metabolism disease, fluid electrolyte disease, hypertensive disease, thrombosis, hypotension, chronic obstructive pulmonary disease (COPD), liver disease and kidney disease. The algorithms are evaluated using multi-label evaluation metrics such as hamming loss, one error, coverage, ranking loss, and average precision.
               
               
                  Results
                  Non-linear dimensionality reduction approaches behave well on medical time series quantized using the BoW algorithm, with results comparable to state-of-the-art multi-label classification algorithms. Chaining the projected features has a positive impact on the performance of the algorithm with respect to pure binary relevance approaches.
               
               
                  Conclusions
                  The evaluation highlights the feasibility of representing medical health records using the BoW for multi-label classification tasks. The study also highlights that dimensionality reduction algorithms based on kernel methods, locality preserving projections or both are good candidates to deal with multi-label classification tasks in medical time series with many missing values and high label density.
               
            

@&#INTRODUCTION@&#

The average lifespan has increased considerably due to the invention of better drugs and improvement of healthcare, but the rate of chronic illnesses per patient has also increased, becoming a burden for the economy of industrialized and emerging countries [1].

The interaction between chronic illnesses and multiple drugs intake make the patient treatment complex to handle for caregivers. The possibility of taking informed decisions about complex patients is important to slow down the development of their illnesses.

Unfortunately, doctors have to take decisions whose consequences will be evaluated only after years of treatment. Furthermore, given the growth in number of chronically ill patients, caregivers are often in charge of hundreds of patients [2]. In addition, patient electronic health records (EHR) often contain the evolution in time of the patient clinical data, which are high dimensional multivariate time series of physiological values.

As reported in [3], physicians would use services that improve their understanding of an illness even if these involve more cognitive effort than in the standard practice. In particular, in the medical informatics and data mining community [4,5] it has already been discussed that classifying patients given their physiological values and laboratory tests may help caregivers’ decision making process.

This paper is motivated by the problem of classifying patients affected by multiple illnesses to enhance the decision support of medical doctors. There are two challenges to overcome in order to define a system capable to correctly classify the multiple illnesses that may affect a chronically ill patient: (a) dealing with irregular multivariate time series; and (b) dealing with the interaction of multiple co-morbidities in a heterogeneous population of patients.

The presence of high dimensional and multivariate data presents a big challenge to standard classification algorithms due to the curse of dimensionality [6]. Clinical time series are often irregular, a patient may present different number of records with respect to another patient and the periods of time in which the values are collected may not be aligned. The challenge is even more difficult if we consider the inherently multi-label properties of medical data, where a patient may present multiple co-morbidities at once.

Concerning irregular time series, quantization algorithms, such as the Bag of Words (BoW) model, have proven successful in several medical tasks [7].

As a matter of fact, BoW is often used in biomedical time series. In [7], Wang et al. present an application of the BoW model to EEG and ECG signals. Similarly to us, the authors of [7] are faced with the issue of time series of different length with possibly heterogeneous patients at hand.

Jiu et al. present a supervised approach towards BoW codebook generation using neural networks in [8]. In particular, the approach uses Multi-Layer Perceptrons (MLP) and the backpropagation algorithm to update the weights of the codewords according to their discrimination capabilities with respect to a set of classes.

Similarly to [8], in [9] Ordonez et al. present a modification of the BoW model to classify medical time series. Such a model uses continuous multivariate time series to compute a symbolic representation of the signals that is then used as the codebook for the classification of the patients.

Concerning multi-label classification algorithms, an extensive review can be found in [10]. Multi-label learning [11] implies training sets where each instance has a labelset and the task is to predict the labelset of unseen instances. As reported in [10], there exist works that combine supervised dimensionality reduction with multi-label learning [12–14]. Furthermore, most of these works focus on applying multi-label techniques on text analysis with static datasets [15].

In general terms multi-label classification of complex patients in discrete medical time series is quite an unexplored issue. Firstly, we think that the main contribution of this paper is to propose the combination of BoW, to quantize irregular time series present in patient health records, and multi-label classification algorithms, to classify the chronic illnesses that a patient may present. These are two established techniques, but in medical settings their combination is quite novel.

Secondly, we believe that this contribution is interesting to biomedical informatics as we evaluate linear and non-linear supervised dimensionality reduction approaches with respect to multi-label classification in medical time series, and we compare these approaches with state-of-the-art multi-label classification algorithms. In doing this, we aim at identifying the most effective supervised dimensionality reduction techniques with respect to medical time series. We aim to confirm the hypothesis that, given the nature of the data at hand, non-linear supervised dimensionality reduction algorithms have a behavior comparable to state of the art multi-label classifiers.

Thirdly, our contribution is also of interest to biomedical research because we perform our evaluation against two real world medical datasets: the Portavita dataset, provided for this study by the Portavita company,
                        1
                        
                           www.portavita.eu.
                     
                     
                        1
                      containing 525 diabetic patients presenting, sometimes simultaneously, hypertension, dyslipidemia or microvascular and macrovascular complications of diabetes type 2 (DT2) [16]; an extraction of 2635 patients from the public MIMIC II database [17], where we consider patients affected simultaneously by thyroid disease, diabetes mellitus, lipoid metabolism disease, fluid electrolyte disease, hypertensive disease, thrombosis, hypotension, chronic obstructive pulmonary disease (COPD), liver disease and kidney disease.

The rest of this paper is organized as follows: Section 2 presents a background on multi-label classification, kernel methods, and supervised dimensionality reduction algorithms; Section 3 presents the Portavita and MIMIC II datasets and their properties; Section 4 presents the training schema for the attempted multi-label classification algorithms; Section 5 presents an evaluation for the multi-label algorithms considered in this paper; finally, Section 6 concludes this paper and draws the lines for future work.

@&#BACKGROUND@&#

In this Section we present the concepts of multi-label classification, kernels, locality preserving projections and multi-class Fisher discriminant analysis. In Section 4 we show how we combined these concepts in a system for classification of multi-label chronically ill patients.

Let X be the domain of observations and let L be the finite set of labels. Given a training set 
                           
                              T
                              =
                              {
                              (
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              )
                              ,
                              (
                              
                                 
                                    x
                                 
                                 
                                    2
                                 
                              
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    2
                                 
                              
                              )
                              ,
                              …
                              ,
                              (
                              
                                 
                                    x
                                 
                                 
                                    n
                                 
                              
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    n
                                 
                              
                              )
                              }
                              
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ∈
                              X
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                              ⊆
                              L
                              )
                           
                         i.i.d. drawn from an unknown distribution D, the goal is to learn a multi-label classifier 
                           
                              h
                              :
                              X
                              →
                              
                                 
                                    2
                                 
                                 
                                    L
                                 
                              
                           
                        . However, it is often more convenient to learn a real-valued scoring function of the form 
                           
                              f
                              :
                              X
                              ×
                              L
                              →
                              R
                           
                        . Given an instance 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         and its associated label set 
                           
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                           
                        , a working system will attempt to produce larger values for labels in 
                           
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                           
                         than those that are not in 
                           
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                           
                        , i.e. 
                           
                              f
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    y
                                 
                                 
                                    1
                                 
                              
                              )
                              >
                              f
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    y
                                 
                                 
                                    2
                                 
                              
                              )
                           
                         for any 
                           
                              
                                 
                                    y
                                 
                                 
                                    1
                                 
                              
                              ∈
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    y
                                 
                                 
                                    2
                                 
                              
                              ∉
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                           
                        . By the use of the function 
                           
                              f
                              (
                              ·
                              ,
                              ·
                              )
                           
                        , we can obtain a multi-label classifier: 
                           
                              h
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              )
                              =
                              {
                              y
                              |
                              f
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              y
                              )
                              >
                              δ
                              ,
                              y
                              ∈
                              L
                              }
                           
                        , where 
                           
                              δ
                           
                         is a threshold to infer from the training set. The function 
                           
                              f
                              (
                              ·
                              ,
                              ·
                              )
                           
                         can also be adapted to a ranking function 
                           
                              
                                 
                                    rank
                                 
                                 
                                    f
                                 
                              
                              (
                              ·
                              ,
                              ·
                              )
                           
                        , which maps the outputs of 
                           
                              f
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              y
                              )
                           
                         for any 
                           
                              y
                              ∈
                              L
                           
                         to 
                           
                              {
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              |
                              L
                              |
                              }
                           
                         such that if 
                           
                              f
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    y
                                 
                                 
                                    1
                                 
                              
                              )
                              >
                              f
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    y
                                 
                                 
                                    2
                                 
                              
                              )
                           
                         then 
                           
                              
                                 
                                    rank
                                 
                                 
                                    f
                                 
                              
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    y
                                 
                                 
                                    1
                                 
                              
                              )
                              <
                              
                                 
                                    rank
                                 
                                 
                                    f
                                 
                              
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    y
                                 
                                 
                                    2
                                 
                              
                              )
                           
                        .

Furthermore, there exist several approaches to train multi-label classifiers (see [10] for a comprehensive review on the subject). The simplest approach, known as binary relevance (BR), is to train one binary classifier per label with traditional classification algorithms, considering each label as a separated problem. BR has the disadvantage of not taking into consideration the relationships existing amongst labels. To overcome this issue, several ensemble methods have been defined in the past, amongst which the most popular ones are classifier chains (CC) and label powersets approaches (LP). CC methods work by recursively training classifiers with the label predicted by the previous classifier as new features. LP methods focus on training classifiers defining classes by means of subsets of the labelset. Despite having been demonstrated effective, CC and LP methods present computational disadvantages with respect to BR methods, whose complexity is linear in respect to the number of labels. In addition, CC methods are difficult to train with classifier presenting many parameters, as each classifier in the chain needs to be optimized differently. Secondly, LP method are computationally infeasible due to the large number of possible classes in a labelset.

Other approaches use the probabilistic distribution of the labels and their dependencies within a neighborhood to tune the classifier output. MLkNN [18] is a successful example of such a method.

Within this paper we will show the effect of using dimensionality reduction algorithms with a BR approach, considering the output of each classifier as separated, or as CC classifiers by concatenating the projected features.

Non-linear subspaces may be suitable to describe clinical datasets as due to their high dimensionality they may lie in complex manifolds. Therefore, we may need to map our input data in terms of clinical datasets to a higher dimensional space using a linearization function. If we consider a set of m samples 
                           
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    x
                                 
                                 
                                    m
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    n
                                 
                              
                           
                        , belonging to c classes, then we can consider a non-linear mapping 
                           
                              ϕ
                              :
                              
                                 
                                    R
                                 
                                 
                                    n
                                 
                              
                              →
                              F
                           
                        , where we choose 
                           
                              ϕ
                           
                         so that 
                           
                              〈
                              ϕ
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              )
                              ,
                              ϕ
                              (
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                              )
                              〉
                              =
                              K
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                              )
                           
                        , where 
                           
                              K
                              (
                              ·
                              ,
                              ·
                              )
                           
                         is a positive semi-definite kernel function.

Performing this map explicitly can be computationally expensive, to avoid it we can apply the Kernel Trick 
                        [19], and calculate the Gram matrix 
                           
                              K
                              (
                              ·
                              ,
                              ·
                              )
                           
                        , containing the inner product between the input vectors in the linearization space. This then allows us to modify linear techniques using the inner product with appropriate kernel functions, opening up the possibility of applying well known approaches in non-linear spaces.

Within this paper we will use the RBF kernel and the histogram intersection kernel 
                        [20]. The RBF kernel is defined as:
                           
                              (1)
                              
                                 K
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             exp
                                          
                                          
                                             -
                                             ‖
                                             x
                                             -
                                             y
                                             
                                                
                                                   ‖
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    
                                       2
                                       
                                          
                                             σ
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        The histogram intersection kernel can be defined starting from two histograms x and y consisting both of m features. We denote the ith features of x as 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         and for y as 
                           
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                           
                        . Then we can define the kernel as:
                           
                              (2)
                              
                                 K
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                       
                                       
                                          m
                                       
                                    
                                 
                                 min
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                           
                        
                     

A big advantage of this kernel is that it is parameterless.

As explained in [21], a LPP projection is a linear transformation for which the data residing in a space 
                           
                              
                                 
                                    R
                                 
                                 
                                    n
                                 
                              
                           
                         are mapped in a subspace 
                           
                              
                                 
                                    R
                                 
                                 
                                    r
                                 
                              
                           
                        , with 
                           
                              r
                              <
                              n
                           
                        , such that nearby data pairs in the original n-dimensional space are also close in the identified subspace. More formally, if we consider a square matrix 
                           
                              A
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    d
                                    ×
                                    d
                                 
                              
                           
                        , where 
                           
                              
                                 
                                    A
                                 
                                 
                                    i
                                    ,
                                    j
                                 
                              
                              ∈
                              [
                              0
                              ,
                              1
                              ]
                           
                        , representing the affinity between the elements 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                           
                         in a dataset with d elements, the 
                           
                              
                                 
                                    T
                                 
                                 
                                    LPP
                                 
                              
                           
                         transformation matrix can be defined as follows:
                           
                              (3)
                              
                                 
                                    
                                       T
                                    
                                    
                                       LPP
                                    
                                 
                                 =
                                 
                                    
                                       
                                          arg
                                          
                                          min
                                       
                                       
                                          T
                                          ∈
                                          
                                             
                                                R
                                             
                                             
                                                d
                                                ×
                                                r
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                1
                                             
                                             
                                                2
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   ,
                                                   j
                                                   =
                                                   1
                                                
                                                
                                                   n
                                                
                                             
                                          
                                          
                                             
                                                A
                                             
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                          ‖
                                          
                                             
                                                T
                                             
                                             
                                                T
                                             
                                          
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          -
                                          
                                             
                                                T
                                             
                                             
                                                T
                                             
                                          
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                          
                                             
                                                ‖
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        Within this paper we are interested in the usage of such a projection within the KLFDA technique, further details on how to calculate 
                           
                              
                                 
                                    T
                                 
                                 
                                    LPP
                                 
                              
                           
                         can be found in [21].

Linear Discriminant Analysis (LDA) [22] is a widely used supervised dimensionality reduction technique that can find the linear transformation which best separates elements of different classes. To achieve this, LDA makes use of the within-class scatter matrix 
                           
                              
                                 
                                    S
                                 
                                 
                                    (
                                    w
                                    )
                                 
                              
                           
                         and of the between-class scatter matrix 
                           
                              
                                 
                                    S
                                 
                                 
                                    (
                                    b
                                    )
                                 
                              
                           
                        . These can be defined as:
                           
                              (4)
                              
                                 
                                    
                                       S
                                    
                                    
                                       (
                                       w
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          C
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          x
                                          ∈
                                          
                                             
                                                E
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 (
                                 x
                                 -
                                 
                                    
                                       μ
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 
                                    
                                       (
                                       x
                                       -
                                       
                                          
                                             μ
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       T
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    μ
                                 
                                 
                                    i
                                 
                              
                           
                         is the mean of class 
                           
                              
                                 
                                    E
                                 
                                 
                                    i
                                 
                              
                           
                        , and
                           
                              (5)
                              
                                 
                                    
                                       S
                                    
                                    
                                       (
                                       b
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          C
                                       
                                    
                                 
                                 
                                    
                                       N
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 
                                    
                                       μ
                                    
                                    
                                       i
                                    
                                 
                                 -
                                 μ
                                 )
                                 
                                    
                                       (
                                       
                                          
                                             μ
                                          
                                          
                                             i
                                          
                                       
                                       -
                                       μ
                                       )
                                    
                                    
                                       T
                                    
                                 
                              
                           
                        where 
                           
                              μ
                           
                         is the global mean and 
                           
                              
                                 
                                    N
                                 
                                 
                                    i
                                 
                              
                           
                         is the number of elements belonging to class 
                           
                              
                                 
                                    E
                                 
                                 
                                    i
                                 
                              
                           
                        . 
                           
                              
                                 
                                    S
                                 
                                 
                                    (
                                    w
                                    )
                                 
                              
                           
                         is a measure of the variance between the elements belonging to the same class, while 
                           
                              
                                 
                                    S
                                 
                                 
                                    (
                                    b
                                    )
                                 
                              
                           
                         is a measure of the variance of the elements belonging to different classes. Ideally, we want the scatter to be minimized for elements of the same class and maximized for elements of different classes. The transformation matrix 
                           
                              
                                 
                                    T
                                 
                                 
                                    LDA
                                 
                              
                           
                         that achieves this is defined as:
                           
                              (6)
                              
                                 
                                    
                                       T
                                    
                                    
                                       LDA
                                    
                                 
                                 =
                                 arg
                                 
                                 max
                                 
                                    
                                       det
                                       (
                                       
                                          
                                             T
                                          
                                          
                                             T
                                          
                                       
                                       
                                          
                                             S
                                          
                                          
                                             (
                                             w
                                             )
                                          
                                       
                                       T
                                       )
                                    
                                    
                                       det
                                       (
                                       
                                          
                                             T
                                          
                                          
                                             T
                                          
                                       
                                       
                                          
                                             S
                                          
                                          
                                             (
                                             b
                                             )
                                          
                                       
                                       T
                                       )
                                    
                                 
                              
                           
                        As explained in [23], to specify a Locality Sensitive LDA (LSDA), we can define the local within-class scatter matrix 
                           
                              
                                 
                                    
                                       
                                          S
                                       
                                       
                                          ∼
                                       
                                    
                                 
                                 
                                    (
                                    w
                                    )
                                 
                              
                           
                         and the local between class scatter matrix 
                           
                              
                                 
                                    
                                       
                                          S
                                       
                                       
                                          ∼
                                       
                                    
                                 
                                 
                                    (
                                    b
                                    )
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    
                                       
                                          
                                             S
                                          
                                          
                                             ∼
                                          
                                       
                                    
                                    
                                       (
                                       w
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          ,
                                          j
                                          =
                                          1
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             W
                                          
                                          
                                             ∼
                                          
                                       
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                    
                                       (
                                       w
                                       )
                                    
                                 
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 -
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 
                                    
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       -
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                    
                                       T
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    
                                       
                                          
                                             S
                                          
                                          
                                             ∼
                                          
                                       
                                    
                                    
                                       (
                                       b
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          ,
                                          j
                                          =
                                          1
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             W
                                          
                                          
                                             ∼
                                          
                                       
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                    
                                       (
                                       b
                                       )
                                    
                                 
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 -
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 
                                    
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       -
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                    
                                       T
                                    
                                 
                              
                           
                        where
                           
                              (9)
                              
                                 
                                    
                                       
                                          
                                             W
                                          
                                          
                                             ∼
                                          
                                       
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                    
                                       (
                                       w
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         A
                                                      
                                                      
                                                         i
                                                         ,
                                                         j
                                                      
                                                   
                                                   /
                                                   
                                                      
                                                         N
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                                
                                                   if
                                                   
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   =
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   =
                                                   c
                                                   ,
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   if
                                                   
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                   ≠
                                                   
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (10)
                              
                                 
                                    
                                       
                                          
                                             W
                                          
                                          
                                             ∼
                                          
                                       
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                    
                                       (
                                       b
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         A
                                                      
                                                      
                                                         i
                                                         ,
                                                         j
                                                      
                                                   
                                                   (
                                                   1
                                                   /
                                                   N
                                                   -
                                                   1
                                                   /
                                                   
                                                      
                                                         N
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   )
                                                
                                                
                                                   if
                                                   
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   =
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   =
                                                   c
                                                   ,
                                                
                                             
                                             
                                                
                                                   1
                                                   /
                                                   N
                                                
                                                
                                                   if
                                                   
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                   ≠
                                                   
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        which implies that we are weighting the pairwise values according to their affinity matrix 
                           
                              
                                 
                                    A
                                 
                                 
                                    i
                                    ,
                                    j
                                 
                              
                              ∈
                              [
                              0
                              ,
                              1
                              ]
                           
                        , with 
                           
                              
                                 
                                    A
                                 
                                 
                                    i
                                    ,
                                    j
                                 
                              
                           
                         closer to 1 if 
                           
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                           
                         is close to 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         and to 0 if they are far apart.

Then, the objective function can be expressed again as a generalized eigenvalue problem:
                           
                              (11)
                              
                                 
                                    
                                       T
                                    
                                    
                                       LSDA
                                    
                                 
                                 =
                                 
                                    
                                       
                                          arg
                                          
                                          max
                                       
                                       
                                          T
                                          ∈
                                          
                                             
                                                R
                                             
                                             
                                                d
                                                ×
                                                r
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          tr
                                          (
                                          
                                             
                                                (
                                                
                                                   
                                                      T
                                                   
                                                   
                                                      T
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            S
                                                         
                                                         
                                                            ∼
                                                         
                                                      
                                                   
                                                   
                                                      (
                                                      w
                                                      )
                                                   
                                                
                                                T
                                                )
                                             
                                             
                                                -
                                                1
                                             
                                          
                                          
                                             
                                                T
                                             
                                             
                                                T
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      S
                                                   
                                                   
                                                      ∼
                                                   
                                                
                                             
                                             
                                                (
                                                b
                                                )
                                             
                                          
                                          T
                                          )
                                          )
                                       
                                    
                                 
                              
                           
                        we refer the interested reader to [23], for further details on how to compute LSDA.

KLFDA [23] is a generalization of the previously presented LSDA using kernel functions. If we consider 
                           
                              
                                 
                                    
                                       
                                          S
                                       
                                       
                                          ∼
                                       
                                    
                                 
                                 
                                    b
                                 
                                 
                                    ϕ
                                 
                              
                              ,
                              
                                 
                                    
                                       
                                          S
                                       
                                       
                                          ∼
                                       
                                    
                                 
                                 
                                    w
                                 
                                 
                                    ϕ
                                 
                              
                           
                         and 
                           
                              
                                 
                                    
                                       
                                          S
                                       
                                       
                                          ∼
                                       
                                    
                                 
                                 
                                    t
                                 
                                 
                                    ϕ
                                 
                              
                           
                         as the local between-class, within-class and total scatter matrices respectively in the space identified by a kernel mapping, then KLFDA seeks to find:
                           
                              (12)
                              
                                 
                                    
                                       T
                                    
                                    
                                       opt
                                    
                                 
                                 =
                                 arg
                                 
                                 max
                                 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             T
                                          
                                       
                                       
                                          
                                             
                                                
                                                   S
                                                
                                                
                                                   ∼
                                                
                                             
                                          
                                          
                                             b
                                          
                                          
                                             ϕ
                                          
                                       
                                       T
                                    
                                    
                                       
                                          
                                             T
                                          
                                          
                                             T
                                          
                                       
                                       
                                          
                                             
                                                
                                                   S
                                                
                                                
                                                   ∼
                                                
                                             
                                          
                                          
                                             w
                                          
                                          
                                             ϕ
                                          
                                       
                                       T
                                    
                                 
                                 =
                                 arg max
                                 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             T
                                          
                                       
                                       
                                          
                                             
                                                
                                                   S
                                                
                                                
                                                   ∼
                                                
                                             
                                          
                                          
                                             b
                                          
                                          
                                             ϕ
                                          
                                       
                                       T
                                    
                                    
                                       
                                          
                                             T
                                          
                                          
                                             T
                                          
                                       
                                       
                                          
                                             
                                                
                                                   S
                                                
                                                
                                                   ∼
                                                
                                             
                                          
                                          
                                             t
                                          
                                          
                                             ϕ
                                          
                                       
                                       T
                                    
                                 
                              
                           
                        We can justify the use of supervised techniques based on LPP and kernel methods with the considerations in [21], for which LPP is particularly useful in applications where by preserving the structure of the neighborhood in the lower dimensional space, nearest neighbor based approaches can still perform well, and the curse of dimensionality is mitigated. Kernel methods are useful in cases where the classes are non-linearly separable. In our case, we apply the version of KLFDA specified in [23] using regularization.

In this Section, we present the descriptive statistics of the two datasets taken into consideration. For multi-label datasets, amongst the descriptive statistics it is important to also consider label cardinality and label density. Given a dataset D, and a set of labels L, where the labels of an example are denoted with 
                        
                           
                              
                                 Y
                              
                              
                                 i
                              
                           
                        
                      we can define label cardinality and label density as below.


                     Label Cardinality: Label cardinality of a dataset D is the average number of labels of the examples in D:
                        
                           (13)
                           
                              LC
                              (
                              D
                              )
                              =
                              
                                 
                                    1
                                 
                                 
                                    |
                                    D
                                    |
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       |
                                       D
                                       |
                                    
                                 
                              
                              |
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                              |
                           
                        
                     
                  


                     Label Density: Label density of D is the average number of labels of the examples in D divided by 
                        
                           |
                           L
                           |
                        
                     
                     
                        
                           (14)
                           
                              LD
                              (
                              D
                              )
                              =
                              
                                 
                                    1
                                 
                                 
                                    |
                                    D
                                    |
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       |
                                       D
                                       |
                                    
                                 
                              
                              
                                 
                                    |
                                    
                                       
                                          Y
                                       
                                       
                                          i
                                       
                                    
                                    |
                                 
                                 
                                    |
                                    L
                                    |
                                 
                              
                           
                        
                     Label cardinality quantifies the average number of alternative labels that characterize the examples in the dataset. With respect to label cardinality, label density also considers the number of labels. The two metrics are important because multi-label algorithms may present a different behavior in datasets with similar cardinality, but different density.

The Portavita dataset is a medical dataset collected during the standard care of DT2 patients. Such a dataset includes 525 diabetic patients affected by four complications which are: hypertension, dyslipidemia, microvascular and macrovascular diseases. A summary showing the distribution of the labels amongst the patients in such a dataset is shown in Table 1
                        .

The Portavita dataset presents a label cardinality of 2.13, a label density of 
                           
                              0.532
                           
                        , with a total of 15 possible symbols (combination of co-occurring labels), all occurring in the dataset. All patients have multiple health records (>3), for an average number of records per patient equal to 
                           
                              6.72
                           
                         and a total number of records equal to 3528, comprising a set of common laboratory tests and physical examinations that are part of normal routine tests in DT2. Table 2
                         gives a summary of the descriptive statistics of such laboratory tests.

Depending on the stability of the diabetic patient physiological values, the data may be collected once every six months, or once every three months, to check for the presence of microvascular or macrovascular complications.

As this is a real world dataset, the presence of a label may simply point towards a suspected issues, requiring further laboratory tests before it can be confirmed. In other cases, the label is assigned at the beginning of the treatment, and then it is never removed even if the patient does not present the complication any more.

We can calculate that the prior probability for a patient to present a label to be 53.33% for each label, which represents a base average precision to compare against with the attempted classifiers. In the Portavita dataset, the tests are performed with a frequency of 3/6months for most of the features, which are conducted at the same time for each patient, and consequently, it is quite easy to produce a set of vectors and to go from the relational model to the multivariate time series associated to a patient for this dataset.

As a second dataset for our study, we decided to use an extraction of 2635 patients from the MIMIC II database. Since MIMIC II is a large database, we decided to select patients that had more than 40 records. Our selection has an average of 
                           
                              60.39
                           
                         records and a total of 159,127 records. The chronic illnesses and number of patients per illness in MIMIC II dataset are shown in Table 3
                        .

The MIMIC II dataset has a label cardinality of 
                           
                              2.54
                           
                         and a label density of 
                           
                              0.254
                           
                        , with 1023 possible symbols, of which 194 are present in the dataset. The patients of MIMIC II are very different from those of Portavita, as MIMIC II is focused on intensive care patients, while Portavita’s patients are standard care patients. This also implies that there are more laboratory tests collected per patient in MIMIC II than in Portavita.

In MIMIC II the frequencies of the laboratory tests depend on the gravity of the patient and not on a treatment. We transformed the patients’ records in multivariate time series by taking the sample frequency of the most frequent laboratory tests for each patient (for example, glucose in serum) and we applied a last observation carried forward (LOCF) to the less frequent measurements considering them as constant between two measurements. We are aware that LOCF underestimates the variability of the data. Our simplifying assumption in applying LOCF is that if the variability between measurements of such values was not crucial for the caregivers of the intensive care units in the first place, then it is acceptable to underestimate variability in our classification analysis. Validating approaches to handle data sampled with different frequencies is an interesting problem that we cannot exhaust within a single contribution, and therefore will be subject of future work.

For those data that are completely missing, we applied the imputation approach explained in the next Section.

The descriptive statistics of MIMIC II dataset, are shown in Table 4
                        . In MIMIC II case, the descriptive statistics for the physiological values are calculated before the LOCF procedure. The missing values rates are calculated after LOCF. A difference between Portavita and MIMIC II datasets is that Portavita has a balanced distribution of labels, whereas in MIMIC II the patient populations are imbalanced. Additionally the two datasets differ in label density. Another difference is that Portavita has a time granularity of months, whereas the tests are performed multiple times per day in MIMIC II. Finally, Portavita has way more missing values than MIMIC II. These differences will allow us to evaluate the considered algorithms in diverse settings and thus also highlight their strengths and weaknesses.

For both of the datasets, the multivariate time series present missing values. In medical datasets, the missing at random assumption does not hold, since if a patient presents missing values for a test, it is often because there was no medical reason to perform it. Thus, removing patients with many missing values would bias the study towards patients with more recognized medical conditions. Similarly, removing features with many missing values implies losing information about the status of the patients.

In the Portavita dataset, some of the features are missing more than 90% of the values. This is quite a normal situation in real world standard care datasets, as the patients considered may have different treatments and needs. To be useful, classification algorithms must be robust to large amounts of missing values and still be able to generalize with respect to unseen data.

It is well known that there is not a single universal approach to deal with missing values [24] in medical datasets. One of the most used approaches is to substitute the mean for the missing values [25], but this is rarely considered acceptable [26]. A more acceptable approach is to use medical knowledge to substitute with values within a likely range [26]. With respect to the mean imputation, this avoids the misleading effect of considering ill someone due to imputing values out of normal ranges.

Given these considerations, we performed plausible physiological values imputation in our multi-label classification analysis. We either impute physiological values in ranges that are likely for the given patient illnesses (putting high blood pressure if the patient has hypertension) or we impute physiological values of a healthy person when the related illness label is absent (normal blood pressure if the patient has not hypertension).

@&#METHODS@&#

In this section we illustrate how we apply a set of multi-label classification algorithms to the selected medical discrete time series datasets.

For each of the algorithms selected we apply the following steps on the data: after transforming our data from medical records to multivariate time series as described in Section 3, we standardize the data to have the same contribution for each feature, we apply a BoW quantization and we standardize the data again to have the same contribution for each codeword. Then, for dimensionality reduction approaches, we apply a dimensionality reduction algorithm and we use a nearest centroid classifier based on the cosine distance. For standard multi-label classifiers, we apply the multi-label classification algorithm after the second standardization step. Fig. 1
                     , inspired by the work of Wang et al. in [7], illustrates the main steps applied by our system in the specific case of KLFDA. For the comparison, we chose the following algorithms, all applied on the model calculated with BoW:
                        
                           •
                           BoW Cosine: This technique applies the cosine distance on the patient histograms and it represents the baseline for the comparison.

LDA-BR, KDA-BR and KLFDA-BR: Linear Discriminant Analysis [27], Kernel Discriminant Analysis and Kernel Local Fisher Discriminant Analysis, with a binary relevance approach, where the classes of the patients are those explained in Section 3.

LDA, KDA and KLFDA: The same as above, but concatenating the features.

MLkNN, DMLkNN, BPMLL, BR-SVM: Multi-label k-nearest neighbors [18], dependent multi-label k-nearest neighbors [28], back propagation multi-label learning [29] and multi-label support vector machines with a binary relevance approach [30].

We purposely decided to use multi-label algorithms capable of handling non-linearly separable data to confirm our hypothesis that supervised dimensionality reduction algorithms such as KLFDA and KDA are suitable candidates for multi-label learning in medical time series. In the rest of this Section we explain how we apply the BoW algorithm, the nearest centroid classifier and finally the metrics used for the evaluation of the multi-label classifiers.

The BoW model was originally introduced for text document analysis [31]. In document retrieval a codebook is defined as a set of pre-selected words, also called codewords. The BoW method counts the codewords per document, reducing each document to a histogram. Adapted versions of the BoW model have been recently applied in the field of computer vision for image classification [32,33], and for biomedical time series classification [7]. When the entities to be analyzed are not documents, but are irregular time series of continuous physiological values, the codebook of the BoW model can be defined using a clustering algorithm. In this paper, the k-means algorithm [34] is used to cluster the multivariate time series obtained from the health records as explained in Section 3, associated to the patients to create a set of centroids. These centroids then become the codewords retained in the codebook.

More formally, if we have a set of health records 
                           
                              X
                              =
                              [
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    x
                                 
                                 
                                    n
                                 
                              
                              ]
                           
                        , with 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    d
                                 
                              
                           
                        , where d are numerical features of each record, associated to a set of patients P, where each patient can have more than one record, and a set of clustering centers 
                           
                              
                                 
                                    c
                                 
                                 
                                    i
                                 
                              
                              ∈
                              [
                              
                                 
                                    c
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    c
                                 
                                 
                                    h
                                 
                              
                              ]
                           
                         calculated with k-means and representing the codebook, then we can set an un-normalized feature f, in an un-normalized histogram 
                           
                              u
                           
                        , for 
                           
                              f
                              =
                              1
                              ,
                              …
                              ,
                              |
                              P
                              |
                           
                        , as:
                           
                              (15)
                              
                                 
                                    
                                       u
                                    
                                    
                                       f
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          
                                             
                                                P
                                             
                                             
                                                r
                                             
                                          
                                       
                                    
                                 
                                 ‖
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 -
                                 
                                    
                                       c
                                    
                                    
                                       f
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    P
                                 
                                 
                                    r
                                 
                              
                           
                         is the number of records associated to a patient, and 
                           
                              ‖
                              ·
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                           
                         is the euclidean norm. After calculating 
                           
                              u
                           
                        , we can calculate a normalized histogram 
                           
                              h
                           
                        , as:
                           
                              (16)
                              
                                 
                                    
                                       h
                                    
                                    
                                       f
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             u
                                          
                                          
                                             f
                                          
                                       
                                    
                                    
                                       ‖
                                       u
                                       
                                          
                                             ‖
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        for 
                           
                              f
                              =
                              1
                              ,
                              …
                              ,
                              |
                              P
                              |
                           
                        . Each patient is then represented in terms of a normalized histogram, allowing us to compare patients even if they have a different number of records.

To classify a new element x we first use the eigenvectors computed with KLFDA to project the testing sample in the identified subspace for a given label k:
                           
                              (17)
                              
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       (
                                       k
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       T
                                    
                                    
                                       opt
                                    
                                    
                                       (
                                       k
                                       )
                                    
                                 
                                 *
                                 ϕ
                                 (
                                 x
                                 )
                              
                           
                        where 
                           
                              
                                 
                                    
                                       
                                          x
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    (
                                    k
                                    )
                                 
                              
                           
                         is the projected testing sample using the transformation matrix 
                           
                              
                                 
                                    T
                                 
                                 
                                    opt
                                 
                                 
                                    (
                                    k
                                    )
                                 
                              
                           
                        , calculated for label k, on the mapping 
                           
                              ϕ
                              (
                              x
                              )
                           
                        .

Second, we concatenate all the test sample projections for each of the labels in a single vector:
                           
                              (18)
                              
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       ˆ
                                    
                                 
                                 =
                                 (
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       (
                                       1
                                       )
                                    
                                 
                                 |
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       (
                                       2
                                       )
                                    
                                 
                                 |
                                 …
                                 |
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       (
                                       k
                                       )
                                    
                                 
                                 )
                              
                           
                        The possibility to concatenate features is a big advantage of dimensionality reduction approaches such as KDA and KLFDA as it allows us to define an easy way to chain the features calculated by the different classifiers, without the need to train another classifier recursively as it happens with classifier chains (CC).

Third, we calculate a cosine distance between the mean of the projected training elements and the projected testing element for each of the labels.
                           
                              (19)
                              
                                 
                                    
                                       d
                                    
                                    
                                       k
                                    
                                 
                                 =
                                 cos
                                 (
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       ˆ
                                    
                                 
                                 ,
                                 
                                    
                                       μ
                                    
                                    
                                       k
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   x
                                                
                                                
                                                   ˆ
                                                
                                             
                                          
                                          
                                             ˆ
                                          
                                       
                                       ·
                                       
                                          
                                             μ
                                          
                                          
                                             k
                                          
                                       
                                    
                                    
                                       ‖
                                       
                                          
                                             
                                                
                                                   x
                                                
                                                
                                                   ˆ
                                                
                                             
                                          
                                          
                                             ˆ
                                          
                                       
                                       ‖
                                       ‖
                                       
                                          
                                             μ
                                          
                                          
                                             k
                                          
                                       
                                       ‖
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    μ
                                 
                                 
                                    k
                                 
                              
                           
                         represent the mean of the concatenation of the features of the training elements in the projected space belonging to label k. To decide whether an element has a label or not, we perform the following test:
                           
                              (20)
                              
                                 
                                    
                                       
                                          
                                             y
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       k
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   if
                                                   
                                                   
                                                      
                                                         d
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   <
                                                   
                                                      
                                                         d
                                                      
                                                      
                                                         ∼
                                                         k
                                                      
                                                   
                                                   ,
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   otherwise
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        Finally, we can define the ranking function 
                           
                              
                                 
                                    rank
                                 
                                 
                                    f
                                 
                              
                              (
                              
                                 
                                    
                                       
                                          x
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    ˆ
                                 
                              
                              ,
                              k
                              )
                           
                         for label k as:
                           
                              (21)
                              
                                 
                                    
                                       rank
                                    
                                    
                                       f
                                    
                                 
                                 (
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       ˆ
                                    
                                 
                                 ,
                                 k
                                 )
                                 =
                                 1
                                 -
                                 
                                    
                                       
                                          
                                             d
                                          
                                          
                                             k
                                          
                                       
                                    
                                    
                                       
                                          
                                             d
                                          
                                          
                                             k
                                          
                                       
                                       +
                                       
                                          
                                             d
                                          
                                          
                                             ∼
                                             k
                                          
                                       
                                    
                                 
                              
                           
                        
                     

As stated in [18,35], multi-label performance metrics differ from single label ones. Following the same approach presented in [36,18], we propose the following five evaluation metrics for multi-label learning.

Let a testing set 
                           
                              S
                              =
                              {
                              (
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              )
                              ,
                              (
                              
                                 
                                    x
                                 
                                 
                                    2
                                 
                              
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    2
                                 
                              
                              )
                              ,
                              …
                              ,
                              (
                              
                                 
                                    x
                                 
                                 
                                    m
                                 
                              
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    m
                                 
                              
                              )
                              }
                           
                        .


                        Hamming loss: evaluates how many times an observation-label pair is misclassified. The score lies between 0 and 1, where 0 is the best:
                           
                              (22)
                              
                                 
                                    
                                       hloss
                                    
                                    
                                       S
                                    
                                 
                                 (
                                 h
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       m
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          m
                                       
                                    
                                 
                                 
                                    
                                       |
                                       h
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       ▵
                                       
                                          
                                             Y
                                          
                                          
                                             i
                                          
                                       
                                       |
                                    
                                    
                                       |
                                       L
                                       |
                                    
                                 
                              
                           
                        
                     


                        One-error: evaluates how many times the top-ranked label is not in the set of proper labels of the observation. The score lies between 0 and 1, where 0 is the best:
                           
                              (23)
                              
                                 one
                                 -
                                 
                                    
                                       error
                                    
                                    
                                       S
                                    
                                 
                                 (
                                 f
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       m
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          m
                                       
                                    
                                 
                                 γ
                                 (
                                 arg
                                 
                                    
                                       
                                          max
                                       
                                       
                                          y
                                          ∈
                                          L
                                       
                                    
                                 
                                 f
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 y
                                 )
                                 )
                                 ,
                              
                           
                        where
                           
                              (24)
                              
                                 γ
                                 (
                                 y
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   if
                                                   
                                                   y
                                                   
                                                   ∉
                                                   
                                                   
                                                      
                                                         Y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   ,
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   otherwise.
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     


                        Coverage: evaluates how far on average we need to traverse the list of labels in order to cover all the proper labels of the observation. A score as small as possible is better:
                           
                              (25)
                              
                                 
                                    
                                       coverage
                                    
                                    
                                       S
                                    
                                 
                                 (
                                 f
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       m
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          m
                                       
                                    
                                 
                                 
                                    
                                       
                                          max
                                       
                                       
                                          y
                                          ∈
                                          
                                             
                                                Y
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       rank
                                    
                                    
                                       f
                                    
                                 
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 y
                                 )
                                 -
                                 1
                                 .
                              
                           
                        
                     


                        Ranking loss: evaluates the average part of label pairs that are ordered in reverse for the observation. The score lies between 0 and 1, where 0 is the best:
                           
                              (26)
                              
                                 
                                    
                                       rloss
                                    
                                    
                                       S
                                    
                                 
                                 (
                                 f
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       m
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          m
                                       
                                    
                                 
                                 
                                    
                                       1
                                    
                                    
                                       |
                                       
                                          
                                             Y
                                          
                                          
                                             i
                                          
                                       
                                       |
                                       |
                                       (
                                       L
                                       ⧹
                                       
                                          
                                             Y
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       |
                                    
                                 
                                 ×
                                 |
                                 {
                                 (
                                 
                                    
                                       y
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       2
                                    
                                 
                                 )
                                 |
                                 f
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 ⩽
                                 f
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       2
                                    
                                 
                                 )
                                 ,
                                 (
                                 
                                    
                                       y
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       2
                                    
                                 
                                 )
                                 ∈
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                                 ×
                                 (
                                 L
                                 ⧹
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 }
                                 |
                              
                           
                        
                     


                        Average precision: evaluates the average fraction of labels ranked above a particular label 
                           
                              y
                              ∈
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                           
                         which actually are in 
                           
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                           
                        . The score lies between 0 and 1, where 1 is the best:
                           
                              (27)
                              
                                 
                                    
                                       avgprec
                                    
                                    
                                       S
                                    
                                 
                                 (
                                 f
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       m
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          m
                                       
                                    
                                 
                                 
                                    
                                       1
                                    
                                    
                                       |
                                       
                                          
                                             Y
                                          
                                          
                                             i
                                          
                                       
                                       |
                                    
                                 
                                 ×
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          y
                                          ∈
                                          
                                             
                                                Y
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       |
                                       {
                                       y
                                       ′
                                       |
                                       
                                          
                                             rank
                                          
                                          
                                             f
                                          
                                       
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       y
                                       ′
                                       )
                                       ⩽
                                       
                                          
                                             rank
                                          
                                          
                                             f
                                          
                                       
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       y
                                       )
                                       ,
                                       y
                                       ′
                                       ∈
                                       
                                          
                                             Y
                                          
                                          
                                             i
                                          
                                       
                                       }
                                       |
                                    
                                    
                                       
                                          
                                             rank
                                          
                                          
                                             f
                                          
                                       
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       y
                                       )
                                    
                                 
                                 .
                              
                           
                        where 
                           
                              ▵
                           
                         represents the symmetric difference, and 
                           
                              ⧹
                           
                         is the set-theoretic difference.

@&#RESULTS@&#

In this section we evaluate the combination of BoW and multi-label classification algorithms. In the Portavita dataset, we perform our evaluation using a leave-one-patient-out cross validation (LOPO CV). LOPO CV proved to be suitable for the medical domain [37], as it avoids situations that happen with leave-one-out (LOO), where records of the same patient are both in the training and testing set. Furthermore, LOPO CV presents an advantage with respect to N-folds CV, in which the selection of the random splits may lead to choose suboptimal parameters. Given the fact that we have 525 patients and 3528 health records, the computational cost of LOPO CV is affordable for the Portavita dataset. For model selection, we split our dataset into a training/validation set and a testing set, applying a LOPO CV on the training/validation set to select the best model for the testing phase. We withheld 375 patients for the training/validation and 150 patients for the testing.

Concerning the MIMIC II dataset extraction, we used a 10-fold CV approach for the grid search, splitting the dataset and keeping 70% of the patients (1844) for training and validation and 30% (791) patients for testing, while keeping the same distribution of labels in the test dataset. 10-folds CV was chosen as this dataset counts 2635 patients for a total of 159,127 health records, and LOPO CV was computationally infeasible to run a grid search.

The combination of BoW and dimensionality reduction techniques involves many parameters: size of the codebook, neighbors for the affinity matrix, the regularization coefficient, and the number of components to retain in the dimensionality reduction. Given the large amount of parameters to evaluate, we decided to run a grid search with a step of 100 for the size of the codebook, identifying 
                           
                              cb
                              =
                              600
                           
                         as the best size for the codebook for all the considered algorithms in the Portavita dataset and 
                           
                              cb
                              =
                              800
                           
                         for the MIMIC II dataset.

After the dimensionality reduction applied by LDA, KDA, and KLFDA, we always retain components that can explain at least 99% of the variance of the model. Fig. 2
                         shows the grid search on average precision and hamming loss, run on the training set, for the other parameters of KLFDA in the Portavita dataset.


                        Table 5
                         summarizes the parameters selection performed with LOPO CV and 10-fold cross validation concerning the algorithms studied for the Portavita and MIMIC II datasets. The parameter N identifies the number of neighbors, while 
                           
                              λ
                           
                         identifies the regularization factor, 
                           
                              σ
                           
                         the smoothing parameter for MLkNN and DMLkNN, 
                           
                              γ
                           
                         the exponent of the RBF kernel, and HN the number of hidden nodes in the BPMLL algorithm. In particular, the most difficult algorithm to train has been BPMLL as it requires more parameters than the other algorithms. To simplify the search, we decided to keep the learning rate constant to the default value 
                           
                              α
                              =
                              0.05
                           
                        .

After training and validation of the model, we perform our testing using 150 additional patients from the Portavita dataset, with respect to the performance measures discussed in Section 4. Table 6
                         shows the results for the selected algorithms on the Portavita dataset, with a confidence interval of 95%. The BoW Cosine approach is taken as a baseline for the comparison with the other algorithms.

The classes of patients of the Portavita dataset do not appear to be linearly separable and linear techniques such as LDA and LDA-BR do not seem to improve the results with respect to a BoW classifier. We think that this is due to the tendency of non-regularized LDA to overfit when the ratio between the classes and the features of the training elements is small [38,39].

KLFDA and KDA with feature concatenation achieve a better hamming and ranking loss than the other considered algorithms. KLFDA also achieves a better average precision. This suggests that in the case of classifying patients affected by DT2, the possibility of using supervised kernel methods and LPP brings an advantage in terms of classification. Another advantage of both KDA and KLFDA when compared to the other considered algorithms is the possibility of concatenating the projected features calculated by each of the classifiers. The KLFDA-BR and KDA-BR algorithms, on the contrary, do not perform much better than the standard BoW approach. In particular KDA-BR performs exactly the same as the BoW case. This is probably due to the fact that the relationship between the labels is not taken into consideration, degrading performances. KLFDA-BR shows an improvement with respect to BoW. This suggests that algorithms considering the locality of the data are likely to perform better than algorithms considering only the labels.

MLkNN performs similarly to KDA and KLFDA, with the best one-error score, confirming that making use of neighborhood properties of the dataset is quite important in the case of DT2 patients. In contrast, BPMLL does not generalize too well with respect to new data after the training. The main issue of BPMLL is the large number of parameters to train, which makes it difficult to tune properly. Furthermore, BPMLL seems to be affected more by the large rate of missing values in the Portavita dataset than the other considered algorithms.

BR-SVM performs well in both training and testing, despite not considering the interaction between the labels, which seems to explain the difference in performance with KLFDA concerning hamming loss and ranking loss.


                        Table 7
                         shows the results for the selected algorithms on the MIMIC II dataset, with a confidence interval of 95%. As it is clear from Table 7, BoW without any transformation is affected by the curse of dimensionality and LDA-BR does not really give meaningful results. LDA manages to improve the results with respect to BoW, but as in the Portavita dataset, the algorithm does not perform well.

First, Table 7 shows that for the MIMIC II dataset, the two best performing algorithms are MLkNN and DMLkNN, while the KDA and KLFDA algorithms perform similarly to MLkNN and DMLkNN. The fact that MIMIC II dataset has less missing values than Portavita, seems to favor the BPMLL algorithm, which performs well from the perspective of the average precision. BPMLL still does not perform well for the hamming loss and the ranking loss, which we believe related to the difficulty in training the algorithm.

Second, binary relevance approaches seem to perform well on MIMIC II, except for BR-SVM. KDA-BR performs similarly to KLFDA-BR: this may happen because MIMIC II has 194 different symbols, and thus the interaction between the illnesses is quite complex, limiting the advantage of LPP projections. Furthermore, KDA-BR and KLFDA-BR seem to have comparable results to KLFDA and KDA, where the calculated features are concatenated. This may be related to the fact that MIMIC II is imbalanced. Concatenating features moves the centroid of a label depending on the features calculated for the other labels, but majority labels may have more impact in defining the centroids, degrading the performance. The difference in performance between BR-SVM, KDA-BR and KLFDA-BR is of more difficult interpretation. This could be caused by the use of regularization or of the nearest centroid classifier in KDA-BR and KLFDA-BR algorithms.

@&#DISCUSSION@&#

The fact that KLFDA and KDA perform better than the other algorithms for the Portavita dataset in respect to hamming loss and ranking loss is quite important in medical applications such as classification of diabetic patients complications. Hamming loss discriminates the capability of the algorithm to identify the presence of a complication, while ranking loss discriminates how well the algorithm ranks the labels. These metrics allow a caregiver to understand which patient illnesses have a strong expression, giving an indication on where to act more promptly.

The performed evaluation illustrates the strengths and weaknesses of KDA and KLFDA for multi-label classification tasks: the behavior of KDA and KLFDA is comparable with that of state-of-the-art multi-label classification algorithms, but they seem to present an advantage with respect to datasets with a large number of missing values and with a high label density such as the Portavita dataset. We can have a better idea of the behavior of KDA and KLFDA by looking at Table 8
                        , comparing the hamming loss per symbol of KLFDA, KDA, BR-SVM and MLkNN, in the Portavita dataset (confidence intervals are omitted as we only have 10 elements per symbol).

In these results, we see that KDA has an advantage where the patients have only one label, which are also those patients presenting many missing values in Portavita dataset. For the other classes, KDA performs similarly to BR-SVM, with some exceptions, probably caused by the fact that BR-SVM finds support vectors, whereas KDA is a variance based method. KLFDA seems to combine the behavior of KDA, BR-SVM and MLkNN: the eigenvectors explaining little variance are discarded just like in KDA; the use of kernel methods allows KLFDA to deal with non-linearity in the data, similarly to BR-SVM; the LPP transformation allows KLFDA to consider the neighborhood of the elements, similarly to MLkNN, but in addition if there are enough elements per symbol, with a high label density, the retained eigenvectors would be able to characterize those symbols expressing most variance. In this sense, when dealing with datasets presenting the three aspects of missing data, high label density and non-linearity, KLFDA may have an advantage with respect to other techniques.

In MIMIC II, KLFDA and KDA perform slightly worse on the average precision than MLkNN and DMLkNN, but they are comparable for hamming loss and ranking loss. A possible reason for this is that MIMIC II dataset is imbalanced. KLFDA and KDA are variance based methods, so an imbalanced estimation of the classes variance impacts the calculated model and its performance. Looking at the hamming loss per symbol in MIMIC II, we found that the absence of missing values in MIMIC II, cancels the advantage of KLFDA and KDA, as they behave similarly to MLkNN for patients with only one complication. Additionally, MIMIC II has a low label density, with 194 symbols and few patients for most of the symbols, which may be difficult to characterize for the variance based model calculated by KLFDA and KDA. If this is the case, only the effect of the LPP projection of KLFDA, and of the nearest centroid classifier for KLFDA and KDA would be present and that would explain the similar behavior of KLFDA and KDA with MlKNN.

Finally, an advantage of KLFDA and KDA it that they compute a model based on eigenvectors, which allows to include new patients’ records by projecting their BoW representation and then recalculating the centroids for each class, while MLkNN and DMLkNN have to store the new instances in memory, that is infeasible with big datasets.

@&#CONCLUSIONS@&#

In this paper we studied the combination of the BoW model in medical time series with dimensionality reduction approaches for multi-label patient classification. When taking the Portavita dataset into consideration, the KLFDA algorithm with a nearest centroid classifier achieves the best results. In the MIMIC II dataset, dimensionality reduction algorithms are comparable to state-of-the-art multi-label classification algorithms, but suffer from the fact that the dataset is imbalanced.

There are several possible extensions to this work. At the moment we are using a single kernel mapping, but extensions of KLFDA and KDA that work with multiple kernel learning have already been defined [40]. Multiple kernels could achieve a better mapping for our data and improve the precision of KLFDA and KDA.

Another promising approach could be to develop a multi-label version of KLFDA and KDA, similarly to what is proposed in [41]. This would require modifying the definition of the scatter matrices in KLFDA and KDA to consider multiple labels, which is quite a challenging problem.

In Section 3, we identified the issue of dealing with values sampled at different frequencies. Quantizing patient data with different sampling frequencies or considering descriptive statistics rather than a codebook, could be suitable approaches. Finally, we could apply a different substitution to LOCF and generate physiological values with a maximum likelihood model, provided that enough patients’ records are available.

@&#ACKNOWLEDGMENTS@&#

This work was partially supported by the FP7 287841 COMMODITY12 project. We thank the anonymous reviewers for the useful comments that allowed us to improve the paper considerably.

@&#REFERENCES@&#

