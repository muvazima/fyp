@&#MAIN-TITLE@&#A concise review of the quality of experience assessment for video streaming

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Concise and up-to-date review of quality assessment for video streaming services.


                        
                        
                           
                           Description of a typical video assessment process.


                        
                        
                           
                           Analysis of current research on subjective, objective, and hybrid QoE assesment.


                        
                        
                           
                           Discussion of future trends and challenges for QoE in video streaming services.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Quality of experience

QoE

Video quality assessment

Video streaming service

@&#ABSTRACT@&#


               
               
                  The widespread use of mobile and high definition video devices is changing Internet traffic, with a significant increase in multimedia content, especially video on demand (VoD) and Internet protocol television (IPTV). However, the success of these services is strongly related to the video quality perceived as by the user, also known as quality of experience (QoE). This paper reviews current methodologies used to evaluate the quality of experience in a video streaming service. A typical video assessment diagram is described, and analyses of the subjective, objective, and hybrid approaches are presented. Finally, considering the moving target scenario of mobile and high definition devices, the text outlines challenges and future research directions that should be considered in the measurement and assessment of the quality of experience for video streaming services.
               
            

@&#INTRODUCTION@&#

The rapid development of mobile and high definition video devices and of the network infrastructure used for video streaming requires a permanent evolution of the techniques used to assess the video quality of experience (QoE). The objective of this paper is to provide a concise, up-to-date view of this research field.

In the last decade, interactive voice traffic (Voice over IP – VoIP) has been added to the traditional network data traffic (web, email, file transfers). Today, VoIP is common in IP networks, and the trend is a rapidly increasing in video traffic, namely, video on demand (VoD) and IP television (IPTV). Moreover, the rapid popularization of mobile devices with video display support, such as notebooks, tablets and smartphones, and the dissemination of wireless networks (WLANs and 3G/4G) contribute to this scenario. In a few years, 90% of the content transmitted over the Internet will be related to videos, which will be viewed by over a billion people [1,2].

These services are transmitted using a streaming technique through an Internet service provider or a private corporate IP network. The contents are presented to the user as they are sent by the source, without the need to store the complete file for later viewing. A buffer is used to store a few seconds of content before their display to minimize sporadic failures or delay fluctuations in the network transmission.

A typical infrastructure used to provide a video streaming service is composed of three elements (see Fig. 1
                     ). In the headend, the contents are created, edited, encoded, and stored in a multimedia database, which is made available by a streaming server. Next, the contents are divided into several IP packets and transmitted to the customers through the core network. Finally, via an access node in a customer network, the contents are displayed on the user’s device, which can be a television, a desktop computer, a notebook, a tablet, or a smartphone.

As the success of a video streaming service is heavily linked to the quality level assurance, the contents are displayed on customer devices with minimal failures or delays. Usually, a network manager monitors network information, such as bandwidth, delay, jitter, throughput, and packet loss, to provide adequate quality for each customer. However, this task becomes difficult due to the complexity of the network infrastructure, and when mobile devices are included in this scenario, the difficulties are even greater due to new problems, such as wireless signal coverage, a high rate of packet loss, and wireless channel instability.

Given the required conditions for video transmission to customers over IP networks, the features offered by the network define the concept of quality of service (QoS). However, other information can also be measured, such as resolution and codification of video contents. All of these factors strongly influence the quality as perceived by the user, which in turn determines the level of quality of experience (QoE). Presently, the rapid development of new technology allows for the emergence of devices with new resolutions, screen sizes, and contrast and brightness features. For this reason, the techniques used to measure perceived quality, as described in the remainder of this paper, must be carefully reexamined.

Various papers have explored the approaches and methodologies used to evaluate video quality in multimedia services. Winkler and Mohandas [3] discuss the evolution of subjective and objective metrics used for video quality measurement and introduce a new hybrid metric named V-Factor. A state-of-the-art perceptual-based audio and video quality assessment is described by You et al. [4] as are some relevant quality metrics to develop a joint audio and video assessment. Seshadrinathan and Bovik [5] present recent developments in a multimedia signal (audio, image, and video) quality assessment with a focus on full-reference methods. A classification scheme for full-reference and reduced-reference video quality assessment methods is introduced by Chikkerur et al. [6] that takes into account the natural visual characteristics (natural visual statistics and natural visual features) and perceptual characteristics (frequency-domain and pixel-domain methods). Yang and Wan [7] analyze the factors that may affect the quality of the networked video method and some bitstream-based methods to evaluate video quality. Finally, a classification of objective video quality and a comparison with different metrics, distortion types, and video databases is provided by Vranješ, Rimac-Drlje, and Grgić [8].

The main goal of this paper is to summarize current and emerging approaches to evaluate the quality of a video streaming service. It presents concepts related to QoS and QoE as well as factors that influence each one. A typical process of video service quality evaluation is detailed and the different assessment methods are divided into subjective, objective and hybrid approaches and compared. A discussion about future trends and challenges in video quality assessment completes the study.

The remainder of this paper is organized as follows. Section 2 defines QoS, QoE and related factors. Section 3 details the video quality assessment process, the available methodologies, and the various approaches. Section 4 discusses future trends and challenges in video quality assessment, and Section 5 presents the conclusions.

QoS is defined by the International Telecommunication Union (ITU) as a set of characteristics of a telecommunication service that focuses on user satisfaction [9,10], while the Internet Engineering Task Force (IETF) summarizes QoS as a collection of requirements to be met by the transport data stream of a particular service [11]. Bandwidth, delay, jitter, and packet loss rate are some of the most common parameters used to measure QoS.

In addition to QoS, the services can also be evaluated according to the grade of service (GoS) and the quality of resilience (QoR). The GoS is related to events that occur during communications between the server and the client, such as the configuration, release, and maintenance [12], and it is based on parameters such as setup time, blocked communication probability, client authentication delay, and connection drop probability. The survival rate of data flow in a network is assessed by the QoR, which considers how long it takes to recover from a broken connection or on the availability of the service from the server [13]. Furthermore, when the connection is restored after a failure in communication, the QoR is responsible for verifying if the level of the GoS and the QoS are the same as they were before the connection failure, if the route reestablished to deliver the contents is congested, and how many packets were lost during the service outage.

In a video streaming service, a QoS measurement occurs inside the network used to transmit data packets from the server to the user’s receiver. This type of assessment is called the network QoS (NQoS). Additionally, it is possible to investigate the relationship between the QoS parameters and the video quality perceived by the user (Perceived QoS – PQoS). Over the years, this term has evolved into QoE, where the focus is on the user experience rather than on the specific quality of service provided.

QoE is an assessment of the user satisfaction with the contents played or displayed on the client’s device [14,15]. It is based on human auditory and visual systems (HAS and HVS, respectively) and relates the perceived auditory and visual experience of the user with the contents. This paper focuses on the visual component of the QoE.

QoE is based on subjective parameters, i.e., it measures the interaction between the contents presented and the user’s perception (color, light intensity or failure of some pixel) and expresses it in words, such as excellent, good, fair, poor or bad. Cost, availability, usability, and fidelity are also taken into consideration [16].

When compared with QoS, QoE does not have well-defined metrics as the evaluation depends on the perception of each user. On the other hand, there does exist a relation between QoS and QoE. A possible relation is shown in the three regions of Fig. 2
                        . In Region 1, there is no disturbance during the video transmission from the server to the receiver, and QoE is considered excellent. In Region 2, there are some failures during the transmission (perhaps due to packet loss or delay), thus the user satisfaction regarding the displayed video decreases. Finally, in Region 3 the user gives up watching the content because the strong degradation of QoS results in an unacceptable QoE.

There are other factors that may influence the evaluation of QoE, as shown in Fig. 3
                        . Baraković, Baraković, and Bajrić [18] classify these factors into five categories: (1) technological performance, (2) usability, (3) subjective evaluation, (4) expectations, and (5) context. In a complementary way, Xue and Chen [19] take into account factors that influence the perception of the users when viewing videos on mobile devices, such as screen size, viewing distance, lighting, and user movement.

The European Network on Quality of Experience in Multimedia Systems and Services (Qualinet) defines three major groups of QoE influencers [20]: (1) human factors, such as demographic and socio-economic background, physical and mental constitution, and user emotional state; (2) system factors, such as media capture, transmission and device playback; and (3) context factors, such as physical user environment, temporal, social, economic and technical factors.

The methodologies used for video quality assessment are usually divided into four stages (see Fig. 4
                     ): (1) selection of reference videos (without introduction of error); (2) creation of distorted videos; (3) evaluation of distorted videos by users; and (4) statistical assessment of distorted videos.

The reference (raw) videos are not encoded by any type of compression. The spatial resolutions commonly used range from CIF (352×288) to high definition (1280×720 and 1920×1080) and, recently, to ultra high definition (3840×2160). Different research groups in the area make the raw videos available on their websites, as shown in Table 1
                     .

In a next step, distorted videos are created. These videos contain errors or variations in coding quality artifacts. Common errors include blocking or blockiness, blurring, color bleeding, DCT basis image effect, staircase effect, ringing, mosquito noise, flickering, packet loss, and jitter [47]. Bitrate and frame rate can be adjusted during the coding process.

Following the previous step, the distorted video quality is evaluated by a group of people that numbers between 4 and 40 [48]. However, at least 15 participants are needed to obtain a statistically significant evaluation of distorted videos [49]. Staelens, den Broeck, and Pitrey [50] show that to prevent evaluator fatigue, the maximum duration of each session to evaluate video quality should not exceed 30min. In addition, before an assessment session, it is recommended that each participant be examined for normal visual acuity (Snellen chart) and normal color vision (Ishihara test) [51].

At the end of this process, a prediction model to evaluate the quality of distorted video is created. The model can be based on the original video or on users’ opinions regarding the quality of the distorted video. Other parameters can be collected from: the application layer, which includes display size and device resolution; the network layer, which typically includes packet loss rate, delay, and jitter; and the contents layer, which includes frame rate, bitrate, spatial resolution, and temporal information. To increase the accuracy of the video quality predictor, it is possible to apply techniques such as principal component analysis (PCA) [52], canonical discriminant analysis (CDA) [53] or rough set theory (RST) [54]. Regression-based models, artificial neural networks (ANN), and fuzzy logic are also common methods employed during the creation of a video quality predictor [55]. The prediction model accuracy is commonly evaluated using mean squared error (MSE), mean absolute error (MAE), Pearson correlation coefficient (PCC), Spearman rank order correlation coefficient (SROCC), and outlier ratio indicators [56].

Considering the typical process of video quality assessment presented in Fig. 4, such assessment can be classified into three main approaches: subjective, objective, and hybrid.

The subjective approach produces a video quality assessment based on the perception of human observers. Experiments are usually performed in laboratories where videos are displayed on a TV set, but natural environments, like home or office, and different media for observing videos, like mobile devices, can be used [57,58].

Originally conceived for telephony [59], the mean opinion score (MOS) is a common used metric whereby an observer evaluates each video by selecting a score from a quality scale that ranges from 1 to 5 (Table 2
                        ) [49]. Another option is the degradation scale (Table 3
                        ) [48], where the observer is asked to report the degradation level of the distorted video by comparing it with the reference video previously shown.

In the subjective approach, the way videos are displayed for each observer is divided into three categories: single stimulus (SS), double stimulus (DS), and comparison stimulus (CS) [48,49].

The distorted videos are displayed one-by-one and evaluated individually by each observer. The reference videos are sometimes presented without informing the observer. The main methods of this category are absolute category rating (ACR), absolute category rating with hidden reference (ACR-HR), and single stimulus continuous quality evaluation (SSCQE) [48,49].

In the ACR category, only distorted videos are displayed to the observer, and they are individually evaluated based on the scale presented in Table 2. These videos can be viewed more than once as the same previously defined conditions remain. Each video has an average duration of 10seconds.

The ACR-HR is similar to the ACR, but in the ACR-HR, the reference videos are used. The observer, however, does not know what type of video is being evaluated. While the quality scale is used for the video evaluation (see Table 2), the final quality of each distorted video is the quality differential score (DMOS) between the distorted video and its respective reference video. This process is known as “hidden reference”.

Finally, the SSCQE enables the continuous evaluation of distorted videos, which is performed while the observer watches them. Each video is shown once for a minimum duration of 5minutes. While the scale used is similar to the quality scale, the SSCQE is continuous and items are normalized to a range between 0 and 100 (Table 4
                           ).

Both reference and distorted videos are displayed to the observer in the double stimulus method. To evaluate the distorted video, the observer is asked to take into account the difference in quality when compared with the reference video. The main methods in the double stimulus class are the double stimulus impairment scale (DSIS), double stimulus continuous quality scale (DSCQS), and simultaneous double stimulus for continuous evaluation (SDSCE) [49].

When using the DSIS, the observer is presented several video pairs in sequence, where the first is the reference video and the second is the distorted video. Each sequence can be reproduced two times, after which the observer evaluates the distorted video using the degradation scale (Table 3).

Similar to the previous method, with respect to the DSCQS, each participant evaluates the quality of both videos, but the observer does not know which video is under evaluation, if the reference video or the distorted video. The scale used in this assessment is the same as that employed in the SSCQE (Table 4).

In the third method, the SDSCE, a pair of videos is shown simultaneously, but the reference video is known by the observer. After observing the differences with the reference video, the distorted video quality assessment is continuous using a standardized scale (Table 4). The duration of each video is approximately 5minutes [49].

The comparison stimulus is similar to the double stimulus. The only difference is that only distorted videos are presented to the observer. Thus, the quality assessment may be conducted in one of two ways. In the first, called the pair comparison (PC) [48], the participant is asked to indicate the better of two videos with respect to quality. The participant is not required to assign a value to the videos. In the second, which is called the stimulus comparison adjectival categorical judgment (SCACJ) [49], the participant indicates, using the scoring scale displayed in Table 5
                           , the quality of the second video compared to the quality of the first video.


                           Table 6
                            provides a comparative analysis of the discussed subjective video assessment methodologies. The methods presented in this section are compared separately based on seven different parameters: stimulus type, explicit and hidden video reference usage, video duration, video repetition, type of quality assessment scale, and continuity of assessment.

A major problem with the subjective approach is that the evaluation of video quality is time-consuming for the participants. Moreover, the human perception can be influenced by external factors such as the environment (home, office or street), emotional and physical states, displayed content type (music, newspaper or sport), and personal profile (education level, age, occupation, etc.) [12].

Another dimension to consider is the intended level of assessment. If one wants to characterize the achievable quality of a video streaming service before launching it, then there is time to perform all the evaluations and any of the methods above can be used. Some require a longer execution time (SSCQE, SDSCE), while others may provide a better evaluation of some specific quality aspects [48,49]. However, if the intention is to obtain the user evaluation of a recently played video (e.g., in his/her smartphone or tablet), then the original video is not available and the assessment approach must be much less time-consuming (for example, a simplified version of ACR).

In the objective approach, mathematical models (algorithms) based on the human visual system (HVS) are created to assess the quality of videos without external interferences. These algorithms are classified as full-reference (FR), reduced-reference (RR), and no-reference (NR) metrics [5,60].

In a full-reference metric (see Fig. 5
                           ), a reference video is compared frame-by-frame with the distorted video to obtain a video quality measure that takes into account several aspects. Such aspects include color processing, spatial and temporal masking, perceived contrast and adaptation to a specific luminance or color, and contrast sensitivity [61]. The most representative approaches using an FR metric are the peak SNR (PSNR), mean squared error (MSE), structural similarity (SSIM), visual signal-to-noise ratio (VSNR), information fidelity criterion (IFC), and visual information fidelity (VIF).

In image and video processing, PSNR is the most used objective quality assessment among the full-reference metrics. It makes a pixel-by-pixel comparison between the reference content and the distorted content, producing a peak SNR figure without considering what the contents actually represent [3]. PSNR can be approximately mapped to MOS using Table 7
                            
                           [66].

MSE is a statistical estimator used to measure the arithmetic difference between the reference content and the distorted content [62]. As such, it evaluates the distortion value of all pixels and does not take into account the visual aspects of human perception. The distorted video quality is good when the MSE returns a low value compared to the signal variance observed.

The SSIM approach [63] is based on the idea that pixels that are spatially close have strong inter-dependencies, reflecting the human perception. The quality of the content is associated with three aspects of information loss: correlation distortion, contrast distortion and luminance distortion. The distorted content quality is considered excellent when the value of the quality evaluation is close to 1.

The VSNR is based on human visual system (HVS) and mainly explores the aspects of contrast sensitivity, working in a two-stage approach. In the first stage, it detects distortions using contrast thresholds. If the distortions are below human perception, the distorted image is adequate (VSNR=infinity) and no further analysis is performed. However, if the distortions are visible, a score is produced based on the perceived contrast and global precedence [64]. Limitations of this method are related to the specific use of the luminance component for obtaining the score and the non-use of spatial distortion.

The IFC [65] uses natural scene models in conjunction with distortion models to quantify the statistical information shared between test and reference images, thus producing a quality value that ranges from zero (no fidelity) to infinity (perfect fidelity). It is based on the statistical properties of the natural environment related to the evolution of the human visual system (HVS) [64].

With respect to the VIF [62], the reference image is modeled as the output of a stochastic natural source that passes through an HVS channel, which is then later processed by the brain. The VIF metric computes the mutual information between the input and the output of the HVS channel and then inserts a distortion channel between the natural source and the HVS channel, thus computing the same measure. The resulting value measures the information that the brain could ideally extract from the test image.

The first three methods essentially compare the reference with the distorted videos and compute a value that considers few (in SSIM) or no aspects (PSNR, MSE) of the human perception. On the other hand, VSNR, IFC, and VIF are more complex approaches, based on the human visual system (HVS) and capable of producing a quality indicator closer to the human experience.

Considering a scenario of video streaming service, the main disadvantage of the full-reference (FR) approach is the impracticability of having the reference video at the receiving end to perform a comparison. In contrast, the reduced-reference (RR) video quality assessment extracts some properties or features from the reference video (e.g., spatial details or motion information) and uses them to evaluate the distorted video quality (see Fig. 6
                           ). The idea is to produce a much lower amount of information for the assessment than the whole original reference video, while keeping the process accuracy at a satisfactory level.

One of the first works related to reduced-reference is that of Webster et al. [67], which describes the development process of a video assessment algorithm. Both reference and distorted videos are processed to extract a large number of features related to the luminance and color video components, such as Sobel filtering, Laplace filtering, fast Fourier transformations, first-order difference, color distortion measurements, and moment calculations. In addition, the reduced-reference metric analyzes how viewers perceive spatial and temporal information. The work reports a correlation coefficient between previously obtained subjective scores and the produced objective scores between 0.92 and 0.94. A real time version of the assessment algorithm is proposed, indicating the need for an extra data channel up to 2400bps to send the RR data.

Gunawan and Ghanbari [68] propose a video quality assessment based on a local harmonic strength (LHS) method. The LHS does not use the HVS model. It performs a frame by frame harmonic analysis of the video, detecting harmonic features related to blocking and blurring distortions. Special care was taken in reducing the data rate of the reduced-reference data sent along with the video signal to figures around 160–400bps. The authors report a 0.86–0.88 Pearson correlation against MOS score from the performed experiments.

A method that takes into account human motion perception in the video is proposed in the work of Rohani et al. [69]. To evaluate the distorted video quality, their method applies a weighting map based on different motion speeds of objects in each frame and combines it with a divisive normalization transform. Experiments show that the proposed method outperforms the full-reference PSNR, MSE, and SSIM methods, but the paper provides no considerations about the transmission of the generated reduced-reference data.

A spatial–temporal assessment quality metric is described by Amirshahi and Larabi [70], who observe HVS sensitivity to sharp changes in the video by analyzing the matching regions in the time domain. The distorted video quality is calculated based on motion activity density. Results show a higher Spearman correlation than the full-reference PSNR, MSE, SSIM, VSNR, and VIF methods. No discussion is made about the transmission of the generated reduced-reference data.

Ma, Li, and Ngan [71] explore the spatial and temporal information from compressed video sequences. They calculate an energy variation descriptor (EVD) that measures the energy change in the space domain. In the temporal domain, they apply a generalized Gaussian density (GGD) function to produce a histogram distribution of the difference between frames in sequence, followed by a city-block distance (CBD) method to compute the histogram difference between the original and the distorted videos. Combining the spatial EVD and the temporal CBD, a final index is produced. The work provides a detailed performance evaluation of the proposed method. The produced RR data rate is around 875bps and the experiments show that it is superior to the full-reference PSNR, SSIM, VSNR, and VIF methods and also superior to the reduced-reference Gunawan and Ghanbari approach.

Apart from the good quality of the proposed RR metrics and the low bitrate needed to transmit RR data, two additional aspects must be taken into account when considering a reduced-reference approach for a streaming video service evaluation. At the video source, all the necessary computation to produce the RR data can be performed offline. At the receiving end, however, the same computation has to be done in real-time, as the distorted video metric has to be compared to the original video metric. If this process is computing intensive, this may not be achieved, particularly in small devices like smartphones and tablets. Another aspect is that a specific metric has to be used by both the sending and receiving sides. This is not a problem for an individual service provider, but different providers may choose different metrics (or no metric at all).

The no-reference metric (see Fig. 7
                           ) does not use any information from the reference video to obtain the distorted video quality, and therefore, it is normally used when it is necessary to obtain a quality score in real-time. Some works related to the no-reference metric are Fu-zheng et al. [72], Saad and Bovik [73], Yao et al. [74], Kim et al. [75], Kawano et al. [76], and Lin, Tian, and Chen [77].

Fu-zheng et al. [72] use a method based on a watermark technique to determine the distorted video quality. This method adds a watermark to the video before the transmission and then extracts it from the video received by the user. The quality assessment is obtained by comparing the extracted watermark with the original watermark. The authors simulated various scenarios in wireless and IP video services, obtaining a correlation with PSNR of 0.992. Although presented as a no-reference approach, considering that it takes no information from the original video, the insertion of a watermark before the video transmission is similar to a reduced-reference method. The experiments indicate an increase in the data rate from 4.0% to 7.1%.

As part of a work for developing a no-reference metric, Saad and Bovik [73] present an algorithm capable of extracting motion statistics from the video optical flow vectors, by computing independent components (ICs) from the flow. Using undistorted and distorted (by packet loss) versions of videos, they show that these ICs are more Laplacian distributed than the raw data. In addition, they observe that a lower video quality implies in a higher mean square error between the Laplacian fits of the extracted ICs.

Yao et al. [74] propose a hybrid quality assessment method that combines bitstream and pixel analysis. They model the relationship between mean square error (MSE) and the feature parameters calculated from pixel domain. Next, they estimate the receiver-side MSE from the decoded pixels, which is used as a quality degradation index. Tests with videos subjected to spatial (slice-mismatch and macroblock boundary mismatch) and temporal (jerkiness) impairments achieved a Pearson correlation coefficient of 0.851.

Kim et al. [75] create a new parameter to augment the accuracy of the no-reference video quality assessment. They observe that a camera, after the capture process of a video sequence, dynamically controls the range of the digital signal due to the quantization process and that if this range is not correctly created, edge values are repeatedly shown. A video quality model is then created that consists of input brightness, motion analysis, contrast, edge, and chrominance parameters. A subjective video quality assessment is performed with 20 students and a Pearson correlation of 0.847 is obtained.

A QoE estimator of video streaming service is proposed by Kawano et al. [76]. The estimator is based on the blockiness and motion-blur artifacts that are extracted from distorted video. Subjective tests permit for an adjustment to the QoE estimator, and its accuracy is then compared to that of the PSNR, showing better results than PSNR for high and low subjective video quality. According to the paper, by using a noise-robust edge detector, it can extract blockiness even if video signals are converted or filtered. Also, its blur-extraction algorithm does not use the edge information of source video signals and thus can be applied to edge-degraded videos.

All of the previously described works take into consideration all pixels contained in each frame. Lin, Tian, and Chen [77] introduce a method based on regions of interest. As the user’s eyes focus only on specific areas (regions of interest – ROIs), these regions can be used to improve the accuracy of video quality predictors. In their work, for each frame, the influence of blockiness and blur artifacts is analyzed and the distorted video quality is predicted using this analysis in all of the regions of interest. Using eight standard video sequences, the method is compared to PSNR and SSIM, showing a Pearson Linear Correlation Coefficient (LCC) of 0.81, against 0.58 for PSNR and 0.76 for SSIM.

The major advantage of a no-reference (NR) metric is that it needs no extra information apart from the received video. On the other hand, it has to provide a level of accuracy similar to a full-reference (FR) or a reduced-reference (RR) approach. Also, as for the RR metrics, the computing power needed to process NR metrics in real-time is an issue.


                           Table 8
                            summarizes a comparative analysis of the objective methodologies for video assessment based on the following parameters: reference approach, metric type used (data, picture or packet), and proposal objective.

In the hybrid approach, the advantages of both subjective and objective approaches are combined. The distorted video quality is evaluated by a group of persons, and a prediction model based on the results of this evaluation is then formulated. Features not related to video content can be applied in the prediction model, such as device characteristics (e.g., resolution and display size) and current information regarding the network (e.g., packet loss, delay, and jitter).

Mohamed and Rubino [78] describe a methodology, later named the pseudo-subjective quality assessment (PSQA) [79], which is used to automate the video quality evaluation at the user’s device. Its initial step is the selection of parameters (codec used, network packet loss rate, end-to-end delay and jitter, etc.), the corresponding parameters range (e.g., 0%, 1%, 2%, and 5% for packet loss) and, as the number of possible configurations is large, the subset of configurations to be evaluated. Using the configuration subset applied to selected video samples, a set of distorted videos is produced. These distorted videos are subjectively evaluated, producing a MOS score. Some of the subjective evaluations are randomly selected as the training set for a random neural network (RNN), while the remaining will be used as a validation set. The experiments carried out show that the RNN did not fail to validate and produced results close to the produced MOS scores. In a following work, Rubino et al. [79] use the PSQA to assess the quality level in a WLAN, when different mechanisms (application level FEC and IP level DiffServ) are applied to improve multimedia communications (VoIP and video).

Rubino et al. [79] use the PSQA to manage the perceived quality by the end-user in real-time. The video quality displayed at the receiver end is controlled by a QoS scheme, which manages admission control, changes the codec parameters, filters the traffic to avoid congestion, and uses a DiffServ scheme to prioritize access. Experiments are analyzed by means of artificial neural networks (ANN) and Bayesian classifiers. Piamrat et al. [80] propose a PSQA admission control mechanism for wireless networks. The radio resource is dynamically controlled at the access point, while the QoE is obtained in real-time by the PSQA. Loss rate and mean size of burst loss are considered in the RNN model to predict the video quality perceived by user. Ghareeb and Viho [81] associate the PSQA in the context of multiple description coding (MDC) to improve the quality of a video streaming service over multiple overlay paths. MDC is a technique to provide error resilience, which fragments a single media stream into sub-streams, referred to as descriptions. The work analyzes the influence of the I, P, and B frame losses, the group of pictures (GoP) size, and the MDC usage in the QoE perceived by the user during the experiments. In a simulated video streaming scenario, using the ns-2 network simulator, the PSQA output is received by the server, which uses it to improve the quality of the transmitted video.

Improving on different aspects of PSQA, Aguiar et al. [82,83] developed an enhanced approach named HyQoE. It adopts a subjective evaluation with more observers (55 individuals), uses 10 videos with diverse characteristics (temporal information, percentage of losses in I, P and B frames, total losses, and GoP size), groups videos into three different profiles (low movement, medium movement, high movement), and collects impairments from a wireless mesh network testbed. As in the PSQA, an RNN is used, as the authors point that “it is closer to biophysical reality and mathematically more tractable than standard neural methods”, producing accurate QoE prediction results that are equal or closer to human scores. The performed experiments revealed that HyQoE produces scores equal to the subjective assessment (MOS) in 70% of the scenarios, compared with a PSQA figure of 24.11%. An almost identical approach is proposed by Cerqueira et al. [84] and used by Rosário et al. [85], using multiple artificial neural networks (MANN) instead of RNN. In MANN (also known as modular artificial neural networks), one uses a series of independent neural networks, with each neural network serving as a module and operating on a separate subtask. The idea is to break the original problem into smaller, more specific (and easy to train the ANN) problems. The experiments show a performance of 90% of the scores identical to a MOS evaluation, for the proposed approach, compared with a PSQA performance of 41%. It is interesting to note an apparent inconsistency here, as for identical experimental conditions, PSQA produces a 41% performance in one work [84] and 24.11% in another [82].

Khan et al. [86] use an artificial neural network (ANN) based on the adaptive neuro-fuzzy inference system (ANFIS) to estimate the MOS score. However, no subjective assessment is performed, as the video quality is obtained from PSNR converted to MOS. In a WLAN scenario, using three distinct content types and a combination of network level and application level parameters (frame rate, send bitrate, link bandwidth, and packet error rate), the ANFIS-based ANN demonstrated good prediction accuracy.

Yamagishi et al. [87] propose a video quality estimation model that analyzes received packet header information as well as video signals characteristics. An initial subjective quality assessment produces MOS scores for 16 videos in different bitrate conditions. Half of the videos are used as training data for the model and temporal information is extracted from the remaining videos. Combining packet layer (bitrate) and video signal (temporal information) features, in the experiments the model is able to estimate quality with a Pearson correlation of 0.96, a root mean square error (RMSE) of 0.37, and an outliers ratio (OR) of 0.36.

The blockiness and blurriness artifacts generated by packet loss are studied in Farias et al. [88]. Some reference videos are compressed with bitrate between 50kbps and 400kbps. For each video, packet losses are simulated with error rates between 0.1% and 10%. The generated video quality during simulation is evaluated by the PSNR and mapped to the MOS. Finally, a video quality metric is created that incorporates input packet loss rate, blockiness, and blurriness measurements. As a result, experiments show a Pearson correlation of 0.79.

The hybrid approach is relatively new and one could argue that it is a variant of the no-reference approach. Some works [86,88], in fact, produce a MOS score derived from a PSNR assessment. However, if a MOS score produced by a subjective evaluation is used, it can fit in the hybrid classification. Apart from that point, Table 9
                         presents a comparative analysis of hybrid methodologies for video assessment based on network type, method used, and features used. It shows that the approaches differ mainly in the assessment method and features. Artificial neural networks are mostly used, especially the RNN, an influence of the PSQA method. Concerning the network features considered in each method, bitrate and losses are the factors that cause major impact on QoE, while GoP and temporal information are the critical video characteristics to take into account.

To improve the techniques used to measure the QoE perceived by users in a video streaming service, a range of issues remains as the state-of-the-art of video quality assessment continues to face important challenges that must be addressed.

The fundamental challenge is related to the quality predictor model. Each model uses a different choice of inputs because of the existence of several information sources such as network infrastructure (jitter, delay, packet loss, and bandwidth), video characteristics (codec, spatial and temporal information, bitrate, GoP) and assessment approach (subjective, objective, and hybrid). It is interesting to note that QoS alone is not a sufficient source of data to judge upon the QoE level, as specific video features, and the user device capability of playing it, may strongly influence the quality of experience. Further hampering the accuracy of each prediction model is that each specific video database is based on a chosen set of parameters, whose values vary independent of each other.

Moreover, the use of mobile devices should be taken into account during the elaboration of the video quality predictor. When compared to standard home displays, these mobile devices have specific limitations, such as battery consumption, small screen size (3″–12″), limited storage, and connectivity problems [89]. The methodologies used to evaluate the QoE on mobile devices are adaptations of ITU-T standards and do not consider these peculiarities.

In these scenarios, wireless links, dynamic environments, and service coverage can influence QoE assessments. Due to the heterogeneous and dynamic characteristics of wireless networks and mobile devices, an adaptive video content delivery method has to be used. For example, network and device features can be collected and used to select a compression strategy that maximizes the video quality [90]. Environment and service attributes, as well as user behavior, cognitive and psychological states, are then analyzed in this context to determine the QoE of the video streaming service [91]. If available to the service provider or network routers, the endpoint QoE score can provide the necessary information to adapt the video source or the network characteristics in a way to guarantee a suitable content delivery.

Another point is that the video quality is usually obtained by an analysis of the full content displayed on a device. However, one can use the concept of regions of interest (ROIs) to determine video quality. By analyzing moving objects and luminance variation in specific regions, adaptive video streaming using the video quality of each ROI can be employed to improve the QoE [92].

Additionally, recent video content trends include the use of autostereoscopic 3DTV (AS3DTV), 4K ultra high definition (4K-UHD), and super HDTV (SHDTV) [93]. Some video streaming services (YouTube, Vimeo, Livestream, Netflix, Amazon Fire TV, Apple TV, and Google TV) already provide 4K-UHD in some videos, while new algorithms to generate 4K-UHD video content, such as high efficiency video coding (HEVC), are in development [94]. The effect of resolution, distance from the viewer’s eyes, and dimensions of the mobile display should be considered in the development of 3D videos assessment [95].

@&#CONCLUSION@&#

Favored by the popularization of mobile devices (tablets and smartphones) and improvements to the Internet infrastructure, the use of video streaming services has grown quickly, allowing customers to watch videos anywhere, anytime, anyplace. Nevertheless, none of this advanced technology has any value if the video content provider cannot guarantee the consumer high quality videos. While the QoE represents the quality level perceived by the end user, there are many challenges remaining to be solved in video quality assessment methods, especially with respect to mobile devices. It is necessary to develop QoE predictors that take into account not only network characteristics such as jitter, bandwidth, packet loss, and delay but also consider the place from which the customer is viewing the video (e.g., bus, car, or home); the content type, such as news, soap opera, and sports; and the device type, such as television, computer, tablet, and smartphone.

Accordingly, this paper concisely reviewed the quality assessment process currently used to obtain QoE feedback regarding video streaming services. Furthermore, the paper summarized the definitions for both QoS and QoE and identified factors that influence each. A typical video assessment process was described, current research activities, including subjective, objective, and hybrid approaches, were analyzed, and a detailed comparison of existing research for each approach was provided. Finally, the paper identified certain challenges that still require innovative solutions to create quality video models to improve the QoE perceived by users of the video streaming service.

@&#ACKNOWLEDGMENTS@&#

This work was supported by the following Brazilian institutions: CAPES (Coordenação de Aperfeiçoamento de Pessoal de Nível Superior), CNPq (Conselho Nacional de Desenvolvimento Científico e Tecnológico), FAPEMIG (Fundação de Amparo à Pesquisa do Estado de Minas Gerais), IFAM (Instituto Federal do Amazonas), CT-PIM (Centro de Ciência, Tecnologia e Inovação do Pólo Industrial de Manaus), and PRPq-UFMG (Pró-Reitoria de Pesquisa da UFMG).

@&#REFERENCES@&#

