@&#MAIN-TITLE@&#POS-RS: A Random Subspace method for sentiment classification based on part-of-speech analysis

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The rise of social media has fueled interest in sentiment classification.


                        
                        
                           
                           POS-RS is proposed for sentiment analysis based on part-of-speech analysis.


                        
                        
                           
                           Ten public datasets were investigated to verify the effectiveness of POS-RS.


                        
                        
                           
                           Experimental results reveal POS-RS can be used as a viable method.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sentiment classification

Random Subspace

Part of speech

Ensemble learning

@&#ABSTRACT@&#


               
               
                  With the rise of Web 2.0 platforms, personal opinions, such as reviews, ratings, recommendations, and other forms of user-generated content, have fueled interest in sentiment classification in both academia and industry. In order to enhance the performance of sentiment classification, ensemble methods have been investigated by previous research and proven to be effective theoretically and empirically. We advance this line of research by proposing an enhanced Random Subspace method, POS-RS, for sentiment classification based on part-of-speech analysis. Unlike existing Random Subspace methods using a single subspace rate to control the diversity of base learners, POS-RS employs two important parameters, i.e. content lexicon subspace rate and function lexicon subspace rate, to control the balance between the accuracy and diversity of base learners. Ten publicly available sentiment datasets were investigated to verify the effectiveness of proposed method. Empirical results reveal that POS-RS achieves the best performance through reducing bias and variance simultaneously compared to the base learner, i.e., Support Vector Machine. These results illustrate that POS-RS can be used as a viable method for sentiment classification and has the potential of being successfully applied to other text classification problems.
               
            

@&#INTRODUCTION@&#

With the rise of Web 2.0 platforms, personal opinions, such as reviews, ratings, recommendations, and other forms of user-generated content, have attracted significant interest from the research community (Pang & Lee, 2008). The sheer volume and exponential growth of this information provides potential value to governments, businesses, and users themselves (He & Zhou, 2011). There is an inherent property called sentiment involved in the vast majority of online-generated content. Sentiment is an opinion or feeling you have about something (Delacroix, 2007). In this study of sentiment classification, we focus on attempts to identify the sentiment polarity of a given text, which is traditionally classified as either positive or negative.

In reality, sentiment classification has many application scenarios, such as applications to review-related websites, applications as a sub-component technology, applications in business and government intelligence, applications across different domains (Pang & Lee, 2008). Based on two surveys of more than two thousands American adults, it was found that people between 73% and 87% reported that online reviews had an important influence on their purchase among readers of online reviews of different services, such as restaurants, hotels (Horrigan, 2008; Pang & Lee, 2008). Moreover, consumers reported that they were willing to pay more for a 5-star-rated goods than a 4-star-rated goods (Horrigan, 2008; Pang & Lee, 2008). Additionally, sentiment classification not only is useful for customers, but also has an important role as enabling technique for other systems (Pang & Lee, 2008). For example, sentiment classification is a supplement to recommendation systems, as it might be proper that a system recommend items that receive a lot of positive feedback. Moreover, sentiment classification can also be applied in government activities. For instance, it is needed for the government to monitor sources in hostile or negative communications (Abbasi, 2007).

Accordingly, sentiment classification has become a popular research topic in both academic and industry fields (Abbasi, Chen, & Salem, 2008a; Boiy & Moens, 2009; Chen & Yang, 2011). The sentiment classification problem was initially tackled granularly at the levels of document, sentence, clause, phrase, and word, depending on the specific objectives of applications. Heuristic-based methods and machine learning approaches were frequently employed in previous research (Abbasi et al., 2008a; Pang & Lee, 2008). Heuristic-based methods were primarily used in conjunction with linguistic properties and semantic features. For example, Turney used mutual information with predefined sentiment words to score other phrase tags, therefore identifying the sentiment of documents (Turney, 2002). In parallel, many studies focused on using machine learning algorithms to classify sentiment. For instance, Support Vector Machines (SVM) and Naive Bayes (NB) are commonly used to identity sentiment, due to their predictive power. Pang et al. conducted an empirical study in sentiment classification, concluding that SVM outperformed other classifiers such as NB (Pang, Lee, & Vaithyanathan, 2002). In recent years, there has been a growing interest in using ensemble learning techniques, which combine the outputs of several base classifiers to form an integrated output, to enhance classification accuracy (Polikar, 2006; Wang, Sun, Ma, Xu, & Gu, 2014). Furthermore, prior research have shown that Bagging, Boosting, and Stacking have performed better than singleton machine learning techniques for sentiment classification (Abbasi, Chen, Thoms, & Fu, 2008b; Wang et al., 2014; Whitehead & Yaeger, 2010; Xia, Zong, & Li, 2011). However, Random Subspace, an important ensemble method that has been successfully applied in other text classification problems, has been under-examined.

In this research, a novel Random Subspace method, POS-RS, is proposed for sentiment classification based on part-of-speech analysis. Random Subspace mainly uses one parameter, i.e., subspace rate, to control the diversity of base learners. However, the performance of ensemble learning methods is influenced not only by the diversity of base learners, but also by the accuracy of base learners. From the perspective of multi-objective optimization, a collaborative mechanism should be considered to balance the accuracy and the diversity of base learners. Accordingly, based on part-of-speech analysis, POS-RS is proposed for sentiment classification, which uses two parameters, i.e., content lexicon subspace rate and function lexicon subspace rate, to simultaneously control the accuracy and the diversity of base learners. Ten public sentiment analysis datasets are used to verify the effectiveness of proposed method. Empirical results show that POS-RS achieves the best results among the compared methods. Moreover, POS-RS can reduce bias and variance simultaneously compared to the base learner, i.e., SVM, and yields the lowest bias among all the ten datasets.

In summary, the main contributions of this paper are the following:
                        
                           (1)
                           From the perspective of multi-objective optimization, a tradeoff mechanism is proposed to control the balance of the accuracy and the diversity of base learners based on the nature of the sentiment classification.

Based on the proposed collaborative mechanism, a new Random Subspace method, POS-RS, is proposed for sentiment classification by simultaneously control content lexicon subspace rate and function lexicon subspace rate.

A detailed comparison with the performance of other popular ensemble methods is provided.

The rest of this paper is organized as follows. Section 2 presents the related work on sentiment classification. The newly proposed method, POS-RS, is described in Section 3. Next, Section 4 describes the experimental setup used for evaluating the proposed approach. Section 5 presents the results and discussions. Finally, Section 6 discusses conclusions and future research directions.

@&#LITERATURE REVIEW@&#

Since the late 1990s, sentiment classification has been an important research topic in the areas of data mining, and natural language processing (Boiy & Moens, 2009; Pang & Lee, 2008). Many researchers have investigated sentiment classification from different perspectives. Due to the linguistic characteristics involved, sentiment analysis is done at different levels of text units. A word, phrase, clause, sentence, or document may become the text unit in analysis (Pang & Lee, 2008). In order to capture the sentiment of individual words or phrases, a measure of the strength of sentiment polarity is often defined to quantify how strongly a word or phrase is judged to be positive or negative (Dang, Zhang, & Chen, 2010; Kim & Hovy, 2004; Subrahmanian & Reforgiato, 2008; Turney, 2002). Furthermore, Thet, Na, and Khoo (2010) computed the sentiment of a clause from individual word sentiment scores, considering the grammatical dependency structure of the clause. Other studies used sentence-level attempts to classify the positive or negative sentiments for each sentence (Yi, Nasukawa, Bunescu, & Niblack, 2003; Zhang, Zeng, Li, Wang, & Zuo, 2009). The greatest amount of work has been done on document level polarity categorization (Abbasi et al., 2008a; Boiy & Moens, 2009; Dave, Lawrence, & Pennock, 2003; Pang et al., 2002; Whitehead & Yaeger, 2010; Xia et al., 2011). This is also the focal level of our study. The techniques for sentiment classification in prior research can be classified into heuristic-based methods and machine learning methods.

By means of predefined lexicons and calculation rules, heuristic-based methods generally classify text sentiments based on the counts of derived positive or negative sentiment features (Pang & Lee, 2008). For example, Hatzivassiloglou and McKeown (1997) considered that adjectives are more predictive of sentiment classification and predicted the sentiment of adjectives by inspecting them in conjunction with “and”, “or”, “but”, “either/or”, and “neither/nor”. However this approach may overestimate the importance of adjectives and underestimate some predictive words of other parts-of-speech. Along this line, Turney (2002) determined the semantic orientation of a phrase using its point-wise mutual information with predefined sentiment words, such as “excellent” and “poor”. Therefore, sentiment classification can be achieved by aggregating the overall sentiment information of phrases. The Adjective–Verb–Adverb (AVA) combinations were thoroughly analyzed in (Subrahmanian & Reforgiato, 2008) for sentiment polarity predication. There are also other fruitful studies following this line (Hu & Li, 2011; Yi et al., 2003).

The heuristic-based methods are by nature knowledge engineering methods. One of the problems of this approach is it relies heavily on pre-defined lexicons and rules that are difficult to update and use in multiple domains (Pang & Lee, 2008). In this research, we focus on machine learning methods for sentiment classification.

Machine learning approaches for sentiment classification have been extensively studied, due to their predominant classification performance (Abbasi et al., 2008b; Pang & Lee, 2008). By constructing predictive models from labeled training datasets, these methods can model more features and adapt to changing inputs more robustly, than heuristic-based methods (Pang & Lee, 2008).

Many machine learning techniques have been investigated for sentiment classification in the literature. However, there is no consensus as to which methodology an algorithm developer should adopt for a given problem in a given domain (Wolpert & Macready, 1997). Recent studies suggest that ensemble learning methods may have potential applicability in sentiment classification (Abbasi et al., 2008b; Wang et al., 2014; Wilson, Wiebe, & Hwa, 2006; Xia et al., 2011). Table 1
                         presents selected previous studies dealing with sentiment classification using ensemble methods.

Prior studies have shown that such ensemble methods have performed better than singleton machine learning technique for sentiment classification (Abbasi et al., 2008b; Su, Zhang, Ji, Wang, & Wu, 2013; Wang et al., 2014; Whitehead & Yaeger, 2010). For example, Wilson et al. first used Boosting for sentiment classification and achieved 23–96% improvements in accuracy (Wilson et al., 2006). Tsutsumi et al. proposed a stacking method for review document classification, which consists of three classifiers based on SVM, ME and score calculation (Tsutsumi, Shimada, & Endo, 2007). Two voting methods and SVM used to the integration process of single classifier. The proposed stacking method improved the accuracy as compared with the three singleton classifiers. Abbasi et al. proposed the support vector regression correlation ensemble (SVRCE) method for enhanced performance of sentiment classification (Abbasi et al., 2008b). The experimental results indicated that SVRCE outperformed comparison techniques. Lu and Tsou presented a novel approach to combine a large sentiment lexicon and machine learning techniques for sentiment classification (Lu & Tsou, 2010). The experiments with the NTCIR opinion data showed that the approach significantly outperformed the baselines. Whitehead and Yaeger compared Bagging, Boosting, and Random Subspace for sentiment classification and shown that ensemble learning methods can increase classification accuracy in the domain of sentiment classification (Whitehead & Yaeger, 2010). Xia et al. made a comparative study of effectiveness of Stacking for sentiment classification (Xia et al., 2011). Experimental results demonstrated that using ensemble technique is an effective way to combine different feature sets and classification algorithms for better classification performance. Su et al. used Stacking for sentiment classification of reviews based on different methods and compared it with majority voting (Su et al., 2013). Stacking has been proven to be consistently effective across multiple domains. Li et al. proposed three heterogeneous ensemble learning methods to integrate the four classifiers (Li, Wang, & Chen, 2012). Experiments on Chinese benchmark datasets showed that Stacking outperformed any individual classifiers. Wang et al. conducted a comparative assessment of the performance of three popular ensemble methods (Bagging, Boosting, and Random Subspace) based on five base learners (Naive Bayes, Maximum Entropy, Decision Tree, K Nearest Neighbor, and Support Vector Machine) for sentiment classification (Wang et al., 2014). Empirical results revealed that ensemble methods substantially improve the performance of individual base learners for sentiment classification.

Although many ensemble learning methods have been proposed for sentiment classification, Random Subspace, an important ensemble learning method that has been successfully applied in other text classification problems (Zhang, 2008), is under-examined in the literature. To fill this research gap, this study proposes a new Random Subspace method, POS-RS, for sentiment classification based on part-of-speech analysis. This research will give more insights into sentiment classification using ensemble learning methods.

Ensemble learning is a machine learning paradigm where multiple learners are trained to solve the same problem (Polikar, 2006; Zhou, 2012). In contrast to ordinary machine learning approaches that try to learn one hypothesis from the training data, ensemble methods try to construct a set of hypotheses and combine them for use (Liu & Zsu, 2009). Learners which an ensemble is composed of are usually called base learners. One of the earliest studies on ensemble learning is Dasarathy and Sheela’s research (Dasarathy & Sheela, 1979), which discussed partitioning the feature space using two or more classifiers. In 1990, Hansen and Salamon showed that the generalization performance of an Artificial Neural Network (ANN) can be improved using an ensemble of similarly configured ANNs (Hansen & Salamon, 1990). Schapire demonstrated that a strong classifier in Probably Approximately Correct (PAC) sense can be generated by combining weak classifiers through Boosting (Schapire, 1990), the predecessor of the suite of AdaBoost algorithms. Since these seminal works, studies in ensemble learning have expanded rapidly, appearing often in the literature under many creative names and ideas (Polikar, 2006).

The generalization ability of an ensemble method is usually much stronger than that of a single learner, which makes ensemble methods very attractive. Dietterich (1997) gave three reasons based on viewing the nature of machine learning as searching in a hypothesis space for the most accurate hypothesis. Firstly, the training data might not provide sufficient information for choosing a single best learner. For example, there may be many base learners performing equally well on the training set. Thus, combining these learners may be a better choice. Secondly, the search processes of the learning algorithms might be imperfect. For example, even if there is a unique best hypothesis, it might be difficult to attain this goal, since running the algorithms results in sub-optimal hypotheses. Thus, ensembles can compensate for such imperfect search processes. Thirdly, the hypothesis space being searched might not contain the true target function, while ensembles can give some good approximation. For example, the classification boundaries of DTs are linear segments parallel to coordinate axes. If the target classification boundary is a smooth diagonal line, using a single DT cannot lead to a good result. But a good approximation can be achieved by combining a set of DTs. Although these intuitive explanations are reasonable, they lack rigorous theoretical analyses.

In practice, to achieve a good ensemble, two necessary conditions should be satisfied: accuracy and diversity (Windeatt & Ardeshir, 2004). The base learner should be more accurate than random guessing, and each base learner should have its own knowledge about the problem, with a different pattern of errors than other base learners. In general, ensemble learning methods can be divided into three categories: instance partitioning methods, feature partitioning methods, and Stacking (Polikar, 2006; Wang, Hao, Ma, & Jiang, 2011). Bagging and Boosting are instance partitioning methods; Random Subspace is a feature partitioning method (Polikar, 2006; Wang & Ma, 2011). The Random Subspace method is an ensemble construction technique proposed by Ho (1998). In Random Subspace, the training dataset is modified as in Bagging. However, this modification is performed in the feature space rather than in the instance space. The Random Subspace method may benefit from using random subspaces for both constructing and aggregating the base learners (Wang & Ma, 2011; Zhang, 2008).

Compared with other ensemble methods, Random Subspace method is more suitable to the problem of sentiment classification. The main reason is that sentiment classification is an instance of text classification problems, which has a large number of relevant and redundant features. Thus, good base learners can be obtained in random subspaces than in the original feature space, when the dataset has many redundant or irrelevant features (Ho, 1998). The combined decision of such base learners may be superior to a single learner constructed on the original training dataset in the complete feature sets. Although Random Subspace has these superiorities, previous studies have not paid attention to it in the area of sentiment classification. Subsequently, in line with the characters of sentiment classification, a new Random Subspace method, POS-RS, is proposed for sentiment classification based on part-of-speech analysis. The framework of POS-RS is shown in Fig. 1
                     . POS-RS has two major steps: extracting a set of features and constructing base learners. These steps are used to carry out sentiment classification.

In ensemble learning, accuracy and diversity of base learners are two important factors for the performance of ensemble learning methods. However, like many multi-objective optimization problems, how to balance these two factors lacks theoretical guidance. Comparative studies have shown that on average AdaBoost is the best method although Random Subspace and Bagging have their application niches as well (Bauer & Kohavi, 1999; Rodriguez, Kuncheva, & Alonso, 2006). Diversity in Bagging is obtained by using bootstrapped replicas of the training data: different training data subsets are randomly drawn—with replacement—from the entire training dataset (Breiman, 1996). Boosting creates different base learners by sequentially reweighting the instances in the training dataset. Each instance misclassified by the previous base learner will get a larger weight in the next round of training (Schapire, 1990). Unlike instance partitioning methods, Random Subspace construct base learners through randomly selecting features. Randomized feature subsets is used to “inject randomness” into base learners in order to increase their diversity in the ensemble, while this method ignores another import factor, accuracy. Accordingly, an interesting question emerges whether there exists a mechanism that is capable of considering accuracy and diversity simultaneously for sentiment classification.

For the sentiment classification problem, the goal is to distinguish reviews’ sentiment polarity, i.e. positive or negative. To achieve this, reviews need to be linguistically analyzed. Based on linguistic theories, a sentence is composed of many relative words. In grammar, a word class is a linguistic category of words, which is generally defined by the syntactic or morphological behavior of the lexical item in a sentence (Winkler, 2012). Words in English can be divided into content words and function words (Winkler, 2012). Content words are words that refer to some object, action, or other non-linguistic meaning. Function words are words that have little lexical meaning, but instead serve to express grammatical relationships. Content words mainly include nouns, verbs, adjectives, and adverbs. Function words mainly include articles, prepositions, conjunctions, and so on (Winkler, 2012). Content words typically hold much more information than function words, while many functional words are used to make sentences fluent and integrated. Just for this reason, WordNet is constructed based on a lexical database for English including nouns, verbs, adjective and adverbs, while not including function words (Miller, 1995). These functional words not only reduce the accuracy of classifier, but also cause longer computing time in classification (D’hondt, Verberne, Weber, Koster, & Boves, 2012; Liu, Yu, Chen, Wang, & Wu, 2010).

Based on the above analysis and justification, we design a mechanism to balance the accuracy and diversity based on part-of-speech analysis. We firstly separate reviews into content lexicons and function lexicons. Then, two parameters, i.e., content lexicon subspace rate and function lexicon subspace rate, are used to control the process of constructing sub datasets for POS-RS. Unlike the original Random Subspace, which uses only one parameter, i.e., subspace rate, to control the diversity of ensemble, POS-RS includes two parameters to simultaneously control the accuracy and the diversity of ensemble.

Specifically, in the feature extraction stage, the Stanford POS Tagger (Toutanova & Manning, 2000) is used to extract content lexicons and function lexicon from the reviews. The content lexicons consist with nouns, verbs, adjectives, and adverbs. Nouns is a word or group of words used for referring to a person, thing, place, or quality, while, verbs is a type of word or phrase that shows an action or a state. Adjectives is a word used for describing a noun or pronoun, while, adverbs is a word used for describing a verb, an adjective, another adverb or a whole sentence. These four sets of content lexicons play a dominant role in classifying sentiment of reviews, as demonstrated by our experiments. Stanford POS Tagger is a piece of software that reads text in some language and assigns parts of speech to each word (and other token), such as noun, verb and adjective (Toutanova & Manning, 2000). Based on the outputs of Sanford POS Tagger, content lexicons and function lexicons are constructed separately.

Random Subspace methods use the subspace rate (the ratio of selected features over total feature stock) to randomly select features to construct sub datasets. In POS-RS, two parameters, i.e. content lexicon subspace rate and function lexicon subspace rate, are proposed to construct sub datasets, which control the accuracy and the diversity of base learners simultaneously. After construction of sub datasets, base learners are trained on the different sub datasets.

The purpose of the model construction module is to learn the pattern in every sub datasets. Although each base learner in the ensemble only has to satisfy the minimum requirement of being more accurate than random guessing, the more accurate base learners can achieve the better performance. SVM is one of the most widely used machine learning techniques. Pang et al. conducted an empirical study in sentiment classification, concluding that SVM outperformed other classifiers (Pang et al., 2002). Therefore, SVM is selected as base learner in this study.

In SVM learning, the original input space is mapped into a high-dimensional dot product space called a feature space, and in the feature space the optimal hyperplane is determined to maximize the generalization ability of the classifier (Vapnik, 2000; Wang & Ma, 2012). The optimal hyperplane is found by exploiting the optimization theory, and respecting insights provide by the statistical learning theory.

Given a set of training data points 
                           
                              
                                 
                                    TR
                                 
                                 
                                    k
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      1
                                                   
                                                   
                                                      k
                                                   
                                                
                                                ,
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      1
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      N
                                                   
                                                   
                                                      k
                                                   
                                                
                                                ,
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      N
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        , where 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                                 
                                    k
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    n
                                 
                              
                           
                         is the vector space pattern, 
                           
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                              ∈
                              {
                              -
                              1
                              ,
                              1
                              }
                           
                         is the class label for a 2-class problem, SVM attempts to find a classifier f(x), which minimizes the expected misclassification rate. A linear classifier f(x) is a hyperplane, and can be represented as f(x)=sgn(wTx
                        +
                        b). Finding the optimal classifier f(x) in SVM is equivalent to solving a convex quadratic optimization problem in (1):
                           
                              (1a)
                              
                                 
                                    
                                       
                                          max
                                       
                                       
                                          w
                                          ,
                                          b
                                       
                                    
                                 
                                 
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 ‖
                                 
                                    
                                       w
                                    
                                    
                                       k
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       C
                                    
                                    
                                       k
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          N
                                       
                                    
                                 
                                 
                                    
                                       ξ
                                    
                                    
                                       i
                                    
                                    
                                       k
                                    
                                 
                              
                           
                        
                        
                           
                              (1b)
                              
                                 Subject to
                                 
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                
                                             
                                          
                                          +
                                          
                                             
                                                b
                                             
                                             
                                                k
                                             
                                          
                                       
                                    
                                 
                                 
                                 ⩾
                                 
                                 1
                                 -
                                 
                                    
                                       ξ
                                    
                                    
                                       i
                                    
                                    
                                       k
                                    
                                 
                                 
                                 (
                                 
                                    
                                       ξ
                                    
                                    
                                       i
                                    
                                    
                                       k
                                    
                                 
                                 
                                 ⩾
                                 
                                 0
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 N
                                 )
                              
                           
                        where Ck
                         is called the regularization parameter, and is used to balance the classifier’s complexity and classification accuracy on the training set 
                           
                              
                                 
                                    TR
                                 
                                 
                                    k
                                 
                              
                           
                        . This quadratic problem is generally solved through its dual formulation. Simply replacing the involved vector inner-product with a non-linear kernel function converts linear SVM into a more flexible non-linear SVM, which is the essence of the famous kernel trick. Any function satisfying Mercer’s condition can be used as the kernel function (Vapnik, 2000).

After training of different base learners, another issue is to combine these results of different base learners. The goal of the fusion module is to aggregate different base learners’ results and reduce the detection errors of every 
                           
                              
                                 
                                    SVM
                                 
                                 
                                    k
                                 
                              
                           
                        . In ensemble learning methods, majority voting is a popular aggregation method whose effectiveness has been proven empirically and theoretically (Breiman, 1996; Polikar, 2006). Accordingly, majority voting rule is used to aggregate different based learners’ results in POS-RS. Given a series of base learners 
                           
                              {
                              
                                 
                                    C
                                 
                                 
                                    i
                                 
                              
                              (
                              x
                              )
                              ,
                              
                              1
                              
                              ⩽
                              
                              i
                              
                              ⩽
                              
                              k
                              }
                           
                        , the majority voting rule can be represented as follows:
                           
                              (2)
                              
                                 
                                    
                                       C
                                    
                                    
                                       ∗
                                    
                                 
                                 (
                                 x
                                 )
                                 =
                                 sgn
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          
                                             
                                                C
                                             
                                             
                                                i
                                             
                                          
                                          (
                                          x
                                          )
                                          -
                                          
                                             
                                                k
                                                -
                                                1
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The POS-RS algorithm works as follow. At first, it uses the Stanford POS Tagger to split the original feature space D into content lexicons DC
                         and function lexicons DF
                        . Then, POS-RS generate K sub datasets by randomly choosing and combining two subsets of features from DC
                         and DF
                        , controlled by the parameters rC
                         and rF
                         respectively. After that, K base learners are trained on K sub datasets. In this research we choose SVM as the base learner. Finally, the predictions of the base learners are combined via majority voting. The pseudo-code of the POS-RS algorithm is shown in Fig. 2
                        . Compared with the original Random Subspace, POS-RS has two important parameters, i.e. content lexicon subspace rate and function lexicon subspace rate.

To verify the effectiveness of POS-RS for sentiment analysis, we investigated ten publicly available sentiment analysis datasets from a wide variety of domains. The Movie dataset was collected from the commonly-used Cornell movie-review dataset (Pang & Lee, 2004). It consists of four collections of movie-review documents labeled with sentiment polarities (positive or negative) or ratings on a scale from 1 to 5, and movie-review sentences labeled with subjectivity statuses (subjective or objective) or polarities. In our experiments, the documents labeled with polarities were chosen as the dataset, which contained 1000 positive and 1000 negative reviews. The other nine sentiment analysis datasets were provided by Whitehead and Yaeger (2009). These datasets include reviews and corresponding ratings, in which the rating of “1” was a positive sentiment and the rating of “−1” was a negative sentiment. Except for the Camera dataset that contain 250 positive instances and 248 negative instances, the other eight datasets have the equal number of positive and negative instances. The summary descriptions of the datasets are shown in Table 2
                        .

@&#PERFORMANCE EVALUATION@&#

The established standard measure in sentiment classification, average accuracy, was adopted to evaluate the performance of the proposed method. The definition of average accuracy can be explained with a confusion matrix as shown in Table 3
                        .

Formally, average accuracy is defined as follows:
                           
                              (3)
                              
                                 Average accuracy
                                 =
                                 
                                    
                                       TP
                                       +
                                       TN
                                    
                                    
                                       TP
                                       +
                                       FP
                                       +
                                       FN
                                       +
                                       TN
                                    
                                 
                              
                           
                        
                     

In order to examine the performance of the proposed method, some baseline methods are selected for comparison purposes. Although the Stacking method is often used for sentiment classification, the performance of Stacking is difficult to analyze theoretically (Polikar, 2006; Zhou, 2012). Similarly, little guidance is available on how to select base learners (Polikar, 2006; Zhou, 2012). In this research, therefore, three popular ensemble methods, i.e., Bagging, Boosting, and the original Random Subspace method, are used as benchmark techniques in the experiments. Like POS-RS, Bagging, Boosting, and Random Subspace all used SVM as base learner. Besides above mentioned ensemble methods, another two popular sentiment classification methods, i.e., Naive Bayes (NB) and Maximum entropy (ME), were selected as benchmark methods. To verify our contribution to sentiment analysis besides machine learning, we also compared our proposed method with three widely used state-of-the-art sentiment analysis toolkits, i.e., the sentiment module in StanfordNLP (Socher et al., 2013), SentiStrength (Thelwall, Buckley, & Paltoglou, 2012), and OpinionFinder (Wilson et al., 2005). The StanfordNLP-based algorithm (denoted as SNLP) uses average sentence polarity to approximate document polarity. SentiStrength (denoted as SS) estimates the strength of positive and negative sentiment simultaneously, and uses the maximum of the two to label the document polarity. OpinionFinder (denoted as OF) provides document-level polarity labels as well as corresponding probability distributions.

Moreover, in order to minimize the influence of variability in the training set, 10-fold cross validation was performed ten times on the ten datasets. In detail, each dataset was partitioned into ten subsets with similar sizes and distributions. Then, the union of nine subsets was used as the training set while the remaining subset is used as the test set. The process was repeated ten times, such that every subset had been used as the test set once. The average test result was regarded as the result of the 10-fold cross validation. The process was repeated for 10 times with random partitions of the ten subsets, and the average results of these different partitions were recorded. The experimental procedure is shown in Fig. 3
                        .

@&#RESULTS AND DISCUSSIONS@&#

We used the data mining toolkit WEKA (Waikato Environment for Knowledge Analysis) version 3.7.0 for implementation of base learners. This open-source toolkit includes a collection of machine learning algorithms for solving data mining problems (Witten, Frank, & Hall, 2011). Additionally, the Stanford POS Tagger (version stanford-postagger-2012-01-06) was employed to extract content lexicons and function lexicons.

In this study, we compared the performances of SVM, Bagging, Boosting, Random Subspace (RS), Naïve Bayes (NB), Maximum Entropy (ME), StanfordNLP (SNLP), SentiStrength (SS), OpinionFinder (OF), and the proposed POS-RS. Among these methods, the SVM algorithm, Bagging algorithm, Boosting algorithm, Random Subspace algorithm, NB algorithm, and ME algorithm were implement by the SMO module, the Bagging module, AdaBoostM1 module, the RandomSubSpace module, the NaiveBayes module and Logistic module (WEKA’s own version of multinomial logistic regression) in WEKA, respectively. The ensemble modules used SVM as base learner. StanfordNLP 3.4, SentiStrengh’s Window version, and OpinionFinder 2.0 were used. As the original datasets were text forms, WEKA’s StringToWordVector filter was used to convert original texts into an N-Gram representation. The parameter of SetWrodsKeep is set to the maximum number of features of datasets. The content lexicon subspace rate rC
                      and function lexicon subspace rate rF
                      varied from 0.1 to 1 with interval 0.1. The subspace rate of Random Subspace varied from 0.1 to 0.9 with interval 0.1. Except when stated otherwise, all the default parameters in WEKA were used.

@&#EXPERIMENTAL RESULTS@&#


                        Table 4
                         presents the average accuracy of all methods, where the values following “±” are standard deviations. The highest average accuracies of different datasets are boldfaced.

Generally speaking, the results in Table 4 show that the performance of proposed POS-RS method is better than that of other methods. Except on the Camera, Music and TV datasets, POS-RS achieves the highest average accuracies of 85.26% (rC
                        
                        =0.5 and rF
                        
                        =0.5) on Camp, 85.03% (rC
                        
                        =0.8 and rF
                        
                        =0.7) on Doctor, 68.82% (rC
                        
                        =0.5 and rF
                        
                        =0.6) on Drug, 79.79% (rC
                        
                        =0.4 and rF
                        
                        =0.9) on Laptop, 83.86% (rC
                        
                        =0.6 and rF
                        
                        =0.2) on Lawyer, 85.55% (rC
                        
                        =0.9 and rF
                        
                        =0.5) on Movie, and 70.66% (rC
                        
                        =0.8 and rF
                        
                        =0.3) on Radio. RS, OF, and SNLP each leads on performance in one dataset. Moreover, as expected, methods only using function lexicons (F2) yield the lowest average accuracies.

@&#DISCUSSIONS@&#

To ensure that the assessment does not occur by chance, we tested the significance of these results. Following (Demšar, 2006; García-Pedrajas, 2009), we firstly conducted an Iman–Davenport test (Iman & Davenport, 1980), to ascertain whether there are significant differences among all methods. Then, pairwise differences were measured using a Wilcoxon test (Demšar, 2006). The formulation of the test (Wilcoxon, 1945) is as follows. Let di
                            be the difference between the error values of the methods in i th data set. These differences are ranked according to their absolute values; in case of ties, an average rank is assigned. Let R
                           + be the sum of ranks for the data sets on which the second algorithm outperformed the first, and let R
                           − be the sum of ranks where the first algorithm outperformed the second. Ranks are split evenly among the sums
                              
                                 (4)
                                 
                                    
                                       
                                          R
                                       
                                       
                                          +
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   d
                                                
                                                
                                                   i
                                                
                                             
                                             >
                                             0
                                          
                                       
                                    
                                    rank
                                    (
                                    
                                       
                                          d
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    +
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   d
                                                
                                                
                                                   i
                                                
                                             
                                             =
                                             0
                                          
                                       
                                    
                                    rank
                                    (
                                    
                                       
                                          d
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                              
                           and
                              
                                 (5)
                                 
                                    
                                       
                                          R
                                       
                                       
                                          -
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   d
                                                
                                                
                                                   i
                                                
                                             
                                             <
                                             0
                                          
                                       
                                    
                                    rank
                                    (
                                    
                                       
                                          d
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    +
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   d
                                                
                                                
                                                   i
                                                
                                             
                                             =
                                             0
                                          
                                       
                                    
                                    rank
                                    (
                                    
                                       
                                          d
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                              
                           
                        

Let T be the smaller of the two sums and N be the number of data sets. For a small N, there are tables with the exact critical values for T. For a larger N, the statistics
                              
                                 (6)
                                 
                                    z
                                    =
                                    
                                       
                                          T
                                          -
                                          
                                             
                                                1
                                             
                                             
                                                4
                                             
                                          
                                          N
                                          (
                                          N
                                          +
                                          1
                                          )
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      1
                                                   
                                                   
                                                      24
                                                   
                                                
                                                N
                                                (
                                                N
                                                +
                                                1
                                                )
                                                (
                                                2
                                                N
                                                +
                                                1
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           is distributed approximately according to N(0,1). We combined these two tests to assess the performance difference of the different algorithms. When the comparison was between two algorithms only the Wilcoxon test was used.

We also employed the statistics used in (Webb, 2000) to compare two learning algorithms across all data sets, namely, the win/draw/loss record. The win/draw/loss record presents three values, the number of data sets for which algorithm A obtained better, equal, or worse performance than algorithm B with respect to classification accuracy. We also reported the statistically significant win/draw/loss record, where a win or loss was only counted if the difference in values was determined to be significant at the 0.05 level by a paired t-test.

As the results of methods only using function lexicons (F2) are obviously worse, we only compared the methods using F1 and F1+F2 with POS-RS. Table 5
                            shows the comparison among the methods. Columns labeled s present the win/draw/loss record, where the first value is the number of data sets for which row
                           <
                           col, the second is the number for which row
                           =
                           col, and the last is the number for which row
                           >
                           col. Columns labeled pw
                            present the results of Wilcoxon tests. For all methods, the Iman–Daveport test had a p-value of 0.000, showing significant differences among them.

As seen in the tables, POS-RS has significantly better win/draw/loss records than other methods. Compared with RS (F1) and RS (F1+F2), POS-RS gets the records 7/2/1 and 5/3/2. Compared with Bagging (F1) and Bagging (F1+F2), POS-RS gets the records 9/1/0 and 10/0/0. Compared with Boosting (F1) and Boosting (F1+F2), POS-RS gets the records 10/0/0 and 10/0/0. Compared with SVM (F1) and SVM (F1+F2), POS-RS get the records 10/0/0 and 10/0/0. Compared with NB (F1+F2), ME (F1+F2), SNLP, SS, and OF, POS-RS get the records 9/0/1, 10/0/0, 8/1/1, 10/0/0, and 8/1/1. In summary, we can confidently confirm the effectiveness of the new proposed POS-RS for sentiment classification.

Besides, some other interesting phenomena were observed in the experiments. The first interesting thing is that compared with SVM (F1), SVM (F1+F2) gets a record 2/2/6. This result indicates that content lexicons have more discriminative power than function lexicons, which is consistent with previous research (D’hondt et al., 2012; Liu et al., 2010). The second interesting observation is that compared with RS (F1), RS (F1+F2) gets a record 3/6/1. A possible explanation is that the performance of Random Subspace is determined by the diversity and the accuracy of base learners. Compared with content lexicons (F1), the richer feature space (F1+F2) can produce base learners with higher diversity, which can compensate the issue of low accuracy. Thirdly, Bagging (F1+F2) gets the equal records with Bagging (F1), and Boosting (F1) gets the better records than Boosting (F1+F2). It is probably due to the fact that Boosting is easily influenced by noisy data (Polikar, 2006; Wang, Ma, & Yang, 2011). At the same time, sentiment classification has many redundant and relevant features (Isa, Lee, Kallimani, & Rajkumar, 2008; Leopold & Kindermann, 2002). Thus, only using content lexicons can reduce such influence.

Subsequently, we will attempt to explain why POS-RS works, using bias and variance decompositions of the classification error. Bias and variance analysis is typically used to guide the design of ensemble learning methods and also provides a tool to analyze learning algorithms. The classification error is the expected loss when classifying a new instance and can be decompose into bias and variance using Kohavi and Wolpert’s definition (Kohavi & Wolpert, 1996).
                              
                                 (7)
                                 
                                    
                                       
                                          Bias
                                       
                                       
                                          x
                                       
                                       
                                          2
                                       
                                    
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             y
                                             ∈
                                             Y
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      P
                                                   
                                                   
                                                      Y
                                                      ,
                                                      X
                                                   
                                                
                                                (
                                                Y
                                                =
                                                y
                                                |
                                                X
                                                =
                                                x
                                                )
                                                -
                                                
                                                   
                                                      P
                                                   
                                                   
                                                      τ
                                                   
                                                
                                                (
                                                L
                                                (
                                                τ
                                                )
                                                (
                                                x
                                                )
                                                =
                                                y
                                                )
                                             
                                          
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           
                           
                              
                                 (8)
                                 
                                    
                                       
                                          Variance
                                       
                                       
                                          x
                                       
                                    
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          
                                             1
                                             -
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      y
                                                      ∈
                                                      Y
                                                   
                                                
                                             
                                             
                                                
                                                   P
                                                
                                                
                                                   τ
                                                
                                             
                                             
                                                
                                                   (
                                                   L
                                                   (
                                                   τ
                                                   )
                                                   (
                                                   x
                                                   )
                                                   =
                                                   y
                                                   )
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (9)
                                 
                                    
                                       
                                          σ
                                       
                                       
                                          x
                                       
                                    
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          
                                             1
                                             -
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      y
                                                      ∈
                                                      Y
                                                   
                                                
                                             
                                             
                                                
                                                   P
                                                
                                                
                                                   Y
                                                   ,
                                                   X
                                                
                                             
                                             
                                                
                                                   (
                                                   Y
                                                   =
                                                   y
                                                   |
                                                   X
                                                   =
                                                   x
                                                   )
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           The third term σx
                            relates to irreducible error. We follow Kohavi and Wolpert’s practice of aggregating this value with bias due to the difficulty of estimating it from observations of classification performance (Kohavi & Wolpert, 1996).


                           Tables 6 and 7
                           
                            show the bias/variance decompositions for compared methods
                              1
                              SNLP and SS do not have probability distributions of labels available, thus are not part of the analysis.
                           
                           
                              1
                            on the ten sentiment datasets. The lowest errors, biases, and variances of different datasets are boldfaced.

As can be seen from Table 6, POS-RS can reduce bias and variance simultaneously compared to the base learner, i.e., SVM. Additionally, except on Camera, Music, and TV datasets, POS-RS all gets the lowest bias of 0.0872 on Camp, 0.0842 on Doctor, 0.1845 on Drug, 0.0997 on Laptop, 0.1006 on Lawyer, 0.0722 on Movie, 0.1655 on Radio. All above results can explain why POS-RS gets the highest average accuracy.

Furthermore, it is interesting that POS-RS does not get any lowest variance. However, among ten lowest variances, Bagging (F1) gets five lowest variances. This result is also consistent with prior research (Breiman, 1998; Webb, 2000). The reason, as Breiman argued, is that Bagging can be viewed as classifying by application of an estimate of the central tendency of base learners (Breiman, 1998).

Random Subspace has an important parameter, i.e., subspace rate, to control the diversity of base learners. In our experiments, subspace rate varies from 0.1 to 0.9. Figs. 4–6
                           
                           
                            show the average accuracy curve when using different feature sets.

As shown in Figs. 4–6, Random Subspace gets lower average accuracy when the subspace ratio is lower than 0.3. After that, average accuracy curves reach the peak and they are becoming smooth as the subspace rate increases. These results are consistent with previous research, which also can explain why subspace rate is recommended to be set to 0.5 (Ho, 1998).

Unlike the original Random Subspace method, POS-RS uses two important parameters, i.e. content lexicon subspace rate and function lexicon subspace rate, to control the balance between accuracy and diversity. A sensible question might be: what are the optimal values of content lexicon subspace rate and function lexicon subspace rate? However, there is not a value of content lexicon subspace rate and function lexicon subspace rate that we can consider optimal. Thus, the impact of using different numbers of content lexicon subspace rate and function lexicon subspace rate are studied further. Figs. 7–16
                           
                           
                           
                           
                           
                           
                           
                           
                           
                            display the classification accuracy surface under different content lexicon subspace rate and function lexicon subspace rate. The X-axis is defined to be the content lexicon subspace rate and the Y-axis is defined to be the function lexicon subspace rate.

As shown in Figs. 7–16, when getting the highest average accuracy, the values of content lexicon subspace rate and function lexicon subspace rate are (rC
                           
                           =0.6 and rF
                           
                           =0.2) on Camera, (rC
                           
                           =0.5 and rF
                           
                           =0.5) on Camp, (rC
                           
                           =0.8 and rF
                           
                           =0.7) on Doctor, (rC
                           
                           =0.5 and rF
                           
                           =0.6) on Drug, (rC
                           
                           =0.4 and rF
                           
                           =0.9) on Laptop, (rC
                           
                           =0.6 and rF
                           
                           =0.2) on Lawyer, (rC
                           
                           =0.9 and rF
                           
                           =0.5) on Movie, (rC
                           
                           =0.6 and rF
                           
                           =0.6) on Music, (rC
                           
                           =0.8 and rF
                           
                           =0.3) on Radio, and (rC
                           
                           =0.7 and rF
                           
                           =0.3) on TV. Based on these results, we can see that for the content lexicon subspace rate, 0.4–0.9 is the more suitable value; it is difficult to specify a recommended value for function lexicon rate, and it will vary with the different content lexicon subspace rate. The reason is that the content lexicon subspace rate is the dominant factor for classification accuracy. However, this does not mean that the function lexicon subspace rate is not important. Just on the contrary, the function lexicon subspace rate has important influence on the diversity of base learners. This has been proved in the experiment where RS (F1) and RS (F1+F2) underperform POS-RS.

The rise of social media has fueled interest in sentiment classification. Promptly and correctly classifying sentiment from the text has become an important task for individuals and companies. Many researchers have investigated sentiment classification problem based on ensemble learning methods, but very few literature focus on the Random Subspace and its variations.

In this research, we proposed a new Random Subspace method, POS-RS, for sentiment analysis based on part-of-speech analysis. Two important parameters, i.e., content lexicon subspace rate and function lexicon subspace rate, are introduced to control the balance between the accuracy and the diversity of base learners. Ten publicly available sentiment analysis datasets were investigated to verify the effectiveness of the proposed method. Empirical results showed that POS-RS can reduce bias and variance simultaneously and achieves the best results overall. All these results illustrate that POS-RS can be used as a viable method for sentiment classification, and possibly other text classification problems.

There are several future research directions for this study. First, although we proposed a new Random Subspace method, POS-RS, for sentiment classification based on part-of-speech analysis, and evaluated the effectiveness on ten datasets, more detailed theoretical analysis are needed further. We introduced two parameters, i.e., content lexicon subspace rate and function lexicon subspace rate, to control the balance the diversity and the accuracy of base learners. It is interesting to formally understand how these two parameters influence the accuracy of POS-RS. Second, in this research, as SVM is a state-of-the-art classifier and has been proved to be an effective method in sentiment classification, we use it as base learner to evaluate our proposed POS-RS. Actually, like other ensemble methods, our proposed POS-RS is a framework, which can also employ other methods, i.e., DT, as base learner. Third, in this study, we use ten popular sentiment datasets to evaluate our proposed POS-RS. In the future research, large and more diverse datasets should be collected and examined for further investigation. Fourth, as ensemble learning methods are typically computationally intensive, parallel computing techniques should be explored to tackle this problem in the future research.

@&#ACKNOWLEDGEMENTS@&#

This work is partially supported by the National Natural Science Foundation of China (71071045, 71131002, 71101042, 71471054), the National Basic Research Program of China (973 Program) (2013CB329603), the Specialized Research Fund for the Doctoral Program of Higher Education (20110111120014), the China Postdoctoral Science Foundation (2011M501041, 2013T60611), the Special Fund of AnHui Province Key Research Institute of Humanities and Social Sciences at Universities (SK2013B400) and the Special Fund of Political Theory Research Center of HeFei University of Technology (2012HGXJ0392).

@&#REFERENCES@&#

