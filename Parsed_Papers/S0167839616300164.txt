@&#MAIN-TITLE@&#Unsupervised 3D shape segmentation and co-segmentation via deep learning

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a novel segmentation and co-segmentation approach for 3D shapes.


                        
                        
                           
                           We introduce deep learning into 3D shape segmentation and co-segmentation.


                        
                        
                           
                           Our method is data-driven but does not need a tedious labeling process.


                        
                        
                           
                           Our algorithm achieves better or comparable performance when compared with the state-of-the-art methods.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

3D shapes

Segmentation

Co-segmentation

Deep learning

High-level features

@&#ABSTRACT@&#


               Graphical abstract
               
                  
               
               
                  
               
            

@&#INTRODUCTION@&#

Automatic segmentation of 3D shapes is a fundamental operation in geometric modeling and shape processing (Wu et al., 2013). It helps shape understanding and is also central to many computer graphics problems, including mesh parameterization, skeleton extraction, resolution modeling, shape retrieval and so on. Most of existing segmentation algorithms partition a single 3D shape depending on a specific kind of signature that is often invariant to a certain transformation group (Veltkamp and Hagedoorn, 2001; Hong and Soatto, 2015).

Known signatures include scale-invariant heat kernel signatures (SIHKS) (Bronstein and Kokkinos, 2010), shape diameter function (SDF) (Shapira et al., 2008), Gaussian curvature (GC) (Gal and Cohen-Or, 2006) and so on. To our knowledge, they can be used for shape segmentation purpose. However, shape understanding is a complicated task and thus we can't rely on a single signature to settle the segmentation problem once and for all. This is due to the fact that a signature, represented as a statistics or deterministic function, can only characterize the geometric shapes from a special perspective.

Recently, researchers find that simultaneously segmenting a set of 3D shapes within the same class into consistent decompositions, i.e., co-segmentation, is possible to achieve a better segmentation result than the traditional segmentation that is targeted at a single object. Some of these algorithms (Kalogerakis et al., 2010; van Kaick et al., 2011) require labeled training data to learn common segmentation rules. Generally speaking, the supervised algorithms are able to produce a desirable segmentation result if the labeled data set is sufficiently large. However, labeling 3D shapes is pretty time-consuming and tedious. By contrast, the unsupervised methods (Sidi et al., 2011; Huang et al., 2011; Hu et al., 2012; Meng et al., 2013; Wu et al., 2013) can segment 3D shapes automatically, without extra labeling. Generally, it has to be at the cost of segmentation performance.

In this paper, we propose an effective unsupervised method for shape segmentation/co-segmentation based on deep learning. In the very beginning, we decompose each 3D shape into primitive patches to generate over-segmentation and compute various shape signatures for the input models. The signatures used in this paper include SIHKS (Bronstein and Kokkinos, 2010), SDF (Shapira et al., 2008) and GC (Gal and Cohen-Or, 2006). However, each separate signature can only characterize part of geometric features. Therefore, in the next stage, we build a deep neural network for unsupervised learning such that high-level features can be extracted from the geometric signatures computed in the first stage. It's noted that this stage doesn't need a labeling operation. With the support of unsupervised deep learning, we can finally segment models guided by the high-level features. Fig. 1
                      shows an example of segmentation and co-segmentation computed by our approach.

We evaluate our approach on two open datasets, including the Princeton Segmentation Benchmark (Chen et al., 2009) and the Shape COSEG Dataset (Wang et al., 2012). Extensive experimental results exhibit the superior segmentation/co-segmentation performance of the proposed method over the previous state-of-the-art approaches.


                     Contributions. Our contributions are twofold.
                        
                           •
                           We introduce deep learning into the problem of shape segmentation and co-segmentation such that various shape signatures can be integrated into a high-level feature space. This algorithmic framework is extensible – it supports a variety of shape signatures and hopefully achieves better segmentation results if some new signatures are considered in this framework.

Our method is data-driven but does not need a tedious labeling process. The new approach, in its nature, is also adaptive to different databases.

The remaining of the paper is organized as follows. Section 2 reviews the related work on model segmentation and shape descriptors. Section 3 presents the overall segmentation framework followed by detailed construction process of high-level features. The unsupervised deep learning technique is detailed in Section 4. After that, we give the segmentation and co-segmentation algorithm in Section 5. In Section 6, we show extensive experimental results, as well as comparisons with the state of the art. Finally, we give limitations and future work in Section 7 and conclude this paper in Section 8.

@&#RELATED WORK@&#

Shape segmentation (Attene et al., 2006; Shamir, 2008) aims at segmenting a 3D shape into meaningful parts and plays an important role in shape analysis and shape understanding. So far, a lot of methods have been proposed for solving this problem. A common practice is to build a shape signature by extracting a kind of geometric properties and then apply it in shape segmentation using some decomposition techniques, such as approximate convexity analysis (Kaick et al., 2014), concavity-aware fields (Au et al., 2012), extreme learning machine (Xie et al., 2014), spectral clustering (Rong and Hao, 2004), K-Means (Shlafman et al., 2002), core extraction (Katz et al., 2005), graph cuts (Golovinskiy and Funkhouser, 2008; Katz and Tal, 2003), random walks (Lai et al., 2008), randomized cuts (Golovinskiy and Funkhouser, 2008), normalized cuts (Golovinskiy and Funkhouser, 2008), and so on. However, shape segmentation depends on the way how people understand a shape and thus is a very challenging task. An individual signature cannot provide sufficient geometric cues to differentiate meaningful parts (Chen et al., 2009).

Recently, researchers find that if the input is a family of 3D models that are deemed to be in the same class, segmenting the models guided by the underlying relevance is possible to get better results than segmenting them individually. This is the so-called co-segmentation problem. For example, Golovinskiy and Funkhouser (2009) transformed co-segmentation into a graph clustering problem. Their assumption is that a global rigid alignment exists between the input shapes, which facilitates iteratively establishing correspondence between respective parts. Later, Xu et al. (2010) used anisotropic part scales to part correspondence and their algorithm performs well on a diverse set of shapes. Zheng et al. (2014) suggested an indirect top-down approach to deal with large shape variations.

Because 3D shape segmentation can actually be regarded as a face clustering process in the feature space, a data-driven method may be used to obtain better performance. For example, Kalogerakis et al. (2010) proposed a supervised approach to do labeling and segmentation simultaneously. They showed that the segmentation performance can be dramatically improved by learning techniques. van Kaick et al. (2011) introduced an approach to part correspondence which incorporates prior knowledge imparted by a training set of pre-segmented, labeled models and combines the knowledge with content-driven analysis based on geometric similarity between the matched shapes. However, these supervised methods often require a huge set of manually labeled segmentation results, which greatly limits its use.

To overcome this problem, Sidi et al. (2011) presented an unsupervised method that takes co-segmentation as a clustering problem in a descriptor space. Huang et al. (2011) proposed to formulate the joint segmentation problem as an integer quadratic programming problem. Experimental results show that co-segmentation significantly outperforms traditional segmentation that targets at a single shape. Hu et al. (2012) presented an unsupervised approach for co-segmentation by over-segmenting the input models into primitive patches and then grouping similar patches via a subspace clustering scheme. Meng et al. (2013) suggested improving the co-segmentation results by a multi-label optimization process. Wang et al. (2012) presented a semi-supervised learning method where the user actively assists in the co-analysis by iteratively providing inputs. Their method requires only a sparse set of constraints to quickly converge toward a consistent and error-free semantic labeling of the set. Recently, Wu et al. (2013) suggested a spectral clustering method that generates consistent segmentation by performing spectral clustering in a fused space of shape descriptors.

To our best knowledge, we are the first to introduce deep learning to 3D shape segmentation. It is worth pointing out that the distinguished feature of our method is that it directly learns from unlabeled input 3D shapes and does not require manual labeling. Our approach is different than previous learning-based methods (Kalogerakis et al., 2010), where users have to label a lot of models for training. It is also different than optimization-based methods that are devised to select a desirable combination of shape features (Hu et al., 2012; Kalogerakis et al., 2010).

Shape descriptors are central to 3D shape segmentation. In recent years, numerous feature descriptors have been proposed, such as GC, SDF, average geodesic distance (AGD) (Shapira et al., 2010) and shape distribution (D2) (Osada et al., 2002). Roughly speaking, existing shape descriptors fall into two categories. One kind is global feature descriptors that describe the geometric properties of the overall shape. For example, Gatzke et al. (2005) built a curvature map signature for model matching based on geodesic distances. Vranic (2003) designed a rotation invariant feature vector based on functions on concentric spheres. Loffler (2000) proposed to convert a 3D shape to a series of 2D images. The other kind is local feature descriptors. For example, Bronstein and Kokkinos (2010) developed a scale-invariant heat kernel descriptor. The construction is based on a logarithmically sampled scale-space in which shape scaling corresponds to a translation. Knopp et al. (2010) presented a local 3D shape descriptor by using Hough-voting. Smeets et al. (2013) proposed a four-step algorithm to generate a local shape descriptor for face recognition under expression variations and partial data.

By contrast, the proposed shape segmentation and co-segmentation method in this paper uses high-level features learned from multiple shape signatures by a deep learning framework, rather than directly employs the input 3D feature descriptors. We use a collection of geometry-based shape descriptors as the input of multiple-level neural networks, which is different than the case in the computer vision field where images are represented as a matrix structure and high-level features are usually learned from pixels or pixel blocks that are the input of neural networks. Generally, the first few layers produce low-level features, while the last few layers produce high-level ones. However, the polygonal mesh based representation is not so regular as images and an input model may have a complicated shape and topology. It does not make sense to directly take triangles as the input of neural networks.

@&#OVERVIEW@&#

Our algorithm works on a 3D model database and it consists of four stages, as illustrated in Fig. 2
                     . First, we compute the primitive patches for each shape independently. Then in the next stage, we calculate feature vectors for each patch. After that, we take the extracted feature vectors as the input of a deep neural network and generate a high-level feature space. Finally, we conduct segmentation and co-segmentation on the dataset by performing a clustering operation in the high-level feature space.

Similar to the super-pixel based image segmentation (Ren and Malik, 2003; Shi and Malik, 2000), we divide each shape into primitive patches in the first stage. In implementation, we convert the input mesh into its dual graph and then associate two weights to each graph arc, i.e., a traversal cost, and a cut cost, which are defined based on dihedral angles. Since low-cost cuts in the graph correspond to favorable segmentation boundaries in the mesh, and low-cost traversal paths in the graph occur between points on the mesh likely to be in the same functional parts, we finally decompose the mesh into a number of patches, like that achieved in Golovinskiy and Funkhouser (2008). In our experiments, the number of patches is set to be 
                           L
                           =
                           50
                        ; see Section 6 for details.

There is a common view in the computer vision field – high-level features are often learned from low-level ones. Patel et al. (2015) developed a novel probabilistic framework that accounts for why deep learning is able to work well in practice. In order to inherit the spirit of deep learning, we use low-level feature descriptors as the input and finally generate a high-level feature space. In the experimental setting, we select three widely known feature descriptors, including SIHKS, SDF and GC. These feature descriptors are deemed to have a capability of characterizing the geometric properties well from different perspectives.

For SDF and GC that are a scalar field on each patch, it is very easy to capture the feature distribution using histograms. For SIHKS that is a vector field on each patch, we extract the 1D feature distribution by the famous bag-of-feature (BoF) technique. It's noted that the number of bins in the histograms and that of the bags for the bag-of-feature representation are both set to be 
                           B
                           =
                           100
                        . In this way, any feature descriptor can be adapted into our algorithm framework. After that, we need to concatenate the low-level feature vectors and take them as the input of a deep neural network. Based on the deep learning technique, we can finally get a new feature space that characterizes the high-level features.

For both segmentation and co-segmentation, we need to perform a clustering operation in the high-level feature space – first use the Gaussian mixture model (GMM) to define the probability for representing the presence of each patch within a cluster and then use the graph cuts algorithm (Boykov et al., 2001) to get the final results. The difference between segmentation and co-segmentation, in our approach, is that co-segmentation needs to define a common GMM to guide the consistent segmentation of a family of models.

In this section, we detail the architecture of the deep neural network used in this paper. For purpose of unsupervised learning, we use the stacked auto-encoders (SAEs) (Bengio et al., 2007) to construct high-level features since it is a commonly used technique in the research community.

The architecture of our deep neural network is summarized in Fig. 3
                        . The deep neural network has five layers in total: the input layer, three hidden layers and the output layer. The first layer takes a collection of 300-dimensional input vectors. The numbers of vector dimensions in the three hidden layers are respectively 350, 200, 150. We get a collection of 100-dimensional high-level feature vectors in the output layer.

SAEs is a neural network composed of multiple layers of sparse auto-encoders. It is often used for training in a fully unsupervised way. As shown in Fig. 4
                        , the auto-encoder consists of an input layer, a hidden layer and an output layer. The auto-encoder is able to learn latent features of the input by minimizing the discrepancy between the input features and the reconstruction one from the latent features.

In Fig. 4, the bottom mapping represents the stage of the encoder, while the top mapping represents that of the decoder. Let 
                           
                              
                                 N
                              
                              
                                 I
                              
                           
                         and 
                           
                              
                                 N
                              
                              
                                 H
                              
                           
                         be the numbers of units in the input layer and the hidden layer respectively. Given a feature vector 
                           x
                           ∈
                           
                              
                                 R
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       I
                                    
                                 
                                 ×
                                 1
                              
                           
                        , the auto-encoder transforms x to a latent representation 
                           
                              
                                 h
                              
                              
                                 1
                              
                           
                         by a compound mapping of a linear transformation and a non-linear activation function s as follows:
                           
                              (1)
                              
                                 
                                    
                                       
                                          h
                                       
                                       
                                          1
                                       
                                    
                                    =
                                    s
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          1
                                       
                                    
                                    x
                                    +
                                    
                                       
                                          b
                                       
                                       
                                          1
                                       
                                       
                                          1
                                       
                                    
                                    )
                                    ,
                                 
                              
                           
                         where 
                           
                              
                                 h
                              
                              
                                 1
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       H
                                    
                                 
                              
                           
                         is the latent data, 
                           
                              
                                 w
                              
                              
                                 1
                              
                              
                                 1
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       H
                                    
                                 
                                 ×
                                 
                                    
                                       N
                                    
                                    
                                       I
                                    
                                 
                              
                           
                         is the encoding weight matrix, 
                           
                              
                                 b
                              
                              
                                 1
                              
                              
                                 1
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       H
                                    
                                 
                              
                           
                         is the bias vector, and 
                           s
                           
                              (
                              ⋅
                              )
                           
                         is the sigmoid function:
                           
                              (2)
                              
                                 
                                    s
                                    
                                       (
                                       a
                                       )
                                    
                                    =
                                    
                                       1
                                       
                                          1
                                          +
                                          exp
                                          ⁡
                                          (
                                          −
                                          a
                                          )
                                       
                                    
                                    .
                                 
                              
                           
                         Then the latent representation 
                           
                              
                                 h
                              
                              
                                 1
                              
                           
                         is mapped to a feature vector 
                           
                              
                                 y
                              
                              
                                 1
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       I
                                    
                                 
                              
                           
                        , which approximately reconstructs the input feature vector x by employing a compound mapping of a linear transformation and a non-linear activation function as follows:
                           
                              (3)
                              
                                 
                                    
                                       
                                          y
                                       
                                       
                                          1
                                       
                                    
                                    =
                                    s
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          1
                                       
                                    
                                    
                                       
                                          h
                                       
                                       
                                          1
                                       
                                    
                                    +
                                    
                                       
                                          b
                                       
                                       
                                          2
                                       
                                       
                                          1
                                       
                                    
                                    )
                                    ,
                                 
                              
                           
                         where 
                           
                              
                                 h
                              
                              
                                 1
                              
                           
                         is the latent data, 
                           
                              
                                 w
                              
                              
                                 2
                              
                              
                                 1
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       I
                                    
                                 
                                 ×
                                 
                                    
                                       N
                                    
                                    
                                       H
                                    
                                 
                              
                           
                         is the decoding weight matrix, and 
                           
                              
                                 b
                              
                              
                                 2
                              
                              
                                 1
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       I
                                    
                                 
                              
                           
                         is the bias vector. Then we can learn the underlying features by minimizing the reconstruction error of the cost function:
                           
                              (4)
                              
                                 
                                    C
                                    
                                       (
                                       x
                                       ,
                                       y
                                       )
                                    
                                    =
                                    
                                       1
                                       2
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       ‖
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       
                                          −
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ‖
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                 
                              
                           
                         where x is the input feature vector, m is the number of the input samples, and y is the reconstructed feature vector. Under certain circumstances, some weights may result in over-fitting (Dietterich, 1995). Hence, a regularization term D, also called the weight decay, is introduced for avoiding over-fitting. And the Equation (4) can be redefined as follows:
                           
                              (5)
                              
                                 
                                    C
                                    
                                       (
                                       x
                                       ,
                                       y
                                       ,
                                       θ
                                       )
                                    
                                    =
                                    
                                       1
                                       2
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       ‖
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       
                                          −
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ‖
                                       
                                       
                                          2
                                       
                                    
                                    +
                                    λ
                                    D
                                    
                                       (
                                       θ
                                       )
                                    
                                    ,
                                 
                              
                           
                         where λ is the weight decay parameter, 
                           θ
                           =
                           
                              {
                              w
                              ,
                              b
                              }
                           
                        , w and b represent the weights and the biases of the auto-encoder respectively, and 
                           
                              D
                              
                                 (
                                 θ
                                 )
                              
                              =
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    |
                                    θ
                                    |
                                 
                              
                              
                                 
                                    θ
                                 
                                 
                                    i
                                 
                                 
                                    2
                                 
                              
                           
                        .

If the number of units in the hidden layer is larger (or equal) than that in the input layer, the auto-encoder may learn some useless knowledge from the input features. In order to avoid this situation, a sparsity constraint can be enforced on the hidden layer. We use a sparsity constraint based on the Kullback–Leibler (KL) divergence (Perez-Cruz, 2008) in this paper. To this end, the optimization problem can be formulated as follows:
                           
                              (6)
                              
                                 
                                    
                                       
                                          arg
                                          ⁡
                                          min
                                       
                                       θ
                                    
                                    
                                    C
                                    
                                       (
                                       x
                                       ,
                                       y
                                       ,
                                       θ
                                       )
                                    
                                    +
                                    τ
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          
                                             N
                                          
                                          
                                             H
                                          
                                       
                                    
                                    
                                       KL
                                    
                                    
                                       (
                                       ρ
                                       ‖
                                       
                                          
                                             
                                                ρ
                                             
                                             
                                                i
                                             
                                          
                                          ∧
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                         where,
                           
                              
                                 
                                    
                                       KL
                                    
                                    
                                       (
                                       ρ
                                       ‖
                                       
                                          
                                             
                                                ρ
                                             
                                             
                                                i
                                             
                                          
                                          ∧
                                       
                                       )
                                    
                                    =
                                    ρ
                                    log
                                    ⁡
                                    
                                       ρ
                                       
                                          
                                             
                                                
                                                   ρ
                                                
                                                
                                                   i
                                                
                                             
                                             ∧
                                          
                                       
                                    
                                    +
                                    
                                       (
                                       1
                                       −
                                       ρ
                                       )
                                    
                                    log
                                    ⁡
                                    
                                       
                                          1
                                          −
                                          ρ
                                       
                                       
                                          1
                                          −
                                          
                                             
                                                
                                                   ρ
                                                
                                                
                                                   i
                                                
                                             
                                             ∧
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                         
                        τ is the weight of sparsity penalty term, 
                           
                              
                                 N
                              
                              
                                 H
                              
                           
                         is the number of the hidden units, ρ is the sparsity parameter, and 
                           
                              
                                 
                                    ρ
                                 
                                 
                                    i
                                 
                              
                              ∧
                           
                           =
                           
                              1
                              
                                 
                                    N
                                 
                                 
                                    I
                                 
                              
                           
                           
                              ∑
                              
                                 j
                                 =
                                 1
                              
                              
                                 
                                    N
                                 
                                 
                                    I
                                 
                              
                           
                           
                              
                                 (
                                 
                                    
                                       h
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                              
                                 j
                              
                           
                         is the average activation of the hidden unit 
                           
                              
                                 h
                              
                              
                                 i
                              
                           
                        . From problem (6), we can see that the sparsity penalty term will vanish if 
                           
                              
                                 
                                    ρ
                                 
                                 
                                    i
                                 
                              
                              ∧
                           
                           =
                           ρ
                        . So the closer 
                           
                              
                                 
                                    ρ
                                 
                                 
                                    i
                                 
                              
                              ∧
                           
                         and ρ are, the sparser the hidden layer will be.

With this formulation, we use the back propagation algorithm (Rummelhart, 1986) and the gradient descent approach to train the auto-encoder by optimizing the cost function with respect to θ.

SAEs is composed of multiple layers of sparse auto-encoders, where the latent features learned in previous auto-encoder are used as the input of the next auto-encoder. The whole training process of a SAEs is carried out in a greedy layer-wise way (Hinton and Salakhutdinov, 2006). Hinton et al. (2006) find that this approach can generate better parameters for deep neural networks and produce desirable results.

The details of the training process can be illustrated as Fig. 5
                        . As shown in Fig. 5, after training each auto-encoder using the method described in Section 4.2, the outputs 
                           
                              
                                 w
                              
                              
                                 2
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                         of the auto-encoder are discarded and the latent features 
                           
                              
                                 h
                              
                              
                                 i
                              
                           
                         of the auto-encoder are used to feed the next auto-encoder. When all auto-encoders are trained, we further employ the forward propagation method, based on the parameters 
                           
                              
                                 w
                              
                              
                                 1
                              
                              
                                 i
                              
                           
                        , to get the high-level feature vectors.

For segmenting a single 3D shape, we cluster the patches of the shape by considering their corresponding high-level feature vectors. GMM is employed for clustering in our method, resulting in a probability matrix depicting the probabilities for a patch belonging to a cluster. GMM enables us to utilize the graph-cut algorithm in the final the segmentation step. It's noted that the energy term is computed like that achieved in Meng et al. (2013).

For co-segmenting a set of 3D shapes from the same class, we cluster all the shape patches on a common basis, assuming that these models have the same number of clusters and a group of corresponding clusters are similar in shape. Again, we use GMM to get the probability matrix of a patch within a cluster, and the graph-cut algorithm to get the final co-segmentation results. To further boost the segmentation performance, we employ fuzzy cuts (Katz and Tal, 2003) to refine the jaggy boundaries between adjacent parts. In our implementation, we refine the boundaries in a fuzzy region that is 10 faces wide.

To demonstrate the effectiveness of high-level features and SAEs, we visualize the segmentation results in Fig. 6
                     , where the features for segmentation are extracted from the corresponding layer. It can be seen that (1) the results are better if we use high-level features to guide segmentation, (2) the feature learning process by using SAEs is very useful for improving the segmentation performance, and (3) even if the Buddha model has a complicated shape and topology, our algorithm can still give a desirable segmentation result. A quantitative comparison for using different feature descriptors is also provided in Table 2 (see the last four columns).

@&#EVALUATION@&#

In this section, we use extensive experimental statistics and results to validate our algorithm.


                     Experimental dataset. We test the segmentation algorithm on the Princeton Segmentation Benchmark (PSB) dataset with 19 different object categories, which is an open dataset for 3D shape segmentation and shape retrieval. In order to test co-segmentation, we use a composed database suggested in Hu et al. (2012) that has 20 different object categories, 16 categories from PSB and 4 from COSEG (Wang et al., 2012). We leave out 3 categories (Bust, Mech and Bearing) from 19 categories in PSB since the shapes in the three categories do not have meaningful correspondence. The detailed statistics of the datasets used in our experiments are shown in Table 1
                     .

For the 4 categories of 3D shapes selected from COSEG, we use the ground-truth provided by the authors (Wang et al., 2012) for evaluation. For the other 16 categories of 3D shapes from PSB, we use the human-generated labeling provided by Chen et al. (2009) as the ground truth of segmentation and the manually labeled training data provided by Kalogerakis et al. (2010) as the ground truth of co-segmentation respectively. Note that, there are 15 human-generated segmentations for each 3D shape in Chen et al. (2009), while only one for each shape in Kalogerakis et al. (2010).


                     Evaluation metrics. To evaluate our segmentation method, we adopt four metrics that are defined by Chen et al. (2009), including Rand Index, Cut Discrepancy, Hamming Distance and Consistency Error. Rand Index, named after William M. Rand, measures the similarity between two segmentations of the same shape. Cut Discrepancy is a boundary-based method evaluating the distance between different cuts. Hamming Distance, named after Richard Hamming, is a region-based method and measures the number of substitutions required to change one region into the other. Consistency Errors, whether the global version (GCE) or local version (LCE), are used to compute the hierarchical differences and similarities between segmentations.


                     Parameter settings. In this paper, the coefficient λ of the weight decay, the weight of sparsity penalty term τ and the sparsity parameter ρ in the optimization problem (6) is set to be 0.0001, 0.01 and 0.05 respectively. In our experiments, we tried various choices of L. Fig. 7
                      shows the average Rand Index scores with regard to different values of L on PSB. We can see that the Rand Index score gets worse if L is far less than 50, since in this case our algorithm cannot capture small parts very well. On the contrary, it is problematic if L is far larger than 50. First, it will make the constructed feature vectors have a huge size if there are only a few triangles in each patch. Second, the algorithm will be very inefficient if there are too many patches. Based on the above observation, the number of patches L is set to 50 for each 3D shape in our experiments.

@&#RESULTS@&#


                        Fig. 8
                         shows the segmentation results for some representative categories of 3D shapes in PSB. Although there are large shape variations, the absolute majority of the segmentation results are desirable and consistent with our perception. The Rand Index score statistics of our segmentation on the PSB dataset, as well as those of other methods, are detailed in Table 2
                        , from which we can see that our algorithm obtains an average Rand Index of 0.118 that outperforms the related algorithms.


                        Fig. 9
                         shows the co-segmentation results of some 3D shapes in PSB and COSEG datasets. We can see that our method produces consistent results even if there are large topological differences. For example, the five Chair models are very different from each other, but our algorithm still gives desirable co-segmentation. The co-segmentation of the Candelabra models also exhibits the powerfulness of our algorithm. For quantitatively evaluating our co-segmentation algorithm, we use the Rand Index metric to carry out the comparison between our co-segmentation method and other co-segmentation methods. The detailed statistics of our co-segmentation results on the PSB and COSEG datasets are shown in Table 3
                        . The overall average Rand Index score is 0.089, which is obtained by first computing the average Rand Index score for each category respectively, and then averaging on all categories in the corresponding dataset.


                        Comparisons with previous segmentation algorithms. We compare our method with the other five segmentation algorithms including Randomized cuts (Golovinskiy and Funkhouser, 2008), Normalized cuts (Golovinskiy and Funkhouser, 2008), Random walks (Lai et al., 2008), K-Means (Shlafman et al., 2002) and approximate convexity analysis (WcSeg) (Kaick et al., 2014) on the PSB database. All the related segmentation algorithms are compared against human-generated segmentations (available in the PSB database). We use four metrics for evaluation and the detailed statistics plots are shown in Fig. 10
                        . It can be seen that our segmentation method outperforms the other segmentation approaches no matter what kind of evaluation metrics is used, except the WcSeg method. But basically, as the Rand Index score indicates, our algorithm is better than the WcSeg algorithm and as far as the other three metrics are concerned, it is comparable to the WcSeg method. In Fig. 10, the leftmost bars of each subfigure with a “Human” label illustrate the performance of the human-generated segmentations. No matter what kind of metrics is used, lower values mean closer similarity to human-generated ground truth. Fig. 11
                         provides a qualitative comparison on the Teddy model, the Cup model and the Armadillo model.


                        Comparisons with previous co-segmentation algorithms. We make comparisons with two state-of-the-art algorithms: the unsupervised subspace clustering method (Hu et al., 2012) and the unsupervised affinity aggregation spectral clustering approach (Wu et al., 2013). The test datasets include PSB and COSEG. Fig. 12
                         and Fig. 13
                         show the comparisons with the methods proposed in Hu et al. (2012) and (Wu et al., 2013) respectively.

In Fig. 12, we can see that the average Rand Index score of the method proposed in Hu et al. (2012) is 0.11, worse than our average Rand Index score 0.089. Fig. 14
                         shows some segmentation results to clearly visualize the difference between the method proposed in Hu et al. (2012) and our algorithm. It can be seen that our method is able to precisely identify the desired boundaries of these Chair models, while their algorithm cannot.

Compared with the unsupervised affinity aggregation spectral clustering approach (Wu et al., 2013), our algorithm also exhibits an advantage. Our average Rand Index score is 0.08 while their average score is 0.093, as shown in Fig. 13. In Fig. 15
                        , we can see that their algorithm cannot separate the top and middle parts of the bottom Cup model (see Fig. 15(b)) while our method can (see Fig. 15(a)).

We implemented the proposed algorithm in Matlab and C++. The computation time costs of the segmentation and co-segmentation algorithms are shown in Table 4
                        . In average, our algorithm takes more than 3 minutes to process a shape, where the time cost for computing low-level features is about 11 seconds and that for computing high-level features and SAEs is about 3 minutes and 10 seconds. However, our current implementation, un-optimized yet, can be further speeded up if the parallel implementation technique is considered into the overall framework.

First, we can't automatically distinguish wanted features and unwanted features in the learning stage. Therefore, even if there are sufficiently many low-level geometric features, we can't guarantee that the segmentation results are definitely better. We need to propose a better strategy in the future.

Second, designing a desirable structure of the SAEs is non-trivial. We will conduct more experiments to obtain a general configuration such that the algorithm framework can truly work well across various model libraries and various basic shape signature combinations.

Finally, the required computation time, especially that spent training the deep neural network and generating the high-level feature space, is still too long. In the future, we will seek a parallelized or distributed implementation, which will greatly reduce the time costs and facilitate its use in practice.

@&#CONCLUSION@&#

In this paper, we propose a novel unsupervised method for segmenting and co-segmenting 3D shapes. After over-segmenting the shapes into primitive patches, we generate the high-level features from the low-level features of each patch by using deep learning. We finally use high-level features for segmenting a single shape or co-segmenting a group of shapes from the same family. We validate our method on two open datasets (PSB and COSEG), and make extensive comparisons with the state-of-the-art approaches on this problem. The experimental results demonstrate that our method can achieve desirable results.

@&#ACKNOWLEDGEMENTS@&#

Many thanks to the anonymous reviewers for their valuable comments and suggestions. We would also like to thank Oliver van Kaick, Zizhao Wu, Yunhai Wang and Ruizhen Hu for their kind help. This work is supported by National Natural Science Foundation of China (11226328, 61222206, 11526212, 61300168, 61273332), Natural Science Foundation of Zhejiang Province (LY13F020018), Opening Foundation of Zhejiang Provincial Top Key Discipline (XKXL1406), Natural Science Foundation of Ningbo City Grant (2015A610123), One Hundred Talent Project of the Chinese Academy of Sciences (Ligang-Liu), and Ningbo Sc. & Tech. (Innovation Team) Plan Project (2014B82015).

@&#REFERENCES@&#

