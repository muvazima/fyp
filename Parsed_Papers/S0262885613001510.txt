@&#MAIN-TITLE@&#Learning structured visual dictionary for object tracking

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A tracking algorithm which exploits both appearance and geometric information


                        
                        
                           
                           Two complementary features are adopted in building the appearance dictionary.


                        
                        
                           
                           A shape context approach to capture the stable geometric patterns of keypoints


                        
                        
                           
                           The proposed tracking algorithm performs well in challenging conditions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Object tracking

Bag of features

Appearance model

Geometric relationship

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Numerous effective tracking approaches have been proposed in the literature [1]. One of the most important factors for effective tracking algorithms is how to account for appearance variation with effective representations. Among the existing algorithms, some focus on holistic appearance models [2–6], whereas others resort to local parts or patches [7–12].

In visual categorization, the bag-of-features (BoF) algorithm has been successfully applied to object and natural scene classification with different codebooks or decision criterion. While most existing works [13–17] use interest point detectors to select patches and SIFT descriptors [18] for object representation, Nowak et al. [19] show that random sampling with bag of features has more discriminative power than that with interest point detectors when a large number of patches are used.

Aiming to develop an effective model with features extracted from image patches, we explore the BoF paradigm to collect representative features that best characterize an object and the background. We cluster a large set of patches into a compact number of groups as a visual dictionary which consists of the most distinctive features of both the foreground object and the background. Although Yang et al. [20] have applied BoF representation to tracking, their generative approach does not use negative samples or geometrical structures. The conventional BoF algorithm used in [20] treats an object image as a collection of independent patches without considering their spatial relationships. Numerous geometric descriptors have been developed to encode spatial information as features for object recognition. Cootes et al. [21] propose the active shape models (ASMs) to describe an object in terms of its contour. The ASMs consist of a set of landmark points representing the shape, which are analogous to the active contour model [22]. The active appearance models (AAMs) are developed to model both shape and texture information of an object [23]. Shape matching is carried out via an optimization process to minimize the difference between the current estimate of appearance and the target image. However, it is not applicable to real-time tracking since the optimization process is time-consuming. Recently, several algorithms have been proposed to incorporate spatial information with the patch collections [24–26] for object categorization and recognition. Nevertheless, these methods require significant numbers of training samples since they need to find statistically the most representative keywords to handle intra-class variance. Moreover, the time-consuming training procedures to describe the spatial relationships among the large training samples and build classification models make the previous works impractical for real-time object tracking. For object tracking, it is desirable to develop effective methods that exploit spatial relationships and constraints among features.

Within the context of object tracking, numerous methods that utilize spatial information have been proposed due to its importance. Some approaches are formulated based on Markov networks [27,28] to encode the relationships of interest points or regions in the image space. However, [27] uses a fixed number of local patches and in [28] both appearance and spatial features entirely rely on the locations of detected keypoints. Therefore, they are not flexible enough to deal with complex scenarios. Similarly, [29,30] also use keypoints to extract local appearance patches, but no local spatial information among keypoints is utilized although [29] takes the overall geometry of the entire object into consideration. In addition, many approaches rely on the motion estimation of interest points (based on features or regions) between successive frames [31,28,32,33]. These methods first match features between two frames and then estimate their corresponding motion parameters. The global object movement is then computed from the motion parameters of local features. However, features in these methods are processed independently and thus rich geometric structures among them within the same image are ignored. Here we propose a method that focuses on statistical relationships of features and does not involve motion estimation.

In this paper, we propose a tracking algorithm based on structured visual dictionary that exploits both appearance and geometric information. The contributions of this paper are summarized as follows. First, we extend the BoF representation scheme in the context of object tracking. We propose a discriminative tracking algorithm for learning an effective and compact visual dictionary of both foreground target and background. Second, we encode and exploit geometric constraints of local invariant features. Instead of computing movement of individual features to infer the global object motion, we model the statistical distribution of features in the 2D image plane. Third, our formulation facilities the integration of appearance and geometric information. Both appearance and geometric information are characterized as histograms so they can be combined in a unified and reasonable way. In addition, the proposed formulation accommodates an update scheme easily, thereby facilitating effective object tracking.

In the first frame, the target object is initialized manually by a bounding box. We use the similarity transform to describe object motion with state variable xt
                        . A small number of training images are collected to construct the structured visual dictionary for the target object as follows. For each training frame, a rectangular bounding box enclosing the target object is bootstrapped either by another tracker (with visual inspection) or by hand. Similar to the sampling scheme in [34], we crop out a set of patches P
                        +
                        ={P| l(P)−
                        l(xt
                        )║<
                        α} of random size within the object as positive samples, where l(P) and l(xt
                        ) denote the locations of small patch P and the object xt
                         at time t. The distance α is a threshold to ensure all patches fall into the object region. For negative samples, we sample another set of patches P
                        −
                        ={P|β
                        <║l(P)−
                        l(xt
                        )║<
                        γ} from the area outside the target. Here the distance β and γ are thresholds, and β is larger than α. These parameters are selected by imposing a range constraint on the central locations of patches such that these patches do not overlap with the object region and are not far away from the object. With several training frames, sufficient positive and negative samples are collected to build the structured visual dictionary at the outset of the tracking process.

In this section, we describe the statistical properties learned from the sampled patch sets. In contrast to conventional BoF approaches that use only appearance or texture to represent objects, we incorporate geometric information into the visual vocabulary, and refer to it as the structured visual dictionary (SVD).

Given two sets of patches P
                           + and P
                           −, descriptors are extracted f(P
                           +) and f(P
                           −) where f is a feature function. Next, these features are grouped into clusters K
                           
                              f
                           
                           
                              
                                 (1)
                                 
                                    
                                       
                                          K
                                          f
                                       
                                       =
                                       
                                          
                                             C
                                             
                                                
                                                   f
                                                   
                                                      
                                                         P
                                                         +
                                                      
                                                   
                                                
                                             
                                             ,
                                             
                                             C
                                             
                                                
                                                   f
                                                   
                                                      
                                                         P
                                                         ‐
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       ,
                                    
                                 
                              
                           where 
                              C
                            denotes a clustering function that returns the center and K
                           
                              f
                           
                           ={K
                              f,m
                           }
                              m
                              =1
                           
                              M
                            is a set of cluster centers that are used as appearance keywords of the dictionary, where M is the number of cluster centers. Note that positive and negative samples are clustered separately in order to better contrast the learned appearance keywords between positive and negative sets. We note that in [20] only positive samples are used to learn visual dictionary, which explains why the tracker is sensitive to visual drift.

To better represent objects, the proposed SVD consists of two sets of appearance keywords obtained from the RGB descriptor and the local binary pattern (LBP) descriptor [35] which are extracted from randomly sampled patches instead of using interest point operators. Therefore, we describe an object from two different aspects which uncover as many representative features as possible. These collaborative sets of keywords derived from two descriptors often complement each other for more effective object representations, thereby facilitating the tracking process.

Aside from appearance keywords, we incorporate geometric constraints of features in the proposed SVD. While some existing methods have utilized geometric information of features [24–26] in the context of object categorization and recognition, these methods are not efficient or effective for object tracking. Within the context of visual tracking, several methods [31,36,28,32,33] that use interest points for describing target objects have been proposed.

Tracking is then carried out by matching individual interest points in consecutive frames. In other words, such approaches treat all the features independently and do not take the spatial relationships of neighboring features into account. However, the geometric information among neighboring features preserves important cues for feature matching and object tracking. Therefore, we utilize the relative positions of detected interest points in a 2D plane to incorporate geometric information for constructing a structured visual dictionary. Our motivation is that the spatial distribution of interest points within an object conforms with a stable pattern which can be statistically described by modeling the spatial relationships among these points. Note that the way of encoding spatial information does not depend on the correspondence of interest points in successive frames.

We detect interest points from an image and extract their SIFT features [18]. As shown in Fig. 2, the relative positions of stable interest points extracted from an object in successive frames almost remain constant. The histograms of the five images from the target object in Fig. 2 are relatively similar. That is, the geometric structure of stable interest points are not affected much by smooth motion, scale change and rotation in successive frames. In contrast, interest points of the regions from the background cannot be modeled well by a specific distribution (See Fig. 2).

Motivated by these observations, we propose an efficient method to exploit geometric structure as discriminative features for object tracking. Inspired by the shape context approach [37], we consider local statistics originating from the center c of the extracted object region to all the other interest points.

Assume that we detect R interest points from a target object, we use Vr
                            to represent a vector from c to the r-th point. The set of local statistics V
                           ={V
                           
                              r
                           }
                              r
                              =1
                           
                              R
                            describes the positions and distances of interest points with respect to c. To obtain a compact and robust representation, we quantize local statistics in V in the polar coordinate system as shown in Fig. 1
                           . Suppose that we divide the plane of polar coordinates into Um
                           
                           ×
                           Ua
                            regions where Um
                            and Ua
                            represent different magnitudes and angles. For a specific region (um
                           , ua
                           ), where um
                            and ua
                            are the indices of magnitude bin and angle bin respectively, we count the number of interest points within this grid, thereby constructing a matrix M(um
                           , ua
                           ) which records the distribution of V (See Fig. 1). We then convert each matrix M(um
                           , ua
                           ) to a histogram Hs
                           (u), which preserves compact spatial information of interest points. Each bin u of the histogram represents a specific magnitude and angle region. In practice, we use 4 bins and 5 bins for quantization of magnitude and angle, respectively. The motivation to use histogram representation for spatial information is to facilitate update and fusion with appearance information in a simple and consistent way.

We also use the same method to compute the spatial histogram of interest points from the background, as shown in Fig. 2
                           . However, since interest points of sampled regions from the background may appear in any configurations, their spatial histograms do not contain representative information. The appearance keywords and the geometric correlations of interest points together are the integral components of the proposed SVD for object tracking.

Once we have the SVD, an image region can be represented by two appearance histograms and a spatial histogram. Within each image region, a set of patches of random sizes P are randomly sampled. The appearance histogram is then created by assigning each patch in P to its nearest keyword in K and computing the occurrence frequency of corresponding keyword within this image region. The spatial histogram characterizes the spatial distribution of a set of interest points. The formulation of spatial histogram is mathematically consistent with appearance histograms, because both are histograms describing the statistical information (appearance or location) of a group of representative features.

For a set of keywords K
                        
                           f
                        , we arrange positive and negative keywords separately, and then concatenate bins of indices from two kinds of keywords. Subsequently, the object is represented by an ordered appearance histogram Hf
                        (u), where u indicates a bin of Hf
                        (u). Assume that Hf
                        (u) has U bins, so the first U/2 bins record the occurrence of positive keywords and the remaining U/2 bins represent that of negative ones. Moreover, we use the following equation rather than raw counts to compute Hf
                        (u):
                           
                              (2)
                              
                                 
                                    
                                       H
                                       f
                                    
                                    
                                       u
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             n
                                             =
                                             1
                                          
                                          N
                                       
                                       
                                    
                                    
                                    exp
                                    
                                    
                                       
                                          −
                                          
                                             d
                                             n
                                          
                                          /
                                          σ
                                       
                                    
                                    ⋅
                                    δ
                                    
                                       
                                          b
                                          
                                             
                                                P
                                                n
                                             
                                          
                                          −
                                          u
                                       
                                    
                                 
                              
                           
                        where Hf
                         denotes the appearance histogram regarding feature f and Pn
                         denotes a patch. In the equation above, N is the number of patches within a candidate, b(∙) indicates the index of bin, and δ(∙) is a delta response function that equals 1 when b(Pn
                        ) equals u. By multiplying a weighting term exp(−
                        dn
                        /σ), the patch more similar to the keyword contributes more to the corresponding bin. In the weighting term, dn
                         is the minimal Euclidean distance between the feature of a patch f(Pn
                        ) and the closest keyword in K
                        
                           f
                         (σ is set to 0.2 in our experiments).

At time t, we draw L candidate states X
                        
                           t
                        
                        ={x
                        
                           t
                        
                        
                           i
                        }
                           i
                           =1
                        
                           L
                         where x
                        
                           t
                        
                        
                           i
                        
                        =(cx
                        
                           t
                        
                        
                           i
                        , cy
                        
                           t
                        
                        
                           i
                        , s
                        
                           t
                        
                        
                           i
                        , θ
                        
                           t
                        
                        
                           i
                        ) around the tracked object at time t−1 using random walks with factorized Gaussians in a way similar to [38]. cx
                        
                           t
                        
                        
                           i
                        , cy
                        
                           t
                        
                        
                           i
                        , s
                        
                           t
                        
                        
                           i
                         and θ
                        
                           t
                        
                        
                           i
                         represent the center, scale and rotation angle of the state x
                        
                           t
                        
                        
                           i
                        . Each state x
                        
                           t
                        
                        
                           i
                         corresponds to a rectangular image region enclosed by a bounding box. Within each bounding box, we randomly sample small patches P
                        
                           t
                        
                        
                           i
                        
                        ={P
                        
                           t,n
                        
                        
                           i
                        }
                           n
                           =1
                        
                           N
                         of random size and extract their RGB features f
                        1(P
                        
                           t
                        
                        
                           i
                        ) and LBP features f
                        2(P
                        
                           t
                        
                        
                           i
                        ). After computing the distances between features of patches f(P
                        
                           t
                        
                        
                           i
                        ) and keywords K
                        
                           f
                        , the image patch region at state x
                        
                           t
                        
                        
                           i
                         is represented by two appearance histograms H
                        
                           f1 and H
                        
                           f2. We also detect interest points in the image patch at state x
                        
                           t
                        
                        
                           i
                         and compute the spatial histogram Hs
                         of these points. Next, we compute the histogram similarity using the appearance and spatial histograms by:
                           
                              (3)
                              
                                 
                                    
                                       C
                                       w
                                    
                                    =
                                    max
                                    
                                    exp
                                    
                                    
                                       
                                          −
                                          
                                             d
                                             
                                                χ
                                                2
                                             
                                          
                                          
                                             
                                                H
                                                w
                                             
                                             
                                                H
                                                w
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    ∀
                                    w
                                    =
                                    
                                       f
                                       1
                                    
                                    ,
                                    
                                       f
                                       2
                                    
                                    ,
                                    s
                                    ,
                                 
                              
                           
                        where Hw
                         is an appearance histogram or spatial histogram of the candidate sample, H
                        
                           w
                         is a set of corresponding histograms computed from the training images, and 
                           
                              d
                              
                                 χ
                                 2
                              
                           
                         is a function that computes χ
                        2-distance. Note that no similarity regarding individual patch is computed, and we focus more on the statistical properties of the group of randomly sampled patches. As summarized in Algorithm 1, we obtain similarities of each sampled candidate region with respect to the target object using appearance and spatial histograms.
                           Algorithm 1
                           SVD matching (w.r.t. one candidate region at x
                              
                                 t
                              
                              
                                 i
                              )

Given: K
                              
                                 f1 and H
                              
                                 f1 for RGB feature, K
                              
                                 f2 and H
                              
                                 f2 for LBP feature, spatial histograms H
                              
                                 s
                              , SIFT features fs
                               of detected interest points from a positive training sample (randomly selected) or a previous tracked object.

Input: Set of patches P
                              ={P
                              
                                 n
                              }
                                 n
                                 =1
                              
                                 N
                              
                              
                                 
                                    1.
                                    for n
                                       =1 to N
                                       
                                          
                                             (a)
                                             Find 
                                                   
                                                      
                                                         d
                                                         
                                                            
                                                               f
                                                               1
                                                            
                                                            ,
                                                            n
                                                         
                                                      
                                                      =
                                                      min
                                                      
                                                         
                                                            
                                                               f
                                                               1
                                                            
                                                            
                                                               
                                                                  P
                                                                  n
                                                               
                                                            
                                                            ,
                                                            
                                                               K
                                                               
                                                                  f
                                                                  1
                                                               
                                                            
                                                         
                                                      
                                                   
                                                 for RGB and index of the keyword 
                                                   
                                                      u
                                                      
                                                         
                                                            f
                                                            1
                                                         
                                                         ,
                                                         n
                                                      
                                                   
                                                
                                             

Find 
                                                   
                                                      
                                                         d
                                                         
                                                            
                                                               f
                                                               2
                                                            
                                                            ,
                                                            n
                                                         
                                                      
                                                      =
                                                      min
                                                      
                                                         
                                                            
                                                               f
                                                               2
                                                            
                                                            
                                                               
                                                                  P
                                                                  n
                                                               
                                                            
                                                            ,
                                                            
                                                               K
                                                               
                                                                  f
                                                                  2
                                                               
                                                            
                                                         
                                                      
                                                   
                                                 for LBP and index of the keyword 
                                                   
                                                      u
                                                      
                                                         
                                                            f
                                                            2
                                                         
                                                         ,
                                                         n
                                                      
                                                   
                                                
                                             

end for

Construct appearance histograms H
                                       
                                          f1 and H
                                       
                                          f2 using (2)

Detect interest points in candidate region at state x
                                       
                                          t
                                       
                                       
                                          i
                                       
                                    

Extract SIFT features to perform interest point matching with fs
                                       
                                    

Construct spatial histogram of matched points Hs
                                       
                                    

Determine appearance similarities C
                                       
                                          f1 and C
                                       
                                          f2 of two features and spatial similarity Cs
                                        using (3)

Output: Similarities C
                              
                                 f1, C
                              
                                 f2 and Cs
                               between a candidate region and trained samples.

We note that it is less effective to simply compute the spatial histogram from all detected points. If detected points are from the background but happen to have similar geometric relationships to those of the foreground, the computed similarity value Cs
                         of the candidate region may be high and leads to inaccurate results. Therefore, we add a constraint for feature matching. In a candidate region, only points that match those from the previous tracked object are used for constructing Hs
                        , whereas other points lying in the background region are discarded. Despite its effectiveness, this strategy may discard some new interest points due to the appearance change of the target object. This issue is addressed by an update scheme discussed in Section 2.5.

The appearance similarities C
                        
                           f1 and C
                        
                           f2, and spatial similarity Cs
                         for a candidate region are integrated by:
                           
                              (4)
                              
                                 
                                    
                                       C
                                       j
                                    
                                    =
                                    exp
                                    
                                    
                                       
                                          
                                             C
                                             
                                                f
                                                1
                                             
                                          
                                          /
                                          
                                             σ
                                             
                                                f
                                                1
                                             
                                             2
                                          
                                          +
                                          
                                             C
                                             
                                                f
                                                2
                                             
                                          
                                          /
                                          
                                             σ
                                             
                                                f
                                                2
                                             
                                             2
                                          
                                          +
                                          
                                             C
                                             s
                                          
                                          /
                                          
                                             σ
                                             s
                                             2
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              σ
                              
                                 f
                                 1
                              
                           
                        , 
                           
                              σ
                              
                                 f
                                 2
                              
                           
                         and σ
                        
                           s
                         are scaling terms that are set empirically to 0.02 and fixed in our experiments. The reliability of different features is computed by histogram similarities with the combination of appearance and spatial information. Thus, the proposed tracker is less likely to drift from the target as long as at least one feature is extracted to provide sufficient information of the target object, as shown in our experiments. A number of candidate regions with highest joint similarities are weighted and considered as the state of the target object at time t by
                           
                              (5)
                              
                                 
                                    
                                       S
                                       t
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             L
                                             ′
                                          
                                       
                                       
                                    
                                    
                                    
                                       C
                                       
                                          t
                                          ,
                                          j
                                       
                                       i
                                    
                                    
                                       x
                                       t
                                       i
                                    
                                    /
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             L
                                             ′
                                          
                                       
                                       
                                    
                                    
                                    
                                       C
                                       
                                          t
                                          ,
                                          j
                                       
                                       i
                                    
                                 
                              
                           
                        where L′ is used to determine how many candidates are used (e.g., L′ equals 5% of L candidate states in our experiments), and C
                        
                           t,j
                        
                        
                           i
                         is the joint similarity for candidate x
                        
                           t
                        
                        
                           i
                         using (4).

Rather than a constant and off-line learned appearance model, all the histograms in the SVD are updated to account for appearance change of the target object.

At each time t, we store the state St
                            of the tracked object and the corresponding accumulative similarity value 
                              
                                 
                                    Z
                                    t
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          L
                                          ′
                                       
                                    
                                    
                                 
                                 
                                 
                                    C
                                    
                                       t
                                       ,
                                       j
                                    
                                    i
                                 
                              
                           . Large Zt
                            means that all selected candidates have high probabilities of being the target. Thus, during the period up to the next update at time t
                           +
                           T, we have a set of states for the tracked objects {S
                           
                              t
                           ,Z
                           
                              t
                           }
                              t
                           
                           
                              t
                              +
                              T
                           , from which we choose the S
                           
                              t
                           
                           ∗ with highest Zt
                           . From the image patch at S
                           
                              t
                           
                           ∗, we construct two appearance histograms based on the RBG and LBP features, and update the corresponding set of trained appearance histograms H
                           
                              f1 and H
                           
                              f2 by discarding the oldest histograms respectively. We choose the histogram that best characterizes the target object to account for the appearance change. The update period T is pre-determined to accommodate typical object motion so that this scheme is not quick to adapt to noise or too slow to respond to the change of the target objects. We show the change of appearance histograms that account for the foreground and background change of a test sequence in Fig. 3
                           .

The update process of a spatial histogram is slightly different from that of an appearance histogram. At each update, the matching process determines a number of interest points from a candidate which are used for computing a spatial histogram. Assume that p detected interest points are stored from the previously tracked object and q points from a candidate St
                            at time t, then r matched points (r
                           ≤min(p,q)) are determined. If r
                           >
                           ωp, where ω is a percentage (e.g., it is set to 80% in our experiments), it means that the tracking result does not deviate much from the target location. Thus, it is reliable to utilize the current tracking result to update the spatial histogram. Similar to the scheme used for the appearance histogram update, new histograms are added to the set and old ones are discarded. When spatial histograms are updated, the corresponding features of old interest points are replaced with those of the new ones for subsequent tracking process. The update process is illustrated in Fig. 4
                           .

In Fig. 5
                           , we show some results of matched interest points for the spatial histogram update and tracked objects of the tom2 image sequence. The change of three similarity values is also presented in Fig. 7. This figure clearly demonstrates that tracking drifts result in few matched points, i.e., low spatial similarity values (see frame #289). Even when there are no matched interest points in some frames, our method is still able to compute the spatial histogram (all bins are 0) and similarity values used for the overall similarity in (4).

Note that the appearance and spatial histograms are updated separately because they do not change simultaneously in most cases. If the appearance of the object changes but there are only few interest points, the spatial histograms do not contribute to the combined similarity much. Therefore, we update only the appearance histograms, but there is no need to update the spatial histograms. Experimental results validate the effectiveness of the update mechanism.

@&#EXPERIMENTS AND RESULTS@&#

We present experimental results with comparisons to four state-of-the-art tracking algorithms and discussions in this section.

We evaluate the proposed tracking algorithm with SVD (referred to as SVDTrack) using several challenging image sequences. The SVDTrack system is implemented by Matlab without code optimization and takes 10s to process a frame on a PC with 2.1GHz CPU and 4G memory. In fact, the keypoint detection takes the most time while the histogram construction and comparison are very efficient. The values of parameters are set empirically and listed as follows. The number of sampled candidates N is set to 200 in all the experiments. Each candidate region is normalized block of 64×64pixels, from which 50 patches ranging from 8×8 to 16×16pixels are randomly selected. For each patch, we extract a 64-dimensional RGB feature vector by quantizing each channel into 4 bins, and a 59-dimensional LBP feature vector using uniform LBP descriptor with 8 neighbors [35]. Both the numbers of positive and negative keywords are set to 10, and the appearance histogram for each feature has 20 bins. As appearance variance of the target object is much less than that of the background in consecutive frames, a small number of keywords is sufficient for object tracking. We detect interest points in each 64×64 candidate region, compute their relative positions with its center, and construct the spatial histogram that is normalized using 
                           
                              l
                              1
                           
                        -norm. At the outset of the tracking process, the first 5 frames of an image sequence are used for training appearance keywords and spatial histograms. The update period T for appearance histogram update is set to 5, 10 or 20 empirically to achieve a trade-off between representative ability and adaptive ability of the tracker. On the other hand, the update period for spatial histograms is entirely determined by the number of interest points during tracking.

We evaluate the proposed SVDTrack method in terms of its capability in handling occlusion, pose and appearance change for object tracking. We compare it with four state-of-the-art tracking methods, the FragTrack [8], TLD [39], MILTrack [34] and VTD tracking [6] algorithms, with the provided source codes for fair performance evaluation. The FragTrack method is designed to handle occlusion by using a template of multiple image fragments for object representation. Our approach is similar in spirit to this method as both utilize local information from a group of small patches. The MILTrack has been shown to be able to handle visual drifts. The TLD tracking method, which simultaneously tracks the object, learns its appearance and detects it whenever it appears in the video, is able to handle large illumination change. Finally, the VTD method, using sparse PCA algorithm to select the best template in each frame, is able to deal with large motion and lighting changes. In our experiments, the search region of FragTrack ranges from 10×10 to 30×30pixels with 16-bin grayscale histogram. The patch size in the TLD method remains 20×20pixels, and the VTD method uses 600 candidate templates. Note that except some parameters specific to individual trackers, all the common parameters are fixed for all tracking methods on each test sequence. All the source codes and data sets will be made available to the public.

We present and discuss tracking results of our approach and other state-of-the-art methods. The test image sequences include all major challenging issues in object tracking: heavy occlusion, illumination change, fast motion, low contrast, camera movement, and pose variation. More tracking results are available at http://www.youtube.com/svdtrack.


                           Fig. 6(a), (b), and (c) show the tracking results on the woman sequence from [8], and two videos, face1 and dance3, from our own dataset. Since the clothes of the person have very similar appearance to the top of the white sedan, this sequence poses significant challenges for object tracking when the subject is partially occluded. Thus, methods relying on global templates (MILTrack and VTD) are not able to handle such scenarios. It is worth noticing that in [8] a smaller tracking window is used (that encloses about two-thirds of the full body). However, if we enlarge the window to enclose the entire human body, the FragTrack method easily loses track of the subject because many background pixels are inevitably considered as parts of the foreground object. In contrast, our SVDTrack method keeps track of the woman well and outperforms all other methods. Although some parts of the target object are invisible in some frames, there are still sufficient representative features that can be used for patch-based tracking methods. In addition, since some interest points can still be detected, the relative positions to the central point of the target object can be exploited. That is, geometric information encoded in our spatial histograms facilitates our tracker to capture the stable geometric distribution of interest points in such scenarios, and thus renders robust results.

The tracking results using the dance3 sequence from our own dataset are presented in Fig. 6(c) where two dancers are occluded by each other. The dancers also change their poses frequently. All other methods fail when the female dancer is fully occluded by the male dancer for a short time, but only our tracker continues to track her when she reappears. Likewise, the results on the face1 sequence (Fig. 6(b)) show that our method is able to track the girl as she swings her head while being heavily occluded.


                           Fig. 6(d) shows the tracking results of the bird2 sequence in which a bird walks back and forth while undergoing large pose variation. Note that the target is also partially occluded at times with shape deformation. The background pixels are likely being considered as parts of foreground objects for template-based methods that rely on rectangles to enclose target objects (unless some mechanisms are introduced to separate foreground and background pixels). Our SVDTrack algorithm successfully keeps track of the bird in this sequence, while other methods render inaccurate results, e.g., the VTD method drifts away and the TLD tracker fails when the bird is partially occluded. The MILTrack method is able to locate the bird more accurately compared with others, but the tracking window size does not adapt to shape deformation. In the sylvester sequence from [34], the target undergoes a large pose variation (see Fig. 6(e)). The FragTrack method loses the target right after a holistic motion blur occurs. Relying on global templates, the MILTrack and VTD methods fail to track the target accurately, which are not robust to pose variation. The proposed SVDTrack successfully keeps track of the target due to the use of interest points which capture the shape variance of the object well. We show another comparison on the david sequence from [3] in Fig. 6(f). Although there is pose variation with illumination change, our SVDTrack method locates the target precisely throughout the sequence. Other methods are not robust enough in dealing with some challenging situations as shown in the figure.

In Fig. 6(g), we present the tracking results of the bolt sequence. Due to fast acceleration and motion of the target athlete, it is difficult for trackers to precisely locate his position. In addition, the camera view change makes this sequence rather challenging. Our SVDTrack algorithm is able to track the athlete throughout the entire sequence, while other methods all drift at the beginning or when the camera angle changes. For instance, the VTD, TLD and MILTrack methods drift away when the athlete just begins to accelerate. As a result of using the proposed SVD, our tracker is able to track fast moving objects in a complex scene with a camera view change.

The tracking results with the tom2 and tom3 sequences are shown in Fig. 6(h) and (i), in which the target objects undergo fast and drastic movements with partial occlusion. The proposed SVDTrack algorithm performs well against other methods, as they either drift (MILTrack, FragTrack and VTD) or fail (TLD) at times.

In the cup sequence, the target object undergoes illumination and scale change, as shown in Fig. 6(j). The FragTrack and MILTrack methods do not handle illumination change or larger scale variation well, whereas our SVDTrack method is able to deal with such situations due to the use of LBP features. The MILTrack method gradually drifts away from the target when there is a large scale change. Although the TLD method is equipped with a detection mechanism, it fails to adapt the model for tracking the target object in this sequence.

The average tracking errors in terms of the central object position is presented in Table 1
                            with manually labeled ground truth data.

@&#DISCUSSIONS@&#

To evaluate the importance of individual terms in (4), we remove one component and run the SVD tracker on all test sequences, resulting in three trackers which are SVDTrack without LBP feature, SVDTrack without RGB feature and SVDTrack without geometric structure (referred as SVD1, SVD2 and SVD3 respectively). The qualitative and quantitative comparisons are shown in Fig. 6 and Table 1. On most sequences, the geometric information helps to improve the tracking performance, especially when the target object undergoes occlusion, large pose change and shape deformation. In some cases, the appearance information is enough for good tracking results, so the importance of geometric information decreases especially when the geometric information is not distinct (the cup sequence). The importance of the individual terms is also reflected in Fig. 7
                        .

Overall, our approach performs favorably in all sequences with lower errors and deviations. In our method, appearance keywords preserve the most representative features of the foreground and background, and spatial histograms are stable to facilitate object tracking. Thus, our tracker does not easily drift away from the target object.

Additionally, we evaluate the effectiveness of online update by showing the change of similarity values of RGB feature, LBP feature and spatial information for sequences tom2 and cup in Fig. 7. It is obvious that the similarity values change throughout the sequence during tracking. Specifically, our tracker successfully captures the change of the object and adjusts the importance of individual features accordingly. For example, at the beginning of the cup sequence, the color feature and LBP feature are equally important. But when the lighting condition changes, the LBP feature becomes more important (as the colors of the cup and background fall into the same bin of the color histograms).

@&#CONCLUSION@&#

In this paper, we propose a tracking algorithm with structured visual dictionary (SVD) named SVDTrack. The proposed SVD exploits both appearance and geometric information. We exploit statistics of relative geometric information of local invariant features. Both represented as histograms, appearance and geometric features are integrated for object tracking, and they contribute to locate objects according to scene change. Experimental results show that our SVDTrack algorithm performs favorably in comparison to several state-of-the-art tracking methods. Our future work will focus on a discriminative classifier to classify positive and negative histograms rather than computing their χ
                     2 distances. In addition, we will explore better histogram update strategies.

@&#REFERENCES@&#

