@&#MAIN-TITLE@&#Assessment of artificial neural network and genetic programming as predictive tools

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Two major soft computing techniques, ANN and GP, are evaluated in detail.


                        
                        
                           
                           A case study in punching shear modeling of RC slabs is modeled.


                        
                        
                           
                           The models are compared based on model complexity, statistical validation and parametric study.


                        
                        
                           
                           Overfitting potential of the models is evaluated and suggestions are provided.


                        
                        
                           
                           The results indicate model acceptance criteria should include engineering analysis.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Artificial neural networks

Genetic programming

Over-fitting

Explicit formulation

Punching shear

RC slabs

Parametric study

@&#ABSTRACT@&#


               
               
                  Soft computing techniques have been widely used during the last two decades for nonlinear system modeling, specifically as predictive tools. In this study, the performances of two well-known soft computing predictive techniques, artificial neural network (ANN) and genetic programming (GP), are evaluated based on several criteria, including over-fitting potential. A case study in punching shear prediction of RC slabs is modeled here using a hybrid ANN (which includes simulated annealing and multi-layer perception) and an established GP variant called gene expression programming. The ANN and GP results are compared to values determined from several design codes. For more verification, external validation and parametric studies were also conducted. The results of this study indicate that model acceptance criteria should include engineering analysis from parametric studies.
               
            

@&#INTRODUCTION@&#

Empirical modeling and formulation by soft computing techniques remain highly-researched topics, especially for engineering modeling [25]. Soft computing-based models differ from conventional models that are based on engineering principles (e.g., elasticity and plasticity theories), as they are based on experimental data rather than theoretical derivations. Soft computing-based models are usually complex and often cannot build an explicit formula. Therefore, they are most appropriate for use as a part of a computer program, limiting their applicability.

The most well-known soft computing predictive tool is the artificial neural network (ANN), which has been used successfully in structural engineering modeling (e.g. [36]. ANNs are inspired by biological neural networks [30]. Although ANNs typically build “black box” models, explicit formulas can be derived for a trained ANN model. A derivative-free optimization algorithm should be added to the training process of the ANN algorithm to avoid local minima, which lead to false convergence of the ANN model [38]. Some researchers have already combined ANN and global optimization algorithms to improve ANN efficiency (e.g., [41,46].

Another robust soft computing technique for modeling is genetic programming (GP), which is inspired by the principle of Darwinian natural selection. The machine code generated by GP can be translated as a mathematical formula, which makes it very suitable for mathematical modeling. GPs, especially new variants such as gene expression programming (GEP), have been successfully applied to several engineering problems, particularly in structural engineering (e.g., [21].

In this study, the performance of ANN and GP techniques are evaluated based on several criteria, including over-fitting potential, parametric study results, and simplicity of the generated formulas. To demonstrate this performance comparison, the punching shear strength of reinforced concrete slabs is modeled using a comprehensive database containing 241 experimental test results. We present the explicit slab strength prediction formulas from a well-trained ANN and from a proposed GP model. A subsequent parametric study was carried out to evaluate the trends of the ANN and GP models with respect to each parameter. The results show that although the ANN model outperforms the GP model in terms of error and correlation, it tends to be overfitted (with respect to design code values) due to its complexity. The GP models tend to have acceptable error and correlation characteristics while performing well in the parametric studies (with respect to the physics of the problem as verified by the design codes).

Soft computing includes, but is not limited to, evolutionary algorithms, ANNs, support vector machine and fuzzy logic. Soft computing predictive tools have wide-ranging applications and are often used to model the nonlinear relationship between input parameters and output value(s). Advances in computer hardware have made soft computing techniques more efficient. In addition, soft computing techniques may be used to model problems where conventional approaches, such as regression analysis, fail or perform poorly (e.g., [34]. In this study, two of the most well-known soft computing techniques, ANN and GP, are applied to an engineering case study and their results are analyzed and compared.

Artificial neural networks (ANNs) were first developed in the early 1940s [42]. ANNs are predictive tools used to build a mathematical model for a complex system. Multi-layer perception (MLP) ANN [14] is the most well-known class of ANNs. MLP ANNs usually have feed-forward architectures and are typically trained with back-propagation algorithms. MLP networks consist of one input layer and one output layer, with at least one additional hidden layer. Each layer has a number of nodes and contains one or more processing units. Each unit in the MLP is fully interconnected with weighted connections (wij
                        ) to the units in the subsequent layer [6]. The output (Y) is obtained by passing the sum of the products of the inputs and weights through an activation function. Fig. 1
                         shows a schematic of a simple MLP ANN.

ANNs are “trained” from a set of data known as the training set. During the training process, the network’s weights are optimized. The training procedure consists of two main steps: initialization and optimization [4]. In the initialization process, initial values are assigned to the weights of the network, either randomly or via a global optimization method such as simulated annealing (SA). The optimization process typically utilizes a gradient-based algorithm that is suitable for local search. Therefore, to be successful, the optimization process requires a starting point obtained from a global search. A robust training process needs both the initialization and optimization processes [38]. A schematic of the hybrid ANN algorithm is presented in Fig. 2
                           . In the first step, the initial weights are optimized by SA. In the second step, the MLP network is used to find the final weights of the network.

SA is very useful for solving nonlinear problems with multiple local optima [1]. SA is a global search algorithm that makes use of the Metropolis algorithm [40] for computer simulation of annealing. Annealing is a process in which a metal is heated to a high temperature and thereafter it is gradually cooled to relieve thermal stresses. The cooling process is simulated by SA to optimize a function in a certain design space. The objective function relates to the energy state, and changing the set of design variables corresponds to changing the crystalline structural state [22]. The abilities and shortcomings of SA are well summarized by Ingber [33]. For the initialization step, SA randomly perturbs the weights of the network during the iterations. When the weights are perturbed, the network performance is evaluated based on the defined objective function. The cooling schedule during iterations can be linear or exponential, and additional iterations at a specific temperature may occur if the objective function is improved [32]. Each time a new solution is generated, the algorithm decides whether the new solution should be accepted or rejected. Metropolis et al. [40] expressed the probability (Pa
                           ) of accepting a new solution as:
                              
                                 (1)
                                 
                                    
                                       
                                          P
                                       
                                       
                                          a
                                       
                                    
                                    (
                                    Δ
                                    E
                                    ,
                                    y
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            e
                                                         
                                                         
                                                            -
                                                            
                                                               
                                                                  κ
                                                                  Δ
                                                                  E
                                                               
                                                               
                                                                  y
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      Δ
                                                      E
                                                      >
                                                      0
                                                   
                                                
                                                
                                                   
                                                      1
                                                   
                                                   
                                                      Δ
                                                      E
                                                      ⩽
                                                      0
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where ΔE is the error, y is the current temperature, and κ is an acceptance constant that depends on the network’s weights range and inputs. As y decreases, the algorithm becomes more selective [38].

When implementing SA for ANN initialization, the most important factor is adjusting the acceptance constant, which is a function of the temperature range, the training dataset, and the allowed weight values [26]. Generally, cooling schedules progress gradually from high temperatures to lower temperatures until a specified target temperature is reached [39]. However, the hybrid SA-ANN method requires the temperature to increase and decrease periodically, following a linear or temperature cycling cooling schedule. In some cases, the temperature cycling schedule can outperform the linear cooling schedule (e.g., [4,38]. Therefore, the temperature cycling cooling schedule is used in this study.

Genetic programming (GP) is a predictive tool that creates computer programs by emulating the biological evolution of living organisms [35]. Friedberg [20] used a learning algorithm to improve a program, which was the first use of GP. Cramer [13] applied genetic algorithms (GAs) to evolve programs and introduced tree-like GP structures. A breakthrough in GP then came with the experiments of Koza [35] on symbolic regression. Most of the genetic operators used in GA, such as crossover and mutation, can be implemented in GP with little changes. In GP, the evolving programs (individuals) are parse trees with variable length rather than fixed-length binary strings. Each program generated by GP is evaluated using a fitness function, which serves as the objective function that GP aims to optimize [23].

Gene expression programming (GEP), introduced by Ferreira [18], is a natural development of GP. Most of the genetic operators used in GAs can also be used in GEP with minor changes. GEP consists of five main components [5]: the function set, the terminal set, the fitness function, control parameters, and a termination condition. GEP uses a fixed length of character strings to represent solutions to the problems, which are afterwards expressed as parse trees of different sizes and shapes. These trees are called GEP expression trees (ETs) [5]. In GEP, the individuals are selected and copied into the next generation according to the fitness by roulette wheel sampling with elitism (citation). This guarantees the survival and cloning of the best individual to the next generation. Variation in the population is introduced by conducting one or more genetic operators (crossover, mutation, or rotation) on selected chromosomes.

One advantage of the GEP technique is that the creation of genetic diversity is extremely simplified, because the genetic operators work at the chromosome level. The multi-genic nature of GEP allows the evolution of more complex programs composed of several subprograms. Each GEP gene contains a fixed-length list of terms; each term can be any element from the function set (e.g., {+,−,×,/,tan}) or from the terminal set (e.g., {a,
                           b,
                           c,5}). The function set and terminal set must have the closure property: each function must able to process any value of a data type that can be returned by a function or assumed by a terminal [5]. A typical GEP gene with the given function and terminal sets can be as follows:
                              
                                 (2)
                                 
                                    
                                       
                                          +
                                          ·
                                          ×
                                          ·
                                          tan
                                          ·
                                          a
                                          ·
                                          -
                                          ·
                                       
                                       
                                          ̲
                                       
                                    
                                    +
                                    ·
                                    +
                                    ·
                                    ×
                                    ·
                                    b
                                    ·
                                    a
                                    ·
                                    c
                                    ·
                                    5
                                    ·
                                    b
                                    ·
                                    c
                                 
                              
                           where a, b and c are variables. The above expression is termed as Karva notation or K-expression [18,23]. A K-expression can be shown using an ET diagram. For example, the sample gene from Eq. (2) can be expressed as shown in Fig. 3
                           .

The conversion starts from the first position in the K-expression, which corresponds to the root of the ET, and reads through the string element by element. The GEP gene from Eq. (2) can also be expressed in a traditional mathematical form as:
                              
                                 (3)
                                 
                                    a
                                    (
                                    (
                                    c
                                    +
                                    5
                                    )
                                    -
                                    (
                                    b
                                    ×
                                    c
                                    )
                                    )
                                    +
                                    tan
                                    (
                                    b
                                    +
                                    a
                                    )
                                 
                              
                           Inversely, an ET can be converted into a K-expression by recording the nodes from left to right in each layer of the ET, from the root layer down to the deepest layer, to form the string.

The simplified GEP algorithm is presented in Fig. 4
                           .

Evaluation of the punching shear strength of concrete slabs has traditionally been a difficult task. Several estimates of punching shear strength have been developed, indicating that punching shear strength is mainly influenced by several important parameters: column side dimension (c), effective depth to the center of the tensile reinforcement (d), concrete compressive strength (f́c
                     ), and flexural reinforcement ratio (ρ). These properties of an RC slab are indicated schematically in Fig. 5
                     .

Different design codes (e.g., BS and EC) use different models to predict punching shear strength; most code predictions use a combination of these four parameters (c, d, f’c
                     , ρ). None of the code formulations considered in this study use shear span and/or shear span to depth ratio. It should be noted that some codes (e.g., ACI and AS) also do not use ρ in the punching shear strength calculation.

Two soft computing techniques (ANN and GP), as an alternative to the code approaches, have also been applied to predict punching shear strength. In a recently proposed ANN model by Said et al. [44], the authors used five different inputs for the ANN model: f’c
                     , d, a/d (the ratio of shear span to effective depth), c/d, and ρ. Another recently proposed ANN model by Elshafey et al. [17] uses the main four parameters (c, d, f’c
                     , and ρ) and yield stress of steel reinforcement (fy
                     ) as input parameters. However, fy
                      may not be a suitable input value, as yielding typically does not occur during a punching shear failure.

After developing several models with different combinations of the input parameters, we have used the basic four parameters in developing the ANN and GP models, and can generally express the punching shear strength, Pu
                     , as a function of those four parameters, Pu
                     
                     =(c,
                     d,
                     f’c
                     ,
                     ρ).

A database including 241 experimental RC slab tests gathered by Elshafey et al. [17] has been used in this study for model development. The performance of any model developed using this database is influenced by the sample size and its variable distributions. Therefore, the data are visualized in Fig. 6
                         as histograms.

The descriptive statistics of the samples shown in Fig. 6 are given in Table 1
                        . For the neural network analysis, the database must be normalized based on the activation functions used in the ANN architecture. For the log-sigmoid function, all values should be between 0 and 1. Therefore, the values of each variable should be divided by a normalization value that is greater than or equal to its maximum value. The normalization values used for this study are shown in the last row of Table 1.

The database was divided into a learning (training and validation) and testing set (unseen during model development; used for testing the model after training) for the analysis. To ensure consistent data division, several combinations of the training and testing sets were considered. For the selection of training and testing subsets, the minimum, maximum, average, and standard deviation of the punching shear values should be consistent between the training and the testing data sets [23]. Of the 241 data sets, 193 records (80%) are taken for the training process and the remaining 48 records were used for validation and testing (10% for each) of the ANN and GP models.

Frank and Todeschini [19] proposed a minimum ratio of the number of database records over the number of input variables for model acceptability as three, and recommended using ratio values greater than five. For the current case study, this ratio for the training and unseen (validation and testing) sets were 193/4=48.25 and 48/4=12, respectively, which both exceed the suggested criteria.

A correlation coefficient by itself is not a good indicator of prediction accuracy of a model, because correlation coefficients are not sensitive to the addition or multiplication of output values by a constant. Therefore, an error function should be used in addition to the correlation coefficient to evaluate model performance. Gandomi et al. [23] proposed a function for performance evaluation of a trained model, taking into account the changes of correlation and error functions together. Based on that function, a performance index (PI) is proposed here to evaluate performance as a function of the relative root mean square error (RRMSE) and the correlation coefficient (R) as follows:
                           
                              (4)
                              
                                 PI
                                 =
                                 
                                    
                                       RRMSE
                                    
                                    
                                       R
                                       +
                                       1
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 RRMSE
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       |
                                       
                                          
                                             
                                                
                                                   h
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          
                                             ¯
                                          
                                       
                                       |
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   n
                                                
                                             
                                             
                                                
                                                   (
                                                   
                                                      
                                                         h
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   -
                                                   
                                                      
                                                         t
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   )
                                                
                                                
                                                   2
                                                
                                             
                                          
                                          
                                             n
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 R
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                       (
                                       
                                          
                                             h
                                          
                                          
                                             i
                                          
                                       
                                       -
                                       
                                          
                                             
                                                
                                                   h
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          
                                             ¯
                                          
                                       
                                       )
                                       (
                                       
                                          
                                             t
                                          
                                          
                                             i
                                          
                                       
                                       -
                                       
                                          
                                             
                                                
                                                   t
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          
                                             ¯
                                          
                                       
                                       )
                                    
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   n
                                                
                                             
                                             
                                                
                                                   (
                                                   
                                                      
                                                         h
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   -
                                                   
                                                      
                                                         
                                                            
                                                               h
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                      
                                                         ¯
                                                      
                                                   
                                                   )
                                                
                                                
                                                   2
                                                
                                             
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   n
                                                
                                             
                                             
                                                
                                                   (
                                                   
                                                      
                                                         t
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   -
                                                   
                                                      
                                                         
                                                            
                                                               t
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                      
                                                         ¯
                                                      
                                                   
                                                   )
                                                
                                                
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where hi
                         and ti
                         are the ith experimental and predicted outputs, respectively; 
                           
                              
                                 
                                    
                                       
                                          h
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    ¯
                                 
                              
                           
                         and 
                           
                              
                                 
                                    
                                       
                                          t
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    ¯
                                 
                              
                           
                         are the average values of the experimental and predicted outputs, respectively; and n is the number of samples.

Higher R values and lower RRMSE values result in lower PI and, consequently, indicate a more precise model. It should be noted that PI values range from 0 to +∞, with smaller values indicating better performance. For the proposed performance index, a PI value close to zero (e.g. PI
                        ⩽0.2) indicates the model predicts the actual values well and is the recommended acceptance threshold.

Over-fitting is a significant issue in machine learning algorithms, especially in soft computing techniques, because of their complexity. To avoid over-fitting, the derived model should be tested based on an unseen data set to find a better generalization. Therefore, the following objective function was constructed as a performance measure of the developed model. The best ANN and GP models were determined by the minimization of the following multi-objective (MO) function:
                           
                              (7)
                              
                                 MO
                                 =
                                 
                                    
                                       w
                                    
                                    
                                       T
                                    
                                 
                                 
                                    
                                       PI
                                    
                                    
                                       T
                                    
                                 
                                 +
                                 
                                    
                                       w
                                    
                                    
                                       V
                                    
                                 
                                 
                                    
                                       PI
                                    
                                    
                                       V
                                    
                                 
                              
                           
                        where
                           
                              
                                 
                                    
                                       w
                                    
                                    
                                       T
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             n
                                          
                                          
                                             T
                                          
                                       
                                       -
                                       
                                          
                                             n
                                          
                                          
                                             V
                                          
                                       
                                    
                                    
                                       n
                                    
                                 
                                 
                                 and
                                 
                                 
                                    
                                       w
                                    
                                    
                                       V
                                    
                                 
                                 =
                                 2
                                 
                                    
                                       
                                          
                                             n
                                          
                                          
                                             V
                                          
                                       
                                    
                                    
                                       n
                                    
                                 
                              
                           
                        where the subscripts of n, T and V, are related to the training and validation (and/or testing) sets, respectively. One advantage of the multi-objective function is that it explicitly considers the relative percentage of the database entries in the training and validation sets. An MO value closer to zero indicates better model calibration.

Parameter selection will affect the model generalization capability of the hybrid ANN. The parameter values used in the predictive algorithm are shown in Table 2
                        . The simulated annealing (SA) parameters were selected based on previously suggested values [4,26,38]. The performance of an ANN model mostly depends on the network architecture. According to the universal approximation theorem, a network with one hidden layer is sufficient to uniformly approximate any continuous and nonlinear function (e.g., [6,14]. The type of activation function, the number of epochs, and the learning rate play important roles in model construction [16]; therefore, in this study, several ANN models were developed with different settings to obtain the optimal configurations. In this study, log-sigmoid and conjugate-gradient were adopted as the transfer function and training algorithm, respectively.


                        Over-fitting. The choice of the number of the hidden nodes is very important, especially because a complex model may exhibit over-fitting. Hecht-Nelson [31] recommended an upper bound for the number of hidden neurons using Kalmogorov’s theorem as nh
                        
                        ⩽2ni
                        
                        +1, where nh
                         and ni
                         are the numbers of hidden neurons and inputs, respectively. Therefore, for the current problem, the upper limit is 2×4+1=9. Another over-fitting criterion recently proposed by Belman-Flores et al. [10] for a system with one hidden layer defined a lower bound for the number of training sets as nT
                        
                        ⩾4(ni
                        
                        +1) nh
                        . Considering biases and number of output(s), this criterion can be expressed as:
                           
                              (8)
                              
                                 
                                    
                                       n
                                    
                                    
                                       T
                                    
                                 
                                 ⩾
                                 c
                                 [
                                 
                                    
                                       n
                                    
                                    
                                       h
                                    
                                 
                                 (
                                 
                                    
                                       n
                                    
                                    
                                       i
                                    
                                 
                                 +
                                 1
                                 )
                                 +
                                 
                                    
                                       n
                                    
                                    
                                       o
                                    
                                 
                                 (
                                 
                                    
                                       n
                                    
                                    
                                       h
                                    
                                 
                                 +
                                 1
                                 )
                                 ]
                              
                           
                        where no
                         is number of outputs and c is a coefficient greater than or equal to 4. Then, the maximum number of hidden neurons in the system with one hidden layer can be expressed as:
                           
                              (9)
                              
                                 
                                    
                                       n
                                    
                                    
                                       h
                                    
                                 
                                 ⩽
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      n
                                                   
                                                   
                                                      T
                                                   
                                                
                                                -
                                                
                                                   
                                                      cn
                                                   
                                                   
                                                      o
                                                   
                                                
                                             
                                             
                                                c
                                                (
                                                
                                                   
                                                      n
                                                   
                                                   
                                                      i
                                                   
                                                
                                                +
                                                
                                                   
                                                      n
                                                   
                                                   
                                                      o
                                                   
                                                
                                                +
                                                1
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        For this problem, considering c
                        =4, the maximum number of hidden neurons can be computed as:
                           
                              (10)
                              
                                 
                                    
                                       n
                                    
                                    
                                       h
                                    
                                 
                                 ⩽
                                 
                                    
                                       
                                          
                                             
                                                193
                                                -
                                                4
                                                ×
                                                1
                                             
                                             
                                                4
                                                (
                                                4
                                                +
                                                1
                                                +
                                                1
                                                )
                                             
                                          
                                       
                                    
                                 
                                 ≈
                                 7
                              
                           
                        The minimum of the two criteria (Hecht-Nelson [31]
                        =9 and Belman-Flores et al. [10]
                        =7) is considered here as the maximum number of hidden neurons in the system with one hidden layer. Therefore, as many as seven hidden neurons were considered in this study. The number of hidden neurons was optimized such that the final model uses the fewest hidden neurons beyond which the testing results do not improve.

The performance results between different ANN or SA-ANN runs may vary considerably, even if all the previous parameter settings and the network architecture are kept constant. This leads to further difficulties in the selection of the optimal ANN parameters and architecture. To overcome these difficulties, the weights and biases were frozen after the network was well trained; the trained ANN models were then translated into explicit forms [3]. The hybrid ANN algorithm was implemented by using the Neural Lab software for ANNs [37].

The model architecture that gave the best results for the formulation of the Pu
                            in terms of c, d, f’c
                            and ρ is shown in Fig. 7
                           . This model had five hidden neurons. The mean square error (MSE) of the training set as a function of time for the normalized output data is presented in Fig. 8
                           . As illustrated in this figure, MSE was initially minimized using the SA algorithm, which took approximately 5s. At that point, the SA had globally reached appropriate initial weights, decreasing the trapping potential to the local minima. The training process then switched to the optimization phase using ANN, which minimized the error using a gradient based algorithm.

Previous studies on punching shear modeling with ANNs did not present closed form solutions. The explicit formulation of punching shear capacity of RC slabs derived using the hybrid ANN algorithm in this study can be expressed as follows:
                              
                                 (11)
                                 
                                    
                                       
                                          P
                                       
                                       
                                          u
                                          ,
                                          NN
                                       
                                    
                                    =
                                    Logsig
                                    
                                       
                                          
                                             B
                                             +
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      h
                                                      =
                                                      1
                                                   
                                                   
                                                      
                                                         
                                                            n
                                                         
                                                         
                                                            h
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   V
                                                
                                                
                                                   h
                                                
                                             
                                             Logsig
                                             
                                                
                                                   
                                                      
                                                         
                                                            b
                                                         
                                                         
                                                            h
                                                         
                                                      
                                                      +
                                                      
                                                         
                                                            
                                                               ∑
                                                            
                                                            
                                                               i
                                                               =
                                                               1
                                                            
                                                            
                                                               
                                                                  
                                                                     n
                                                                  
                                                                  
                                                                     i
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            w
                                                         
                                                         
                                                            ih
                                                         
                                                      
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           The weights and biases of the final hybrid ANN model are presented in Tables 3 and 4
                           
                           .

A comparison of the actual and predicted punching shear values for the whole data set is shown in Fig. 9
                           . The diagonal “ideal fit” line indicates when the predicted value equals the actual value; value closer to the diagonal line are more accurate predictions. The ANN model successfully predicted the punching shear values. The statistical parameters of both the training and testing data sets are presented in Table 5
                           . The MO is close to zero, which shows that the model has low error over the full range of data.

In this study, the GP technique was employed to obtain meaningful relationships between the punching shear and the mechanical and geometrical parameters discussed earlier (c, d, f’c
                         and ρ). GP algorithm parameter selection will affect the model generalization capability of GP; therefore, in this study, these parameters were selected based on previously suggested values in the literature [9,24]. The parameter settings for the GP algorithm used in this study are presented in Table 6
                        . For developing the GP-based empirical models, a commercial software program, GeneXproTools [28], was utilized.

The expression trees corresponding to the best GP model are depicted in Fig. 10
                           . The related GP-based formulation of the punching shear, Pu,GP
                           , is:
                              
                                 (12)
                                 
                                    
                                       
                                          P
                                       
                                       
                                          u
                                          ,
                                          GP
                                       
                                    
                                    =
                                    d
                                    
                                       
                                          
                                             
                                                
                                                   d
                                                
                                                
                                                   4
                                                
                                             
                                             -
                                             1.1
                                             
                                                
                                                   d
                                                   
                                                      
                                                         ρ
                                                      
                                                   
                                                
                                                
                                                   3
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             1
                                             -
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               f
                                                            
                                                            
                                                               c
                                                            
                                                         
                                                         
                                                            
                                                               C
                                                            
                                                         
                                                      
                                                      
                                                         5
                                                      
                                                   
                                                
                                                
                                                   5
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           A comparison of the experimental and predicted punching shear values for the training, validation and testing data is shown in Fig. 11
                           . From this figure, the predicted values are very close to the actual values, as all values fall near the “ideal fit” line. The statistical parameters of the both training and testing data sets are presented in Table 7
                           . The statistical results of the training results and the validation and testing results are very similar, which generally indicates that the model is well trained.

Model validity and accuracy can be determined based on high values of correlation (R) and low values of error (e.g., RRMSE) [45]. According to these criteria, the proposed ANN and GP models, which have high R values and low RRMSE values (as indicated in Tables 5 and 7) predict the punching shear capacity with a high degree of accuracy. The performance of the models on the training, validation, and testing subsets reveals that they have good predictive ability. Moreover, Golbraikh and Tropsha [29] suggested some new criteria for external verification of a proposed model on the testing data sets: the slope (k or k’) of the regression line between the actual data (hi
                        ) and the predicted data (ti
                        ) should be close to 1, and that the performance indexes |m| and |n| should be lower than 0.1. Recently, Roy and Roy [43] introduced an index (Rm
                        ) for external predictability evaluation of models; their validation criterion is satisfied for Rm
                        
                        >0.5.

The external validation criteria results for the developed models are shown in Table 8
                        . The derived SA-ANN and GP models satisfy all proposed conditions.

As described above, two different formulas were obtained for the assessment of the punching shear of RC slabs by means of a hybrid ANN formulation and a GP formulation. Overall statistical performance of the developed models and five other code models, including formulations from the American Concrete Institute ACI 318 [2], the European code CE2 [12], the British Standard BS8110 [11], the Australian Standard AS3600 [7] and the German code DIN 1045-1 [15] are summarized in Table 9
                        . Comparing the performance of these two soft computing predictive tools for the full data set, the ANN model produced the best (highest) R values and the best (lowest) RRMSE values. With respect to these criteria, the GP model was outperformed only by the ANN model. From Table 9, it is clear that only four models have low PI values (PI
                        <0.1): ANN, GP, EC, and BS. It should be noted that the best code model (according to the statistical criteria) among the five studied codes is the BS, which has a good correlation and covariance as well as an acceptable value of PI.

Bagheri et al. [8] recommended that model prediction capabilities should be compared on the basis of relative error distribution. To that end, the absolute relative error (ARE) percentage is calculated as:
                           
                              (13)
                              
                                 ARE
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      h
                                                   
                                                   
                                                      i
                                                   
                                                
                                                -
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      h
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 ×
                                 100
                              
                           
                        
                        Fig. 12
                         illustrates the ARE distribution for the proposed models and five available code models. Ideally, the frequency shown in the figure should decrease with increasing ARE. The ANN and GP models have the highest frequencies of low ARE (ARE
                        <5%) and lowest frequencies of high ARE (30%⩽
                        ARE). Among all studied models, only ANN, GP, and BS have acceptable error distributions, though EC also has a relatively low frequency of high ARE.

For further verification of the developed ANN and GP models of punching shear strength, a parametric analysis was performed in this study. This study is primarily intended to evaluate the effect of individual parameters on the punching shear strength (Pu
                        ) and compare the trends with those exhibited by the current code models. Fig. 13
                         shows the predicted values of the punching shear obtained by the ANN, GP, the best two code models (BS and EC), and the popular ACI code, modeled as a function of each parameter. Fig. 13(a)–(d) show punching shear as a function of column side dimension (c), the effective depth to the center of the tensile reinforcement (d), the concrete strength (fc’), and reinforcement ratio (ρ), respectively. Note that Fig. 13(d) reflects the fact that ρ is not an input in the ACI model.

As shown in Fig. 13(a), the column side dimension (c) has a positive linear effect on the code predictions of punching shear capacity. In the GP model, c has a slightly nonlinear trend, but the trend (positive slope) is in general agreement with the code values. For the ANN model, however, the slope of the line is positive for low c values but becomes negative for high c values, which is not reasonable with respect to the mechanics of punching shear. As shown in the histogram plot of the column side dimension values (Fig. 6(a)), the frequencies of the higher c values are low, indicating that ANN cannot accurately predict the behavior for the column side frequency range in Fig. 6(a).

As shown in Fig. 12(b), the predicted punching shear of RC slabs increases nonlinearly with increasing effective depth in all models. However, for the ANN model, is the prediction converges to a constant value for large d values (d
                           ⩾375mm), which is inconsistent with the other models. As shown in Fig. 6(b), few experimental tests have high d values. Therefore, the ANN model cannot be trained well for the d values in this low-frequency range. However, the GP model seems well trained and its trend is consistent with the code models, particularly that of the BS 8110 model.

The punching shear of an RC slab is related to the concrete’s tensile strength, and concrete’s tensile strength is related to its compressive strength. As shown in Fig. 13(c), punching shear strength increases in all models with increasing fc’ as expected. The GP, ACI, and BS models have very similar behavior. The ANN model predicts different behavior for high strength concrete (HSC) due to the low frequencies of HSC data (as shown in Fig. 6(c)).

As shown in Fig. 13(d), the GP and BS models have similar trends with respect to reinforcement ratio (ρ). As previously mentioned, the ACI model does not include ρ as an input parameter, and is therefore not sensitive to changes in ρ. The ANN model converges to the constant ACI prediction value for higher values of ρ. However, Fig. 6(d) shows that the data set has limited data for higher ρ values.

Generally, the ANN training process is faster than that of GP, even when ANN is hybridized with a global optimization algorithm. However, for ANN modeling, we must initially find the best network architecture. One of the main problems in statistical and machine learning problems is multicollinearity, which is more critical in ANN than GP [27]. Therefore, instead of combining independent variables to create dimensionless parameters, in this study we have decided to use the basic variables, as there is no strong correlation between them.

After soft computing models have been trained, they must be evaluated with respect to applicability and reliability. Most soft computing techniques (e.g., ANNs) are very complex and only appropriate to be used as a computer program. However, the GP-based equations are generally short and simple, and can be used for routine design practice via hand calculations.

Over-fitting is the most essential problem in statistical and machine learning techniques. Complex methods have high over-fitting potential. As ANN models are more complex than those of GP, they have more over-fitting potential. Before starting the modeling, the model complexity of the ANN models is minimized by using only one hidden layer. Furthermore, the number of hidden neurons is kept below the upper bound (which is equal to seven for the models developed in this study).

After training the models used in this study, several steps were taken to avoid over-fitting: (1) derive the final models based on training and validation sets; (2) compute several external validation criteria using unseen data to verify the models; and (3) conduct a parametric study based on engineering principals and the physics of the problem.

The first two steps are purely statistical; however, the third step is based in engineering principals and should be done by an engineer who understands the problem that is being modeled. The first and second steps are used here for the first time for this problem. The third step indicates that the ANN model does not work properly for some variable ranges, especially for those ranges in which the variables do not have high frequencies (e.g., the ranges in which the model extrapolates beyond the majority of the available data). This demonstrates how the sample size and input distributions influence the proposed models.

@&#CONCLUSIONS@&#

In this study, two established soft computing techniques, a hybrid artificial neural network (ANN) and a robust variant of genetic programming (GP), were utilized to assess an engineering modeling case, predicting punching shear strength of RC slabs. Two different formulations were developed for the punching shear prediction. The models were verified based on several existing external model validation criteria. The GP- and ANN-based correlations were benchmarked against five code models for punching shear capacity. The following conclusions can be derived from the results presented in this research:
                        
                           •
                           For modeling using ANN, data should initially be normalized based on the suitable activation function and the best network architecture should be determined. GP formulations, however, inherently find the best structure after evaluation of millions of programs throughout multiple generations.

Both ANN and GP-based correlations are capable of predicting the punching shear of RC slabs with high accuracy and without any prior assumptions (i.e., the models were developed independent of any engineering mechanics concepts).

The GP-based equations are much simpler than the ANN equations. The developed GP formulation can be used for routine design practice.

Several steps, including data division, minimized model complexity, external validation, and a parametric study, are required to avoid over-fitting of the ANN model, and were discussed in this paper in detail.

Further research is needed to develop criteria to quantify over-fitting potential or complexity of GP-based formulations.

Over-fitting of the ANN model mostly happened in variable ranges with low frequencies of occurrence in the database, for which the algorithms were not trained very well.

The main conclusion of this study is that although statistical parameters and indices indicate there is no over-fitting in the ANN model, a parametric study is necessary to determine the usefulness of the model and to avoid over-fitting.

@&#REFERENCES@&#

