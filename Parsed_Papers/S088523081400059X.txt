@&#MAIN-TITLE@&#Leveraging social Q&A collections for improving complex question answering

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           The proposed approach leverages social Q&A collections to improve automatic complex QA system.


                        
                        
                           
                           There is no need to manually collect training Q&A pairs that are necessary for supervised machine learning approaches.


                        
                        
                           
                           Extensive comparison experiments are conducted, i.e., LexRank, question-specific and translation-based approaches are compared.


                        
                        
                           
                           Experiments on the extension of NTCIR 2008 test questions indicate that the proposed approach is more effective.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Complex question answering

Web mining

Summarization

@&#ABSTRACT@&#


               
               
                  This paper regards social question-and-answer (Q&A) collections such as Yahoo! Answers as knowledge repositories and investigates techniques to mine knowledge from them to improve sentence-based complex question answering (QA) systems. Specifically, we present a question-type-specific method (QTSM) that extracts question-type-dependent cue expressions from social Q&A pairs in which the question types are the same as the submitted questions. We compare our approach with the question-specific and monolingual translation-based methods presented in previous works. The question-specific method (QSM) extracts question-dependent answer words from social Q&A pairs in which the questions resemble the submitted question. The monolingual translation-based method (MTM) learns word-to-word translation probabilities from all of the social Q&A pairs without considering the question or its type. Experiments on the extension of the NTCIR 2008 Chinese test data set demonstrate that our models that exploit social Q&A collections are significantly more effective than baseline methods such as LexRank. The performance ranking of these methods is QTSM>{QSM, MTM}. The largest F3 improvements in our proposed QTSM over QSM and MTM reach 6.0% and 5.8%, respectively.
               
            

@&#INTRODUCTION@&#

Unlike conventional search engines, which find relevant documents on the web, question-answering (QA) systems are designed to return much more focused answers, for example:
                        
                           Q:
                           In which city will the 2020 Olympic games be held?

Tokyo

QA research attempts to deal with a wide range of question types, including factoid, list,
                        1
                     
                     
                        1
                        Example question: Name 20 countries other than the United States that have a McDonald's restaurant.
                      and complex questions. In the past, substantial progress has been made with factoid and list questions. In February 2011, IBM's Watson QA system (Ferrucci et al., 2010) defeated human grand champions in the game of Jeopardy!
                        2
                     
                     
                        2
                        
                           http://www.jeopardy.com/.
                      In this paper, we focus on complex questions because – apart from definitional
                        3
                     
                     
                        3
                        Example questions: Who is Aaron Copland? What is a golden parachute?
                      (Voorhees, 2003; Mitamura et al., 2008), reason (Higashinaka and Isozaki, 2008; Oh et al., 2013), and opinion (Dang, 2008) complex questions – many types remain largely unexplored.
                        4
                     
                     
                        4
                        Most complex questions have generally been referred to as what-questions in previous studies. This paper argues that it is helpful to treat them discriminatively.
                     
                  

Compared to factoid and list questions that can be answered by short phrases, such as persons, organizations, locations and dates, complex questions, whose answers generally consist of a list of “nuggets” (Voorhees, 2003; Mitamura et al., 2008), are more difficult to answer. For instance, the factoid question, “In which city will the 2020 Olympic games be held?” asks for a city name, and thus it is easier to impose some constraints on the plausible answers for this type of question and significantly reduce the search space of plausible answers. However, complex questions such as “What are the hazards of global warming?” often seek multiple, different types of information simultaneously, making it difficult to screen plausible answers. Moreover, complex QA tasks require inferring and synthesizing information from multiple documents to provide multiple nuggets as answers (Dang, 2006; Chali et al., 2009). To answer complex questions, we often need to go through complex procedures.

Many approaches have been proposed to answer factoid, definitional, reason, and opinion questions. Among them, machine learning techniques have proven to be effective in constructing QA components from scratch, but these supervised techniques require a certain quantity of question and answer (Q&A) pairs as training data. For example, Echihabi and Marcu (2003) and Sasaki (2005) constructed 90,000 English and 2000 Japanese Q&A pairs for their factoid QA systems, respectively. Cui et al. (2004) collected 76 term-definition pairs for their definitional QA system. Higashinaka and Isozaki (2008) used 4849 positive and 521,177 negative examples in their reason QA system. Stoyanov et al. (2005) required a known subjective vocabulary for their opinion QA system. To answer other types of complex questions using supervised techniques, we need to collect Q&A pairs for each type of complex question to train models, even though this is an extremely expensive and labor-intensive task. Fortunately, many user-generated Q&A pairs can be found in social QA websites such as Yahoo! Answers,
                        5
                     
                     
                        5
                        
                           http://answers.yahoo.com/.
                      Baidu Zhidao,
                        6
                     
                     
                        6
                        
                           http://zhidao.baidu.com/.
                      and Answers.com.
                        7
                     
                     
                        7
                        
                           http://www.answers.com/.
                     
                  

This paper explores the automatic learning of Q&A training pairs and the mining of needed knowledge from social Q&A collections such as Yahoo! Answers. We are interested in whether millions of typically noisy user-generated Q&A pairs can be exploited for automatic QA systems. If so, a plethora of Q&A training data is already readily available.

Many studies (Riezler et al., 2007; Surdeanu et al., 2008; Duan et al., 2008; Wang et al., 2010) have retrieved similar Q&A pairs from social QA websites as answers to test questions; accordingly, answers cannot be generated for questions that have not previously been answered on such sites. Our study, however, regards social Q&A websites as knowledge repositories and exploits their knowledge to synthesize answers to questions that have not yet been answered. Even for questions that have been answered, it is necessary to perform answer summarization (Liu et al., 2008; Chan et al., 2012). Our approach can also be used for this purpose. To the best of our knowledge, very few works in the literature have addressed this aspect.

Various kinds of knowledge can be mined from social Q&A collections to support complex QA systems. In this paper, we present a question-type-specific method (QTSM) to mine question-type-specific knowledge and compare it with the question-specific and monolingual translation-based methods proposed in related work. Given question Q, whose question type Q
                     
                        t
                      is automatically recognized from Q, three kinds of methods can be applied here.
                        
                           •
                           Our proposed QTSM collects Q&A pairs in which the question types are the same as Q
                              
                                 t
                               and extracts salient cue expressions that are indicative of possible answers to question type Q
                              
                                 t
                              . It uses the expressions and the Q&A pairs to train a binary classifier to remove noisy candidate answers.

The question-specific method (QSM, introduced in Section 5.1) collects Q&A pairs that resemble Q from social Q&A collections and extracts question-dependent answer words to improve complex QA systems.

The monolingual translation-based method (MTM) employs all of the social Q&A pairs and learns word-to-word translation probabilities, without considering question Q and question type Q
                              
                                 t
                              , to solve the lexical gap problem between question and answer. MTM will be discussed in Section 5.2.

We evaluated three methods in terms of the extension of the NTCIR (NII Testbeds and Community for Information access Research) 2008 test data set. We employed the Pourpre v1.0c evaluation tool (Lin and Demner-Fushman, 2006), which was also adopted to evaluate the TREC (Text REtrieval Conference) QA systems. Experiments showed that our proposed QTSM is the most effective; for instance, the largest F3/NR improvements of QTSM over the baseline, QSM, and MTM models reached 8.6%/12.6%, 6.0%/6.7%, and 5.8%/7.1%, respectively. The performance ranking of these methods was QTSM>{QSM, MTM}. LexRank (Erkan and Radev, 2004), which is commonly used in query-based summarization tasks, was implemented as the baseline. Our experiments showed that the three models designed for exploiting social Q&A collections are significantly better than LexRank.

Social QA websites such as Yahoo! Answers and Baidu Zhidao provide interactive platforms for users to post questions and answers. After users answer the questions, the best answer can be chosen by the asker or nominated by the community. Table 1
                      demonstrates an example of these Q&A pairs, whose numbers have risen dramatically on such sites. Table 2
                      shows the statistics of Q&A pairs crawled from Baidu Zhidao in December 2008. The pairs collectively form a source of training data, which is essential for supervised machine-learning-based QA systems.

This paper exploits such user-generated Q&A collections to improve complex QA systems by the automatic learning of Q&A training pairs and the mining of needed knowledge from them. Social collections, however, have two salient characteristics: the textual mismatch between questions and answers (i.e., question words are not necessarily used in answers), and user-generated trolling or flippant answers, which are unfavorable factors in our study. Therefore, we only extract questions and their best answers to form the Q&A training pairs, where the best answers are longer than an empirical threshold (20 words).
                        8
                     
                     
                        8
                        The average length of the answer candidates is 29 words. Therefore, we set the threshold at 20 to filter out those Q&A pairs that are not likely to answer complex questions.
                      Eventually, we extracted 15,678,739 Q&A pairs and used them as training data.

A typical complex QA system is a cascade of three modules. The Question Analyzer analyzes the test question and identifies its type. The Document Retriever & Answer Candidate Extractor retrieves documents related to questions from the given collection (Xinhua and Lianhe Zaobao newspapers published 1998–2001 were used in this study) for consideration and segments them into sentences as answer candidates. The Answer Ranker applies sentence retrieval techniques (Balasubramanian et al., 2007) to estimate the “similarities” between the sentences (we used 1024 sentences) and questions, and ranks them based on their similarities. This paper simply employs a Kullback-Leibler (KL)-divergence language model approach. The top N sentences are deemed the final answers.

As an example, consider Table 3
                     , in which test question Q
                     1 and three candidate answers extracted by the Document Retriever & Answer Candidate Extractor are given. For the Answer Ranker, which directly calculates the similarities between the question and the answer candidates, it is difficult to correctly select a
                     2 as an answer because the three candidates contain the same question keywords. To improve this architecture, external knowledge must be incorporated. As introduced in Section 2, social Q&A collections are a good choice for mining the necessary knowledge.

In this paper, we propose a question-type-specific technique of exploiting social Q&A collections (Section 4) to mine knowledge and compare it with question-specific (Section 5.1) and monolingual translation-based (Section 5.2) methods in the experimental section. We also compare the three models with LexRank, which has been widely used in query-based summarization tasks.

Based on our observation that answers to certain types of complex questions usually contain question-type-dependent cue expressions that are helpful for answering such questions (Table 6), we propose a QTSM that learns these cue expressions for each type of question and utilizes them to improve complex QA systems. For each test question, QTSM performs the following steps:
                        
                           •
                           Recognizes the type of test question.

Learns the positive and negative Q&A training pairs for the particular type of question from social Q&A collections.

Extracts salient question-type-specific cue expressions from the Q&A pairs and then utilizes them with the Q&A pairs to build a binary classifier for the type of test question.

Employs the learned classifier to remove noise from the candidate answers before using the Answer Ranker to select the final answers to the test question.


                     Fig. 1
                      shows the architecture of the QTSM system.

Previous work on factoid QA systems recognized question types by classification techniques (Li and Roth, 2002) that require the taxonomy of the question types and the training instances for each type. This algorithm may be inappropriate for complex QA systems since (a) hundreds of question types exist and (b) we have little prior knowledge for defining complex QA-oriented taxonomy. Therefore, such related studies as NTCIR (Mitamura et al., 2008) classify questions into a specific question type: the EVENT type. In this paper, we categorize complex questions into fine-grained question types by identifying their question focus.

Question focus is defined as a short sub-sequence of tokens (typically one to three words) in a question that are adequate for indicating the question type. For example, consider Q
                        1
                        =“What are the hazards of global warming?” and Q
                        2
                        =“What were the casualties of the Indonesian tsunami?.” “Hazard” and “casualties” are the corresponding question focuses based on the above definition. Note that the question focus used here resembles the term “lexical answer type (LAT)” (Ferrucci et al., 2010) in which answers to such factoid questions as “In which city is the River Seine?” must be instances of LATs. This paper uses “question focus” because answers to complex questions are sets of information nuggets.

To recognize the question type, we simply assume that the type of complex question is only determined by its question focus; we use question-type and question focus interchangeably in this paper. Based on this assumption, questions Q
                        1 and Q
                        2 belong to hazard and casualty-type questions, respectively. Krishnan et al. (2005) showed that (a) the accuracy of recognizing question types reached 92.2% using only question focuses and (b) the recognition accuracy of the question focuses was 84.6%. This indicates that most questions contain question focuses, and thus it is practical to use them in representing question types. This shifts the task of recognizing question types to recognizing question focuses in questions. Nevertheless, our approach to recognizing question types has the following potential disadvantages: (1) questions belonging to identical question types may variously use different question focuses and (2) questions do not necessarily contain a question focus. Identifying synonyms of question focuses can partially solve the first problem, and here this is introduced in Section 4.2. Patterns can be used to address the second problem, but that approach is beyond the scope of this paper.

We regard question focus recognition as a sequence-tagging problem and employ conditional random fields (CRFs) (Lafferty et al., 2001) because many studies have proven their consistent advantage in sequence tagging. We manually annotated 4770 questions with question focuses to train a CRF model that classifies each question word into a set of tags O
                        ={I
                        
                           B
                        , I
                        
                           I
                        , I
                        
                           O
                        }, where I
                        
                           B
                         is a word that begins a focus, I
                        
                           I
                         is a word occurring in the middle of a focus, and I
                        
                           O
                         is a word outside of a focus. In the feature templates used in the CRF model, 
                           
                              w
                              n
                           
                         and t
                        
                           n
                         refer to word and part-of-speech (POS), where n refers to the relative position from current word n
                        =0. These feature templates have four types: unigrams of 
                           
                              w
                              n
                           
                         and t
                        
                           n
                        , where n
                        =−2, −1, 0, 1, 2; bigrams of 
                           
                              w
                              n
                           
                           
                              w
                              
                                 n
                                 +
                                 1
                              
                           
                         and t
                        
                           n
                        
                        t
                        
                           n+1, where n
                        =−1, 0; trigrams of 
                           
                              w
                              n
                           
                           
                              w
                              
                                 n
                                 +
                                 1
                              
                           
                           
                              w
                              
                                 n
                                 +
                                 2
                              
                           
                         and t
                        
                           n
                        
                        t
                        
                           n+1
                        t
                        
                           n+2, where n
                        =−2, −1, 0; and bigrams of O
                        
                           n
                        
                        O
                        
                           n+1, where n
                        =−1, 0. We employ an open source CRF tool, CRF++,
                           9
                        
                        
                           9
                           
                              http://code.google.com/p/crfpp/.
                         and set the parameters to default values.

Among the 4770 questions obtained, 1500 were extracted for the test set, and the others were used for training. Our experiment shows that the precision of the CRF model on the test set was 89.5%. Offline, the CRF model was used to recognize question focuses in the questions of social Q&A pairs. The distributions of the identified question focuses are shown in Figs. 2 and 3
                        
                        . We recognized 103 question focuses for which the frequencies exceeded 10,000. The numbers of question focuses for which the frequencies exceeded 100, 1000, and 5000 are 4714, 807, and 194, respectively. Among the 4714 recognized question focuses, 87% were not included in the training questions. In the online phase, we used the CRF model to identify the question focuses of the test questions.

Once question types are recognized from social Q&A pairs, positive and negative training Q&A pairs for the recognized question types can be automatically learned.

For question-type X, the social Q&A pairs, for which the question focuses are the same as X, are regarded as basic positive Q&A pairs QA
                           
                              basic
                            of X-type questions. Formally, QA
                           
                              basic
                           
                           ={QA
                           
                              i
                           |QF
                           
                              i
                           
                           =
                           X}, where QA
                           
                              i
                            denotes a Q&A pair and QF
                           
                              i
                            denotes the question focus of QA
                           
                              i
                           . Table 4
                            gives the number of Q&A pairs for each type of question in an extension of the NTCIR 2008 test set (discussed in the experimental section). For example, 10,362 Q&A pairs are learned for answering hazard-type questions. Table 5
                            lists some of the questions used with their best answers as basic positive training pairs of the corresponding type of complex questions.

For question types such as casualty (
                              
                           )-type, for which only a few basic positive Q&A pairs are learned, it is possible to use Q&A pairs for similar question types like fatality (
                              
                           )-type. We adopted HowNet
                              10
                           
                           
                              10
                              We used the version released in 2000.
                            (Zhendong and Qiang, 1999), which is a lexical knowledge base with rich semantic information that serves as a powerful tool in meaning computation, for bootstrapping the basic positive Q&A pairs. In HowNet, a word may represent multiple concepts, each of which consists of a group of sememes. We demonstrate this with the following two examples: 
                              
                                 
                                    
                                    
                                       
                                          
                                             
                                                
                                             
                                          
                                       
                                    
                                 
                              
                            Here, word 
                              
                            (casualty) represents one concept (c
                           1) that is composed of four sememes, such as phenomena|
                              
                           . Word 
                              
                            (damage) has two concepts (c
                           1 and c
                           2), each of which consists of a sememe.

The similarity between the two words can be estimated by
                              
                                 
                                    sim
                                    (
                                    
                                       w
                                       1
                                    
                                    ,
                                    
                                       w
                                       2
                                    
                                    )
                                    =
                                    
                                       max
                                       
                                          1
                                          ≤
                                          i
                                          ≤
                                          |
                                          
                                             w
                                             1
                                          
                                          |
                                          ;
                                          1
                                          ≤
                                          j
                                          ≤
                                          |
                                          
                                             w
                                             2
                                          
                                          |
                                       
                                    
                                    sim
                                    (
                                    
                                       c
                                       i
                                    
                                    ,
                                    
                                       c
                                       j
                                    
                                    )
                                    ,
                                 
                              
                           
                           
                              
                                 
                                    sim
                                    (
                                    
                                       c
                                       i
                                    
                                    ,
                                    
                                       c
                                       j
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                1
                                                ≤
                                                k
                                                ≤
                                                |
                                                
                                                   c
                                                   i
                                                
                                                |
                                             
                                          
                                          
                                             max
                                             
                                                1
                                                ≤
                                                z
                                                ≤
                                                |
                                                
                                                   c
                                                   j
                                                
                                                |
                                             
                                          
                                          sim
                                          (
                                          
                                             se
                                             
                                                i
                                                ,
                                                k
                                             
                                          
                                          ,
                                          
                                             se
                                             
                                                j
                                                ,
                                                z
                                             
                                          
                                          )
                                       
                                       
                                          |
                                          
                                             c
                                             i
                                          
                                          |
                                       
                                    
                                    ,
                                 
                              
                           where c
                           
                              i
                            and c
                           
                              j
                            represent the ith and jth concepts of words 
                              
                                 w
                                 1
                              
                            and 
                              
                                 w
                                 2
                              
                           , respectively, 
                              |
                              
                                 w
                                 1
                              
                              |
                            is the number of concepts represented by 
                              
                                 w
                                 1
                              
                           , se
                           
                              i,k
                            denotes the kth sememe of concept c
                           
                              i
                           , |c
                           
                              i
                           | is the number of sememes of concept c
                           
                              i
                           , and sim(se
                           
                              i,k
                           , se
                           
                              j,z
                           ) is set to 1 if they are the same (otherwise, to 0). Using this algorithm, we get similarity sim(casualty|
                              
                           , damage|
                              
                           )=0.25.

Accordingly, the bootstrapping positive Q&A pairs QA
                           
                              boot
                            of X-type questions are composed of the Q&A pairs in which the question focuses resemble X. Formally, QA
                           
                              boot
                           
                           ={QA
                           
                              j
                           |sim(QF
                           
                              j
                           , X)>
                           θ
                           1}, where QF
                           
                              j
                            is the question focus of QA
                           
                              j
                            and θ
                           1 is the similarity threshold.

For each type of question, we also randomly selected Q&A pairs that do not contain question focuses and their similar words in questions as negative Q&A training pairs.

We preprocessed the training data, including word segmentation, POS tagging, named entity recognition (Wu et al., 2005), and dependency parsing (Chen et al., 2009). We also replaced each named entity
                              11
                           
                           
                              11
                              We used five unique entities: person, location, organization name, time, and numeric expressions.
                            with its tag type to extract the class/lexical-based cue expressions.

In this paper, we extracted two kinds of cue expressions: n-grams at the sequential level and dependency patterns at the syntactic level. Cue expression mining extracts a set of frequent class/lexical-based and POS-based subsequences that are indicative of the answers to a type of question. n-gram cue expressions include the following:
                           
                              •
                              Lexical unigrams, selected using formula 
                                    
                                       TFIDF
                                       w
                                    
                                    =
                                    
                                       tf
                                       w
                                    
                                    ×
                                    log
                                    (
                                    N
                                    /
                                    (
                                    
                                       
                                          df
                                          w
                                       
                                    
                                    )
                                    )
                                 , where 
                                    
                                       tf
                                       w
                                    
                                  denotes the frequency of word 
                                    w
                                 , 
                                    
                                       df
                                       w
                                    
                                  denotes the number of documents where word 
                                    w
                                  appears, and N is the total number of Q&A pairs. Table 6
                                  shows the most frequent unigrams learned for each type of question.

Class/lexical-based bigrams and trigrams that contain the selected unigrams. In theory, we could feed all features to the support vector machine (SVM) classifier and let it select the informative features, but we restricted the feature set for practical reasons: (1) the number of features would become tremendously large and (2) the feature vector would be highly sparse with a huge number of infrequent features, and thus the SVM learning would become very time-consuming. Therefore, we ranked the class/lexical-based bigrams and trigrams (including the following POS-based bigrams and dependency patterns) by their TFIDFs, and chose them at an appropriate percentage (in Section 6.2, we compare various percentages).

All POS-based unigrams and POS-based bigrams at an appropriate percentage.

A dependency pattern is defined as the relation among the words of a dependency tree. Fig. 4
                         shows an example. Similarly, we selected the frequent class/lexical-based and POS-based dependency patterns at an appropriate percentage. Note that we used only a percentage for both n-gram and dependency cue expressions.

We also assigned each extracted cue expression ce
                        
                           i
                         a weight calculated using 
                           
                              weight
                              
                                 
                                    ce
                                    i
                                 
                              
                           
                           =
                           
                              c
                              1
                              
                                 
                                    ce
                                    i
                                 
                              
                           
                           /
                           (
                           
                              c
                              1
                              
                                 
                                    ce
                                    i
                                 
                              
                           
                           +
                           
                              c
                              2
                              
                                 
                                    ce
                                    i
                                 
                              
                           
                           )
                        , where, 
                           
                              c
                              1
                              
                                 
                                    ce
                                    i
                                 
                              
                           
                         and 
                           
                              c
                              2
                              
                                 
                                    ce
                                    i
                                 
                              
                           
                         denote the frequencies in positive and negative Q&A training pairs, respectively. The weights are used as feature values in the SVM classifier.

As mentioned above, we used the extracted cue expressions and collected Q&A pairs for each type of question to build a question-type-specific classifier for removing noisy sentences from answer candidates. Many studies indicated that SVM performed very well in such classification tasks as question classification (Zhang and Lee, 2003), factoid QA (Suzuki et al., 2002), biographical sentence classification (Biadsy et al., 2008), and text classification (Joachims, 1998). Finally, we employed multivariate classification SVMs (Joachims, 2005)
                           12
                        
                        
                           12
                           
                              http://svmlight.joachims.org.
                         that can directly optimize a large class of performance measures like F1-Score, prec@k (precision of a classifier that predicts exactly k
                        =100 examples to be positive), and error-rate (percentage of errors in predictions). Instead of learning a univariate rule that predicts the label of a single example in conventional SVMs (Vapnik, 1998), multivariate SVMs formulate the learning problem as a multivariate prediction of all the examples in the data set. Considering hypotheses 
                           
                              h
                              ¯
                           
                         that map tuple 
                           
                              
                                 
                                    x
                                 
                              
                              ¯
                           
                         of n feature vectors 
                           
                              
                                 
                                    x
                                 
                              
                              ¯
                           
                           =
                           (
                           
                              
                                 
                                    
                                       x
                                       1
                                    
                                 
                              
                           
                           ,
                           .
                           .
                           .
                           ,
                           
                              
                                 
                                    
                                       x
                                       n
                                    
                                 
                              
                           
                           )
                         to tuple 
                           
                              y
                              ¯
                           
                         of n labels 
                           
                              
                                 y
                                 ¯
                              
                           
                           =
                           (
                           
                              y
                              1
                           
                           ,
                           .
                           .
                           .
                           ,
                           
                              y
                              n
                           
                           )
                        , multivariate SVMs learn a classifier
                           
                              (1)
                              
                                 
                                    
                                       
                                          h
                                          ¯
                                       
                                    
                                    
                                       
                                          w
                                       
                                    
                                 
                                 (
                                 
                                    
                                       
                                          x
                                       
                                    
                                    ¯
                                 
                                 )
                                 =
                                 
                                    argmax
                                    
                                       
                                          
                                             
                                                y
                                                ¯
                                             
                                          
                                          ′
                                       
                                       ∈
                                       
                                          
                                             Y
                                             ¯
                                          
                                       
                                    
                                 
                                 {
                                 
                                    
                                       
                                          w
                                       
                                    
                                    T
                                 
                                 Ψ
                                 (
                                 
                                    
                                       x
                                       ¯
                                    
                                 
                                 ,
                                 
                                    
                                       
                                          y
                                          ¯
                                       
                                    
                                    ′
                                 
                                 )
                                 }
                                 ,
                              
                           
                        by solving the following optimization problem:
                           
                              (2)
                              
                                 
                                    min
                                    
                                       
                                          
                                             w
                                          
                                       
                                       ,
                                       ξ
                                       ≥
                                       0
                                    
                                 
                                 
                                 
                                    1
                                    2
                                 
                                 ∥
                                 
                                    
                                       w
                                    
                                 
                                 
                                    ∥
                                    2
                                 
                                 +
                                 C
                                 ξ
                                 ,
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    
                                       
                                          s
                                          .
                                          t
                                          .
                                          :
                                          ∀
                                          
                                             
                                                
                                                   y
                                                   ¯
                                                
                                             
                                             ′
                                          
                                          ∈
                                          
                                             
                                                Y
                                                ¯
                                             
                                          
                                          ∖
                                          
                                             
                                                y
                                                ¯
                                             
                                          
                                          :
                                          
                                             
                                                
                                                   w
                                                
                                             
                                             T
                                          
                                          [
                                          Ψ
                                          (
                                          
                                             
                                                x
                                                ¯
                                             
                                          
                                          ,
                                          
                                             
                                                y
                                                ¯
                                             
                                          
                                          )
                                          −
                                          Ψ
                                          (
                                          
                                             
                                                x
                                                ¯
                                             
                                          
                                          ,
                                          
                                             
                                                
                                                   y
                                                   ¯
                                                
                                             
                                             ′
                                          
                                          )
                                          ]
                                          ≥
                                          Δ
                                          (
                                          
                                             
                                                
                                                   y
                                                   ¯
                                                
                                             
                                             ′
                                          
                                          ,
                                          
                                             
                                                y
                                                ¯
                                             
                                          
                                          )
                                          −
                                          ξ
                                          ,
                                       
                                       
                                    
                                 
                              
                           
                        where w is a parameter vector, Ψ is a function that returns a feature vector describing the match between (x1
                        
                        ...
                        xn
                        ) and (
                           
                              y
                              1
                              ′
                           
                           .
                           .
                           .
                           
                              y
                              n
                              ′
                           
                        ), Δ denotes the types of multivariate loss functions, and ξ is a slack variable.

Our complex QA system is related to the query-based summarization task of DUC (Document Understanding Conferences) (Dang, 2006; Harabagiu et al., 2006), and in this regard LexRank (Erkan and Radev, 2004; Otterbacher et al., 2005) is commonly used in query-based summarization tasks. Therefore, we first re-implement LexRank as a baseline. In addition, previous studies proposed the QSM (Kaisser et al., 2006; Chen et al., 2006) and MTM methods (Berger and Lafferty, 1999; Riezler et al., 2007; Xue et al., 2008; Bernhard and Gurevych, 2009) for generating and retrieving answers to complex questions. In this paper, we also re-implement those approaches for performance comparison.

The complex QA system discussed in this paper is also related to the query-based summarization of DUC (Dang, 2006; Harabagiu et al., 2006), which synthesizes a fluent, well-organized 250-word summary for a given topic description and a collection of manually generated relevant documents. The topic descriptions usually consist of several complex questions such as “Describe the theories on the causes and effects of global warming and the arguments against them.” LexRank is commonly used in query-based summarization tasks (Erkan and Radev, 2004; Otterbacher et al., 2005; Chali et al., 2009).

The concept of LexRank is illustrated by the following model:
                           
                              (4)
                              
                                 p
                                 (
                                 
                                    a
                                    i
                                 
                                 |
                                 Q
                                 )
                                 =
                                 d
                                 ×
                                 
                                    
                                       rel
                                       (
                                       
                                          a
                                          i
                                       
                                       |
                                       Q
                                       )
                                    
                                    
                                       
                                          ∑
                                          z
                                       
                                       rel
                                       (
                                       z
                                       |
                                       Q
                                       )
                                    
                                 
                                 +
                                 (
                                 1
                                 −
                                 d
                                 )
                                 ×
                                 
                                    ∑
                                    v
                                 
                                 
                                    
                                       sim
                                       (
                                       
                                          a
                                          i
                                       
                                       ,
                                       v
                                       )
                                    
                                    
                                       
                                          ∑
                                          z
                                       
                                       sim
                                       (
                                       z
                                       ,
                                       v
                                       )
                                    
                                 
                                 ×
                                 p
                                 (
                                 v
                                 |
                                 Q
                                 )
                                 ,
                              
                           
                        where p(a
                        
                           i
                        |Q) is the score of sentence a
                        
                           i
                         given question Q, which is determined as the sum of its relevance to the question and the similarity to the other sentences among the candidates, and d is “question bias,” which is a trade-off between relevance and similarity. For details, refer to the previous works (Otterbacher et al., 2005; Chali et al., 2010).

Eq. (4) can be explained as follows. Candidate a
                        
                           i
                        , which achieved a high relevance score, is likely to contain an answer; candidate a
                        
                           j
                        , which may not resemble the question itself, is also likely to contain an answer if it resembles a
                        
                           i
                        .

QSM first learns the potential answer words to the question and then re-ranks the candidates by incorporating their “similarities” with the answer words. Fig. 5
                         shows the architecture. For each submitted question, the following steps are performed. (1) In Retrieving Similar Q&A Pairs, an information retrieval (IR) algorithm retrieves the most similar Q&A pairs (the top 50 in our experiments) to the question from the social Q&A collection. (2) The Learning Answer Profile gives weights to all of the non-stop words from the retrieved Q&A pairs using the TFIDF scores and then selects the top M words to form an answer profile: Ap. (3) The Answer Ranker re-ranks the answer candidates based on a similarity formula sim(a
                        
                           i
                        )=
                        γrel(a
                        
                           i
                        |Q)+(1−
                        γ)sim(a
                        
                           i
                        , Ap), where rel(a
                        
                           i
                        |Q) denotes the relevance between question Q and candidates a
                        
                           i
                        , sim(a
                        
                           i
                        , Ap) means the similarity between candidates and answer profile Ap, and γ is the question bias weight that is empirically set to 0.95 in our experiments. (4) Finally, the top N candidates are selected as answers to question Q.

Following the earlier work (Otterbacher et al., 2005; Chali et al., 2010), we computed rel(a
                        
                           i
                        |Q) and sim(a
                        
                           i
                        , Ap) using the following equations:
                           
                              (5)
                              
                                 rel
                                 (
                                 
                                    a
                                    i
                                 
                                 |
                                 Q
                                 )
                                 =
                                 
                                    ∑
                                    
                                       w
                                       ∈
                                       Q
                                    
                                 
                                 log
                                 (
                                 
                                    tf
                                    
                                       w
                                       ,
                                       
                                          a
                                          i
                                       
                                    
                                 
                                 +
                                 1
                                 )
                                 ×
                                 log
                                 (
                                 
                                    tf
                                    
                                       w
                                       ,
                                       Q
                                    
                                 
                                 +
                                 1
                                 )
                                 ×
                                 
                                    idf
                                    w
                                 
                                 ,
                              
                           
                        
                        
                           
                              (6)
                              
                                 sim
                                 (
                                 
                                    a
                                    i
                                 
                                 ,
                                 Ap
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             w
                                             ∈
                                             
                                                a
                                                i
                                             
                                             ,
                                             Ap
                                          
                                       
                                       
                                          tf
                                          
                                             w
                                             ,
                                             
                                                a
                                                i
                                             
                                          
                                       
                                       
                                          tf
                                          
                                             w
                                             ,
                                             Ap
                                          
                                       
                                       
                                          
                                             (
                                             
                                                idf
                                                w
                                             
                                             )
                                          
                                          2
                                       
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   
                                                      x
                                                      i
                                                   
                                                   ∈
                                                   
                                                      a
                                                      i
                                                   
                                                
                                             
                                             
                                                
                                                   (
                                                   
                                                      tf
                                                      
                                                         
                                                            x
                                                            i
                                                         
                                                         ,
                                                         
                                                            a
                                                            i
                                                         
                                                      
                                                   
                                                   
                                                      idf
                                                      
                                                         
                                                            x
                                                            i
                                                         
                                                      
                                                   
                                                   )
                                                
                                                2
                                             
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   
                                                      y
                                                      i
                                                   
                                                   ∈
                                                   Ap
                                                
                                             
                                             
                                                
                                                   (
                                                   
                                                      tf
                                                      
                                                         
                                                            y
                                                            i
                                                         
                                                         ,
                                                         Ap
                                                      
                                                   
                                                   
                                                      idf
                                                      
                                                         
                                                            y
                                                            i
                                                         
                                                      
                                                   
                                                   )
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              tf
                              
                                 w
                                 ,
                                 x
                              
                           
                         is the number of times 
                           w
                         appears in x.

QSM was first proposed for answering definitional questions and TREC “other” questions (Kaisser et al., 2006; Chen et al., 2006); however, it learns answer words from the most relevant snippets returned by a web search engine. Section 6 compares a QSM based on the 50 most relevant social Q&A pairs with a QSM based on the 50 most relevant snippets returned by Yahoo!.

MTM learns the word-to-word translation probability from all social Q&A pairs without considering the question and the question type to improve complex QA systems. Fig. 6
                         shows the architecture of an MTM-based system.

A monolingual translation-based method treats Q&A pairs as a parallel corpus, where the questions correspond to the “source” language and the answers to the “target” language. Monolingual translation models have recently been introduced to solve the lexical gap problem in IR and QA systems (Berger and Lafferty, 1999; Riezler et al., 2007; Xue et al., 2008; Bernhard and Gurevych, 2009). A monolingual translation-based method for our complex QA system can be expressed by:
                           
                              (7)
                              
                                 
                                    
                                       
                                       
                                          P
                                          (
                                          Q
                                          |
                                          
                                             a
                                             i
                                          
                                          )
                                          =
                                          
                                             ∏
                                             
                                                w
                                                ∈
                                                Q
                                             
                                          
                                          (
                                          (
                                          1
                                          −
                                          γ
                                          )
                                          
                                             P
                                             mx
                                          
                                          (
                                          w
                                          |
                                          
                                             a
                                             i
                                          
                                          )
                                          +
                                          γ
                                          
                                             P
                                             ml
                                          
                                          (
                                          w
                                          |
                                          C
                                          )
                                          )
                                          ,
                                       
                                    
                                    
                                       
                                       
                                          
                                             P
                                             mx
                                          
                                          (
                                          w
                                          |
                                          
                                             a
                                             i
                                          
                                          )
                                          =
                                          (
                                          1
                                          −
                                          ζ
                                          )
                                          
                                             P
                                             ml
                                          
                                          (
                                          w
                                          |
                                          
                                             a
                                             i
                                          
                                          )
                                          +
                                          ζ
                                          
                                             ∑
                                             
                                                t
                                                ∈
                                                S
                                             
                                          
                                          P
                                          (
                                          w
                                          |
                                          t
                                          )
                                          
                                             P
                                             ml
                                          
                                          (
                                          t
                                          |
                                          
                                             a
                                             i
                                          
                                          )
                                          ,
                                       
                                    
                                 
                              
                           
                        where Q is the question, a
                        
                           i
                         is the candidate answer, γ is the smoothing parameter for the entire Q&A collection C, 
                           
                              P
                              ml
                           
                           (
                           t
                           |
                           
                              a
                              i
                           
                           )
                           =
                           
                              
                                 #
                                 (
                                 t
                                 ,
                                 
                                    a
                                    i
                                 
                                 )
                              
                              
                                 |
                                 
                                    a
                                    i
                                 
                                 |
                              
                           
                        , #(t, a
                        
                           i
                        ) denotes the frequency of term t in a
                        
                           i
                        , and 
                           P
                           (
                           w
                           |
                           t
                           )
                         is the probability of translating answer term t to question term 
                           w
                        , which is obtained using GIZA++ (Och and Ney, 2003).

Adopting the common practice used in translation-based retrieval, we utilized IBM Model 1 to obtain word-to-word probability 
                           P
                           (
                           w
                           |
                           t
                           )
                         from 6.0 million social Q&A pairs. The preprocessing of the Q&A pairs only involved word segmentation (Wu et al., 2005) and stop-word removal.

@&#EXPERIMENTS@&#

As explained in Section 4.1, since hundreds of types of complex questions exist, it is difficult to evaluate our approach on all of them. In this paper, we used the question types contained in the NTCIR 2008 test set (Mitamura et al., 2008, 2010), which contains the 30 complex questions
                        13
                     
                     
                        13
                        The definitional, biography and relationship questions in the NTCIR 2008 test set are not discussed here.
                      we discuss here. However, only a few test questions are included for certain question types. For example, the NTCIR 2008 test set contains only one hazard-type, one scale-type, and three significance-type questions. To form a more complete test set, we created another 57 test questions by SEPIA,
                        14
                     
                     
                        14
                        
                           http://code.google.com/p/sepia.
                      which was also used for the official evaluation of NTCIR 2008. The test data used in this paper include 87 questions and are considered an extension of the NTCIR 2008 test data set. For each test question, we also provide a list of weighted answer nuggets as the gold standard answers for evaluation. Three independent assessors voted on the answer nuggets. Inter-assessor agreement was measured by Cohen's Kappa statistic (Cohen, 1960). Cohen's Kappa was 0.54, which shows moderate agreement (Landis and Koch, 1977). This suggests the risk of relying on votes from a single assessor. Therefore, the answer nuggets are weighted based on the annotations of three assessors and a previously proposed weighting scheme (Lin and Demner-Fushman, 2006). Table 7
                      shows an example. The evaluation was conducted by employing a Pourpre v1.0c tool that uses the standard scoring methodology for TREC “other” questions (Voorhees, 2003).
                        15
                     
                     
                        15
                        Pourpre is a technique for automatically evaluating answers to definitional and complex questions. It simple counts the answer nuggets that are covered by the system response by examining nontrivial unigrams shared between them.
                      Each question is scored using nugget recall NR, nugget precision NP, and combination score F3 of NR and NP (Lin and Demner-Fushman, 2006). The final score of a system run is the mean of the scores across all test questions.


                        Table 8
                         summarizes the evaluation results of the systems. The baseline system is a special case of LexRank in which question bias weight d in Eq. (4) was set to 1. QSM
                           web
                         and QSM
                           qa
                         are QSMs that learned the answer words from the web and social Q&A pairs, respectively. QTSM
                           prec
                         denotes a QTSM based on classifier optimizing performance prec@k. It is important to note that comparison with the NTCIR-2008 participant systems is difficult because the automatic evaluation results on the complex questions discussed here were not reported (Mitamura et al., 2008).

This table indicates that complex QA performance can be clearly improved by exploiting social Q&A collections. In particular, we observe the following:
                           
                              1.
                              LexRank only slightly improved the baseline. A query-based summarization task is given a set of manually generated relevant documents, but our QA systems need to automatically retrieve relevant documents, and much noise exists that might negatively impact LexRank.

QTSM obtained the best performance: the F3 improvements of QTSM
                                    prec
                                  over MTM and QSM
                                    qa
                                  in terms of N
                                 =10 are 5.8% and 6.0%, respectively. The improvements are significant at the p
                                 =0.01 level using two-sided t-tests.

QSM
                                    qa
                                  outperformed QSM
                                    web
                                  by 2.0% in terms of the F3 metric when N
                                 =10. Further analysis shows that the average number of gold-standard answer words learned in QSM
                                    web
                                  (42.9%) is smaller than that of those learned in QSM
                                    qa
                                  (58.1%), perhaps because the Q&A pairs are more complete and complementary than snippets that only contain the length-limited contexts of question words. This proves that answer profile Ap learned from social Q&A pairs is superior to that from the snippets returned by a conventional web search engine.

The performance ranking of these models is: QTSM
                                    prec
                                  > {MTM, QSM
                                    qa
                                 } > QSM
                                    web
                                  > {LexRank, Baseline}. QSM
                                    qa
                                  depends on very specific knowledge, i.e. answer words to each question, which may fail if the social Q&A collection does not contain similar Q&A pairs or similar Q&A pairs do not contain answer words to the question. MTM learns very general knowledge from a social Q&A collection, i.e., word-to-word translation probability, which is not the most suitable for any question, any type of question, or any domain question. QTSM
                                    prec
                                 , however, learns question-type-specific salient expressions, which have granularity between QSM
                                    qa
                                  and MTM. This may explain why QTSM
                                    prec
                                  achieves better performance.

For further comparison, Fig. 7
                         shows how well QTSM
                           prec
                         performs for each type of question when N
                        =10. This figure clarifies that our method improves QSM
                           qa
                         on most types of test questions; the F3 improvements on function and hazard-type questions are 20.0% and 14%, respectively. Note that QSM
                           qa
                         outperforms QTSM
                           prec
                         on event-type questions. We interpret this to mean that the extracted salient cue expressions may not characterize answers to event-type questions. More complex features such as the templates used in MUC-3
                           16
                        
                        
                           16
                           
                              http://www-nlpir.nist.gov/related_projects/muc/index.html.
                         may be needed. Fig. 8
                         shows the NR recall curves of the three models that characterize the amount of relevant information contained within a fixed-length text segment (Lin, 2007). QTSM
                           prec
                         greatly improved MTM and QSM
                           qa
                         at every answer length. For example, the improvement of QTSM
                           prec
                         over MTM is about 10% when the answer length is 400 words, although we found no remarkable difference between MTM and QSM
                           qa
                        .

To find the optimal number of lexical unigrams that are important to extract bigram and trigram cue expressions (as described in Section 4.3), we conducted comparison experiments in which only lexical unigrams are used, and the results are shown in Table 9
                        . The experiments indicate that QTSM
                           prec
                         achieved the best F3 score when the number of lexical unigrams was set to 3000.


                        Table 10
                         illustrates the performance of QTSM
                           prec
                         based on various percentages for selecting bigram, trigram, and dependency cue expressions (as described in Section 4.3). In this table, 10% means we fed 10% of the extracted cue expressions to the SVM classifier. From our experiment, we set the percentage to 50%.

To evaluate the contributions of the individual features to QTSM, we gradually added them in this experiment. Table 11
                         shows the performance of QTSM
                           prec
                         on different sets of features. Lex and POS represent class/lexical-based and POS-based n-gram cue expressions. This table clarifies that all of the lexical and POS features can positively impact the F3, NR and NP metrics. However, the dependency patterns significantly improve the NR metric by 1.5% but only slightly decrease the NP metric. Finally, the F3 improvement from dependency patterns is relatively small, e.g. 0.7%. This experiment's results imply that longer sentences are ultimately selected as answers after using dependency patterns.

Pourpre v1.0c evaluation is based on the n-gram overlap between the automatically produced answers and the human-generated reference answers. Consequently, it cannot measure conceptual equivalents. In subjective evaluations, the answer sentences returned by the QA systems were labeled by two native Chinese assessors. Given a pair of answers for each question, they determined which summary had better content for the question or whether both were equally responsive. If their judgments differed, they conferred until they reached a final judgment. Cohen's Kappa of the inter-assessor decision was 0.71 (substantial agreement). Such an evaluation was also carried out in previous work (Biadsy et al., 2008; Liu et al., 2008). The results shown in Table 12
                         indicate that QTSM
                           prec
                         is much better than MTM and QSM
                           qa
                        . For example, 56.3% of these judgments preferred the answers produced by QTSM
                           prec
                         over those produced by MTM. Note that this subjective evaluation is a relative comparison among systems, which is faster than the subjective evaluation method in NTCIR-8. Table 13
                         compares the top three answers to question Q
                        1 answered by QSM
                           qa
                        , MTM, and QTSM
                           prec
                        .

@&#RELATED WORK@&#

Research on complex question answering can be classified into two categories: Retrieval complex QA involves finding good question-answer pairs in social Q&A sites and online forums for new queried questions; Generation complex QA focuses on inferring and synthesizing answers from multiple documents (Dang, 2006; Chali et al., 2009). Our work belongs to the latter category.

As with most information retrieval tasks, the major challenge for a retrieval complex QA is word mismatch between the user's question and the question-answer pairs on the social Q&A sites. To solve the word mismatch problem, many approaches have been proposed (Wang et al., 2010; Zhou et al., 2013; Qiu et al., 2013). One study (Xue et al., 2008) proposed a retrieval model that combines a translation-based language model for the question part with a query likelihood model for the answer part. Another approach (Surdeanu et al., 2008) put forth a supervised ranking model for this task and investigated a wide range of feature types, including similarity features, translation features, density/frequency features, and web correlation features. One attempt (Duan et al., 2008) used the MDL (minimum description length)-based tree cut model for identifying the question topic and question focus and then modeled them in a language-modeling framework for retrieval. Another (Luo et al., 2013) focused on extracting and ranking terms and phrases that are critically important for retrieving answers from social Q&A data sets. Recently, a deep belief network was proposed to model the semantic relevance for question-answer pairs (Wang et al., 2010).

However, retrieval complex QA cannot generate answers to questions that have not been solved on social sites. Even for questions that have been answered, it is necessary to carry out answer summarization (Liu et al., 2008; Chan et al., 2012). This is because the “best answer” of a complex question misses valuable information that is contained in other answers. Going through tedious and redundant answers to look for exact answers would be very time-consuming.

To solve the above problems, generation complex QA is proposed, which aims to find answer sentences from a set of relevant documents to a complex question and synthesize them to form the final summarized answer. The graph-based random walk method (Erkan and Radev, 2004; Otterbacher et al., 2005) has proven to be very successful for generic summarization. This method, however, only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information. Later, another work (Chali et al., 2010) considered the various textual similarity measurement techniques of the ROUGE similarity measure, BE (basic element) overlap, syntactic similarity measure, semantic similarity measure, and extended string subsequence kernel. Recently, a study (Chali and Hasan, 2012) used ILP (integer linear programming)-based sentence compression models with syntactic and semantic information for complex QA. Another work (Oh et al., 2013) recognized the intra- and inter-sentential causal relations between terms or clauses as evidence for answering why-questions. Though significant progress has been made on incorporating lexical, syntactic, and semantic analysis, the lexical mismatch/gap between question and answer (as shown in Table 3) is still very challenging. Among many proposed approaches, one (Chali and Hasan, 2012) used synonym, hypernym/hyponym in WordNet to bridge the lexical gap. Another (Morita et al., 2011) constructed a co-occurrence graph to obtain words that augment the original question terms. QSM (introduced in Section 5.2) has been proposed to learn question-dependent answer words from a small number of relevant snippets returned by a Web search engine (Kaisser et al., 2006; Chen et al., 2006). Consequently, the learned answer words may be very noisy. MTM (discussed in Section 5.3) learned word-to-word translation probabilities for bridging the gap between question words and answer words (Berger and Lafferty, 1999; Riezler et al., 2007; Xue et al., 2008; Bernhard and Gurevych, 2009). Although MTM relies on large-scale Q&A collection, it learns general knowledge, which is not the most suitable kind for any given question.

In this paper, we regard social Q&A collections as external knowledge repositories and mine question-type-specific knowledge (lexical, syntactic, structural) from them to address the word mismatch problem. To the best of our knowledge, work taking this point of view is scarce. One group (Mori et al., 2008) proposed a QSM method for improving complex Japanese QA. They simply collected Q&A pairs based on seven-grams whose centers are interrogatives for learning answer words.

In addition, the TREC and NTCIR participant systems have also proposed many interesting approaches. One of them (Harabagiu et al., 2006) introduced a new paradigm for processing complex questions that relies on question decompositions (of the complex question) and multi-document summarization techniques (to fuse together the answers provided for each decomposed question). Another work (Shima et al., 2008) treated complex question answering as a sequential classification and a multi-document summarization task. Several other systems employed pattern-based approaches (Harabagiu et al., 2005; Shima and Mitamura, 2010).

@&#CONCLUSION@&#

This paper investigated techniques for mining knowledge from social Q&A websites to improve a sentence-based complex QA system. Our proposed question-type-specific method (QTSM) explored social Q&A collections to automatically learn question-type-specific Q&A training pairs and cue expressions and created a question-type-specific classifier for each type of question to filter out noise sentences before answer selection. Experiments on the extension of NTCIR 2008 test questions indicated that QTSM is more effective than question-specific (QSM) and monolingual translation-based (MTM) methods. The largest improvements in F3 over QSM and MTM reached 6.0% and 5.8%, respectively.

The main limitations of our approach are, first of all, as mentioned in Section 4.1, the fact that complex questions (such as “What causes global warming?”) having no explicit question focuses are beyond the scope of this paper and cannot be adequately handled by our approach, second, the same types of cue expressions are used for all questions, which should be improved in the future. For example, discourse expressions might be effective to answer procedure-type questions such as “How do I assemble a bicycle?” For event-type questions, event-templates need to be considered.

Future work will therefore focus on solving these key limitations: (1) reducing noise in the training Q&A pairs; (2) designing more characteristic cue expressions for various types of questions, such as event-templates for event-type questions (Chinchor, 1991); (3) adapting QTSM to summarize answers in social QA sites (Liu et al., 2008; Chan et al., 2012); (4) learning paraphrases to recognize types of questions that do not contain question focuses – for example, the question, “What causes global warming?” can be classified as a reason-type question by using the paraphrases, “what causes X” and “what is the reason for X”; and (5) adapting the QA system to a topic-based summarization system, which will, for instance, summarize accidents based on “casualty” and “reason” and events according to “reason,” “measure” and “impact.”

@&#ACKNOWLEDGEMENTS@&#

We would like to thank this paper's reviewers for their very constructive and detailed comments.

@&#REFERENCES@&#

