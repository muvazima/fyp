@&#MAIN-TITLE@&#Improved open-vocabulary spoken content retrieval with word and subword lattices using acoustic feature similarity

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Acoustic feature similarity between spoken segments can improve spoken term detection.


                        
                        
                           
                           Pseudo-relevance feedback and graph-based re-ranking approach using acoustic feature similarity are proposed.


                        
                        
                           
                           A generalized framework for these approaches is presented.


                        
                        
                           
                           Significant improvements with both in-vocabulary and out-of-vocabulary queries were observed.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Spoken content retrieval

Spoken term detection

Pseudo-relevance feedback

Random walk

@&#ABSTRACT@&#


               
               
                  Spoken content retrieval will be very important for retrieving and browsing multimedia content over the Internet, and spoken term detection (STD) is one of the key technologies for spoken content retrieval. In this paper, we show acoustic feature similarity between spoken segments used with pseudo-relevance feedback and graph-based re-ranking can improve the performance of STD. This is based on the concept that spoken segments similar in acoustic feature vector sequences to those with higher/lower relevance scores should have higher/lower scores, while graph-based re-ranking further uses a graph to consider the similarity structure among all the segments retrieved in the first pass. These approaches are formulated on both word and subword lattices, and a complete framework of using them in open vocabulary retrieval of spoken content is presented. Significant improvements for these approaches with both in-vocabulary and out-of-vocabulary queries were observed in preliminary experiments.
               
            

@&#INTRODUCTION@&#

In the Internet era, digital content over the Internet covers almost all the information and activities of human life. The most attractive form of network content is multimedia, which includes audio signals. The subjects, topics, and core concepts of such multimedia content can very often be identified based on the speech information within the audio part of the content. Hence, in the future, spoken content retrieval will be very important in helping users retrieve and browse efficiently across the huge qualities of multimedia content (Lee and Chen, 2005). Spoken term detection (STD) is a subtask of the above spoken content retrieval, in which the query is a term (a word or a phrase of a few words) in text form and a spoken segment is taken as relevant if it includes the query term. The work of this paper is primarily for STD, although it is certainly possible to generalize the discussions here to other tasks in spoken content retrieval.

Substantial research has been conducted in spoken content retrieval, and many successful techniques have been developed. Lattice-based approaches taking into account multiple recognition hypotheses (Saraclar, 2004; Chelba et al., 2007) have been used to mitigate the relatively low accuracy of 1-best transcriptions. Lattices were usually converted into sausage-like structures to facilitate indexing and to reduce memory requirements. Examples of such sausage-like lattice-based structures include position-specific posterior lattices (PSPL) (Chelba and Acero, 2005; Pan and Lee, 2007) and confusion networks (CN) (Mangu et al., 2000; Hori et al., 2007; Pan and Lee, 2007). As an alternative, the weighted finite state transducer (WFST) algorithm can also be used to index and retrieve lattices (Allauzen et al., 2004). The out-of-vocabulary (OOV) query is another important issue because queries often contain OOV words (Logan et al., 2000). The most fundamental approach for handling the OOV problem is to represent both the queries and the spoken segments by subword units and then match them on the subword unit level (Akbacak et al., 2008; Pan et al., 2007; Logan et al., 2005; Wallace et al., 2007; Turunen, 2008; Turunen and Kurimo, 2007; Wang et al., 2008; Itoh et al., 2007; Garcia and Gish, 2006; Ng, 2000). Word-based and subword-based indexing can be further integrated to yield better performance (Pan et al., 2007). Many successful applications of spoken content retrieval have been demonstrated including those for broadcast news (Pan et al., 2012), course lectures (Kong et al., 2009; Glass et al., 2007), historical spoken archives (Hansen et al., 2004; Oard et al., 2004), podcasts (Goto et al., 2007), and YouTube videos (Alberti et al., 2009).

In general, there are two stages in conventional spoken content retrieval (Chelba et al., 2008). In the first stage, the audio content is recognized and transformed into transcriptions or lattices by a recognition engine using a set of acoustic models and language models. In the second stage, after the user enters a query, the retrieval engine searches through the recognition output and returns a list of relevant spoken segments to the user. The returned segments are usually ranked by the relevance scores derived from the recognition output. In the above two-stage framework, the spoken content retrieval techniques were actually applied on top of ASR output, either 1-bests or lattices. The performance of spoken content retrieval is thus inevitably limited by ASR performance, and in many cases the ASR performance is still unpredictable today, especially for spontaneous speech produced under adverse environments, and speech produced in the languages with limited resources for acoustic and language model training (Akbacak, 2009). Also, in many application tasks, it is practically very difficult, if not impossible, to obtain acoustic and language models that are robust enough to produce good ASR performance for the huge quantities of target spoken archives which are generated under different acoustic conditions by larger number of speakers for different scenarios with different subject domains. It is not surprising that such mismatched models may result in relatively poor recognition output. In such cases even very robust retrieval approaches are not able to compensate for the recognition errors.

In text-based information retrieval, even if the texts to be retrieved include all precise words, it is still difficult to retrieve precisely all documents relevant to the query. One major reason for this is many queries are too short to completely represent the user's intent. However, related documents may have many words in common; thus if a given document contains many words that also appear in documents judged to be relevant in the first retrieval pass, this document may have a higher probability to be relevant. In other words, it is possible to get better retrieval results by considering the “similarity” (common words) between documents. Pseudo-relevance feedback (PRF) (Kurland et al., 2005; Tao and Zhai, 2006; Cao et al., 2008; Lv and Zhai, 2009, 2010), also known as blind relevance feedback, is one way to accomplish this. PRF assumes that a small number of top-ranked documents in the first-pass retrieved results are relevant (or “pseudo-relevant”), and sometimes also assumes that low-ranked documents are irrelevant (or “pseudo-irrelevant”); the documents retrieved in the first pass are then re-ranked based on their similarity (and dissimilarity) to the pseudo-relevant (and/or -irrelevant) documents.

If the spoken content to be retrieved from can be transcribed into text, the PRF methods developed for text information retrieval can be directly applied on the transcriptions (Zhou, 2003; Lee et al., 2012a). However, since the transcriptions may include many recognition errors, when transcribing speech signals into text, much information is lost and not recoverable. A better approach of utilizing the idea of PRF is not directly applying it on the transcriptions but on the speech signal level with a hope to better use the information carried by the signals (Parada et al., 2009; Chen et al., 2010). The basic idea is if a spoken segment has an acoustic feature vector sequence very similar in some way to those of other spoken segments judged to include the target query term in the first retrieval pass, it may have a higher probability to include the target query term. In this approach, given a user query, the retrieval engine first searches through the lattices to produce a first-pass returned list ranked according to a relevance score directly derived from the lattices. The returned segments with the highest and lowest relevance scores are then respectively defined as the pseudo-relevant and -irrelevant sets. The similarities between each first-pass retrieved spoken segment and the pseudo-relevant and -irrelevant sets can be computed based on the acoustic feature vector sequences of the query hypotheses, and the first-pass returned list is re-ranked accordingly. Furthermore, it is also possible to take the pseudo-relevant and -irrelevant sets respectively as positive and negative examples to train a binary classifier by machine learning techniques, and then use the binary classifier to determine the relevance of the spoken segments retrieved in the first pass (Lee and Lee, 2013).

The PRF approach can be taken one step further with graph-based re-ranking (Chen et al., 2011). In this approach, for each query entered we construct a graph for the first-pass retrieved spoken segments, in which each node represents a spoken segment and the edges represent the acoustic feature similarity between the segments’ query hypotheses. With this graph, the above concept now translates to the concept that segments strongly connected to many segments with higher/lower scores on the graph should have higher/lower scores. The relevance scores for the segments therefore propagate over the graph, and the segments are re-ranked accordingly. In this way, all the spoken segments in the first-pass returned list are considered globally, rather than assuming pseudo-relevant and -irrelevant sets in the PRF approach. For STD utilizing machine learning models to determine the relevance of the spoken segments, similar graph-based concept have been applied for selecting pseudo training examples not restricted to top/bottom-ranked spoken segments in the first-pass retrieved results (Lee and Lee, 2013). These approaches are similar to the very successful PageRank (Langville and Meyer, 2005; Brin and Page, 1998) used to rank web pages; PageRank considers the hyperlink between every two pages and computes a converged importance score for each page. Similar approaches have also been found useful in video search (Hsu et al., 2007; Tian et al., 2008) and extractive summarization (Otterbacher et al., 2009; Lee et al., 2011), in which the similarities between each pair of videos or sentences
                        1
                     
                     
                        1
                        Or utterances for spoken documents.
                      are respectively used to formulate the ranking problem over graphs.

Although the approaches utilizing acoustic feature similarity in STD were shown to be helpful, in the prior works (Chen et al., 2010, 2011; Tu et al., 2011; Lee and Lee, 2013) they were formulated based on a relatively limited task in which the query includes only a single in-vocabulary (IV) word, and the whole retrieval process was based on word lattices. Here in this paper we focus on a generalized framework for these approaches for a more complete task: the query can be shorter or longer, including one to several words, IV or OOV, and the retrieval is considered on both word- and subword-based lattices. The formulations of the approaches proposed previously (Chen et al., 2010, 2011) are modified to consider the case that the query can include several words or be represented as a sequence of subword units. Furthermore, in this paper it is verified that these approaches can improve the retrieval performance of OOV queries, which has not been investigated before.

Part of the generalized framework was mentioned previously (Lee et al., 2012b), but here the influences of the sizes of pseudo-relevant/-irrelevant sets and different graph construction methods are explored, and deeper analysis based on detection error trade-off (DET) curves and the discussion of time complexity are also included in this paper. Below, the generalized framework for both approaches using PRF and graph-based re-ranking is presented in Section 2. Experiments are presented in Sections 3 and 4, and we conclude in Section 5.

The framework for the proposed approach for the task considered here is shown in Fig. 1
                     . The spoken segments are first transcribed into word or subword lattices by a speech recognizer. When the user enters a query, which can be shorter or longer including IV or OOV words, the retrieval engine searches over the lattices and produces the first-pass returned list as described in Section 2.1. The acoustic feature similarity between every two retrieved segments is then computed as presented in Section 2.2. Based on this similarity, the list is re-ranked using either pseudo-relevance feedback (PRF) in Section 2.3 or graph-based re-ranking in Section 2.4.

Given query Q, the system returns the spoken segments x
                        
                           i
                         with relevance scores R(x
                        
                           i
                        , Q) higher than a threshold, and ranks these segments according to the values of R(x
                        
                           i
                        , Q) as the first-pass retrieval result 
                           X
                        . The relevance score R(x
                        
                           i
                        , Q) used to produce the first-pass returned list can be derived from either word or subword lattices. Relevance scores from word lattices are usually more accurate than those from subword lattices, but we must rely on the latter when the query Q consists of OOV words. Below we first show how to determine R(x
                        
                           i
                        , Q) using word lattices, and then show that the subword-based scores can be similarly obtained from subword-based lattices.

We are given the query Q represented as one to several words, 
                           
                              Q
                              w
                           
                           =
                           {
                           
                              w
                              j
                           
                           ,
                           j
                           =
                           1
                           ,
                           2
                           ,
                           …
                           ,
                           N
                           }
                        , 
                           
                              w
                              j
                           
                         being the jth word and N the number of words in Q. To compute the word-based relevance score 
                           R
                           (
                           
                              x
                              i
                           
                           ,
                           
                              Q
                              w
                           
                           )
                         for a segment x
                        
                           i
                         from the word lattice, we calculate the expected count for each n-gram {
                           
                              w
                              k
                           
                           ,
                           …
                           ,
                           
                              w
                              
                                 k
                                 +
                                 n
                                 −
                                 1
                              
                           
                        }, k
                        =1, …, N
                        −
                        n
                        +1, in the query from the segment's lattice as in (1), and then aggregate the results for all such n-grams to produce the word-based n-gram score 
                           
                              R
                              
                                 n
                                 -gram
                              
                           
                           (
                           
                              x
                              i
                           
                           ,
                           
                              Q
                              w
                           
                           )
                         for each order of n in (2).


                        
                           
                              (1)
                              
                                 E
                                 [
                                 
                                    w
                                    k
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    
                                       k
                                       +
                                       n
                                       −
                                       1
                                    
                                 
                                 |
                                 
                                    x
                                    i
                                 
                                 ]
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             u
                                             ∈
                                             W
                                             (
                                             
                                                x
                                                i
                                             
                                             )
                                          
                                       
                                       P
                                       (
                                       
                                          x
                                          i
                                       
                                       |
                                       u
                                       )
                                       P
                                       (
                                       u
                                       )
                                       C
                                       (
                                       u
                                       ,
                                       {
                                       
                                          w
                                          k
                                       
                                       ,
                                       …
                                       ,
                                       
                                          w
                                          
                                             k
                                             +
                                             n
                                             −
                                             1
                                          
                                       
                                       }
                                       )
                                    
                                    
                                       
                                          ∑
                                          
                                             u
                                             ∈
                                             W
                                             (
                                             
                                                x
                                                i
                                             
                                             )
                                          
                                       
                                       P
                                       (
                                       
                                          x
                                          i
                                       
                                       |
                                       u
                                       )
                                       P
                                       (
                                       u
                                       )
                                    
                                 
                                 ,
                              
                           
                        where W(x
                        
                           i
                        ) is the set of all allowed paths in the lattice of x
                        
                           i
                        , u one of the allowed paths, P(x
                        
                           i
                        |u) the likelihood for the observation sequence of x
                        
                           i
                         given the path u based on the acoustic model set, P(u) the prior probability of u from the language model, and 
                           C
                           (
                           u
                           ,
                           {
                           
                              w
                              k
                           
                           ,
                           …
                           ,
                           
                              w
                              
                                 k
                                 +
                                 n
                                 −
                                 1
                              
                           
                           }
                           )
                         the occurrence count of the n-gram 
                           {
                           
                              w
                              k
                           
                           ,
                           …
                           ,
                           
                              w
                              
                                 k
                                 +
                                 n
                                 −
                                 1
                              
                           
                           }
                         in u, and


                        
                           
                              (2)
                              
                                 
                                    R
                                    
                                       n
                                       -gram
                                    
                                 
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    Q
                                    w
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    
                                       N
                                       −
                                       n
                                       +
                                       1
                                    
                                 
                                 E
                                 [
                                 
                                    w
                                    k
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    
                                       k
                                       +
                                       n
                                       −
                                       1
                                    
                                 
                                 |
                                 
                                    x
                                    i
                                 
                                 ]
                                 .
                              
                           
                        The different proximity types, one for each n-gram order n allowed by the query length, are finally integrated in a weighted sum to yield word-based relevance score 
                           R
                           (
                           
                              x
                              i
                           
                           ,
                           
                              Q
                              w
                           
                           )
                         for word lattices as


                        
                           
                              (3)
                              
                                 R
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    Q
                                    w
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       n
                                       =
                                       1
                                    
                                    N
                                 
                                 
                                    a
                                    n
                                 
                                 
                                    R
                                    
                                       n
                                       -gram
                                    
                                 
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    Q
                                    w
                                 
                                 )
                                 ,
                              
                           
                        where a
                        
                           n
                         is a weight parameter. Since 
                           R
                           (
                           
                              x
                              i
                           
                           ,
                           
                              Q
                              w
                           
                           )
                         here is the aggregation of all the possible n-grams in the query, segments that only partially match the query can still be retrieved; this may increase the recall rate of the retrieval results but not necessary decrease the precision if a
                        
                           n
                         are properly set (Meng et al., 2009).

To retrieve the subword-based lattices, the query Q is represented as a sequence of subword units instead, Q
                        
                           s
                        
                        ={s
                        
                           j
                        , j
                        =1, 2, …, M}, where s
                        
                           j
                         is the jth subword unit and M the number of subword units in Q. The subword-based relevance score R(x
                        
                           i
                        , Q
                        
                           s
                        ) can be obtained in exactly the same way as that in (1)–(3), except that E[s
                        
                           k
                        , …, s
                        
                           k+n−1|x
                        
                           i
                        ] is computed on a subword lattice.


                        
                           
                              (4)
                              
                                 E
                                 [
                                 
                                    s
                                    k
                                 
                                 ,
                                 …
                                 ,
                                 
                                    s
                                    
                                       k
                                       +
                                       n
                                       −
                                       1
                                    
                                 
                                 |
                                 
                                    x
                                    i
                                 
                                 ]
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             u
                                             ∈
                                             W
                                             (
                                             
                                                x
                                                i
                                             
                                             )
                                          
                                       
                                       P
                                       (
                                       
                                          x
                                          i
                                       
                                       |
                                       u
                                       )
                                       P
                                       (
                                       u
                                       )
                                       C
                                       (
                                       u
                                       ,
                                       {
                                       
                                          s
                                          k
                                       
                                       ,
                                       …
                                       ,
                                       
                                          s
                                          
                                             k
                                             +
                                             n
                                             −
                                             1
                                          
                                       
                                       }
                                       )
                                    
                                    
                                       
                                          ∑
                                          
                                             u
                                             ∈
                                             W
                                             (
                                             
                                                x
                                                i
                                             
                                             )
                                          
                                       
                                       P
                                       (
                                       
                                          x
                                          i
                                       
                                       |
                                       u
                                       )
                                       P
                                       (
                                       u
                                       )
                                    
                                 
                                 ,
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    R
                                    
                                       n
                                       -gram
                                    
                                 
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    Q
                                    s
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    
                                       M
                                       −
                                       n
                                       +
                                       1
                                    
                                 
                                 E
                                 [
                                 
                                    s
                                    k
                                 
                                 ,
                                 …
                                 ,
                                 
                                    s
                                    
                                       k
                                       +
                                       n
                                       −
                                       1
                                    
                                 
                                 |
                                 
                                    x
                                    i
                                 
                                 ]
                                 ,
                              
                           
                        
                        
                           
                              (6)
                              
                                 R
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    Q
                                    s
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       n
                                       =
                                       1
                                    
                                    M
                                 
                                 
                                    a
                                    n
                                 
                                 ′
                                 
                                    R
                                    
                                       n
                                       -gram
                                    
                                 
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    Q
                                    s
                                 
                                 )
                                 .
                              
                           
                        Here (4), (5) and (6) are exactly the same as (1), (2) and (3) except that the word 
                           
                              w
                              j
                           
                         is replaced by the subword unit s
                        
                           j
                        , R
                        
                           n-gram(x
                        
                           i
                        , Q
                        
                           s
                        ) and R(x
                        
                           i
                        , Q
                        
                           s
                        ) are subword-based n-gram score and subword-based relevance score respectively, and a
                        
                           n
                        ′ is the corresponding parameter.

Here the similarity S(x
                        
                           i
                        , x
                        
                           j
                        ) between the acoustic feature vector sequences for two retrieved segments x
                        
                           i
                         and x
                        
                           j
                        , referred to as acoustic feature similarity below, is computed, which will be used in both PRF and graph-based re-ranking in the next two subsections. S(x
                        
                           i
                        , x
                        
                           j
                        ) can be obtained again based on either word or subword units; here we show the word-based version first for demonstration.

Given query Q consisting of a sequence of words 
                           
                              Q
                              w
                           
                           =
                           {
                           
                              w
                              j
                           
                           ,
                           j
                           =
                           1
                           ,
                           2
                           ,
                           …
                           ,
                           N
                           }
                        , for each n-gram 
                           {
                           
                              w
                              k
                           
                           ,
                           …
                           ,
                           
                              w
                              
                                 k
                                 +
                                 n
                                 −
                                 1
                              
                           
                           }
                         in Q where k
                        =1, …, N
                        −
                        n
                        +1, the dynamic time warping (DTW) distance (Aradilla et al., 2006) is first calculated between the acoustic feature sequences corresponding to the subpaths with word hypothesis sequences 
                           {
                           
                              w
                              k
                           
                           ,
                           …
                           ,
                           
                              w
                              
                                 k
                                 +
                                 n
                                 −
                                 1
                              
                           
                           }
                         in the lattices of x
                        
                           i
                         and x
                        
                           j
                        
                        
                           2
                        
                        
                           2
                           If there are multiple subpaths whose word hypotheses are {
                                 
                                    w
                                    k
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    
                                       k
                                       +
                                       n
                                       −
                                       1
                                    
                                 
                              } in a lattice, only the one with the highest posterior probability is considered. Instead of picking the subpaths with the highest posterior probability, there are other reasonable alternatives. For example, first cluster the subpaths with the same hypothesis sequences and similar time spans into groups. Use the time span of the subpath with the highest posterior probability in each group to represent the time span of the group, and take the summation of the posterior probabilities of all the elements in a group as its score. Then use the time span of the group of subpaths {
                                 
                                    w
                                    k
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    
                                       k
                                       +
                                       n
                                       −
                                       1
                                    
                                 
                              } with the highest score to compute the DTW distances. However, this alternative did not lead to too much difference from picking the highest subpaths in terms of the experimental results, but required extra computing efforts, so for simplicity we do not report its results here.
                        . An example is shown in Fig. 2
                        . This yields 
                           d
                           (
                           
                              x
                              i
                           
                           ,
                           
                              x
                              j
                           
                           ;
                           {
                           
                              w
                              k
                           
                           ,
                           …
                           ,
                           
                              w
                              
                                 k
                                 +
                                 n
                                 −
                                 1
                              
                           
                           }
                           )
                        , the word-based DTW distance between x
                        
                           i
                         and x
                        
                           j
                         considering the n-gram {
                           
                              w
                              k
                           
                           ,
                           …
                           ,
                           
                              w
                              
                                 k
                                 +
                                 n
                                 −
                                 1
                              
                           
                        } in the query. The word-based similarity between x
                        
                           i
                         and x
                        
                           j
                         considering {
                           
                              w
                              k
                           
                           ,
                           …
                           ,
                           
                              w
                              
                                 k
                                 +
                                 n
                                 −
                                 1
                              
                           
                        } is then


                        
                           
                              (7)
                              
                                 S
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 ;
                                 {
                                 
                                    w
                                    k
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    
                                       k
                                       +
                                       n
                                       −
                                       1
                                    
                                 
                                 }
                                 )
                                 =
                                 1
                                 −
                                 
                                    
                                       d
                                       (
                                       
                                          x
                                          i
                                       
                                       ,
                                       
                                          x
                                          j
                                       
                                       ;
                                       {
                                       
                                          w
                                          k
                                       
                                       ,
                                       …
                                       ,
                                       
                                          w
                                          
                                             k
                                             +
                                             n
                                             −
                                             1
                                          
                                       
                                       }
                                       )
                                       −
                                       
                                          d
                                          min
                                       
                                    
                                    
                                       
                                          d
                                          max
                                       
                                       −
                                       
                                          d
                                          min
                                       
                                    
                                 
                                 ,
                              
                           
                        where d
                        
                           max
                         and d
                        
                           min
                         are the largest and smallest values of 
                           d
                           (
                           
                              x
                              i
                           
                           ,
                           
                              x
                              j
                           
                           ;
                           {
                           
                              w
                              k
                           
                           ,
                           …
                           ,
                           
                              w
                              
                                 k
                                 +
                                 n
                                 −
                                 1
                              
                           
                           }
                           )
                         for all pairs of segments in the first-pass returned list. Eq. (7) simply linearly normalizes the DTW distance and transforms it into a similarity score between 0 and 1. If the n-gram {
                           
                              w
                              k
                           
                           ,
                           …
                           ,
                           
                              w
                              
                                 k
                                 +
                                 n
                                 −
                                 1
                              
                           
                        } does not exist in the lattice of either x
                        
                           i
                         or x
                        
                           j
                        , 
                           S
                           (
                           
                              x
                              i
                           
                           ,
                           
                              x
                              j
                           
                           ;
                           {
                           
                              w
                              k
                           
                           ,
                           …
                           ,
                           
                              w
                              
                                 k
                                 +
                                 n
                                 −
                                 1
                              
                           
                           }
                           )
                         is set to 0. We then aggregate the similarities considering all such n-grams to produce word-based score 
                           
                              S
                              
                                 n
                                 -gram
                              
                           
                           (
                           
                              x
                              i
                           
                           ,
                           
                              x
                              j
                           
                           ;
                           
                              Q
                              w
                           
                           )
                         for each order of n as
                           
                              (8)
                              
                                 
                                    S
                                    
                                       n
                                       -gram
                                    
                                 
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 ;
                                 
                                    Q
                                    w
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    
                                       N
                                       −
                                       n
                                       +
                                       1
                                    
                                 
                                 S
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 ;
                                 {
                                 
                                    w
                                    k
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    
                                       k
                                       +
                                       n
                                       −
                                       1
                                    
                                 
                                 }
                                 )
                                 .
                              
                           
                        The different proximity types are finally integrated as a weighted sum to yield the word-based similarity between x
                        
                           i
                         and x
                        
                           j
                        :
                           
                              (9)
                              
                                 S
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 ;
                                 
                                    Q
                                    w
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       n
                                       =
                                       1
                                    
                                    N
                                 
                                 
                                    b
                                    n
                                 
                                 
                                    S
                                    
                                       n
                                       -gram
                                    
                                 
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 ;
                                 
                                    Q
                                    w
                                 
                                 )
                                 ,
                              
                           
                        where b
                        
                           n
                         is another weight parameter.

To retrieve subword-based lattices, the query Q is represented as a sequence of subword units instead, Q
                        
                           s
                        
                        ={s
                        
                           j
                        , j
                        =1, 2, …, M}. The computation of subword-based similarity S(x
                        
                           i
                        , x
                        
                           j
                        
                        ;
                        Q
                        
                           s
                        ) is exactly the same as that in (7)–(9), except that the word sequence 
                           
                              Q
                              w
                           
                         is replaced by Q
                        
                           s
                        , and each word 
                           
                              w
                              i
                           
                         is replaced by subword unit s
                        
                           j
                        .
                           
                              (10)
                              
                                 S
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 ;
                                 {
                                 
                                    s
                                    k
                                 
                                 ,
                                 …
                                 ,
                                 
                                    s
                                    
                                       k
                                       +
                                       n
                                       −
                                       1
                                    
                                 
                                 }
                                 )
                                 =
                                 1
                                 −
                                 
                                    
                                       d
                                       (
                                       
                                          x
                                          i
                                       
                                       ,
                                       
                                          x
                                          j
                                       
                                       ;
                                       {
                                       
                                          s
                                          k
                                       
                                       ,
                                       …
                                       ,
                                       
                                          s
                                          
                                             k
                                             +
                                             n
                                             −
                                             1
                                          
                                       
                                       }
                                       )
                                       −
                                       
                                          d
                                          min
                                          ′
                                       
                                    
                                    
                                       
                                          d
                                          max
                                          ′
                                       
                                       −
                                       
                                          d
                                          min
                                          ′
                                       
                                    
                                 
                                 ,
                              
                           
                        
                        
                           
                              (11)
                              
                                 
                                    S
                                    
                                       n
                                       -gram
                                    
                                 
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 ;
                                 
                                    Q
                                    s
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    
                                       M
                                       −
                                       n
                                       +
                                       1
                                    
                                 
                                 S
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 ;
                                 {
                                 
                                    s
                                    k
                                 
                                 ,
                                 …
                                 ,
                                 
                                    s
                                    
                                       k
                                       +
                                       n
                                       −
                                       1
                                    
                                 
                                 }
                                 )
                                 ,
                              
                           
                        
                        
                           
                              (12)
                              
                                 S
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 ;
                                 
                                    Q
                                    s
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       n
                                       =
                                       1
                                    
                                    M
                                 
                                 
                                    b
                                    n
                                 
                                 ′
                                 
                                    S
                                    
                                       n
                                       -gram
                                    
                                 
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 ;
                                 
                                    Q
                                    s
                                 
                                 )
                                 .
                              
                           
                        Here (10), (11) and (12) are exactly the same as (7), (8) and (9), except that the word sequence 
                           
                              Q
                              w
                           
                         is replaced by Q
                        
                           s
                        , and each word 
                           
                              w
                              i
                           
                         is replaced by subword unit s
                        
                           j
                        , and d
                        
                           max
                        ′, d
                        
                           min
                        ′ and b
                        
                           n
                        ′ are the corresponding parameters.

Although we can obtain the relevance score 
                           R
                           (
                           
                              x
                              i
                           
                           ,
                           
                              Q
                              w
                           
                           )
                         and R(x
                        
                           i
                        , Q
                        
                           s
                        ), and similarity 
                           S
                           (
                           
                              x
                              i
                           
                           ,
                           
                              x
                              j
                           
                           ;
                           
                              Q
                              w
                           
                           )
                         and S(x
                        
                           i
                        , x
                        
                           j
                        
                        ;
                        Q
                        
                           s
                        ) based on different units and use them together – for example, it is possible to derive 
                           R
                           (
                           
                              x
                              i
                           
                           ,
                           
                              Q
                              w
                           
                           )
                         in (3) from word lattices but compute S(x
                        
                           i
                        , x
                        
                           j
                        
                        ;
                        Q
                        
                           s
                        ) in (12) on subword lattices and use them together – for simplicity in the experiments below, we always use 
                           R
                           (
                           
                              x
                              i
                           
                           ,
                           
                              Q
                              w
                           
                           )
                        /R(x
                        
                           i
                        , Q
                        
                           s
                        ) and 
                           S
                           (
                           
                              x
                              i
                           
                           ,
                           
                              x
                              j
                           
                           ;
                           
                              Q
                              w
                           
                           )
                        /S(x
                        
                           i
                        , x
                        
                           j
                        
                        ;
                        Q
                        
                           s
                        ) obtained from the same type (word or subword) of lattices together. Below for simplicity in notation, we simply use R(x
                        
                           i
                        , Q) to denote relevance score and S(x
                        
                           i
                        , x
                        
                           j
                        ) to denote similarity, regardless of whether they are obtained from word or subword lattices.

Although we here report only experiments using MFCC features for the DTW distances, other acoustic features could be used, such as phone posteriorgrams (Hazen et al., 2009) or Gaussian posteriorgrams (Zhang and Glass, 2010, 2009), and evaluating the acoustic similarity between two acoustic feature sequences based on models is even preferred (Chan and Lee, 2011; Wang et al., 2012). These approaches may provide less speaker-dependent DTW distance measures and could thus be useful if the target spoken segments are produced by many different speakers.

In PRF, the top-ranked y segments with the highest relevance scores R(x
                        
                           i
                        , Q) in (3) or (6) are selected as pseudo-relevant set 
                           Y
                        ; the bottom-ranked z segments with the lowest R(x
                        
                           i
                        , Q) are selected as pseudo-irrelevant set 
                           Z
                        . The similarity between each segment x
                        
                           i
                         in the first-pass retrieved results 
                           X
                         and the pseudo-relevant and -irrelevant sets is then defined as


                        
                           
                              (13)
                              
                                 SIM
                                 (
                                 
                                    x
                                    i
                                 
                                 )
                                 =
                                 
                                    1
                                    y
                                 
                                 
                                    ∑
                                    
                                       x
                                       ∈
                                       Y
                                    
                                 
                                 S
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 x
                                 )
                                 −
                                 
                                    1
                                    z
                                 
                                 
                                    ∑
                                    
                                       x
                                       ∈
                                       Z
                                    
                                 
                                 S
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 x
                                 )
                                 ,
                              
                           
                        where y and z are the sizes of the pseudo-relevant and -irrelevant sets.

The value of SIM(x
                        
                           i
                        ) is then linearly normalized into a number between 0 and 1 as SIM′(x
                        
                           i
                        ),


                        
                           
                              (14)
                              
                                 
                                    SIM
                                    ′
                                 
                                 (
                                 
                                    x
                                    i
                                 
                                 )
                                 =
                                 
                                    
                                       SIM
                                       (
                                       
                                          x
                                          i
                                       
                                       )
                                       −
                                       
                                          S
                                          min
                                       
                                    
                                    
                                       
                                          S
                                          max
                                       
                                       −
                                       
                                          S
                                          min
                                       
                                    
                                 
                                 ,
                              
                           
                        where S
                        
                           max
                         and S
                        
                           min
                         are the largest and smallest values of SIM(x
                        
                           i
                        ) among all spoken segments x
                        
                           i
                         retrieved.

The relevance score R(x
                        
                           i
                        , Q) for each segment x
                        
                           i
                         is then updated into a new relevance score


                        
                           
                              (15)
                              
                                 
                                    R
                                    p
                                 
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 Q
                                 )
                                 =
                                 R
                                 
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       ,
                                       Q
                                       )
                                    
                                    
                                       1
                                       −
                                       
                                          δ
                                          1
                                       
                                    
                                 
                                 
                                    SIM
                                    ′
                                 
                                 
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       )
                                    
                                    
                                       
                                          δ
                                          1
                                       
                                    
                                 
                                 ,
                              
                           
                        where δ
                        1 is a weight parameter between 0 and 1. The segments in 
                           X
                         are then re-ranked according to R
                        
                           p
                        (x
                        
                           i
                        , Q), and then displayed to the user.

Usually in the literature the sizes of the pseudo-relevant/-irrelevant objects 
                           Y
                         and 
                           Z
                         are fixed for all of the queries. However, in text information retrieval, it has been found that the optimal number of pseudo-relevant objects varies from query to query (Montgomery et al., 2004). This observation suggests 
                           Y
                         and 
                           Z
                         should be determined dynamically for different queries based on their properties, which is not an easy task.
                           3
                        
                        
                           3
                           In the preliminary experiments, we have investigated different alternatives for dynamically determining the size of pseudo-relevant set for each queries. For example, take the segments whose relevance scores higher than a threshold as pseudo-relevant. In this way, different queries have different pseudo-relevant sets with different sizes. However, due to the diverse properties of the queries, these alternatives did not outperform that of simply taking a fixed number of top-ranked segments as pseudo-relevant, so they are not reported here.
                         The graph-based re-ranking presented in the next subsection may achieve this goal to some extent. Instead of selecting a set of pseudo-relevant/-irrelevant spoken segments, in the graph-based re-ranking, the contribution of a spoken segment to the final scores of other segments is based on the acoustic similarity structure between the segments in the first-pass retrieved result (represented as a graph). For instance, the spoken segments ranked in the first place of the first-pass retrieved results of two different queries have different influences to the other spoken segments in the retrieved results because the acoustic similarity structures are different for the two first-pass results.

An alternative to PRF for using acoustic feature similarity is graph-based re-ranking, which involves first constructing a graph for the first-pass retrieved segments for each query (Section 2.4.1) and then applying a random walk for relevance score propagation over the graph (Section 2.4.2).

Here for each query a directed graph is constructed from the first-pass returned list 
                              X
                           , in which each node represents a segment. A simplified example for such a graph is shown in Fig. 3
                           . Because directions are needed for score propagation over the graph, the edges between nodes need to have directions. There can be at least several approaches for connecting the edges with directions for the graph. In the first two cases below, we first connect each pair of nodes for segments x
                           
                              i
                            and x
                           
                              j
                            with a pair of edges in both directions (x
                           
                              i
                           
                           →
                           x
                           
                              j
                            and x
                           
                              i
                           
                           ←
                           x
                           
                              j
                           ). The weight of edge from x
                           
                              i
                            to x
                           
                              j
                            (x
                           
                              i
                           
                           →
                           x
                           
                              j
                           ) is S(x
                           
                              i
                           , x
                           
                              j
                           ) in (9) or (12).
                              4
                           
                           
                              4
                              Because S(x
                                 
                                    i
                                 , x
                                 
                                    j
                                 )=
                                 S(x
                                 
                                    j
                                 , x
                                 
                                    i
                                 ), the edges connecting x
                                 
                                    i
                                  and x
                                 
                                    j
                                  in both directions (x
                                 
                                    i
                                 
                                 →
                                 x
                                 
                                    j
                                  and x
                                 
                                    i
                                 
                                 ←
                                 x
                                 
                                    j
                                 ) have equal weights.
                            We then prune those edges with lower weights in two different ways as listed below.
                              
                                 •
                                 
                                    Fixed Number of Outgoing Edges (OUT): Each segment (or node) x
                                    
                                       i
                                     only keeps the K outgoing edges with the highest weights. In this way, each node in the graph has a fixed number K of outgoing edges but a variable number of incoming edges. In other words, during the score propagation, each node influences equal number of nodes.


                                    Fixed Number of Incoming Edges (IN): Each segment (or node) x
                                    
                                       i
                                     only keeps the K incoming edges with the highest weights. Thus each node in the graph has a fixed number K of incoming edges but a variable number of outgoing edges. In other words, the score of each node is influenced by equal number of nodes during the score propagation.


                                    K-nearest Neighbor (KNN): Nodes x
                                    
                                       i
                                     and x
                                    
                                       j
                                     are connected to each other in both directions (x
                                    
                                       i
                                    
                                    →
                                    x
                                    
                                       j
                                     and x
                                    
                                       i
                                    
                                    ←
                                    x
                                    
                                       j
                                    ) if x
                                    
                                       i
                                     is among the K-nearest neighbors of x
                                    
                                       j
                                     (or given x
                                    
                                       j
                                    , S(x
                                    
                                       i
                                    , x
                                    
                                       j
                                    ) is among the K highest of all x
                                    
                                       i
                                    ), or if x
                                    
                                       j
                                     is among the K-nearest neighbors of x
                                    
                                       i
                                     (or given x
                                    
                                       i
                                    , S(x
                                    
                                       i
                                    , x
                                    
                                       j
                                    ) is among the K highest of all x
                                    
                                       j
                                    ).


                                    Mutual K-nearest Neighbor (M-KNN): Similar as the above, except both directions of K-nearest neighbor relationships are required. Nodes x
                                    
                                       i
                                     and x
                                    
                                       j
                                     are connected to each other in both directions if x
                                    
                                       i
                                     is among the K-nearest neighbors of x
                                    
                                       j
                                    , and x
                                    
                                       j
                                     is among the K-nearest neighbors of x
                                    
                                       i
                                    .

A new set of graph-based relevance scores R
                           
                              g
                           ′(x
                           
                              i
                           , Q) for all x
                           
                              i
                            in the first-pass returned list 
                              X
                            can be obtained via score propagation on the graph, which can be expressed as


                           
                              
                                 (16)
                                 
                                    
                                       R
                                       g
                                    
                                    ′
                                    (
                                    
                                       x
                                       i
                                    
                                    ,
                                    Q
                                    )
                                    =
                                    (
                                    1
                                    −
                                    α
                                    )
                                    R
                                    (
                                    
                                       x
                                       i
                                    
                                    ,
                                    Q
                                    )
                                    +
                                    α
                                    
                                       ∑
                                       
                                          
                                             x
                                             j
                                          
                                          ∈
                                          
                                             B
                                             i
                                          
                                       
                                    
                                    
                                       
                                          
                                             R
                                             ′
                                          
                                          g
                                       
                                       (
                                       
                                          x
                                          j
                                       
                                       ,
                                       Q
                                       )
                                       
                                          
                                             S
                                             ˆ
                                          
                                       
                                       (
                                       
                                          x
                                          j
                                       
                                       ,
                                       
                                          x
                                          i
                                       
                                       )
                                    
                                    ,
                                 
                              
                           where R(x
                           
                              i
                           , Q) is the relevance score in (3) or (6), α is an interpolation weight between 0 and 1, B
                           
                              i
                            is the set of all segments connected to x
                           
                              i
                            by incoming edges as in Fig. 3, and x
                           
                              j
                            is a node in B
                           
                              i
                           . 
                              
                                 
                                    S
                                    ˆ
                                 
                              
                              (
                              
                                 x
                                 j
                              
                              ,
                              
                                 x
                                 i
                              
                              )
                            is the normalized edge weight S(x
                           
                              j
                           , x
                           
                              i
                           ) over all edges outgoing from node x
                           
                              j
                            on the graph
                              5
                           
                           
                              5
                              Note that 
                                    
                                       
                                          S
                                          ˆ
                                       
                                    
                                    (
                                    
                                       x
                                       j
                                    
                                    ,
                                    
                                       x
                                       i
                                    
                                    )
                                    ≠
                                    
                                       
                                          S
                                          ˆ
                                       
                                    
                                    (
                                    
                                       x
                                       i
                                    
                                    ,
                                    
                                       x
                                       j
                                    
                                    )
                                 .
                           :


                           
                              
                                 (17)
                                 
                                    
                                       
                                          S
                                          ˆ
                                       
                                    
                                    (
                                    
                                       x
                                       j
                                    
                                    ,
                                    
                                       x
                                       i
                                    
                                    )
                                    =
                                    
                                       
                                          S
                                          (
                                          
                                             x
                                             j
                                          
                                          ,
                                          
                                             x
                                             i
                                          
                                          )
                                       
                                       
                                          
                                             ∑
                                             
                                                
                                                   x
                                                   k
                                                
                                                ∈
                                                
                                                   A
                                                   j
                                                
                                             
                                          
                                          
                                             S
                                             (
                                             
                                                x
                                                j
                                             
                                             ,
                                             
                                                x
                                                k
                                             
                                             )
                                          
                                       
                                    
                                    ,
                                 
                              
                           where A
                           
                              j
                            is the set of segments connected to x
                           
                              j
                            by outgoing edges as in Fig. 3. In (16) the graph-based score R
                           
                              g
                           ′(x
                           
                              i
                           , Q) of a segment x
                           
                              i
                            depends on two factors interpolated by α: the relevance score in (3) or (6) (the first term on the right hand side of (16)) and the score propagation over the graph from all nodes x
                           
                              j
                            in B
                           
                              i
                            to x
                           
                              i
                            via incoming edges based on the normalized edge weights 
                              
                                 
                                    S
                                    ˆ
                                 
                              
                              (
                              
                                 x
                                 j
                              
                              ,
                              
                                 x
                                 i
                              
                              )
                            (the second term on the right hand side).

Based on (16), a segment x
                           
                              i
                            would have large R
                           
                              g
                           ′(x
                           
                              i
                           , Q) under the following two conditions:
                              
                                 1.
                                 Original relevance score R(x
                                    
                                       i
                                    , Q) is large, or the confidence of the occurrence of the query in x
                                    
                                       i
                                     is high based on its lattice.


                                    x
                                    
                                       i
                                     is connected to other nodes x
                                    
                                       j
                                     with large R
                                    
                                       g
                                    ′(x
                                    
                                       j
                                    , Q), or x
                                    
                                       i
                                     is acoustically similar to other spoken segments x
                                    
                                       j
                                     with larger probabilities of containing the query.

The normalization in (17) formulates (16) as a random walk problem on the graph; random walk theory guarantees that a set of unique solutions of R
                           
                              g
                           ′(x
                           
                              i
                           , Q) can be found. For all retrieved spoken segments x
                           
                              i
                           , R
                           
                              g
                           ′(x
                           
                              i
                           , Q) in (16) can be found efficiently by power method (Langville and Meyer, 2005).
                              6
                           
                           
                              6
                              It is also possible to obtain R
                                 
                                    g
                                 ′(x
                                 
                                    i
                                 , Q) in (16) by searching for the eigenvalues of a matrix, but this approach has much larger time complexity than power method.
                           
                        

Each node x
                           
                              i
                            is first given an initial value 
                              
                                 R
                                 g
                                 0
                              
                              (
                              
                                 x
                                 i
                              
                              ,
                              Q
                              )
                           .
                              7
                           
                           
                              7
                              The initial values would not influence the final results (Meye, 2000).
                            Then at each iteration t, 
                              
                                 R
                                 g
                                 
                                    t
                                    −
                                    1
                                 
                              
                              (
                              
                                 x
                                 i
                              
                              ,
                              Q
                              )
                            obtained in the last iteration are updated to 
                              
                                 R
                                 g
                                 t
                              
                              (
                              
                                 x
                                 i
                              
                              ,
                              Q
                              )
                            as below:


                           
                              
                                 (18)
                                 
                                    
                                       R
                                       g
                                       t
                                    
                                    (
                                    
                                       x
                                       i
                                    
                                    ,
                                    Q
                                    )
                                    =
                                    (
                                    1
                                    −
                                    α
                                    )
                                    R
                                    (
                                    
                                       x
                                       i
                                    
                                    ,
                                    Q
                                    )
                                    +
                                    α
                                    
                                       ∑
                                       
                                          
                                             x
                                             j
                                          
                                          ∈
                                          
                                             A
                                             i
                                          
                                       
                                    
                                    
                                       
                                          R
                                          g
                                          
                                             t
                                             −
                                             1
                                          
                                       
                                       (
                                       
                                          x
                                          j
                                       
                                       ,
                                       Q
                                       )
                                       
                                          
                                             S
                                             ˆ
                                          
                                       
                                       (
                                       
                                          x
                                          j
                                       
                                       ,
                                       
                                          x
                                          i
                                       
                                       )
                                    
                                    .
                                 
                              
                           Eq. (18) is parallel to (16), except that 
                              
                                 R
                                 g
                                 t
                              
                              (
                              
                                 x
                                 i
                              
                              ,
                              Q
                              )
                            is at the left hand side of the equation, and 
                              
                                 R
                                 g
                                 
                                    t
                                    −
                                    1
                                 
                              
                              (
                              
                                 x
                                 j
                              
                              )
                            at the right hand side. Whenever the results converge, that is, 
                              
                                 R
                                 g
                                 
                                    t
                                    −
                                    1
                                 
                              
                              (
                              
                                 x
                                 i
                              
                              ,
                              Q
                              )
                            and 
                              
                                 R
                                 g
                                 t
                              
                              (
                              
                                 x
                                 i
                              
                              ,
                              Q
                              )
                            are sufficiently close, 
                              
                                 R
                                 g
                                 t
                              
                              (
                              
                                 x
                                 i
                              
                              ,
                              Q
                              )
                            can be taken as the scores 
                              
                                 R
                                 g
                                 ′
                              
                              (
                              
                                 x
                                 i
                              
                              ,
                              Q
                              )
                            satisfying (16).


                           R
                           
                              g
                           ′(x
                           
                              i
                           , Q) is finally integrated with R(x
                           
                              i
                           , Q) to become a new relevance score for re-ranking,


                           
                              
                                 (19)
                                 
                                    
                                       R
                                       g
                                    
                                    (
                                    
                                       x
                                       i
                                    
                                    ,
                                    Q
                                    )
                                    =
                                    R
                                    
                                       
                                          (
                                          
                                             x
                                             i
                                          
                                          ,
                                          Q
                                          )
                                       
                                       
                                          1
                                          −
                                          
                                             δ
                                             2
                                          
                                       
                                    
                                    
                                       
                                          R
                                          ′
                                       
                                       g
                                    
                                    
                                       
                                          (
                                          
                                             x
                                             i
                                          
                                          ,
                                          Q
                                          )
                                       
                                       
                                          
                                             δ
                                             2
                                          
                                       
                                    
                                    ,
                                 
                              
                           where δ
                           2 is a parameter between 0 and 1. The final retrieval results ranked according to R
                           
                              g
                           (x
                           
                              i
                           , Q) in (19) are then displayed to the user.
                              8
                           
                           
                              8
                              Although the original scores R(x
                                 
                                    i
                                 , Q) have been considered when computing R
                                 
                                    g
                                 ′(x
                                 
                                    i
                                 , Q) in (16), integrating R(x
                                 
                                    i
                                 , Q) and R
                                 
                                    g
                                 ′(x
                                 
                                    i
                                 , Q) again in (19) empirically lead to better performance. Because R(x
                                 
                                    i
                                 , Q) and R
                                 
                                    g
                                 ′(x
                                 
                                    i
                                 , Q) are added in (16) but multiplied in (19), considering both integration mechanism leads to optimal performance.
                           
                        

There are two stages in graph-based re-ranking: graph construction in Section 2.4.1 and random walk in Section 2.4.2. In the following, the complexity of these two stages is analyzed.

During the graph construction in Section 2.4.1, the system first constructs a fully connected graph, and then prunes the edges with low weights. Suppose the number of spoken segments retrieved in the first pass is G, or there are G nodes in the graph. The system should compute the distances between the G nodes, or the weights of G(G
                           −1) edges. G is usually small because the number of segments retrieved in the first pass is usually limited, although there are a large amount of spoken segments in the spoken archive. In real implementation, it is possible to construct the graph in more effective way. The system can use the coarse but fast approaches (Jansen and Durme, 2012) to first compute the approximate weights for all of the G(G
                           −1) edges in the fully connected graph to decide the edges to be pruned, and then use the fine but slow ways to exactly compute the weights of the remaining edges.

The complexity of the random walk in Section 2.4.2 is low. For each iteration in power method, there are G equations like (18) for every x
                           
                              i
                           , and in general each equation has at most G additions (because the number of elements in A
                           
                              i
                            can never exceed G). Therefore, if power method has T iterations, the complexity of random walk is less than O(G
                           2
                           T). G is a small number as described in the last paragraph, and T is also small because empirically tens of iterations is sufficient for the power method to converge (even if there are millions of nodes in a graph) (Langville and Meyer, 2005). In fact, in the experiments here, O(G
                           2
                           T) excessively overestimates the complexity. For example, based on the graph construction with Fixed Number of Incoming Edges (IN) in Section 2.4.1, because the size of A
                           
                              i
                            is always K, there are only K
                           +1 additions in (18), and K is usually much smaller than G. Therefore, one of the G in O(G
                           2
                           T) should be replaced by K, and thereby the complexity of random walk is only O(KGT) in such case. There are other approaches to speed power method (Kamvar et al., 2003; Manaskasemsak and Rungsawang, 2005), but it is out of the scope here.

The testing spoken archive is a corpus of 45h of recorded lectures for a course offered at National Taiwan University taught by a single instructor; the corpus is quite noisy and spontaneous (Lee et al., 2009).

The spoken archive was divided into about 23,000 spoken segments based on silences, and the lengths of the segments were 3.6s on average.

The lectures were given primarily in Mandarin Chinese but with some English terms and phrases embedded within the Mandarin utterances.

Those embedded English words or phrases are usually very short, very often with a length of only one to three words. The English words or phrases occurred in the utterances in the following two conditions. In the first case, almost all terminologies for this course are directly produced by the instructor in English without trying to translate them into Chinese. For example, in the utterance, “
                        
                      speech recognition 
                        
                     , 
                        
                      indexing 
                        
                      retrieval 
                        
                     ” (Except for speech recognition technology, we also need technologies about indexing and retrieval.)”, the phrase “speech recognition” and the words “indexing” and “retrieval” were produced in English, while other parts of the utterance are in Mandarin. In the second case, the instructor may prefer to use some commonly used English words in his utterances, which are not terminologies at all, probably because the concept can be easily or naturally expressed in English in this way. For example, in the utterance, “
                        
                      somehow handle 
                        
                      (I can somehow handle this problem)”, the words “somehow handle” were produced in English, but other parts of the utterance were in Mandarin.

The ratio of the number of Mandarin characters to that of English words is nine to one in the spoken archive we used. Many course lectures are presented in this code-switching way in Taiwan. In fact, such code-switching speech is very common for speakers whose native languages are not English but speak fluent English in the daily lives. They naturally speak the native languages as the daily language, but spontaneously embed some English words in their native language utterances. For example, many Asian whose native languages are not English speak in this way. Hence, the task domain considered here is very important and representative, although not yet investigated extensively.

We split the corpus into two parts: 12 hours for acoustic and language model training and 33h for retrieval testing.

In the following experiments, mean average precision (MAP) (Garofolo et al., 2000) was used as the retrieval performance measure. The pair-wise t-test with a significance level of 0.05 was used to gauge the significance of performance improvements. Here the STD system only returns the spoken segments containing the query terms without locating their exact positions in the spoken archive, because the spoken segments here were short enough to be used as the pointer to the positions. This task definition is the same as the STD task in the 9th NTCIR workshop (Akiba et al., 2011), but slightly different from that in NIST 2006 (http://www.itl.nist.gov/iad/mig/tests/std/2006/index.html), in which the positions of the query terms in the spoken archive should be located. For computing the DTW distances in Section 2.2, MFCCs were used as the acoustic features, and Euclidean distance was applied as the distance measure between two acoustic features. Some parameters in the experiments were set empirically as below. a
                     
                        n
                      and a
                     
                        n
                     ′ in (3) and (6) were both set to 105n
                      to favor longer n-grams. The expected term frequencies of longer n-grams should have more influence on the relevance scores in (3) and (6) because the observation of a query's longer n-grams in the lattices provides more confidence about the existence of the query than shorter n-grams. Due to the same reason, b
                     
                        n
                      and b
                     
                        n
                     ′ in (9) and (12) were set equal to a
                     
                        n
                      and a
                     
                        n
                     ′.

The influence of δ
                     1 in (15), δ
                     2 in (19) and α in (16) has been explored in previous studies on the same audio but with another query set (Chen, 2011; Chen et al., 2011). In the previous studies (Chen, 2011), larger δ
                     1 and δ
                     2 implied better results unless they were too close to 1 (for example, larger than 0.99) because SIM′(x
                     
                        i
                     ) in (14) and 
                        
                           R
                           g
                           t
                        
                        (
                        
                           x
                           i
                        
                        ,
                        Q
                        )
                      in (16) were more reliable than the original relevance scores R(x
                     
                        i
                     , Q). Therefore, δ
                     1 and δ
                     2 were both set to 0.9 here. For graph-based re-ranking, α close to 1 was optimal for not only spoken term detection (Chen et al., 2011) but also video search (Hsu et al., 2007), so α was set to 0.9 here.

In order to evaluate the retrieval performance with respect to acoustic models of different matched conditions, we used three sets of acoustic models:
                        
                           •
                           Speaker-independent models (SI) trained on a Mandarin corpus of 24.6h of read speech, produced by 100 male and 100 female speakers, plus the Sinica L2 Taiwanese English corpus with 59.7h of English read speech, produced by 229 male and 256 female Taiwanese speakers.

Speaker-adaptive models (SA) adapted by MLLR with 256 classes cascaded with the maximum a posterior estimation from the above SI model based on 500 utterances taken from the training set of the lecture corpus.

Speaker-dependent models (SD) trained on the 12-h training set of the lecture corpus.

Two sets of experiments respectively with in-vocabulary (IV) and OOV queries were performed as mentioned below.

The IV query set included 275 Chinese queries, each composed of 1–3 words, or 2–7 Chinese characters. The number of relevant spoken segments for each IV query ranged from 5 to 714 with an average of 38.2. In the experiments here, a language model trained with the manual transcriptions of the training set of the lecture corpus was used. A close-to-oracle lexicon was used which included 11K Chinese words plus 2K English words covering all words in the testing archive. Each utterance was transcribed into a bilingual word lattice. Then we transformed each Chinese word arc into a sequence of concatenated corresponding Chinese character and Mandarin syllable arcs to respectively form character and syllable lattices. The English word arcs remained unchanged. Therefore, for each utterance there were three lattices: word-, character-, and syllable-based. The word recognition accuracies for Chinese characters and English words evaluated together were 49.7%, 80.8%, and 88.0% respectively for the SI, SA, and SD models, and the inclusion rates of the lattices
                           9
                        
                        
                           9
                           The highest accuracy among the path hypotheses in each lattice.
                         were 72.7%, 86.8%, and 92.0% respectively for the SI, SA, and SD models. Note that the three different sets of acoustic models together with the relatively matched language model and lexicon gave different levels of recognition accuracies. In this way, we wish to show that the proposed approaches can offer performance improvements regardless of whether the recognition accuracies are lower or higher.

For the OOV query set we used 110 English queries, each consisting of a single word. The number of relevant spoken segments for each OOV query ranged from 2 to 268 with an average of 39.8. First, we assume we do not know the pronunciation of these OOV words, so we trained a 6-gram joint-sequence model from the CMU dictionary with 130K words (http://www.speech.cs.cmu.edu/cgi-bin/cmudict) to be used as the grapheme-to-phoneme converter to predict the pronunciations for the OOV queries (Bisani and Ney, 2008).
                           10
                        
                        
                           10
                           The terms used in OOV queries were excluded from the CMU dictionary during training.
                         The canonical pronunciation for each OOV query was also used in the experiments for comparison. Using the canonical pronunciation as the reference, the pronunciation was estimated perfectly (exactly the same as the reference) for 81 of the 110 OOV queries, or with an accuracy of 73.6%, while the pronunciation estimation accuracies on syllable and phoneme levels were 85.8% and 93.8% respectively.

We used a word/subword hybrid system to transcribe each spoken segment, which is a widely used approach for handling the OOV problem (Rastrow et al., 2009; Akbacak et al., 2008; Szoke et al., 2008). In this experiment, we used a lexicon composed of 11K Chinese words, 5K English words from the standard Aurora-4 lexicon,
                           11
                        
                        
                           11
                           This lexicon is quite disjoint from the content of the target corpus for retrieval, so the 110 English queries were not included in this lexicon.
                         and 10K English syllables automatically generated from the CMU dictionary based on some syllable segmentation rules. 20,000 English documents from the 20Newsgroups corpus
                           12
                        
                        
                           12
                           
                              http://people.csail.mit.edu/jrennie/20Newsgroups/.
                         were then used to train an English language model based on the above lexicon of 5K English words and 10K English syllables, which included mixed trigrams for English words and syllables (for example, a word following two concatenated syllables can be a trigram item). In other words, those words in the English training documents but not within the above selected lexicon of 5K English words were segmented into syllable sequences to be used together with the other words in the 5K English word lexicon to train an English trigram language model for mixed words and syllables. A Chinese word-based trigram language model was trained on the lecture corpus training set. These two language models were then interpolated to produce the lattices composed of a mixture of arcs for Chinese words, English words, and English syllables. We further substituted the Chinese and English word arcs in the lattices with their corresponding syllables to obtain a set of syllable-based lattices. Thus for each spoken segment we generated two lattices: one composed of Chinese and English words plus English syllables, and the other composed solely of Chinese and English syllables. Because the English recognition accuracies for SI and SA were not good enough to offer reasonable results, we used only SD models for the OOV query experiments. Since the accuracy for the word/syllable hybrid recognition output is not easy to define, we only evaluated the English syllable accuracy here. The English syllable accuracy of the one-best transcriptions and English syllable inclusion rate on the lattices were respectively 43.6% and 59.5% for the SD models.

@&#EXPERIMENTAL RESULTS@&#

Sections 4.1, 4.2 and 4.3 are the results for the IV queries. In Section 4.1, we demonstrate the performance of PRF with different numbers of pseudo-relevant/-irrelevant spoken segments. In Section 4.2, we investigate different graph construction approaches for graph-based re-ranking, and the results of PRF and graph-based re-ranking for word lattices are compared. In Section 4.3, PRF and graph-based re-ranking were applied on the subword-based lattices, and the results of word and subword units were further integrated. Finally, to investigate the usefulness of PRF and graph-based re-ranking for OOV queries, the results of OOV queries are reported in Section 4.4.


                        Table 1
                         shows the MAP performance for word lattices with IV queries yielded by PRF with different numbers of pseudo-relevant segments (different y in (13)) and 40 pseudo-irrelevant segments (z
                        =40 in (13)). The first-pass retrieval results are taken as the baseline. The three columns SI, SA and SD correspond to the three sets of acoustic models with different quality. The superscript * indicates significantly better than the baselines. First of all, we found that PRF outperformed the baselines except when y
                        =1. We also observed that as the number of pseudo-relevant segments was raised the MAP first increased and then decreased. This is reasonable because a larger y implies that more segments are considered when computing the similarities, and therefore disturbances caused by noisy pseudo-relevant segments (irrelevant segments assumed to be relevant) are diluted. However, when y was too large, more irrelevant segments were inevitably included in the pseudo-relevant segment set, and the MAP naturally degraded.

Because for the IV queries tested here the number of relevant segments ranged from 5 to hundreds, the pseudo-relevant sets including more than 10 segments inherently cover more than 50% incorrect examples for some queries. This may explains why the optimal y for the three recognition conditions, SI, SA and SD, was all around 10.

Notwithstanding we did not develop techniques to automatically decide the optimal y, PRF is still an useful approach because no matter the value of y, significant improvements were always observed (except y
                        =1).


                        Table 2
                         shows similar MAP performance as those in Table 1 with the number of pseudo-relevant segments fixed at 9 (y
                        =9 in (13)) but with different numbers of pseudo-irrelevant segments (different z in (13)). The superscript * indicates significantly better than the baselines. In contrast to Table 1, we observed that as the number of pseudo-irrelevant segments was raised the MAP first increased and then saturated without too much degradation. This may be because the irrelevant segments form the majority of the retrieved segments, so most pseudo-irrelevant segments are truly irrelevant even when we took very large number of them.

In this section, the experimental results show that PRF was helpful for the IV queries with one or several words, and the influence of the sizes of pseudo-relevant/-irrelevant sets, y and z, was investigated.


                        Table 3
                         shows the results of graph-based re-ranking started with word lattices yielded by the different graph construction strategies of Section 2.4.1 with the IV query set and SI models. The four columns of the table correspond to the Fixed Number of Outgoing (OUT) and Incoming (IN) Edges, K-nearest Neighbor (KNN), and Mutual K-nearest Neighbor (M-KNN); K in Table 3 is the number of outgoing edges for OUT, incoming edges for IN, or the number of nearest neighbors considered for KNN and M-KNN. The best results in each column are in bold. Clearly Fixed number of Incoming Edges (IN) is the best graph construction strategy. Consider Eq. (16). We mentioned that a spoken segment x
                        
                           i
                         can have larger 
                           
                              R
                              g
                              ′
                           
                           (
                           
                              x
                              i
                           
                           ,
                           Q
                           )
                         in (16) if x
                        
                           i
                        's first-pass score R(x
                        
                           i
                        , Q) is higher (the first term in (16)), or if the nodes connected by incoming edges to x
                        
                           i
                         have large scores R
                        
                           g
                        ′(x
                        
                           j
                        , Q) (the second term in (16)). There is however another implicit factor with the latter which may influence the score of R
                        
                           g
                        ′(x
                        
                           i
                        , Q): given a large number of nodes connected by incoming edges to x
                        
                           i
                         (or a large set B
                        
                           i
                        ), even though the scores of the individual nodes in B
                        
                           i
                         are small, R
                        
                           g
                        ′(x
                        
                           i
                        , Q) can still become high. In our task, this phenomenon is undesirable because an irrelevant segment x
                        
                           i
                         may be somehow similar to many other irrelevant segments, and that may result in a higher score R
                        
                           g
                        ′(x
                        
                           i
                        , Q) through the many incoming edges from many other irrelevant segments.
                           13
                        
                        
                           13
                           In the PageRank scenario, this is desired because a web page that many other pages link to is deemed to be a famous page.
                         By fixing the size of B
                        
                           i
                        , Fixed number of Incoming Edges (IN) solved this problem, yielding the best results in our task here; it is therefore used in all following experiments.


                        Table 4
                         shows the results of graph-based re-ranking using the graph with Fixed Numbers of Incoming Edges (IN) for different values of K with three different sets of acoustic models, one for each column. The graphs constructed for SI, SA and SD models had respectively 313.4, 171.1 and 146.7 nodes on average, that is, there were 313.4, 171.1 and 146.7 spoken segments respectively in the first pass for SI, SA and SD models on average. Because in Table 4 the number of incoming edges for each node was fixed to K, the number of edges in a graph was simply K times the number of nodes. The superscripts * and † respectively indicate significantly better than the baselines and PRF. The best results of PRF are reported here in the second row, in which the numbers of pseudo-relevant (y) and -irrelevant (z) segments used were tuned to maximize the MAP values on the testing query set, resulting in unrealistically high performance for PRF (higher than all numbers in Tables 1 and 2).
                           14
                        
                        
                           14
                           We tuned the parameters y and z in this way to emphasize the power of graph-based re-ranking.
                         The results in Table 4 are represented in Fig. 4
                         as well. The blue and red lines are respectively for the first-pass retrieval results and PRF, and the green curves are for graph-based re-ranking. The horizontal scales in the figures are the numbers of incoming edges K. Fig. 4(a), (b) and (c) are respectively for SI, SA and SD models. From Table 4 and Fig. 4, we found that graph-based re-ranking outperformed the baseline in all cases except when K
                        =1. We also found that graph-based re-ranking was so powerful that even though the parameters for PRF were carefully tuned, graph-based re-ranking still outperformed PRF significantly if K was large enough.

In graph-based re-ranking, it is ideal if the nodes representing relevant segments only connect to other relevant segments. If K was too large, since the number of relevant segments is limited for each query, a relevant spoken segments would unavoidably connect to some irrelevant segments. Because some queries only had 5 relevant segments, when K was larger than 10, for such queries the relevant segments had to connect to more nodes representing irrelevant segments than the relevant ones. This may be why in Table 4 the optimal K for different models ranged from 5 to 10.

Then we analyzed the execution time of the graph-based re-ranking. As mentioned in Section 2.4.3, there are two stages in graph-based re-ranking: graph construction and random walk. In the first stage, the DTW distances between all the hypothesized regions are computed. Since the acoustic feature sequences for hypothesized regions were usually short, the computation of the DTW distance for a region pair took less than one millisecond on a regular Linux machine.
                           15
                        
                        
                           15
                           With 2.66GHz Intel processor.
                         Moreover, in real implementation, the computation of the distances can be parallel. In the second stage, power method took less than a second on a regular machine.


                        Fig. 5
                         is the detection error trade-off (DET) curves of the first-pass retrieval results (baselines), PRF and graph-based re-ranking for SI, SA and SD models (all results have been shown in Table 4 in terms of MAP). The red and blue curves are respectively for the first-pass retrieval results and PRF, and the green curves are for graph-based re-ranking with Fixed Numbers of Incoming Edges (IN) and K
                        =10. The horizontal and vertical scales are respectively for false alarm rate and missed detection rate in log scale between 0 and 50%. Each point on the curve represents an operation point, or a specific threshold. The spoken segments with relevance scores higher than the threshold would be taken as relevant otherwise irrelevant. Thus each threshold corresponds to a set of false alarm and missed detection rates, or a point on the curves.

From Fig. 5(a), we found that for SI models PRF and graph-based re-ranking outperformed the baselines regardless of the operation points in terms of missed detection and false alarm rates. However, in Fig. 5(b) and (c), for SA and SD models when operating at higher threshold with higher missed detection rates (or only a few top-ranked segments were regarded as relevant), the baselines had lower false alarm rates than PRF and graph-based re-ranking. In other words, PRF and graph-based re-ranking actually increased the false alarm rates for the top-ranked segments. Because SA and SD models were of high quality, based on these models, the top-ranked segments given large original relevance scores from lattices were already ranked well, so PRF and graph-based re-ranking only slightly disturbed the perfect ranking under such conditions. On the contrary, in Fig. 5(b) and (c), when operated at low missed detection rates (or taking more segments as relevant), PRF and graph-based re-ranking offered much lower false alarm rates than the baselines. This shows that the segments besides the top-ranked ones were ranked better after PRF or graph-based re-ranking. Therefore, nevertheless the acoustic models were already of high quality, PRF and graph-based re-ranking still helped discriminate the segments whose relevance was not confidence enough based on the recognition results.

Compare the DET curves of PRF and graph-based re-ranking in Fig. 5. We found that when operating at low missed detection rates, the results of PRF were closer to the baselines than graph-based re-ranking for all the models, SI, SA and SD. Because PRF took the top-ranked segments as pseudo-relevant, in general the ranking of the top-ranked segments did not modify dramatically after PRF. In Fig. 5(a), although the results of PRF and graph-based re-ranking were close at the operation points of low missed detection rates (the right hand side of the figure), at the operation points of high missed detection rates (the upper left corner of the figure), graph-based re-ranking had lower false alarm rates than PRF. This reveals that due to the low quality of the SI models the top-ranked segments with high relevance scores from the lattices still left some rooms for improvement, but PRF which taking the top-ranked segments as pseudo-relevant missed this opportunities. This explains why graph-based re-ranking yielded greatest improvements over PRF for SI models in Table 4.

In this section, on a set of IV queries, we found that fixed the number of incoming edges for each node is the best graph construction approach, and the comparison of PRF and graph-based re-ranking found that graph-based re-ranking outperformed PRF regardless of whether the sizes of pseudo-relevant/-irrelevant sets were tuned to be optimal.


                        Table 5
                         shows the results for IV queries with lattices of word and subword units. Parts (a), (b), and (c) are respectively with word-, character- or syllable-based lattices, and columns SI, SA, and SD correspond to the different sets of acoustic models. For each case we report the results of the first pass (baseline), PRF, and graph-based re-ranking (Graph). The superscripts * and † respectively indicate significantly better than the baselines and PRF. The numbers of pseudo-relevant (y) and irrelevant (z) segments for PRF were tuned on the testing queries. Compared the first pass retrieval results of the three units. It is clear that PRF always yielded improvements over the baseline, and the graph-based re-ranking always offered still further improvements regardless of the acoustic model set or the units selected. Note also that even though the word-based first-pass results were much better than the subword-based results, PRF and graph-based re-ranking yielded larger improvements for subword lattices. Because both PRF and graph-based re-ranking only re-rank the first-pass retrieved results, segments that were not retrieved in the first pass could never be retrieved. Since subword units offered higher recall than words in the lattices, the proposed approach yielded greater improvements for subword units.

Since different units contain complementary information, the integration of the results based on different units after graph-based re-ranking may outperform each individual. Fig.  6
                         shows the integration of the results from word-, character-, syllable-based lattices. The four curves in Fig. 6 are the MAP performances of graph-based re-ranking for graphs constructed with Fixed Number of Incoming Edges (IN) from word-, character-, syllable-based lattices and the integration of the three. Fig. 6(a), (b), and (c) are respectively for SI, SA, and SD models. The horizontal scales in the figures are the numbers of incoming edges K for the graphs. In Fig. 6, the results for the integration were always significantly better than the individuals regardless of the acoustic model set or the number of incoming edges K with only one exception in Fig. 6(b), that is, for SA models when K
                        =8 the improvement of the integration over the character was not significant.

For all the recognition conditions, SI, SA, and SD, the integration in Fig. 6 was achieved by a weighted sum of the relevance scores obtained from word-, character-, and syllable-based lattices with weights 1.0, 0.2 and 0.04
                           16
                        
                        
                           16
                           That is, (0.2)2.
                         respectively. These weights were set based on the following reasons. Because in Mandarin Chinese a word consists of one to several subwords, in Sections 2.1 and 2.2, a query's word sequence representation 
                           
                              Q
                              w
                           
                           =
                           {
                           
                              w
                              j
                           
                           ,
                           j
                           =
                           1
                           ,
                           2
                           ,
                           …
                           ,
                           N
                           }
                         would be much shorter than the subword version Q
                        
                           s
                        
                        ={s
                        
                           j
                        , j
                        =1, 2, …, M}, or N
                        <
                        M. Because a
                        
                           n
                         and a
                        
                           n
                        ′ respectively in 
                           R
                           (
                           
                              x
                              i
                           
                           ,
                           
                              Q
                              w
                           
                           )
                         in (3) and R(x
                        
                           i
                        , Q
                        
                           s
                        ) in (6) raised as n was increased, 
                           R
                           (
                           
                              x
                              i
                           
                           ,
                           
                              Q
                              w
                           
                           )
                         is inherently smaller than R(x
                        
                           i
                        , Q
                        
                           s
                        ) due to N
                        <
                        M. Therefore, to let the results from word and subword-based lattices have comparable influence for the integration, it is reasonable to give the results from word lattices larger weights than subword lattices. On the other hand, each character is produced as a monosyllable in Mandarin Chinese, so very often many different characters with very different meanings may correspond to the same syllable (much less number of distinct syllables than distinct characters). Since in Mandarin Chinese different terms may have the same syllable sequence, and syllable is not as discriminative as character, syllable was given smaller weights than character during the integration. To achieve better integration results, the weights of different units can be learned by the learning-to-rank techniques from a set of training queries as in the previous studies (Meng et al., 2009), but this is out of the scope here because we only aim at showing that after graph-based re-ranking the integration of different units can yield further improvements.

In this subsection, we show that PRF and graph-based re-ranking not only improved word-based retrieval but also character- and syllable-based retrieval, and the integration of the results based on word and different subword units yielded further improvement.


                        Table 6
                         shows the results for the OOV query experiments as summarized in Section 3.2 on lattices for words plus syllables (Hybrid) or for syllables only (Syllable), with PRF or the graph-based re-ranking applied. Section (a) (canonical) is the case assuming the canonical pronunciation of each OOV query was known, while Section (b) (g2p) shows the results based on the pronunciation estimated using the grapheme-to-phoneme approach, each including Column (1) (Hybrid) for lattices composed of Chinese and English word arcs plus English syllable arcs, and Column (2) (Syllable) for lattices containing Chinese and English syllable arcs only. In PRF approach, the numbers of pseudo-relevant (y) and -irrelevant (z) segments were determined by 4-fold cross validation. That is, the testing queries were first separated into 4 parts. In each trial, one part was selected as the development query set for parameter tuning, while the other three parts tested, and this was repeated 4 times.

We can see that the grapheme-to-phoneme-based pronunciations yielded reasonable performance, although naturally lower compared with the canonical pronunciation (Sections (b) vs (a)). Also, the lattices composed of syllables only outperformed the hybrid lattices (Columns (2) vs (1)). Because some of the English queries were incorrectly recognized as words with similar pronunciations in the lexicon in the hybrid case, transforming those words into corresponding syllable sequences increased the recall rates and thus improved the results.

It is clear that remarkable improvements were achieved by both PRF and graph-based re-ranking in all cases. However, we also observed that the graph-based re-ranking did not outperform PRF on the OOV query set. This is probably because of the relatively poor recognition results for the OOV terms, or the relevance scores R(x
                        
                           i
                        , Q
                        
                           s
                        ) in (6) could be unreliable. Since the graph-based re-ranking in (16) was directly applied on these relevance scores, the random walk may be relatively sensitive to the noisy relevance scores for the individual segments from the first pass. On the other hand, PRF considered the pseudo-relevant and -irrelevant groups of segments as a whole which were obtained based on the ranking of the first pass. As a result, the disturbances of the individual scores did not necessarily change the pseudo-relevant and -irrelevant groups.

In this section, we found that both PRF and graph-based re-ranking can improve the performance of the OOV queries, but different from the results in IV queries, PRF was comparable to graph-based re-ranking here.

@&#CONCLUSION@&#

In this paper, we tested approaches that take into account acoustic feature similarity including PRF and graph-based re-ranking, and extended them to the retrieval based on subword-based lattices and OOV queries. Different graph construction approaches were investigated, and we found that fixed the number of incoming edges for each node in the graphs was the best approach. For the results of IV queries under different recognition conditions, both PRF and graph-based re-ranking yielded remarkable improvements for the results obtained from word-, character- and syllable-based lattices, and the integration of the results from different units after graph-based re-ranking gave further improvements over each individual. In addition, graph-based re-ranking outperformed PRF in most cases for the IV queries tested here. Finally, PRF and graph-based re-ranking were also applied on an open-vocabulary spoken content retrieval system based on a hybrid language model of words and syllables, and it was found that both approaches offered significant improvements for a set of OOV queries.

@&#REFERENCES@&#

