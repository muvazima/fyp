@&#MAIN-TITLE@&#Genetic programming-based feature learning for question answering

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new framework for answering definitional and factoid questions.


                        
                        
                           
                           Producing new features by combining effective features with arithmetic operators.


                        
                        
                           
                           Genetic Programming (GP) algorithm has been employed for feature learning.


                        
                        
                           
                           Three discriminant-based methods have been used for learning features weights.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Question Answering (QA)

Feature learning

Genetic Programming (GP) algorithm

Feature weight learning

Factoid questions

Information Extraction (IE)

@&#ABSTRACT@&#


               
               
                  Question Answering (QA) systems are developed to answer human questions. In this paper, we have proposed a framework for answering definitional and factoid questions, enriched by machine learning and evolutionary methods and integrated in a web-based QA system. Our main purpose is to build new features by combining state-of-the-art features with arithmetic operators. To accomplish this goal, we have presented a Genetic Programming (GP)-based approach. The exact GP duty is to find the most promising formulas, made by a set of features and operators, which can accurately rank paragraphs, sentences, and words. We have also developed a QA system in order to test the new features. The input of our system is texts of documents retrieved by a search engine. To answer definitional questions, our system performs paragraph ranking and returns the most related paragraph. Moreover, in order to answer factoid questions, the system evaluates sentences of the filtered paragraphs ranked by the previous module of our framework. After this phase, the system extracts one or more words from the ranked sentences based on a set of hand-made patterns and ranks them to find the final answer. We have used Text Retrieval Conference (TREC) QA track questions, web data, and AQUAINT and AQUAINT-2 datasets for training and testing our system. Results show that the learned features can perform a better ranking in comparison with other evaluation formulas.
               
            

@&#INTRODUCTION@&#

Question Answering systems are advanced search engines that can provide the least brief and the most complete answer to users instead of making them read a set of documents. QA systems are essential tools for dealing with the fast-growing global information. However, upgrading a search engine to a QA system is a complex and open-ended problem (Zadeh, 2003). Machine-based human-like answering has been a dream that Artificial Intelligence (AI) scientists have been trying to achieve. Based on Russell and Norvig (2010), AI field has four definition groups and one of them is based on the Turing test, which is about the ability of machines to communicate or answer like a human. Moreover, Arthur Samuel (Samuel, 1983) in his talk titled “AI: where it has been and where it is going” stated the main goal of AI and machine learning as: “to get machines to exhibit behavior, which if done by humans, would be assumed to involve the use of intelligence.”

At the early years, the fundamental problem of QA was converting a natural language question to a Structured Query Language (SQL) query and retrieving answers from structured data. These convert-to-query-based systems have been called restricted-domain systems because they can only answer questions related to their already-provided structured data (Indurkhya & Damerau, 2010). However, with rapid enlargement of data in unstructured format, extracting answers from domain-independent sources became the main challenge of QA. These QA systems, which operate on sources that could be general and free of specific domain, are called open-domain QA systems. The first web-based QA system developed in 2004, named Start (Katz, Lin, & Felshin, 2002), and contemporary systems are Wolfram
                        1
                     
                     
                        1
                        
                           www.wolframalpha.com
                        
                      from IBM, and AskHERMES (Yen et al., 2013b).

The simplest form of answering for a QA system is returning a paragraph to a definitional question. However, factoid question is the most discussed question type. The answer of a factoid question is a simple fact such as name of a person or a location that can be found in a sentence (Jurafsky & Martin, 2009). In addition to these two question types, there are others such as list, hypothetical, causal, relationship, procedural, and confirmation questions. In this paper, we have worked on definitional and factoid questions with a four-phase framework including paragraph ranking, sentence ranking, word extraction, and word ranking.

In order to select an answer from a set of candidates, they must be ranked. The ranking problem includes computing a score based on a set of features. Computing the sum of all feature values or 
                        
                           
                              ∑
                              i
                           
                           
                              (
                              
                                 f
                                 e
                                 a
                                 t
                                 u
                                 r
                                 
                                    e
                                    i
                                 
                              
                              )
                           
                        
                      formula cannot reflect the true value of an answer because each feature has a weight. Finding the weights is a supervised learning problem that can be solved by a discriminant-based classification algorithm. In this paper, we used three methods for this task including Linear Discriminant Analysis (LDA), Logistic regression, and Support Vector Machines (SVM). Taking into account the weights, the score computation formula will become
                        
                           
                           
                              ∑
                              i
                           
                           
                              (
                              
                                 f
                                 e
                                 a
                                 t
                                 u
                                 r
                                 
                                    e
                                    i
                                 
                                 
                                 ×
                                 
                                 w
                                 e
                                 i
                                 g
                                 h
                                 
                                    t
                                    i
                                 
                              
                              )
                           
                        
                     .

Following these two formulas, one possible continuation is using other arithmetic operators such as multiplication, division, exponential, and logarithm, which is the main purpose of this paper. Here a challenging problem is finding a promising ranking formula based on a set of operators and features. In order to solve such kind of problems, evolutionary approaches can be used since they are capable of searching in large-scale search spaces effectively. Among different types of evolutionary algorithms, Genetic Programming (GP) whose individuals are trees would be the best candidate. Since our problem is finding a ranking formula, which can be modeled as a tree of operators and features, GP would be a perfect choice. The main contribution of this paper is learning efficient features via GP algorithm for a Question Answering system.

The remainder of this paper is organized as follows: in Section 2, we will discuss about related works. In Section 3, the structure of our proposed QA system will be presented in detail. Sections 4 and 5 deal with answering definitional and factoid questions. The last three sections describe experimental results, discussion, and conclusions respectively.

@&#RELATED WORKS@&#

Question answering subject is discussed by chapter books in Indurkhya and Damerau (2010), and Jurafsky and Martin (2009), and by survey papers such as Kolomiyets and Moens (2011).

Previous works on feature engineering task includes Severyn and Moschitti (2013), Severyn et al. (2013), Tymoshenko, Moschitti, and Severyn (2014), and Severyn and Moschitti (2012). Tymoshenko et al. (2014) represented question and candidate answer passages with pairs of shallow syntactic/semantic trees whose constituents are connected using Linked Open Data (LOD). The trees are processed by SVM and tree kernels, which can automatically exploit tree fragments. Severyn and Moschitti (2013) used automatic feature engineering for answer-selection task as an alternative to manual rule definition. Severyn, Nicosia, and Moschitti (2013) proposed a method to learn automatically complex patterns such as relational semantic structures occur in questions and their answer passages. They achieved this task by providing their learning algorithm with the trees derived from the syntactic trees of questions and passages connected by relational tags, where the latter are provided by automatic classifiers. Severyn and Moschitti (2012) defined a novel supervised approach that exploits structural relationships between a question and its candidate passages to learn a re-ranking model. They encoded structures in SVM by means of sequence and tree kernels, which can implicitly represent question and answer pairs in huge feature spaces.

State-of-the-art text-ranking papers include Yen et al. (2013a), Moschitti and Quarteroni (2011), Heie et al. (2012), Moreda et al. (2011), and Ko et al. (2010). Yen et al. (2013a) proposed an SVM-based model for context ranking, which focuses on weighting the Named Entities (NEs) terms based on their contextual clues such as position information, Named-Entity words, phrase structures, and question terms. They combined rich features to predict whether the input content is relevant to the question type. Moschitti and Quarteroni (2011) explored linguistic kernels for answer re-ranking. They used supervised discriminative models that learn to rank answers using examples of question and answer pairs. They used four features: bag-of-words, N-grams, syntactic chunks, and head noun phrase-verb phrase-prepositional phrase (NP-VP-PP) groups. Heie et al. (2012) proposed a statistical language modeling-based ranking using keywords and question type features, and a mathematical model for answer extraction. Their results show that the best performance is achieved when they are using web data and exploiting data redundancy. Using semantic features in QA is discussed in Moreda, Llorens, Saquete, and Palomar (2011). They investigated the influence of using two semantic-based features: semantic roles and WordNet, for the answer extraction module of a general open-domain QA system. Ko et al. (2010) proposed a unified probabilistic framework, which combines multiple evidences to address challenges in answer ranking and answer merging. Their framework combines answer relevance and similarity features, and uses five extractors for evaluation including finite state transducers and SVM.

Web-based QA systems generally perform a big-domain search in web data and evolutionary algorithms can be employed in the main structure of these systems. Evolutionary methods with web-based QA systems are discussed in Khodadi and Saniee (2014), Atkinson, Figueroa, and Andrade (2013), and Figueroa and Neumann (2008). In Khodadi and Saniee (2014), we used Memetic Algorithm (MA) for searching among sentences of retrieved documents from a search engine. We obtained a threshold value for efficient number of retrieved documents with respect to accuracy and process time. Atkinson et al. (2013) proposed an evolutionary model for ranking answers of how-to questions in a community QA system. Their approach combines evolutionary computation techniques and clustering methods to effectively rate the best answers from web-based user-generated contents. Figueroa and Neumann (2008) used Genetic Algorithm (GA) for searching among N-grams in order to find a factoid answer. The N-grams are extracted from the best snippets, which have been identified by a search engine. Their main idea is searching for those substrings in the snippets, whose contexts are most similar to contexts of already known answers.

State-of-the-art papers in question classification include Toba, Ming, Adriani, and Chua (2014), Croce, Moschitti, and Basili (2011), and Yu et al. (2010). Toba et al. (2014) used a hierarchy of classifiers for providing high quality answers in community QA. Croce et al. (2011) merged convolution dependency tree kernels with lexical similarities for question classification. Yu et al. (2010) used co-training style semi-supervised learning for question classification. Their method extracts high frequency keywords as classification features and uses word semantic similarity to adjust feature weights.

Most of the papers that are discussed targeted factoid questions. Definitional questions are investigated in Han, Song, Kim, and Rim (2007) and Yu et al. (2007). Han et al. (2007) used linguistic features and definition terminology for answer extraction and ranking strategies in definitional QA. Yu et al. (2007) developed a medical definitional QA system called MedQA that automatically analyze a large number of electronic documents to generate short and coherent answers in response to definitional questions like “What is X?”. They classified definitional questions into five categories, including what, how, do, can, and others. The first step for our feature learning task is exploring high-quality features. We have gathered a list of features that has been used in these papers and they are illustrated in Table 1
                     . We used some of these features in our system. These features will be described in Section 4.

The structure of proposed QA system is illustrated in Fig. 1. The first input is question and it is given to a search engine. Contents of retrieved sources are the second input. The Question_analyzer part extracts type and features of the question. The question type is determined based on the Question_types collection, which is a set of handcrafted question type patterns that we made. The contents of retrieved sources are given to the Paragraph_feature_extractor and the Pre-processor parts for extracting their features and editing them for next parts. The pre-processed texts, features of them, features of the question, and the question type are given to the Paragraph_ranker part. This part ranks the paragraphs with respect to their features and their weights from Weights collection. The output of the Paragraph_ranker part can be the best paragraph as the Definitional_answer or a portion of the paragraphs for the next parts.

The Sentence_feature_extractor part extracts features of the sentences and the Sentence_ranker part evaluates each sentence and eliminates a portion of them. The qualified sentences are passed to the Word_extractor part and this part extracts one or more words from each sentence as factoid answers based on the Question_types collection. The extracted words should be ranked in order to find the best one. After eliminating clear errors by the Word_pre_processor part, the Word_feature_extractor part extracts features of the factoid answers and the Word_ranker part evaluates each one with respect to the features and their weights, and sends the best one as the final answer. More information about each part will be represented in the next sections.

We illustrated the Question Answering problem from this paper point of view and our main purpose in Fig. 2
                     . As we discussed earlier, the problem of QA leads us to a ranking problem in order to rank the extracted information. The ranking problem includes defining a set of features and this task leads us to find the weights of these features. Our main purpose is creating a formula that is a combination of features and arithmetic operators, and using it as a new feature to perform a better ranking.

In order to determine the question type, we used a set of handcrafted patterns. We classified questions by their main question words such as “where” and “when”, and the answer is extracted based on three elements: prepositions of the answer sentence such as “in” and “on”, keywords of the question, and noun words of the question. Our question types are illustrated in Fig. 3
                      and the factoid types and their handcrafted patterns mentioned in our previous work (Khodadi & Saniee, 2014) are illustrated in Table 2. The patterns of Table 2 are not comparable with state-of-the-art papers. However, when we extract words from a set of sentences with remarkable redundancy, the weakness of the patterns will not be noticed. In the following two sections, we will describe our answering framework with more details.

In order to answer a definitional question, our system ranks paragraphs of the related documents, which are retrieved by a search engine. The ranking task is based on 
                        
                           
                              ∑
                              i
                           
                           
                              (
                              
                                 f
                                 e
                                 a
                                 t
                                 u
                                 r
                                 
                                    e
                                    i
                                 
                                 
                                 ×
                                 
                                 w
                                 e
                                 i
                                 g
                                 h
                                 
                                    t
                                    i
                                 
                              
                              )
                           
                        
                      formula. The features will be described later, but the weights are learned by discriminant-based classification methods and details about this task will be described in the following paragraphs.

In supervised machine learning, the output is estimated from the input based on the learned model. This model is determined from a training set of input and output pairs. One category of the supervised approaches is discriminant-based methods, whose learned model is a line that separates the input examples into two groups. The coefficients of this line are weights of the features or the effects of features in the separation task. The result of 
                        
                           
                              ∑
                              i
                           
                           
                              (
                              
                                 f
                                 e
                                 a
                                 t
                                 u
                                 r
                                 
                                    e
                                    i
                                 
                                 
                                 ×
                                 
                                 w
                                 e
                                 i
                                 g
                                 h
                                 
                                    t
                                    i
                                 
                              
                              )
                           
                        
                      formula can be used both as the score of an input and the determination of the input class.

In order to learn weights of definitional questions features, inputs are a set of questions and a set of paragraphs with their features values, the output is the score of the paragraphs, and the classes are Related_to_question and Not_ related_to_question. The formula of computing the paragraph score is based on Eq. (1).

                        
                           (1)
                           
                              
                                 g
                                 
                                    (
                                    
                                       x
                                       |
                                       W
                                    
                                    )
                                 
                                 =
                                 
                                    w
                                    0
                                 
                                 +
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 
                                    (
                                    
                                       
                                          w
                                          i
                                       
                                       ×
                                       
                                          x
                                          i
                                       
                                    
                                    )
                                 
                              
                           
                        
                     
                  

In this formula, x is a new input (here a paragraph and a sentence or a word in the next section), W is the set of weights learned from the training phase, g(x|W) is the score of the input, w
                     0 andwi
                      (i from 1 to n) are the learned weights, and xi
                      (i from 1 to n) are the feature values. The magnitude of the wi
                      shows the importance of xi
                      and its sign indicates the positivity or negativity of its effect. After computingg(x|W), we should filter candidates based on a threshold. Based on Eq. (2), the threshold is a, and y is the binary result of filtering. The value of a can be set before the training process and w
                     0 transfers the values to satisfy a. We have set
                        
                           a
                           =
                           0
                        
                      however, in the experimental result section we will test the outcome of changinga, after the training process.

                        
                           (2)
                           
                              
                                 y
                                 =
                                 
                                    {
                                    
                                       
                                          
                                             1
                                          
                                          
                                             
                                                i
                                                f
                                                
                                                
                                                
                                                g
                                                (
                                                
                                                   x
                                                   |
                                                   W
                                                
                                                )
                                                >
                                                a
                                             
                                          
                                       
                                       
                                          
                                             0
                                          
                                          
                                             
                                                i
                                                f
                                                
                                                
                                                
                                                g
                                                (
                                                
                                                   x
                                                   |
                                                   W
                                                
                                                )
                                                <
                                                −
                                                a
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                  

The features of paragraphs categorized by their types including lexical, syntactic, and semantic, are illustrated in Fig. 4
                     . Furthermore, these features, their description, and their formulas are illustrated in Table 3
                     . In these features, Q is the question and C is a candidate or a paragraph. The result of each feature is normalized and it is between 0 and 1. First, the system investigates existence of a question keyword in the candidate, and in condition that it is absent; it probes stem and synonym of the question keyword. Therefore, the sum of the mutual keywords, stems, and synonyms between the candidate and the question is less or equal than the question keywords, based on Eq. (3).

                        
                           (3)
                           
                              
                                 
                                    |
                                 
                                 
                                    
                                       K
                                       e
                                       y
                                       w
                                       o
                                       r
                                       d
                                       s
                                       
                                          (
                                          Q
                                          )
                                       
                                       |
                                       ≥
                                       |
                                       K
                                       e
                                       y
                                       w
                                       o
                                       r
                                       d
                                       s
                                       
                                          (
                                          C
                                          )
                                       
                                    
                                    
                                       ∩
                                       K
                                    
                                    
                                       e
                                       y
                                       w
                                       o
                                       r
                                       d
                                       s
                                       
                                          (
                                          Q
                                          )
                                       
                                       |
                                       +
                                       |
                                       S
                                       y
                                       n
                                       o
                                       n
                                       y
                                       m
                                       s
                                       
                                          (
                                          C
                                          )
                                       
                                    
                                    
                                       ∩
                                       K
                                    
                                    e
                                    y
                                    w
                                    o
                                    r
                                    d
                                    s
                                    
                                       (
                                       Q
                                       )
                                    
                                    
                                       |
                                       +
                                       |
                                    
                                    S
                                    t
                                    e
                                    m
                                    s
                                    
                                       (
                                       C
                                       )
                                    
                                    
                                       ∩
                                       K
                                    
                                    e
                                    y
                                    w
                                    o
                                    r
                                    d
                                    s
                                    
                                       (
                                       Q
                                       )
                                    
                                 
                                 
                                    |
                                 
                              
                           
                        
                     
                  

In this equation, |A| means the length of set A.

We used bigrams, trigrams, and longest mutual sequence of words features for exploiting N-grams. The formulas of keyword, synonym, stem, bigram, trigram, and longest sequence similarities are normalized by dividing the number of similar elements between the question and the candidate by the number of question elements. We also used average similarities for keywords, synonyms, stems, bigrams, and trigrams. In the average-based features, the results are normalized by the number of question elements multiplied by the number of paragraph sentences. The purpose of using these average-based features is discriminating between paragraphs that have only one or few sentences about the questions and paragraphs with more sentences about the question.

In the keyword similarity feature, each similar keyword is counted only one time due to the normalization task. However, we should take into account the paragraphs that have repeated question keywords. In the All mutual keywords feature, all similar keywords are counted and the result is divided by Nw
                     , which is the number of paragraph words. The number of sentences and words in the paragraph are also used as features. The rank of the document in search engine is also usable because the search engine had already performed a ranking.

The final feature will be a combination of the mentioned features and a set of operators. A complete description of the proposed feature learning method will be described in Section 4.1. With these features, the score of each paragraph is computed based on Eq. (4).

                        
                           (4)
                           
                              
                                 S
                                 c
                                 o
                                 r
                                 e
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             f
                                             e
                                             a
                                             t
                                             u
                                             r
                                             e
                                             s
                                             _
                                             c
                                             o
                                             u
                                             n
                                             t
                                          
                                       
                                       
                                          (
                                          
                                             f
                                             e
                                             a
                                             t
                                             u
                                             r
                                             
                                                e
                                                i
                                             
                                             
                                             ×
                                             
                                             w
                                             e
                                             i
                                             g
                                             h
                                             
                                                t
                                                i
                                             
                                          
                                          )
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             w
                                             e
                                             i
                                             g
                                             h
                                             t
                                             s
                                             _
                                             c
                                             o
                                             u
                                             n
                                             t
                                          
                                       
                                       w
                                       e
                                       i
                                       g
                                       h
                                       
                                          t
                                          i
                                       
                                    
                                 
                              
                           
                        
                     
                  

The features that we used are similar to the features in Table 1. However, our features are in a uniform format and we did not use some of the features in Table 1 such as complex syntactic and semantic features because the probability of finding a promising formula will decrease with useless increasing of search domain. Moreover, some of these features are similar in context.

Now with these features we want to produce a new one that is a combination of the features and arithmetic operators. The proposed feature learning task will be described in the following part.

We used Genetic Programming algorithm for our feature learning task. GP is an evolutionary algorithm whose individuals are trees. Moreover, GP solves problems without any need to specify form of solution or the best tree in advance (Poli, Langdon, & McPhee, 2008). The trees in GP algorithm are computable or process-able objects such as a formula or a computer program. Leaf nodes or terminals are concepts such as a feature and non-leaf nodes or functions are tools to behave with these concepts such as arithmetic operators. The purpose is reaching a tree by evolutionary methods that can perform a high-quality ranking. The pseudo-code of the GP algorithm with the specific operators that we used is illustrated in Fig. 5
                        . The inputs are population size, number of parents, crossover probability, and mutation probability, and the output is the best-visited formula. Description about main parts of the algorithm will be represented in Sections 4.1.1 to 4.1.5. Furthermore, the summarized details of our implementation are illustrated in Table 4. The purpose is finding the most promising formula that can rank the paragraphs based on function and terminal sets. Function set includes six operators {+, –, ×, ÷, ^2, lg()} and terminal set includes sixteen features (Table 3 features except GPF). An individual or a population member is a formula with some features from 2 to 16, and some operators from 1 to 31 (16 unary and 15 binary operators). For example, feature
                        1 × ((feature
                        2 + lg(feature
                        3)) – feature
                        4/feature
                        5) can be one of the individuals. More details about each part will be represented in the following parts.

There are two famous methods to initialize the population in GP: Full and Grow. In both, initial individuals are generated so that they do not exceed a specified depth. In the Full method, so named because it generates full trees, nodes are taken randomly from the function set until the maximum depth is reached, and beyond that depth, only terminals can be chosen. The Grow method allows creating trees with more varied sizes and shapes. Nodes are selected from the functions and terminals sets until the depth limit is reached and like the Full method once the depth limit is reached only terminals can be chosen (Poli et al., 2008). Following these methods, Koza (1992) proposed an approach named Ramped half-and-half that we used for our initialization phase. The Ramped approach is a combination of the Full and Grow approaches and it constructs half the initial population using the Full method and half using the Grow method in order to ensure diversity in the population. This is done using a range of depth limits. Hence, the method is called Ramped.

Pseudo-code of the Ramped method is represented in Fig. 6
                           . The inputs of this code are maximum depth and population size, and the output is the initialized population. To learn the combined feature of paragraph ranking, the depth of the Full trees is set to 4 and the maximum depth of the Grow trees are set to 15, which are the maximum lengths for 16 leaves. Furthermore, in the Full method, lg(x) and ^2 operators only can be chosen in the last depth. Fig. 7
                            shows two trees that have been built by the Grow and Full methods (together the ramped method) and result of the crossover operator on them.

@&#EVALUATION@&#

In our implementation, fitness of an individual is the sum of absolute errors between a set of hand-made rankings and the rankings that the individual produces based on Eq. (5). The most promising or the individual with best-fitness is the one with the minimum error so Fitness = 0 means that ranking of individual is completely correct.

                              
                                 (5)
                                 
                                    
                                       F
                                       i
                                       t
                                       n
                                       e
                                       s
                                       s
                                       
                                          (
                                          
                                             L
                                             e
                                             a
                                             r
                                             n
                                             e
                                             d
                                             F
                                             a
                                             t
                                             u
                                             r
                                             
                                                e
                                                i
                                             
                                          
                                          )
                                       
                                       =
                                       
                                          ∑
                                          
                                             j
                                             ∈
                                             Q
                                             S
                                          
                                       
                                       
                                          ∑
                                          
                                             k
                                             ∈
                                             
                                                C
                                                j
                                             
                                          
                                       
                                       |
                                       
                                          R
                                          e
                                          a
                                          l
                                          R
                                          a
                                          n
                                          
                                             k
                                             k
                                             j
                                          
                                          −
                                          E
                                          s
                                          t
                                          i
                                          m
                                          a
                                          t
                                          e
                                          d
                                          R
                                          a
                                          n
                                          
                                             k
                                             k
                                             j
                                          
                                       
                                       |
                                    
                                 
                              
                           
                        


                           LearnedFaturei
                            is one of the individuals and it is a combination of features and operators. QS is the training question set and it contains a set of 50 questions with 20 manually-ranked candidates for each one. These questions are from TREC 2007 QA Track (Dang, Kelly, & Lin, 2008) and the manually ranked paragraphs are from search engine retrieved sources. Three manually ranked candidates for an example from QS, which is “Who invented the Rubik's Cube?” are represented in Table 5
                           . Fitness is computed based on the ability of the individual to rank the labeled candidates of each question in QS. Furthermore, Cj
                            is candidate set for jth question in QS, 
                              
                                 R
                                 e
                                 a
                                 l
                                 R
                                 a
                                 n
                                 
                                    k
                                    k
                                    j
                                 
                              
                            is actual hand-made rank of kth candidate for jth question, and 
                              
                                 E
                                 s
                                 t
                                 i
                                 m
                                 a
                                 t
                                 e
                                 d
                                 R
                                 a
                                 n
                                 
                                    k
                                    k
                                    j
                                 
                              
                            is estimated rank by ith individual. An example of computing the fitness of a formula is represented in Table 6
                           , which is based on the resulted ranks for 3 candidates of a question and their hand-made ranks.

The parent selection process is performed by the Proportional selection approach (Holland, 1975). Based on this method, each individual can be a parent with a probability based on Eq. (6) until the selected parents become equal to parent_size argument.

                              
                                 (6)
                                 
                                    
                                       
                                          P
                                          
                                             p
                                             r
                                             o
                                             p
                                             o
                                             r
                                             t
                                             i
                                             o
                                             n
                                             a
                                             l
                                             _
                                             s
                                             e
                                             l
                                             e
                                             c
                                             t
                                             i
                                             o
                                             n
                                          
                                       
                                       
                                          (
                                          x
                                          )
                                       
                                       =
                                       
                                          
                                             f
                                             i
                                             t
                                             n
                                             e
                                             s
                                             s
                                             
                                                (
                                                x
                                                )
                                             
                                          
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   p
                                                   o
                                                   p
                                                   u
                                                   l
                                                   a
                                                   t
                                                   i
                                                   o
                                                   n
                                                   _
                                                   s
                                                   i
                                                   z
                                                   e
                                                
                                             
                                             f
                                             i
                                             t
                                             n
                                             e
                                             s
                                             s
                                             
                                                (
                                                i
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The subtree method is used for the crossover function. Given two parents, subtree crossover randomly selects a crossover point in each parent tree and creates two offspring by exchanging the subtrees rooted at the crossover points in the two parents. An example for the crossover function is represented in Fig. 7. Two trees that have been built with different approaches are combined in order to build two new individuals. Replacing a promising sub-tree in a low-quality individual can enhance its ranking capability.

The subtree approach is used for the mutation function, which randomly selects a mutation point in a tree and substitutes the subtree rooted there with a randomly generated subtree.

@&#RESULTS@&#

With the mentioned settings we executed the algorithm and the best two results are represented in Table 7
                           . Morover, the tree structure of the first best learned feature is illustrated in Fig. 8
                           . We used the first learned feature of Table 7 as a new feature for paragraph ranking. We will show in the experimental results that this feature alone can rank paragraphs with a high accuracy.

In order to rank paragraphs, all features must be multiplied by a set of weights. These weights can be learned by discriminant-based classification methods. We used LDA, Logistic regression, and SVM for learning this task. The output of these methods is similar, which is a separator line, but they have diverse approaches. We used three methods to find out which one has a better result.

Our training data for learning process were a set of 200 hand-made definitional questions with 10 labeled candidates for each question and values of the features. Furthermore, the paragraphs are from search engine retrieved sources. An example of our training data is illustrated in Table 8
                        . We used two-fold cross validation for the learning process. It means we performed the training process for each set of 100 questions and tested the results with the other question set. The evaluation metric that is used is Top 1 accuracy based on Eq. (7). The Top 1 accuracies for each learning method with each test set are illustrated in Table 9
                        . The best results were for ranking with SVM weights and these weights are illustrated in Table 10.
                        
                        
                           
                              (7)
                              
                                 
                                    T
                                    o
                                    p
                                    
                                    1
                                    
                                    a
                                    c
                                    c
                                    u
                                    r
                                    a
                                    c
                                    c
                                    y
                                    =
                                    
                                       
                                          n
                                          u
                                          m
                                          b
                                          e
                                          r
                                          
                                          o
                                          f
                                          
                                          c
                                          o
                                          r
                                          r
                                          e
                                          c
                                          t
                                          
                                          a
                                          n
                                          s
                                          w
                                          e
                                          r
                                          s
                                       
                                       
                                          n
                                          u
                                          m
                                          b
                                          e
                                          r
                                          
                                          o
                                          f
                                          
                                          q
                                          u
                                          e
                                          s
                                          t
                                          i
                                          o
                                          n
                                          s
                                       
                                    
                                 
                              
                           
                        
                     

We should also address the NIL-answering matter. In the paragraph answering task, the best paragraph will be returned and our system does not return NIL. However, in order to answer a factoid question, paragraphs are filtered by a based on Eq. (2) and it is possible that system returns NIL for a factoid question.

In order to rank the sentences, which are sentences of the filtered paragraphs from the previous part, we used 12 features and one learned feature. These features categorized in three groups are illustrated in Fig. 9
                         and those features that are not described in Table 3 are illustrated in Table 11
                        . One of the new features is Question type adaptation and it is based on the analogy of a sentence with expected answer type of the question. This feature contains three inner features and each one is whether 0 or 1 including: adaptation of main verb, adaptation of main noun, and adaptation of main preposition. Table 2 contains all feasible prepositions for each question type.

Other new features for sentences are Cosine distance, which is the vector-based distance between the question and the candidate vectors of words. Paragraph value is the last new feature, which is the score from the previous phase for paragraph of a sentence. In the proposed approach, the result of each ranking is a feature in the next one.

Here we also used GP algorithm for the feature learning task and the two best results are represented in Table 12
                        . Morover, the tree structure of the first best learned feature is illustrated in Fig. 10
                        . We used the first learned feature of Table 12 as a feature for sentence ranking.

Moreover, for the weight learning task, the training and testing data were 200 questions from the TREC 2005 (Voorhees & Dang, 2006) and 2006 (Dang, Lin, & Kelly, 2007) QA Tracks with 10 candidates (sentence) for each question. We used two-fold cross validation for each learning method and the results of Top 1 accuracies for each set and with each learning method are illustrated in Table 13
                        . The weights of the best results were for SVM and they are illustrated in Table 14.
                        
                     

The sentences ranking task filters sentences and passes them to the IE part, which extracts words or factoid answers from them. This task will be discussed in the following part.

In order to prepare a factoid answer we must extract words from each sentence and rank them. For the word extraction task, we used our hand-made patterns that are illustrated in Table 2. Moreover, we used three features and one learned feature for word ranking, and they are illustrated in Table 15
                         with their description, formula, and type. Following the hierarchical rankings, we used result of the sentence ranking as a feature in this phase. The other feature for this part is Popularity, which is the number of times that each word of a candidate is repeated in the other candidates. This feature is useful for exploiting web redundancy. The last feature is number of words.

Here we also used the GP algorithm for the feature learning task and the two best results are represented in Table 16
                        . Furthermore, the tree structure of the first best learned feature is illustrated in Fig. 11
                        . We used the first learned feature of Table 16 as a feature for word ranking.

Moreover, for the weight learning task, the training and testing data were the questions from the previous phase and manually labeled extracted word(s) from each sentence. Similar to the previous phases, two-fold cross validation is used and the Top 1 accuracies of each learning method with each testing set are illustrated in Table 17
                         and the weights of the best result that were for SVM are illustrated in Table 18
                        . After this phase, the highest-ranked factoid answer is returned to the user.

@&#EXPERIMENTAL RESULTS@&#

The accuracy of the system has been computed for answering TREC 2004 (Voorhees, 2005) and 2007 QA Track factoid questions with texts from top-n Google-retrieved documents, and AQUAINT (Graff, 2002) and AQUAINT-2 (Voorhees & Graff, 2008) datasets. These results are illustrated in Table 19
                     . The answering is based on the value of n, which is the number of retrieved documents. We used different values for n including 1, 5, 10, 15, and 50. Web-based data provided by Google API
                        2
                     
                     
                        2
                        
                           developers.google.com
                        
                      and AQUAINT and AQUAINT-2 corpora are indexed with Lemur toolkit
                        3
                     
                     
                        3
                        
                           www.lemurproject.org
                        
                     . Furthermore, we should address NIL answering based on AQUAINT datasets because answer of some of TREC questions does not exist in their related corpus (AQUAINT for 2004 and AQUAINT-2 for 2007). As a result, the formula of Top 1 accuracy for AQUAINT-based answering is based on Eq. (8).

                        
                           (8)
                           
                              
                                 T
                                 o
                                 p
                                 1
                                 
                                 a
                                 c
                                 c
                                 u
                                 r
                                 a
                                 c
                                 c
                                 y
                                 =
                                 
                                    
                                       n
                                       u
                                       m
                                       b
                                       e
                                       r
                                       
                                       o
                                       f
                                       
                                       c
                                       o
                                       r
                                       r
                                       e
                                       c
                                       t
                                       
                                       a
                                       n
                                       s
                                       w
                                       e
                                       r
                                       s
                                       +
                                       n
                                       u
                                       m
                                       b
                                       e
                                       r
                                       
                                       o
                                       f
                                       
                                       c
                                       o
                                       r
                                       r
                                       e
                                       c
                                       t
                                       l
                                       y
                                       
                                       n
                                       i
                                       l
                                       
                                       a
                                       n
                                       s
                                       w
                                       e
                                       r
                                       e
                                       d
                                       
                                       q
                                       u
                                       e
                                       s
                                       t
                                       i
                                       o
                                       n
                                       s
                                    
                                    
                                       n
                                       u
                                       m
                                       b
                                       e
                                       r
                                       
                                       o
                                       f
                                       
                                       q
                                       u
                                       e
                                       s
                                       t
                                       i
                                       o
                                       n
                                       s
                                    
                                 
                              
                           
                        
                     
                  

We also compared our system with Heie, Whittaker, and Furui (2012) and Google snippets in answering TREC 2004 questions with web data and AQUAINT dataset. These results are illustrated in Table 20
                     . However, the values of web-based results are not comparable because retrieved documents of search engine are not equal in different times.

Moreover, we compared our results in answering TREC 2004 and 2007 QA Track questions with participants of these competitions and the results are illustrated in Table 21
                      and 22.
                     
                  

The main purpose of this paper is finding a promising formula for evaluating a text. Therefore, we compared capability of our learned features with other possible evaluation formulas. The Top 1 accuracy of answering TREC 2007 questions with web data is computed with five formulas, including:

                        
                           •
                           
                              
                                 
                                    
                                       ∑
                                       i
                                    
                                    
                                       (
                                       
                                          f
                                          e
                                          a
                                          t
                                          u
                                          r
                                          
                                             e
                                             i
                                          
                                          
                                          ×
                                          w
                                          e
                                          i
                                          g
                                          h
                                          
                                             t
                                             i
                                          
                                       
                                       )
                                    
                                 
                               or SumX: with normal features or features of Fig. 4, Fig. 9, and Table 15, excluding the GP-based learned features;


                              
                                 
                                    
                                       ∑
                                       i
                                    
                                    
                                       (
                                       
                                          f
                                          e
                                          a
                                          t
                                          u
                                          r
                                          
                                             e
                                             i
                                             2
                                          
                                          
                                          ×
                                          
                                          w
                                          e
                                          i
                                          g
                                          h
                                          
                                             t
                                             i
                                          
                                       
                                       )
                                    
                                 
                               or SumX2: with normal features or features of Fig. 4, Fig. 9, and Table 15, excluding the GP-based learned features;


                              
                                 
                                    
                                       ∏
                                       i
                                    
                                    f
                                    e
                                    a
                                    t
                                    u
                                    r
                                    
                                       e
                                       i
                                    
                                 
                               or MulX: with normal features or features of Fig. 4, Fig. 9, and Table 15, excluding the GP-based learned features;


                              LearnedFeature or LF: using only the learned features, which are represented in Figs. 8, 10, and 11;


                              
                                 
                                    
                                       ∑
                                       i
                                    
                                    
                                       (
                                       
                                          f
                                          e
                                          a
                                          t
                                          u
                                          r
                                          
                                             e
                                             i
                                          
                                          
                                          ×
                                          
                                          w
                                          e
                                          i
                                          g
                                          h
                                          
                                             t
                                             i
                                          
                                       
                                       )
                                    
                                 
                               or SumAllX: with all features of Fig. 4, Fig. 9, and Table 15 (including the GP-based learned features).

The results of these experiment show the Top 1 accuracies for paragraph, sentence, and word rankings, with different number of retrieved sources, including 1, 5, 10, 15, and 50, with different evaluation formulas. These results, which are separated for paragraph, sentence, and word rankings, are illustrated in Figs. 12–14
                     
                     
                     . The results show that GP-based learned features (LF) can perform a better ranking in comparison with other formulas. However, using all features (SumAllX) produced the best accuracies. As the result, the learned features should be used as complementary features in order to increase quality of ranking.

Another considerable evaluation is finding the right threshold for the classification formula or Eq. (2). Changing the threshold will affect the number of filtered elements and the accuracy. We used different values for threshold, including: 0, 0.25, 0.5, and 0.75, and different number of retrieved documents values (1, 5, 10, 15, and 50) in answering TREC 2007 questions with web data. These results are illustrated in Fig. 15
                      and they show that increasing the threshold decreases the accuracy because with fewer candidates, the probability of finding answer will decrease.

@&#DISCUSSION@&#

The reason for which the learned formulas have proved to be more valuable than others is their combination of features and operators that can better reflect the true value of a text and discriminate a set of candidates. The multiplication, division, and exponential operators enforce a useful distance between candidates based on key features.

GP algorithm can find the accurate position of each feature and connect them with accurate operators. Moreover, GP algorithm can take into account the intra relations between features. In order to describe these “intra relations”, we represent an example: there are two important features and each has a great weight. However, there is a hidden relation between them, in which if the both features are high, the score of the candidate is more than addition of them. This intra relation can be used with the exponential operator.

One possible side effect of learning a complex formula is over-fitting. However, we have used four series of TREC QA Track questions for learning and testing our learned formulas. We believe that with a set of features, there is a unique formula containing some features and operators that can produce the best ranking. In this paper, we tried to achieve this perfect unique combination of features. We also believe that with these features, the resulted formulas are applicable to other corpora because they are generalized with web data, which does not have a unified format. Moreover, the formulas are only dependent on the features, not the corpus. Our approach can be used in other ranking problems with choosing the right operators and features.

@&#CONCLUSIONS@&#

We proposed a Genetic Programming-based approach for learning new features based on a set of features and arithmetic operators. The learned features proved that they can perform a better ranking in comparison with three types of evaluation formulas, including 
                        
                           
                              ∑
                              i
                           
                           
                              (
                              
                                 f
                                 e
                                 a
                                 t
                                 u
                                 r
                                 
                                    e
                                    i
                                 
                                 ×
                                 w
                                 e
                                 i
                                 g
                                 h
                                 
                                    t
                                    i
                                 
                              
                              )
                           
                        
                     ,
                        
                           
                              ∑
                              i
                           
                           
                              (
                              
                                 f
                                 e
                                 a
                                 t
                                 u
                                 r
                                 
                                    e
                                    i
                                    2
                                 
                                 ×
                                 w
                                 e
                                 i
                                 g
                                 h
                                 
                                    t
                                    i
                                 
                              
                              )
                           
                        
                     , and 
                        
                           
                              ∏
                              i
                           
                           f
                           e
                           a
                           t
                           u
                           r
                           
                              e
                              i
                           
                        
                     . The weights of the learned features, which have higher values than the other features, also show the importance of them.

We also developed a QA framework with three hierarchical ranking phases to test the learned features. In our framework, we used result of search engine ranking as a feature in the paragraph ranking. Moreover, result of the paragraph ranking is a feature in the sentence ranking and the result of the later is a feature in the word ranking.

Discriminant-based classification methods are used to learn the weights of the features and SVM has the best results for all the three phases. This result is also describable by theory; trying to maintain a margin along with separation will cause better results. The threshold of the filtering formula is also tested with different values in order to find the best result.

Our system is evaluated with web data, and AQUAINT and AQUAINT-2 datasets. However, web-based answering has better accuracy because it uses redundancy in web. We used this property via the Popularity feature in the word ranking and using n-retrieved documents.

Moreover, we explored the answering of factoid and definitional questions and future works can include answering other questions like list, procedural, and causal that includes investigating their features and defining new phases for them. Another possible future work for our approach can be a dynamic system that can update its evaluation formulas with respect to feedbacks from users.

@&#REFERENCES@&#

