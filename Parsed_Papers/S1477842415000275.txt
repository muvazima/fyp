@&#MAIN-TITLE@&#Automatic validation for binary translation

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           An automatic validator supports static, dynamic, and hybrid binary translator.


                        
                        
                           
                           Instruction-level validation by comparing the architecture states and stored values.


                        
                        
                           
                           We propose two mechanisms to make the comparisons into simple equality checks.


                        
                        
                           
                           Two acceleration method provided to make the validation process faster.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Binary translation

Validation

ARM

QEMU

Architecture state

@&#ABSTRACT@&#


               
               
                  Binary translation is an important technique for porting programs as it allows binary code for one platform to execute on another. It is widely used in virtual machines and emulators. However, implementing a correct (and efficient) binary translator is still very challenging because many delicate details must be handled smartly. Manually identifying mistranslated instructions in an application program is difficult, especially when the application is large. Therefore, automatic validation tools are needed urgently to uncover hidden problems in a binary translator. We developed a new validation tool for binary translators. In our validation tool, the original binary code and the translated binary code run simultaneously. Both versions of the binary code continuously send their architecture states and the stored values, which are the values stored into memory cells, to a third process, the validator. Since most mistranslated instructions will result in wrong architecture states during execution, our validator can catch most mistranslated instructions emitted by a binary translator by comparing the corresponding architecture states. Corresponding architecture states may differ due to (1) translation errors, (2) different (but correct) memory layouts, and (3) return values of certain system calls. The need to differentiate the three sources of differences makes comparing architecture states very difficult, if not impossible. In our validator, we take special care to make memory layouts exactly the same and make the corresponding system calls always return exactly the same values in the original and in the translated binaries. Therefore, any differences in the corresponding architecture states indicate mistranslated instructions emitted by the binary translator. Besides solving the architecture-state-comparison problems, we also propose several methods to speed up the automatic validation. The first is the validation-block method, which reduces the number of validations while keeping the accuracy of instruction-level validation. The second is quick validation, which provides extremely fast validation at the expense of less accurate error information. Our validator can be applied to different binary translators. In our experiment, the validator has successfully validated programs translated by static, dynamic, and hybrid binary translators.
               
            

@&#INTRODUCTION@&#

Binary translation [1, p. 49–52] is an important technique for migrating application binaries from one platform to another. It is widely used in virtual machines and emulators. There are three categories of binary translators: static binary translators (SBT), which translate code offline, dynamic binary translators (DBT), which translate code at run time, and hybrid binary translators (HBT), which translate code both offline and at run time. UQBT [2], FX!32 [3], and LLBT [4] are considered as static binary translators while Aries [5] and QEMU [6] are dynamic. Shen et al. [7] also implement a hybrid binary translator, which runs statically translated code and invokes the dynamic binary translator whenever the code-location problem [1] is encountered.

Developing quality binary translators is challenging because many issues must be handled carefully, such as code translation (including the code-discovery and code-location problems [1]), speculative execution and system calls. Lots of details must be handled correctly in order to produce a correct translation.

An intuitive way to validate the translated program is manually checking machine states after each instruction is executed. However, this approach is both slow and error-prone. An efficient and automatic validation tool is eagerly called for.

Traditional validation methods fall into two categories. The first approach compares the outputs of the original program and the translated program. This is similar to black-box testing in software engineering. Though simple, this approach is unsuitable for validating binary translators since a correct translated program may not always produce exactly the same output as the original program due to different run-time environments. Moreover, this approach would yield little information to assist debugging.

The second approach performs validation based on the basic blocks of the programs. This approach is widely applied in the validation of optimizing compilers [8,9]. However, the basic-block validation is not suitable for validating static binary translators because the target addresses of indirect branches may not be resolved completely at static time, which implies that it may not be possible to build an accurate control flow graph from binary code. In contrast, we choose to perform validation based on individual instructions since per-instruction validation can help to detect the mistranslated instructions more accurately.

Our validator attempts to validate binary translators that translates ARM code into x86 code. In our experiments, we validate all three kinds of binary translators. The validator is composed of three processes: two processes run the original code and the translated code, respectively, and the third process compares the corresponding architecture states generated from the first two processes. The first two processes respectively send the architecture state immediately after each ARM instruction is executed to the third process. Instrumentation code is inserted in both the ARM program and the translated x86 program at the corresponding places, which serve as checkpoints. The original ARM program and the translated program run side by side. The instrumentation code collects data in both the ARM and the translated binaries, including the emulated architecture states and stored values, and then sends the data to the validation process for comparison.

We try to keep our validation strategy as simple and intuitive as possible. However, there are still many challenging delicate issues to overcome. For example, the registers may hold addresses of variables. The same variable may be allocated at different addresses in the original ARM program and the translated program, respectively, due to different memory layouts or different dynamic memory allocations. Therefore, the same register in the original ARM program and in the translated program may hold different values even if the translated x86 program is considered correct. In order to simplify the comparison of architecture states, we need to make the addresses of the global variables, the run-time stack, and the allocated memory blocks exactly the same in the original ARM version and the translated version.

System calls also bring about challenges. For example, the system call sbrk returns the address of the allocated memory block. The complex allocation mechanisms make the same sbrk system call return different values when the underlying system-call handlers are different. Because the ARM version and the translated version use different system-call handlers, the system call sbrk may return different values. We need both the ARM version and the translated version allocate memory at exactly the same addresses in order to facilitate the comparison of architecture states.

Another challenge is that system calls, such as getpid, that may return system-dependent values must also be forced to return identical values in both the ARM binary and the translated binary.

Performance is also an important issue. Instruction-level validation brings about a large number of messages between the emulation processes and the validator. The tremendous number of messages take a lot of execution time and make the validation a time-consuming and impractical task. We propose two improvements in an attempt to reduce the number of messages. The first is the validation blocks, the purpose of which is to reduce the number of validations while maintaining the accuracy of the validation. Several consecutive instructions are grouped into a single validation block, which is similar to a basic block. In a validation block, a register or variable is defined no more than once. The avoidance of redefinition helps to identify the mistranslated instructions once any register or variable is found to hold a wrong value. On the other hand, architecture-state comparison is performed once per validation block, rather than once per instruction.

The second improvement is quick validation, which adopts a code-coverage based validation. It only validates an instruction when that instruction is executed for the first time. Although it sacrifices some degree of correctness, quick validation can be used as an immediate validation for minor modifications to the translator. The rigorous validation is left to specific milestones.

In this paper, we propose a methodology to validate binary translators. Our methodology is implemented in a real tool that works by comparing the execution states of the original ARM program and the translated x86 program. We run the ARM program with an emulator, QEMU, on an x86 platform instead of running the ARM program on a real ARM board. The translated x86 program also runs on the same x86 platform concurrently. QEMU can be run as a process-level virtual machine, which means it does not need to emulate a whole operating system. This makes QEMU efficient and, sometimes, even faster than a real ARM device, especially when there are a large amount of messages between QEMU and the validator. The popularity of multi-core microprocessors also makes the parallel validation model on a single machine feasible. Another reason is that the process-level virtual machine is much easier to be adapted for validation. In summary, we make the following contributions:
                        
                           •
                           We develop an automatic validation tool that can validate all three kinds of binary translators: static, dynamic, and hybrid.

We propose two mechanisms that reduce the comparison of architecture states to a simple equality check.

We also come up with two improvements that significantly speed up the validation process.

The remainder of this paper is structured as follows. Section 2 sketches the binary translators to be validated and the emulator QEMU, which is used to validate the translators. Section 3 describes the challenges of automatic validation. Section 4 illustrates the implementation details and shows how to overcome the challenges described in Section 3. Section 5 mentions some tips to notice when modifying the validator to support other frontend ISAs. Section 6 discusses the experimental results. Section 7 suggests some testsuite to help the validator provide a sound solution. Section 8 lists related works on debugging, testing, and validation. Section 9 summaries and concludes this paper.

@&#BACKGROUND@&#

In this section, we will first introduce the workflow of the validation system. Then the translators to be validated are introduced, including static, dynamic, and hybrid binary translators. QEMU is the emulation platform used to validate the translators.

In this research, we attempt to validate three kinds of binary translators: static, dynamic, and hybrid. The static binary translator to be validated is LLBT [4], which translates ARM programs to the target versions through the LLVM infrastructure [10] offline. The dynamic and the hybrid binary translators also translate ARM programs to LLVM IR, and their implementations are described in [7]. In this paper, LLBT, DBT, and HBT denote the static, dynamic, and hybrid binary translator to be validated, respectively.

The validation model is composed of three processes: two processes execute the original ARM program and the translated program, respectively, and the third process serves as the validator. The ARM program may be translated offline or at run time. The translated x86 binary runs directly on an x86 system while the original ARM binary runs on an ARM platform. The ARM platform could be a real ARM device, such as a smartphone, or an ARM emulator, such as QEMU. In this paper, we choose QEMU to emulate the ARM binary, and the reasons will be discussed in Section 2.3. During run time, both the ARM binary and the translated x86 binary produce a sequence of data, for example, the architecture states. The validator receives the data through communication channels, compares the data, and reports any disagreements. Fig. 1
                         is a simple diagram sketching the flow of the validation.

The data can be compared once per instruction or once per basic block. Per-basic-block comparison is more efficient. However, this approach is less accurate in that some mistranslated instructions may be left undetected. For example, suppose a variable is assigned values twice in a basic block. The first assignment statement is mistranslated but the second assignment statement is translated correctly. If we compare the architecture states at the end of the basic block, the first (incorrectly translated) assignment statement may not be discovered by the validator. Moreover, generating an accurate control flow graph from a binary program is challenging. Our basic implementation adopts the per-instruction comparison approach. Comparing architecture states immediately after every instruction is executed guarantee the mistranslated instructions that are always detected by the validator as long as the mistranslated instructions produce wrong architecture states. A much faster improvement will be described in a later section.

The translators we validate are all targeting at LLVM (Low Level Virtual Machine). The ARM code is translated into LLVM IR [10], and then the LLVM backend is invoked to optimize the IR and to generate the target machine code. Since the LLVM backend supports many target platforms, the translators can translate an ARM program into programs for various platforms, such as ARM, x86, or MIPS etc. In this paper, the target platform we use is x86.

The ARM architecture state includes all 16 general-purpose registers and 4 condition flags from the current program status register (CPSR). Many instructions, such as compare (CMP) and arithmetic instructions with the s-bit set, may modify these flags. Furthermore, most ARM instructions may be executed conditionally: their execution is predicated on the related condition flags.

In the translated program, all the general-purpose registers and the condition flags are emulated with virtual registers (i.e., variables) in LLVM IR. That is, whenever an instruction accesses a register or a condition flag, it will access them through the load or store LLVM IR. These registers and flags are kept in the memory and are loaded or stored when they are required or updated.

It seems natural that the ARM program should run on a real ARM system, instead of being emulated on an x86 system. There are two reasons we use emulation instead: the first is to reduce communication overhead. In our validation system, the architecture state after every instruction is executed is transferred to the validator. This incurs massive communication volume. The communication between two standalone processors is much slower than that between two processes in the same physical machine. Moreover, in practice, some ARM-instruction-set simulators, such as QEMU, running on x86 platforms have competitive performance compared with a real ARM processor. Glamn et al. [11] used real hardware to validate an instruction-set simulator and got a 50,000x slowdown. Validation with a real hardware processor is only feasible for short code traces or micro-benchmarks.

The second reason is to reduce the development efforts. Our validator needs to compare architecture states. If a real ARM system were used, we would have to modify the execution environment, including the OS kernel and the C library, in order to extract the architecture states. In contrast, modifying a software-based emulator, such as QEMU, is a more tangible solution since the emulator can run as a process virtual machine. A program can run directly within a process virtual machine on a foreign platform without the help of the OS kernel of a real ARM processor. Moreover, debugging a process virtual machine is easier than debugging a kernel.

The instruction-set simulator we choose is QEMU, which is a generic virtual machine [6]. Two modes of emulation are supported by QEMU: system mode and process mode. In the system mode, QEMU attempts to emulate a whole foreign system, including I/O devices, memory space, interrupts and timers, etc. A guest operating system can be emulated on top of the current host system. On the other hand, QEMU can only provide certain functionalities from the kernel, such as system-call emulation, in the process mode. Thanks to the technique of binary translation, process virtual machines for different ISAs could be implemented. In this research, we use the process mode of QEMU to implement our validator.

This section addresses the challenges in the development of the validation system. Most challenges are due to the unpredictable values introduced from various root causes, such as memory addresses or some environment variables. Others are related to the efficiency issue. The three main challenges are
                        
                           •
                           difference in memory layout

system-call validation

performance challenge

ARM is a register-based load-and-store architecture. All memory accesses are performed by the load or store instructions. Before an instruction can access a memory cell, the address of the memory cell must be loaded into a register. It is common to store memory addresses in registers. For example, the stack-pointer register SP holds the address of the stack top and is often used to operate on the process stack.

In our validation model, the original ARM code is emulated by QEMU while the translated x86 code runs directly in a separate process. Since the original ARM code needs a run-time stack, there is an emulated ARM stack in QEMU. Furthermore, there is also an emulated ARM stack in the translated x86 code. Both emulated ARM stacks must be located in exactly the same addresses and operate in exactly the same lock-steps. That is, their SPs increment and decrement are exactly in the same way.

In QEMU, the emulated ARM stack is located in the heap. However, for the translated x86 code generated by LLBT, the emulated ARM stack is located in the x86 stack. It is obvious that the emulated ARM stacks in the original ARM program and in the translated x86 program are located in different addresses. For translated programs generated by DBT and HBT, the emulated ARM stacks are also located in the x86 heap, similar to that in QEMU. However, the locations of the emulated ARM stacks in the heap are not fixed. The emulated stacks could still be located in different addresses.

The same problem also happens for the emulated ARM heaps. Without careful arrangement, it is very likely that heap activities in the original ARM program and in the translated x86 program become totally different. For example, the base addresses of the emulated ARM heaps may located in different locations, just like the emulated ARM stacks. How heap allocation works is another issue. The memory blocks located in the heap are acquired through system calls like brk or mmap. On different platforms, the system call handlers may act in a different manner and the heap allocation mechanisms may also be different. This makes the same consequence of heap allocation system calls return a consequence of memory locations on one platform but return a consequence of different memory locations on the other platform. The differences do not affect the correctness of the translated program but may hinder the comparison of architecture states.

System calls are used to invoke system services. They are invoked with specific assembly instructions with unique system-call id׳s. For example, svc is the system-call instruction in the ARM ISA. Different platforms provide different system-call handlers. The same system call may return different on different, or even the same, platforms. However, for the purpose of validation, we have to force all the corresponding system calls on all platforms to return identical values.

One of the example is the heap allocation which is mentioned in Section 3.1. The allocated memory blocks may not be located in exactly the same consequence of addresses in the original ARM program (which is emulated with QEMU) and the translated x86 program. Other system calls may return values that depend on the underlying platforms. For example, the system call gettimoftheday returns the current time. However, because the calls to gettimoftheday in the original ARM program and in the translated x86 program are hardly invoked at the exactly same time, their return values almost certainly differ. The system call getpid is another example: no two processes have the same process id. Different return values may further cause the program to follow different control-flow paths, resulting in totally different computation even if the translated code is considered correct.

Validating the architecture states after every instruction brings a huge amount of messages. Although putting the validator, the original ARM program (emulated by QEMU) and the translated x86 program on the same physical machine can help to reduce the message propagation time, the amount of messages still cannot be ignored. Long validation time may render the validator impractical for long-running programs. We propose a few techniques that significantly reduce comparisons while sacrificing little validation accuracy.

This section illustrates the implementation of the validation system and various methods we developed to overcome the challenges mentioned in the previous section, including memory allocation, system calls, and performance of validation.


                        Fig. 2
                         shows the work flow of the validator. For a static or hybrid binary translator, such as LLBT or HBT, the original ARM program is fed into the translator to produce the translated x86 program. The target binary is built as a loadable shared object. For a dynamic translator, such as DBT, since the translation is carried out in run time, there is no translation offline. Instead, the DBT itself is built as a loadable shared object, similar to the translated program produced by LLBT/HBT. In the run time, the loaded DBT will load the source binary and translate it into target binary code.

Besides preparing the translated program, two analyses of the source binary are also performed offline. One is the store-instruction analysis, which collects information about the store instructions. It will be described in Section 4.2. The other is the validation-block analysis, which helps to construct the validation blocks of the source binary. The validation blocks can help to reduce the amount of validation. More details are described in Section 4.5.

The validator process is actually a modified process QEMU. The first step is to initialize the memory layout, including the emulated ARM stack and the emulated ARM heap. Then the validator forks two new child processes. The parent process is the validator itself, which compares the architecture states received from the two child processes. One child process runs the translated program. This process first loads the translated program and then performs additional works required to solve the challenges mentioned in Section 3. The details will be described in Sections 4.3 and 4.4. After that, it starts executing the translated x86 code and sends architecture states to the validator. The other child process continues running QEMU to emulate the original ARM program.

Both the original ARM program (which is emulated by QEMU) and the translated x86 program send data to the validator process after every ARM instruction is executed. The data includes the architecture states and the stored values. As described in Section 2.2, each ARM instruction is translated into a sequence of LLVM IR. At the end of this sequence, a call to an external helper function is added. The helper function sends the architecture states and the store values to the validator. How to obtain the store values from the architecture states is described in Section 4.2. QEMU emulates the ARM instructions one by one. We also modified QEMU so that, after each instruction is emulated by QEMU, the same external function is also invoked. This function similarly sends the architecture states and the store values to the validator.

Besides validating the architecture states, the values stored to the memory are also validated. The values stored to the memory are called stored values. Unlike the architecture states, the stored values are only validated when the store instructions are executed. How to ensure the store instruction is correctly translated is a problem. One approach is to compare the whole virtual memory of the two processes. It compares every pair of corresponding memory cells of the two processes for any discrepancies. This approach obviously wastes a lot of time because the whole virtual memory is examined when only a single cell is modified. In contrast, comparing only the modified memory cells is more practical.

For a store instruction, the validator must make sure that the correct value is stored into the correct memory cell. Although an incorrect store can also be detected by the load instruction executed later, the load instruction cannot point out the mistranslated store instruction. On the contrary, the store instruction validation can detect the mistranslated store instruction on time. The stored value can be fetched according to two pieces of information: the store-address, which denotes the address of the modified memory cell, and the store-length, which is the length of the stored value. When an ARM store instruction is emulated, say by QEMU, the store-address and the store-length are calculated.

In the helper function that sends data to the validator process, there are several options to fetch the stored value:
                           
                              1.
                              the helper function simply accepts the stored value from the QEMU emulator or the translated x86 program, and then sends the stored value to the validator for comparison;

the helper function accepts the store-address and the store-length from the QEMU emulator or the translated x86 program, retrieves the stored value with the store-address and the store-length, and sends the stored value to the validator for comparison finally;

the helper function re-calculates the store-address and the store-length from the original ARM instruction, retrieves the stored value according to the calculated store-address and store-length, and sends the stored value to the validator for comparison.

Note that it is not possible to re-calculate the store-address and the store-length from the architecture state alone due to the rich addressing modes provided by the ARM processor. Additional information about the addressing mode of the store instruction, which is not part of the architecture state, is needed.

For example, the instruction str R3, [SP, 24] stores the contents of register r3 into the address which is the sum of the register SP and the constant 24. The store-length is 4bytes since the opcode is str. The additional information for this instruction includes the opcode, the base register index, and the immediate constant. The additional information is called the store information.

The store information is gathered by the offline store-instruction analysis, shown in the upper left corner in Fig. 2. The validator process makes use of a mapping table to look up the store information of each store instruction. In Fig. 3
                        a, the instruction str R3, [SP, 24] at address 0x803c is associated with the store information (4, SP, 23). On the other hand, the instruction mov R0, R6 is not associated with any store information.

The helper function depends on the store information as well as the architecture state to re-calculate the store-address and the store-length. Finally, the stored value is loaded from memory based on the store-address and the store-length and is sent to the validator. Fig. 3b shows these steps.

To circumvent the problem of different memory addresses caused by different memory layouts and different dynamic memory allocation, we may consider two approaches:
                           
                              1.
                              We may allow different memory layouts and different dynamic memory allocation and adjust the addresses in registers accordingly during the comparison of architecture states.

If the emulated ARM code and the translated x86 code always allocate the ARM stack and the heap at the same virtual addresses, all the registers in the emulated ARM code and the corresponding registers in the translated x86 code must always hold exactly the same contents, be it an ordinary value or an address. This approach makes architecture-state comparison straightforward.

In our validation model, QEMU will first allocate the emulated ARM stack, and then fork two processes to run QEMU and the translated program, respectively. Note that both child processes automatically inherit the entire virtual address space from the parent (QEMU) process, which includes the emulated ARM stack. This method makes the emulated ARM stack of QEMU and the emulated ARM stack of the translated program be located at exactly the same address. The emulated ARM heap of the translated program is also inherited from the parent QEMU process. This ensures the emulated ARM heap grows from the same base address. How to make the emulated ARM heap grows exactly the same in both the emulated ARM program and the translated x86 program is solved by using the same system call handler. The details are left to Section 4.4.

There are two kinds of system calls that may produce different architecture states in the original ARM program (which is emulated by QEMU) and in the translated program even if the translated program is considered correct: heap allocation and environment-dependent system calls. For heap allocation, we first need to make the heaps in both child processes start from the same base addresses. This is achieved by running the translated x86 code in a child process. After the base addresses of the heaps are made equivalent, we need to make each pair of corresponding system calls that always allocate memory at the same addresses. This is achieved by intercepting these system calls.

A system call in ARM ISA includes two steps:
                           
                              1.
                              Store a system-call number in register R7.

Raise a software interrupt by the svc (or swi) instruction.


                        LLBT translates the svc instruction into a call to a wrapper function which invokes the system call whose number is in R7 in the target platform. For brk, the wrapper function uses mmap to allocate a block of memory at a specific address. QEMU memory management is implemented similarly.

To make the memory allocation completely identical, the two wrapper calls (in the translated x86 code and in the QEMU process, which emulates the original ARM program, respectively) are bound to the same wrapper function. This can be achieved automatically if the translated x86 code runs in a child process forked from the QEMU process. Because the translated program is loaded into the process (which is forked from the QEMU process) as a shared library, the ARM code sections in QEMU and in the translated program could be made to occupy the same memory space. It is therefore possible to bind the system-call wrapper in the translated program to the same system-call wrapper in QEMU. By binding the system-call wrapper in translated x86 code to the same system-call wrapper in QEMU, the emulated system calls for heap allocation are guaranteed to return the same addresses to the two processes.

For system calls that depend on the execution environments, such as getpid or gettimeofday, it is hardly possible to make them always return identical values in the two processes. Therefore, we made up artificial system-call handlers in the system-call wrapper in QEMU. These artificial system-call handlers always return the same values to the two processes. The objective of the validation system is to automatically validate the translated code. Under this consideration, the return values of the system calls are some default values in order to make the validation continue smoothly.

Although the equality of the stack bases, the equality of the heap bases, and the shared system-call wrappers make automatic validation easier and possible, the massive overhead brought up by the large amount of messages among the processes still makes the validation impractical. To ensure the correctness of each translated instruction, our validation system adopts an instruction-level validation approach. This means that the instrumentation code is executed and architecture-state comparison is performed once after every ARM instruction is executed/emulated. Note that the cost of executing the instrumentation code and comparing architecture states is much more than that of emulating a single ARM instruction. Therefore, reducing the number of times the instrumentation code is executed and the number of state comparisons is important for the efficiency consideration.

We propose a method, validation blocks (abbreviated as vblocks), for reducing instrumentation-code executions and architecture-state comparisons without sacrificing the accuracy of validation.

A validation block is a longest sequence of consecutive instructions, satisfying the following conditions:
                           
                              1.
                              Once an instruction in a validation block is executed, all the instructions that follow in the same validation block must also be executed;

No two instructions in a validation block define the same registers or modify the same condition flags;

Every instruction in the program belongs to exactly one validation block; and

A vblock ends immediately after a branch or store instruction is encountered.


                        Fig. 4
                         shows an example of a validation block. The first instruction in the sequence defines register R5 and the remaining instructions define R6, R1, R2, R3, R0, and IP, respectively.

For validation purpose, we may add the instrumentation code after each ARM instruction, after each vblock, or after each basic block. These methods are called per-instruction validation, vblock validation, and basic-block validation, respectively. Vblock validation is as precise as per-instruction validation as long as no exceptions occur. Vblock validation is much faster than per-instruction validation. Basic-block validation, though three times faster than vblock validation (according to our experiments), suffers from the inaccurate identification of the mistranslated instructions.

In this subsection we show that the vblock validation can identify the mistranslated instructions that cannot be identified accurately by the basic-block validation. Consider the following basic block: 
                              
                                 
                                    
                                    
                                    
                                       
                                          
                                             add R1, R2, R3;
                                          
                                             assume the add instruction is mistranslated in this
                                          
                                       
                                       
                                          
                                             sub R4, R5, R1;
                                          
                                             example.
                                       
                                       
                                          
                                             mul R1, R4, R2
                                          
                                          
                                       
                                    
                                 
                              
                           assume the add instruction is mistranslated in this example. After executing the (mis)translation of this code sequence, register R1 and register R4 would contain the wrong values.

In the basic-block validation, since all three instructions belong to the same basic block, there is only one checkpoint, which is located at the very end of the instruction sequence. The checkpoint reports register R1 and register R4 contain the wrong values. To find the mistranslated instruction we often backtrace to the earliest possibly mistranslated instruction [12]. However, it is hard to decide which instruction is the earliest mistranslated instruction. In this example, the earliest mistranslated instruction could be sub since it modifies R4 and then the incorrect value in R4 is used in mul to modify R1. Nevertheless, add is also a suspect since it modifies R1 whose value is later used in sub to modify R4. Multiple suspects make it necessary for the developer of the translator to manually check all instructions in the basic block to find the mistranslated instruction, which is time consuming and error prone.

On the other hand, in the vblock validation, because both add and mul define the same register, i.e. 
                           R1, the basic block is divided into two vblocks: add and sub in the first vblock and mul in the second. After executing add and sub, the first checkpoint reports register R1 and register R4 contain the wrong values. We can conclude that the add instruction is the earliest mistranslated instruction because add and sub modify R1 and R4, respectively, and the value of R1 is used in mul to compute R4. Based on the above example, we may say that the vblock validation is more accurate than the basic-block validation.

We need to identify the vblocks in a binary program. This process is called the vblock analysis, which is performed offline. It first collects information such as the registers and condition flags defined by each instruction and the condition code used by each instruction. Then a linear scan of the instructions from the beginning of the program is performed. A vblock ends immediately before a register is defined twice or immediately after a branch or store instruction is encountered. When one vblock ends, the following one starts immediately.
                              1
                           
                           
                              1
                              Strictly speaking, the vblock analysis also suffers from the code discovery problem and the code location problem because this analysis is also performed before the program executes. There are cases for which exact control flow graphs cannot be constructed.
                           
                        

The algorithm for the vblock analysis is shown in Fig. 5
                           , Algorithm 1. The algorithm first checks if the previous instruction is the end of a validation block. This check is shown in Lines 6–8. It checks if there are multiple definitions to the same register or the condition flags are set again. If so, the previous instruction is marked as the end of a vblock. And then it checks if the current instruction is the end of a validation block, shown in Lines 11–16. Line 11 checks if it is a branch instruction, and Line 14 checks if the instruction is a store instruction. In either case the current instruction is marked as the end of a vblock.

Following is an example that illustrates the algorithm. The left-hand side is the code sequence, and the right-hand side is the same code sequence which is already divided into several vblocks. 
                              
                                 
                                    
                                    
                                    
                                    
                                    
                                       
                                          
                                             adds
                                          
                                          
                                             r0, r0, r2, lsr r5
                                          
                                          
                                             adds
                                          
                                          
                                             r0, r0, r2, lsr r5
                                          
                                       
                                       
                                          
                                             adc
                                          
                                          
                                             r1, r1, #0
                                          
                                          
                                             adc
                                          
                                          
                                             r1, r1, #0
                                          
                                       
                                       
                                          
                                             adds
                                          
                                          
                                             r0, r0, r3, lsl lr
                                          
                                          ==========================
                                       
                                       
                                          
                                             b#
                                          
                                             
                                                <
                                                
                                                   func
                                                
                                                >
                                             
                                          
                                          
                                             adds
                                          
                                          
                                             r0, r0, r3, lsl lr
                                          
                                       
                                       
                                          
                                             sub
                                          
                                          
                                             r5, r5, #32
                                          
                                          
                                             b
                                          
                                          
                                             
                                                #
                                                <
                                                
                                                   func
                                                
                                                >
                                             
                                          
                                       
                                       
                                          
                                             add
                                          
                                          
                                             lr, lr, #32
                                          
                                          ==========================
                                       
                                       
                                          
                                             cmp
                                          
                                          
                                             r2, #1
                                          
                                          
                                             sub
                                          
                                          
                                             r5, r5, #32
                                          
                                       
                                       
                                          
                                             lsl
                                          
                                          
                                             ip, r3, lr
                                          
                                          
                                             add
                                          
                                          
                                             lr, lr, #32
                                          
                                       
                                       
                                          
                                             orrcs
                                          
                                          
                                             ip, ip, #2
                                          
                                          
                                             cmp
                                          
                                          
                                             r2, #1
                                          
                                       
                                       
                                          
                                             str
                                          
                                          
                                             ip, [sp, #-4]
                                          
                                          
                                             lsl
                                          
                                          
                                             ip, r3, lr
                                          
                                       
                                       
                                          
                                          
                                          ===========================
                                       
                                       
                                          
                                          
                                          
                                             orrcs
                                          
                                          
                                             ip, ip, #2
                                          
                                       
                                       
                                          
                                          
                                          
                                             str
                                          
                                          
                                             ip, [sp, #-4]
                                          
                                       
                                    
                                 
                              
                           
                        

This code sequence can be divided into 4 vblocks. The analysis starts from the first instruction and finds that the CPSR is redefined in the third instruction, so the first vblock is composed by the first two instructions. Because the fourth instruction is a branch instruction, it also marks an end of vblock. There is no register redefined between the fifth instruction and the ninth instruction, so these instructions also compose a vblock. The 12th instruction is a store instruction, so the last two instructions also compose a vblock.

The vblock validation technique reduces the number of checkpoints. However, the execution time can be further reduced if we can tolerate a little inaccuracy in identifying the mistranslated instructions. The inaccuracy can be compensated with other means. Because certain code in a program may be executed repeatedly, the accompanying validation is also carried out repeatedly. In many cases, such repeated validations are of little benefits and could be avoided. We incorporate the code-coverage technique in software testing into our validation tool. In short, each instruction will be validated only when it is executed for the first time. This new validation technique is called quick validation.

Although the code-coverage approach avoids repeated validations, it sacrifices the accuracy of validation because a mistranslated instruction may look normal when it is executed for the first time but goes wrong later. Quick validation will be unable to discover such an anomaly. In the code example below, if block will be generated as a TST instruction which update the condition flags and a conditional executed MUL. 
                           
                              
                                 
                                 
                                 
                                 
                                    
                                       
                                          if(cond) {
                                       
                                          tst
                                       
                                       
                                          R4, #1
                                       
                                    
                                    
                                       
                                          
                                          r=x⁎y;
                                       
                                          mulne
                                       
                                       
                                          R7, R5, r0
                                       
                                    
                                    
                                       }
                                       …
                                       
                                    
                                 
                              
                           
                        
                     

If the instruction MUL is mistranslated, and the variable cond is false when the block is executed at the first time, the error will not be detected since the content of register R7 remains the same. Although the mistranslated part will be executed when the variable cond is true later, the quick validation will not validate the instruction MUL since it is already validated. However, it is much faster than the full, rigorous vblock validation. Quick validation is suitable for minor modifications to a binary translator.

To ensure an instruction is validated at most once, each piece of instrumentation code is turned off after it is executed. To implement quick validation, we adopt different approaches for QEMU (which emulates the original ARM code) and for different translators. In QEMU (in Fig. 6
                        ), the basic unit of the translated code is a translation block (TB). When an ARM instruction is to be executed, QEMU would search for the corresponding TB first. If the TB is not found (this implies that the instruction had not been translated previously), QEMU translates the instruction and possibly some instructions that follow, generates a TB, executes the TB, and finally marks that TB invalid. In our implementation, each TB would contain one or more pieces of the instrumentation code. If an invalid TB is found, the TB will be generated again and then executed. However, the instrumentation code will not be included and the newly generated TB is marked valid this time. If a valid TB is found, the TB is executed.

The binary code translated by LLBT cannot be invalidated in the same way as that generated by QEMU. Instead, we adopt a table-lookup mechanism to prevent sending the architecture states and the stored values more than once for each ARM instruction. In the lookup table, there is one entry for each ARM instruction. All the entries are initialized as false, which means that all the ARM instructions have not been executed yet. Whenever an ARM instruction invokes the helper function to send the architecture state and the stored value, the corresponding entry is set to true. When the next time the ARM instruction invokes the helper function, the lookup table tells the helper function that the current ARM instruction has been visited. The helper function returns immediately without sending anything.

For DBT, although it also translates the ARM code on-the-fly, it adopts the same approach as LLBT since the mechanism used by QEMU cannot guarantee each ARM instruction sends the architecture state and the stored value at most once. HBT has the same concerns as DBT since it may invoke DBT during the execution, so it also adopts the table-lookup approach.

In this section, we are going to introduce several tips to make the validator support different frontend ISAs such as x86 or MIPS.

Different architectures have different architecture states. The number of general purpose register may vary among different architectures. For example, x86 has only 8 registers, and MIPS has 32 registers. The size of the register may also be different. Unlike the 32-bit version x86, the 64-bit version x86 has 64-bit register. The validator needs to adjust the sizes of validation data according to different combinations of the register numbers and the register sizes.

The conditional flags are also a part of the architecture state. Some architectures can ignore this issue since they do not have conditional flags. MIPS is an example. For others, such as ARM and x86, the emulated conditional flags should also be sent to the validator as the general purpose registers. The number of the conditional flags of different architectures is also different.

To validate the stored value, a preliminary analysis at static time is required. The analysis finds the instructions which may update the memory content and collects required information to load the stored value in the run time. For both ARM and MIPS, only the instructions which are designed to store values can update memory content, such as STR in ARM or SW in MIPS. For x86, this is not the case. Lots of instructions in x86 can directly update the memory content. For example, the destination operand of an ADD instruction can be either a register or a memory address. The store instruction analysis for the x86 instruction should examine whether the operand of the given instruction is a memory address.

In this paper we propose several method to simplify the validator such as making the stack pointer register starting from the same address or using the same system call wrapper. Different ISAs have different paths to set up the stack pointer register in QEMU, so it is necessary to take care of this on a per-ISA basis. It is necessary to make sure that this kind of initializations is properly handled when applying to another frontend ISA or binary translator.

In this paper we provide two methods to enhance the performance of the validator. For other frontend ISAs, there may have target specific optimization opportunities. A simple example is the zero register of MIPS, which can be ignored since it is constant.

We use the EEMBC 1.1 benchmark to evaluate our validator. The experiments are conducted on an Intel Xeon 5506 processor with 12GB ram. EEMBC 1.1 is a popular benchmark for embedded systems. It is compiled by the ARM gcc compiler, version 4.7.0. LLVM in our experiment is version 3.4.0 and QEMU is version 1.7.0.

There are three experiments:
                        
                           •
                           The first experiment compares the execution time with and without various validations. The purpose of this experiment is to measure the overhead of various validations. This experiment includes per-instruction validation and vblock validation. More details are in Section 6.2.

The second experiment further compares the performance of per-instruction validation, vblock validation, and basic-block validation. This experiment helps to characterize the performance gap between instruction-level validation and basic-block-level validation. The stored-value validation is disabled in this experiment because the stored-value validation brings about extra checkpoints. The checkpoints introduced by the store instructions will further divide the basic blocks and the validation blocks into even smaller pieces and reduce the performance differences among the three kinds of validations. Both the ratios of the execution time and the ratios of the numbers of the validations are provided in this experiment. More details are in Section 6.3.

The last experiment shows the execution time ratios of the quick-mode validation. The purpose is to show that the quick-mode validation is indeed much quicker and can be used as a fast check. The execution time ratios between the validator and QEMU will be provided. More details are in Section 6.4.

A validator is actually a debugging tool. Our validator indeed helped us to discover three bugs that slipped away from manual debugging. The three bugs are listed below.
                           
                              
                                 
                                    •
                                 
                              
                              
                                 ldrd (load double words) and strd (store double words): This bug was found when we validate the math program, which is a test case from the math library. In the ARM architecture, two base-register addressing modes can be used in load-and-store operations.
                                    
                                       
                                          
                                             ○
                                          
                                       
                                       
                                          Pre-indexed addressing mode comes in two formats: ldr R0, [R1, #4] and ldr R0, [R1, #4]!. In both formats, the contents of the word at address R1 + 4 would be loaded into the register R0. Note that in the second format, the exclamation mark means to increment the base register R1 by 4. So the second instruction is equivalent to two instructions: ldr R0, [R1, #4] followed by add R1, R1, #4.


                                          Post-indexed addressing mode increments the base register after an instruction is executed. For example, the instruction ldr R0, [R1], #4 loads the value in the word whose address is R1 into register R0, and then increment the value of register R1 by 4.


                                 blx: This bug was found when validating the djpeg benchmark in the EEMBC consumer test suite. It occurs when the operand is the register FP in the blx FP instruction. LLBT would mistakenly regard the operand FP as an immediate value in translation.


                                 Set condition flags: This bug was found when validating a test program that casts floating-point numbers to integer numbers. The value of the carry flag was wrong after the sbc (subtract with carry) instruction is executed. The carry flag should be set when no borrow occurs in the sbc operation. But the code translated by LLBT mistakenly sets the carry flag when a borrow occurs.


                        Fig. 7
                         shows the execution time ratios of the EEMBC benchmark. This experiment compares the execution time of our validator (together with the three binary translators LLBT, DBT and HBT) and the execution time of directly emulating the benchmark on QEMU without any validation. There are two modes in this experiment:
                           
                              •
                              
                                 per-instruction mode: Validation is performed after every ARM instruction is executed, including architecture states and stored values.


                                 vblock mode: Validation is performed after every validation block is executed, including architecture states and stored values.


                        Fig. 7 shows the execution time ratios. The baseline is the execution time of directly emulating the benchmark on QEMU. Fig. 7a, b, and c shows the execution time ratios when validating LLBT, DBT, and HBT, respectively. The execution time ratio in the per-instruction mode ranges from 9.5 to 63.6 times slower for LLBT. The average is 29.5. For DBT, the execution time is much longer since its implementation of ARM register emulation is different from that of LLBT, which makes it pay more overhead to pass the architecture states. The execution time in the per-instruction mode ranges from 20.1 to 129.1 times slower. The average is 61.5. Its execution time is twice that of LLBT. Because HBT shares the same code base with DBT, their overhead is quite similar. The execution time in the per-instruction mode ranges from 17.2 to 115.8 times slower and the average is 54.3. Although the overhead is still large (29.7 for LLBT, 61.5 for DBT, and 54.3 for HBT), it is much faster than previous research. Glamn et al. [11] validate an instruction-set simulator with a real machine. Their result is 50,000 times slower when running the SPEC95 benchmark.

Finally, consider the vblock mode. For LLBT, the average execution time ratio is reduced to 13.7. The vblock mode for LLBT is 2.15 times faster than the per-instruction mode. For DBT, the average execution time ratio is reduced to 26.1. The vblock mode for DBT is 2.36 times faster. For HBT, the average execution time ratio is reduced to 23. The vblock mode for HBT is 2.37 times faster. According to the results, we can say that the vblock effectively speeds up the validation.

In this experiment we compare the efficiency of vblock validation and basic-block validation. Because it is impossible to build an accurate CFG, the validation is performed immediately after every branch instruction is executed instead of at the end of basic blocks. This experiment counts the numbers of validations performed and measures the execution time.

Note that the stored-value validation is disabled in this experiment since the store instructions may break basic blocks and vblocks into even smaller pieces. Breaking basic blocks and vblocks into smaller pieces increases the numbers of validations and reduces the differences between the two approaches. Unfortunately, some programs in the EEMBC suite contain store instructions in the hot paths, and the instruction count of these programs raises significantly when the stored-value validation is enabled. Fig. 8
                         shows the difference when the store-instruction validation is enabled and disabled for the basic-block validation. In this figure, the baseline is the number of validations when the stored-value validation is disabled. According to the results, several programs requires twice more validations when the stored-value validation is enabled. For the program aiifft01, the number of validations reaches 9 times more when the stored-value validation is enabled. To avoid this kind of interference, the stored-value validation is disabled in this experiment.

There are three modes in this experiment:
                           
                              •
                              
                                 per-instruction mode: Validation is performed after every ARM instruction is executed. Only the architecture state is validated.


                                 vblock mode: Validation is performed after every vblock is executed. Only the architecture state is validated.


                                 branch mode: Validation is performed after every branch instruction is executed. Only the architecture state is validated.


                           Fig. 9
                            shows the number of the validations performed in the vblock mode and branch mode, respectively, relative to that in the per-instruction mode. In the vblock mode, fewer validations are performed for programs with fewer conditional and branch instructions. In our experiment, the ratio is between 27% and 50%. The geometric mean of the ratio is 38%. In other words, vblock validation can eliminate 62% of the time spent in validation code. In the branch mode, the average ratio is 9%. Although the v-block approach cannot achieve the same performance as the basic-block approach (i.e., the branch mode in the experiment), it still eliminates a half of the comparisons required by the per-instruction mode. Moreover, it can locate the mistranslated instructions accurately.


                           Fig. 10
                            shows the execution time ratio. The baseline is the execution time of directly emulating the benchmark on QEMU. Fig. 10a shows the execution time ratio when validating programs translated by LLBT. According to the figure, a naive implementation of the instrumentation ranges from 8.6 to 55 times slower than direct emulation on QEMU. The average is 25.3. With the help of vblock validation, the execution time ratio is reduced to 10.4. According to the number of validations performed, the execution time is proportionally reduced. The execution time ratio in the branch mode is 3.5. The branch mode only spends about one-third execution time compared with the vblock mode. Fig. 10b shows the execution time ratio when validating DBT. The execution time ratio of per-instruction validation ranges from 19.4 to 123.1. The average is 56.5. The vblock mode reduces the execution time ratio to 22.3. The execution time ratio of the branch mode is 6.7 times slower, and it costs about 30% of the execution time of the vblock mode. Fig. 10c shows the execution time ratio when validating HBT. The execution time ratio of per-instruction validation ranges from 16.3 to 105.6. The average is 48.9 times slower. The vblock mode reduces the execution time ratio to 18.8. In the branch mode, the execution time ratio is 4.9, which is 26% of the execution time of the vblock mode.

According to the experimental results, the vblock approach needs about 3–4 times of the execution time compared with the basic-block validation. Although the performance is not close between the two, the performance of the vblock approach is still acceptable since it can point out the mistranslated instructions accurately.

The last experiment shows the execution time in quick validation. In quick validation, each instruction is validated at most once. It is expected to reduce the execution time significantly. Fig. 11
                         shows the execution time ratio in which the baseline is directly emulating the programs on QEMU without any validation. There are two modes in this experiment, similar to that Section 6.2:
                           
                              •
                              
                                 per-instruction mode: Validation is performed after every ARM instruction is executed. Validation includes architecture states and stored values.


                                 vblock mode: Validation is performed after every vblock is executed. Validation includes architecture states and stored values.

For LLBT, the experimental results of quick validation in the per-instruction mode and in the vblock mode are shown in Fig. 11a. The execution time in the per-instruction mode is only 5.6 times slower than direct emulation on QEMU, and the vblock mode only reduces it to 2.5. The quick mode could be used to quickly identify errors in translations during early development stages. The slower but more accurate modes (such as the vblock mode) can be used for final validation.

For DBT and HBT, the execution time in quick validation is longer. For DBT, the execution time ratio in the per-instruction mode is 7.8 times slower and the execution time ratio in the vblock mode is 3.7 times slower. The results are shown in Fig. 11b. For HBT, the execution time ratio in the per-instruction mode is 6 times slower and the execution time ratio in the vblock mode is 2.5 times slower. The results are shown in Fig. 11c.

According to the experimental results, quick validation significantly reduces the validation time. It only needs 2–4 times more execution time of directly emulating ARM programs on QEMU. Although the quick validation cannot guarantee the correctness of the translator, its performance makes it suitable for checking the correctness of minor modifications.

This section introduce a set of test suites which can be used with the validator to check the correctness of the binary translator. With the help of the test suites, the validation system makes the binary translator a more sound solution.

The first is the test suite provided by the assembler, for example, the GNU assembler(GAS). In the test suite of the GNU assembler, there are lots of test cases for different ISAs. The test suite covers most of the instructions for each ISA to ensure the correctness of the assembler. The high coverage of the tested instructions in the test suite also guarantees that the validator can validate most of the instructions of a given ISA when validating the translated test suites. The test suites are often composed by lots of short assembly code patterns, and the code patterns can be extracted to create the testing programs.

The second is the test suite provided by the compiler. Both GCC and LLVM provide their own test suites. Some of the tests focus on the front end analysis, and some others focus on the generated code. Those checks which focus on the execution can also be used by the validator to check the correctness of the binary translator. Although the test suites from the compiler is not intended to cover all the instructions of an ISA like what the test suites from the assembler do, it still helps since it enriches the set of test programs. Moreover, since lots of the binaries are generated by the compilers from the source code, validating the test suites provided by compilers can still help to find bugs.

The other sources are the test suites from libraries, especially libraries that support compilers. For example, LLVM has a library which is named compiler-rt. It provides target specific function hooks to support some target specific features such as soft float functions of the ARM architecture. To check the correctness of the implementation of the soft float functions, the test suite uses different possible corner cases as input values to the soft float functions. These corner case tests can help to check the calculation errors of arithmetic operations.

@&#RELATED WORK@&#

In this section we discuss related work on automatic validation of binary translators and instruction-set simulators. Furthermore, some runtime debugging or testing techniques will be mentioned, including runtime code-coverage analysis and mutation and automatic test-case generation.

Automatic functional validation is rarely mentioned as an independent topic. Zheng et al. create a framework that generates random PA-RISC instruction sequences and runs each sequence twice to validate the correctness of Aries [5], which translates PA-RISC binary to IA-64 equivalent. The code sequence first runs on a real PA-RISC platform and then runs on the Aries, which is executed on an IA-64 platform. The architecture states at the end of execution are compared to validate the correctness of Aries. Due to machine limitations, their work does not run the two programs in parallel but one after the other. Other related research includes validating instruction-set simulators on real processors. Bensal et al. [13] use pattern matching and a set of peephole rules for binary translation. The rules map a sequence of source instructions into a sequence of target instructions. During the binary translation, the translator will choose the best-fit sequence of target instructions according to the rules. To ensure the correctness of the translation, they use two approaches to verifying the code sequences. One is a fast and immediate check. Random inputs are generated to test each code sequence. The other is a boolean test. Both the source instruction sequence and the target instruction sequence are turned into boolean formulae which are fed into a satisfiability solver to prove the two formulae are equal. Hwang et al. [14] directly compare the execution results of input data sets to check the correctness of the binary translator.

Runtime validation of the architecture states with applications is common when the development of a processor becomes more and more complex. The complex logic of modern processors makes developing instruction-set simulators more difficult. Runtime validation is employed to ensure the results are correct. Glamn et al. [11] intend to validate an instruction-set simulator by running the simulator and a real machine in parallel. It will validate the architecture state after each instruction is executed on the two platforms. The result of their experiment shows that verification with a real hardware could be 50,000 times slower and it may require several hundred hours to run the SPEC95 benchmark.

One of the approaches to validating the correctness of an application is coverage based validation. A single execution seldom exercises (i.e., covers) all the instructions in a program. It usually requires to intelligently choose several set of input data in order to collectively cover all the instructions of the tested application. In recent years, the code coverage approach is commonly combined with the symbolic execution. Godefroid et al. propose a method, called DART [15], which increases the code coverage by executing alternative paths. It tests a program by both randomly generating input vectors and calculating the symbolic constraints of the input vectors. With the two mechanisms the amount of testing input vectors is reduced. Sen et al. provide a method, called CUTE [16], which is based on DART. It tends to solve the pointers which point to the dynamic data structures in the given program. They use pointer arithmetic to replace the symbolic execution when calculating the constraints. Cadar et al. build KLEE [17], which aims at high code coverage on existing real programs. The real programs consume more verification time and may interact with the environment variables. KLEE is designed to be a high performance tool and can achieve high coverage even if some input comes from the environment.

Mutation testing is another approach to detecting errors by executing different paths. It creates many mutants of the original program with syntactic changes to various program components, such as the condition checks of the branches. By these changes, the same input may traverse different paths in the program. Jia et al. [18] have a broad and detailed survey about this topic.

Random test-case generators are developed to detect bugs in the compiler or runtime. Yang et al. develop CSmith [19], which can randomly generate test cases for C compilers. The test cases are generated with a subset of the C grammar and the generated code includes different C statements such as type definitions, variable declarations, or function calls, etc. The generated code is well guarded to prevent the undefined behaviors. With the help of CSmith, they found hundreds of unknown compiler bugs. Regehr et al.׳s work [20] targets on reducing the size of the test cases generated by the random test-case generator. The test case is simplified to make debugging easier, and the bug-triggering feature is preserved. Besides the simplification of the generated test cases, the large amount of test cases is another issue. In the large amount of test cases, some bugs are repeatedly produced. The developer needs to spend lots of time to filter out the repeated bugs. Chen et al. provide an algorithm [21] which ranks the bugs to help programmers focus on the interesting bugs.

@&#CONCLUSIONS@&#

We developed an automatic validator to validate binary translators. The validator works on the instruction level and can support different binary translators. In our experiments, we validate the static, dynamic, and hybrid binary translators from different code bases. The validator takes care of memory layouts and system calls of the emulated programs so that architecture states obtained from two emulation processes (one for QEMU and the other running the translated code) can be directly compared. Besides the architecture states, the stored values of each store instruction are also validated. The validator is carefully designed to make as few modifications to the binary translators as possible. In average, the execution time ratio of the validator versus directly emulating ARM programs on QEMU is 29.7 for the static binary translator LLBT. For the dynamic and hybrid binary translator, the execution time ratios are 61.5 and 54.3, respectively.

Furthermore, in order to speed up the validation process, we propose the validation-blocks, which could significantly reduce the communications between the validation process and the emulation processes without sacrificing the accuracy of instruction-level validation. The average execution time ratio is reduced to 13.8 for LLBT. For the dynamic and hybrid binary translator, the execution time ratios are reduced to 23 and 26.1, respectively. The validation-block approach speeds up about 2.1x to 2.4x compared with the per-instruction validation.

Finally, the quick validation based on the code-coverage techniques offers a very fast validation to uncover translation errors. For LLBT, the execution time ratio is reduced to 2.8 times of direct emulation on QEMU so that it may be applied to cover more applications with many different input data sets. For dynamic and hybrid binary translators, the execution time is also reduced to 3.7 times and 2.5 times of direct emulation on QEMU.

@&#REFERENCES@&#

