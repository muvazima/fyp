@&#MAIN-TITLE@&#Use of graphics processing units for automatic synthesis of programs

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new quantum-inspired linear genetic programming system that runs on the GPU.


                        
                        
                           
                           Allows the synthesis of solutions for large-scale real-world problems.


                        
                        
                           
                           Eliminates the overhead of copying the fitness results from the GPU to the CPU.


                        
                        
                           
                           Proposes a new selection mechanism to recognize the programs with best evaluations.


                        
                        
                           
                           Improves performance of the GP execution through exploiting the GPU environment.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Genetic programming

GPU acceleration

Machine code

Quantum-inspired algorithms

Massive parallelism

@&#ABSTRACT@&#


               
               
                  Genetic programming (GP) is an evolutionary method that allows computers to solve problems automatically. However, the computational power required for the evaluation of billions of programs imposes a serious limitation on the problem size. This work focuses on accelerating GP to support the synthesis of large problems. This is done by completely exploiting the highly parallel environment of graphics processing units (GPUs). Here, we propose a new quantum-inspired linear GP approach that implements all the GP steps in the GPU and provides the following: (1) significant performance improvements in the GP steps, (2) elimination of the overhead of copying the fitness results from the GPU to the CPU, and (3) incorporation of a new selection mechanism to recognize the programs with the best evaluations. The proposed approach outperforms the previous approach for large-scale synthetic and real-world problems. Further, it provides a remarkable speedup over the CPU execution.
               
            

@&#INTRODUCTION@&#

The idea of enabling the computer to automatically create programs that solve problems establishes a new paradigm for developing reliable applications. The field of genetic programming (GP) has demonstrated that devising computer programs on the basis of a high-level description is viable.

Genetic programming extends conventional evolutionary algorithms to deal with computer programs. The essence of GP is to use the Darwinian principle of natural selection in which a population of computer programs is maintained and modified, according to genetic variation. A GP system progresses toward a solution by stochastically transforming populations of programs into better populations of programs until a stopping criterion is met.

In the past few years, GP has been successfully applied to a wide variety of problems, including automatic design, pattern recognition, financial prediction, robotic control, data mining, image processing, and synthesis of analog electrical circuits [1–7]. However, a major drawback of GP is that the search space of candidate programs can become enormous. For example, to solve the 20-bit Boolean multiplexer problem, a total of 1,310,720,000 candidate programs have to be evaluated [8]. In addition, the evaluation of the fitness of a single program in the search space may demand testing this program with numerous different combinations of input data. Consequently, the time required to evaluate the programs may be unreasonable. The computational power required by GP to evaluate billions of programs with hundreds or thousands of input data can be a huge obstacle to solving large real-world problems.

The last few years have witnessed remarkable advances in the design of parallel processors. In particular, graphics processing units (GPUs) have become increasingly popular. Further, their high computational power, low cost, and reasonable floating-point capabilities have made them attractive platforms for speeding up GP. Modern GPUs contain thousands of cores, and the massive parallelism provided is highly suitable to process GP in parallel.

The power of the GPU has been previously exploited to accelerate GP by using different methodologies. The compilation methodology [9–12] generates programs using the GPU high-level language, and each of these programs has to be compiled before its evaluation. The pseudo-assembly methodology [13,14] creates programs in the pseudo-assembly code of the GPU, and a just-in-time (JIT) compilation is performed for each individual before the evaluation. The interpretation methodology [15–17] interprets programs during its evaluation. The machine code methodology [18] generates programs in the GPU machine code and does not require any compilation step before evaluation.

The above mentioned methodologies have been used with different levels of success. However, the machine code methodology, called GPU machine code genetic programming (GMGP) [18], has exhibited significant performance gains over the others. Avoiding the compilation overhead without including the cost of parsing the evolved program is the key for severe reductions in the computational time. In addition, GMGP implements linear genetic programming by using a quantum-inspired evolutionary algorithm. The use of a quantum-inspired evolutionary algorithm provides a more efficient evolutionary algorithm that includes the past evaluation history to improve the generation of new programs. The linear genetic programming approach is more appropriate for machine code programs, as computer architectures require programs to be provided as linear sequences of instructions.

This work extends GMGP in two directions: First, we propose a new approach where the power of the GPU is fully exploited in the GP algorithm. Second, we assess the impact of selecting programs with the best evaluations on the best final solutions (i.e. programs). We have developed two approaches: GMGP-gpu and GMGP-gpu+. GMGP-gpu implements all the GP steps in the GPU and provides the following: (1) significant performance improvements in the GP steps and (2) elimination of the overhead when copying the fitness results from the GPU to the CPU through the PCIe bus. GMGP-gpu+ incorporates a new selection mechanism to recognize programs with the best evaluations. The new selection mechanism produces a more efficient comparison of the past population with the current population, bringing more diversity to the search.

The two proposed approaches were compared to GMGP and found to outperform GMGP for large-scale synthetic and real-world problems. The speedups ranged from 1.3 to 2.6. They also provided substantial speedups over CPU execution; the parallel GPGMGP-gpu+ executed up to 325.5 times faster than the CPU version.

The remainder of this paper is organized as follows: Section 2 presents previous work on parallel GP. Section 3 briefly introduces GP and quantum-inspired GP. Section 4 describes the GMGP system. Section 5 describes our approach to fully exploit the power of the GPU in GMGP. Section 6 presents the experimental results obtained for synthetic and real-world problems. Finally, Section 7 draws some conclusions and presents future research directions.

@&#RELATED WORK@&#

GP has been extensively used to solve a wide range of problems. The problem of a high computational burden associated with GP has been tackled with a variety of parallel processing methods: distributed algorithms [19], hardware implementation [20], and FPGA implementation [21].

Recently, the power of GPUs has been harnessed to accelerate GP processing in different ways. The first GPU implementations of GP were proposed by Chitty [9] and Harding and Banzhaf [10]. They used the compilation methodology and a tree representation for the programs. A single program was evaluated in parallel on the GPU with different input data. They obtained modest gains when only a little input data was used, and more prominent gains for larger datasets. Langdon and Banzhaf [15] proposed the interpretation methodology to evaluate programs and avoiding the compilation overhead of the previous approaches. The parallel algorithm evaluates the entire population of programs in the GPU by using the C++ RapidMind interpreter. The results indicated moderate speedups but demonstrated performance gains even for a very small input dataset. The interpreter proposed was further used for predicting the breast cancer survival rate in [16]. Robilliard et al. [17] studied the interpretation methodology as well. They implemented the interpreter in CUDA (Compute Unified Device Architecture) and avoided conditional branches. They obtained better performance gains than Langdon and Banzhaf [15]. Harding and Banzhaf [11] proposed a GPU implementation based on the compilation methodology, and a cluster of GPUs was used for alleviating the program compilation overhead. Speedups were obtained for very large datasets. Langdon and Harman [12] used the compilation methodology to automatically create an nVidia CUDA kernel. Numerous simplifications were employed. However, they could not automatically verify the speedup obtained as compared to the sequential CPU version. Cupertino et al. [13] proposed the use of a quantum-inspired algorithm to parallel process linear GP by using the NVidia PTX assembly language to create programs. They eliminated some compilation phases and observed gains for large datasets. However, the compilation overhead still dominated the execution time. Silva et al. [18] also used a quantum-inspired parallel linear GP but created programs in the GPU machine code. They compared their methodology with the existing compilation and interpretation methodologies and obtained significant performance gains.

None of the previous work on accelerating machine code GP on GPU mapped the whole GP computation to the GPU. In [14], a GP mapping was proposed, but it used grammatical evolution, which is an interpreted form of GP. Our work is an extension of the work of Silva et al. [18]. We propose here a new mapping of the problem for the GPU.

In GP, a population of computer programs is evolved. Each computer program is called an individual and represents a potential solution to the problem. The basic algorithm defines the following [22]:
                        
                           1.
                           
                              Terminal Set: A set of input variables and/or constants.


                              Function Set: A set of instructions that, combined with the terminals, are used to build a program.


                              Fitness Function: A score value assigned to each individual that gives the measure of the program fitness to the problem stated, i.e., how well the results of this execution match the desired results. For example, the mean absolute error (MAE) can be used as the fitness function.


                              Termination Condition: It can be a predefined number of generations or an error tolerance to the fitness.

The algorithm starts by randomly creating an initial population of individuals. Then, each individual is executed and its fitness value is computed. The individuals with good fitness have a higher probability of being chosen to create new individuals. The creation of new individuals follows the application of genetic operators, which produces diversification to the search. The new individuals follow the same steps until the termination condition is met.

The evolutionary algorithm used in this work is based on the quantum-inspired linear genetic programming (QILGP) algorithm proposed by Dias and Pacheco [23]. QILGP uses the multilevel quantum bit, called qudit, as the smallest information unit. A qudit may take any of d values or a superposition of d states. The superposition of states allows quantum-inspired evolutionary algorithms to represent the individuals probabilistically and has the following two main advantages: (i) global search capability with faster convergence and (ii) smaller population size than that of traditional evolutionary algorithms.

The evolutionary algorithm of QILGP differs from the conventional evolutionary algorithms in that it has a quantum representation for the individuals and applies a quantum operator, Operator P, to drive the creation of new individuals. The quantum individual consists of a number, 
                           
                              
                                 
                                    N
                                 
                                 
                                    max
                                 
                              
                           
                        , of instructions qudits and terminal qudits. 
                           
                              
                                 
                                    N
                                 
                                 
                                    max
                                 
                              
                           
                         represents the maximum length of a program in the evolutionary process.
                           1
                           Programs with different sizes can be created using the NOP instruction.
                        
                        
                           1
                         An instruction qudit represents the superposition of all the instructions in the function set, providing a probabilistic representation, as illustrated in Fig. 1
                        . Each instruction representation in the qudit has an occurrence probability of 
                           
                              
                                 
                                    p
                                 
                                 
                                    i
                                 
                              
                           
                         and depicts a state. A state 
                           
                              
                                 
                                    st
                                 
                                 
                                    i
                                 
                              
                           
                         is enclosed in a probability interval 
                           
                              
                                 
                                    I
                                 
                                 
                                    i
                                 
                              
                           
                         whose end points are the cumulative probability of the start and end of the state. The greater the probability of one state, the greater is the interval size. A terminal qudit has a similar representation.

A quantum individual has to be observed to generate a classical individual. A classical individual is the individual in traditional GP, i.e., one program. The observation process consists of 
                           
                              2
                              ×
                              k
                           
                         observations, one for each pair of instruction qudit and terminal qudit. One qudit is observed by randomly generating a value r 
                        
                           
                              {
                              r
                              ∈
                              R
                              
                              ∣
                              
                              0
                              ⩽
                              r
                              ⩽
                              1
                              }
                           
                         and searching for the interval 
                           
                              
                                 
                                    I
                                 
                                 
                                    i
                                 
                              
                           
                         to which r belongs. Bigger intervals have a greater probability of being chosen randomly.

QILGP does not use the usual genetic algorithm mutation and crossover operators to create new populations of individuals. It uses the quantum operator P. The operator P manipulates the probability 
                           
                              
                                 
                                    p
                                 
                                 
                                    i
                                 
                              
                           
                         of a qudit state, satisfying the normalization condition 
                           
                              
                                 
                                    ∑
                                 
                                 
                                    i
                                    =
                                    0
                                 
                                 
                                    d
                                    -
                                    1
                                 
                              
                              
                                 
                                    p
                                 
                                 
                                    i
                                 
                              
                              =
                              1
                           
                        , where d denotes the qudit size. Operator P works in two main steps. First, it increases the given probability of a state by 
                           
                              
                                 
                                    p
                                 
                                 
                                    i
                                 
                              
                              ←
                              
                                 
                                    p
                                 
                                 
                                    i
                                 
                              
                              +
                              s
                              ×
                              (
                              1
                              -
                              
                                 
                                    p
                                 
                                 
                                    i
                                 
                              
                              )
                           
                        , where s denotes a parameter called step size, which can assume any real value between 0 and 1. The second step is to adjust the values of all of the probabilities of the qudit in order to satisfy the normalization condition.

The evolutionary algorithm of QILGP follows five steps, as shown in Fig. 2
                        . Initially, the population of quantum individuals is created. The size of the population 
                           
                              
                                 
                                    P
                                 
                                 
                                    size
                                 
                              
                           
                         is an input parameter and the length of each individual is 
                           
                              
                                 
                                    N
                                 
                                 
                                    max
                                 
                              
                           
                        . The qudit cardinality d is the number of instructions in the function set. The probabilities of all states in a qudit are the same at the beginning, except for the NOP instruction (which depends on a user-defined parameter). The algorithm proceeds by performing the following steps iteratively until a termination condition is met (e.g., a predefined number of generations or the maximum computational time):
                           
                              1.
                              Each quantum individual of the population is observed, resulting in the respective classical individual.

The classical individuals are executed and the corresponding results are evaluated according to the defined fitness function.

The classical individuals of the current and previous populations are jointly sorted by their evaluations, ordered from the best to the worst.

The operator P is applied to each quantum individual by using the corresponding classical individual as reference.

The best classical individual evaluated in the current generation is stored in the solution, if it is better than the previous best classical individual.

GPU machine code genetic programming (GMGP) is the first system to automatically synthesize computer programs based on the synthesis of GPU machine code programs [18]. GMGP uses a quantum-inspired linear GP algorithm, based on QILGP. The basic algorithm of GMGP follows the evolutionary algorithm of Fig. 2. However, the evaluation step is executed in parallel on the GPU. The initialization, observation, selection, and operator P steps are executed on the CPU.

The evaluation step is the most expensive step in the entire evolutionary algorithm. GMGP focuses on reducing the execution time of this step. The evaluation process in GMGP is performed with a high degree of parallelism: different individuals are evaluated in parallel in different thread blocks and within each thread block where each thread evaluates a different fitness case; the fitness cases correspond to different input data for the individual. This parallelization scheme avoids code divergence as each thread in a block executes the same instruction over different fitness cases (input data) and different individuals are executed by different thread blocks.

The main goal of GMGP is to create classical individuals by using the GPU machine code. For each individual, GMGP creates a GPU machine code kernel. The kernels are loaded to the GPU program memory and executed in parallel. These individuals do not require any compilation or interpretation step. GMGP also proposes a new methodology for acquiring the GPU machine code and obtains impressive performance gains over other GPU-based GP systems found in the literature.

This work proposes a novel approach to explore the power of GPUs within the framework of GP. Our approach is called GMGP-gpu and aims at including the other genetic programming steps in the parallel execution of the GPU. GMGP-gpu is based on the observation that large problems imply in large programs and large search spaces. Large programs have a large number of instructions, and both the observation and the operator P steps of the quantum-inspired evolutionary algorithm can take a substantial amount of time to execute. In GMGP, the execution time increases proportionally with the increase in the number of instructions of the individuals. In addition, a large search space leads to the creation of a large number of individuals. Further, the selection step, where the algorithm selects the best individuals, can have a significant impact on the algorithm performance.

GMGP-gpu implements all the evolutionary steps on the GPU. The idea is to fully exploit the massive parallelism offered by the GPU in the observation, operator P, and selection steps, whereas these steps have to be executed in sequence. Our goal is to make the execution time of the evolutionary algorithm less dependent on the number of instructions of the programs and to avoid the overhead of copying the fitness results from the GPU to the CPU through the PCIe bus.

GMGP-gpu is designed to evolve linear sequences of single-precision floating point operations or linear sequences of Boolean operations. The floating point function set contains the following instructions: addition, subtraction, multiplication, division, data transfer, trigonometric, and other arithmetic operations. The Boolean function set contains the instructions: AND, OR, NAND, NOR, and NOT. The instructions have one operation and one or two arguments. Each argument can be a register or a memory position. As a register, the argument varies from 
                        
                           R
                           0
                        
                      to 
                        
                           R
                           7
                        
                     , and as a memory position, it can be used to load input data or a constant value.

GMGP-gpu is composed of five kernels, one for each step of the algorithm shown in Fig. 2. The first kernel performs the initialization of the quantum individuals. For each individual i, the first kernel creates in the GPU memory three matrices 
                        
                           
                              
                                 S
                              
                              
                                 f
                              
                           
                           ,
                           
                              
                                 S
                              
                              
                                 tm
                              
                           
                        
                     , and 
                        
                           
                              
                                 S
                              
                              
                                 tr
                              
                           
                        
                     . 
                        
                           
                              
                                 S
                              
                              
                                 f
                              
                           
                        
                      contains the instruction qudits. 
                        
                           
                              
                                 S
                              
                              
                                 tm
                              
                           
                        
                      contains the terminal qudits that represent memory accesses, and 
                        
                           
                              
                                 S
                              
                              
                                 tr
                              
                           
                        
                      the terminal qudits that represent registers. Each thread computes one value of each of the three matrices 
                        
                           
                              
                                 S
                              
                              
                                 f
                              
                           
                           ,
                           
                              
                                 S
                              
                              
                                 tm
                              
                           
                        
                     , and 
                        
                           
                              
                                 S
                              
                              
                                 tr
                              
                           
                        
                     . Thus, all the instruction qudits and terminal qudits for all the individuals of the population are initialized in parallel in the GPU memory.

The second kernel performs the observation of the quantum individuals and generates the classical individuals. Each thread inside a block is responsible for observing one instruction qudit and subsequently observing the terminal qudit. These two qudits cannot be observed in parallel; depending on the instruction observed, the terminals are different. A thread block observes all the qudits of a single individual, and the different individuals are observed in parallel by different thread blocks. The thread blocks are unidimensional with size 
                        
                           
                              
                                 N
                              
                              
                                 max
                              
                           
                        
                     . The grid is also unidimensional with the size equal to the number of individuals in the population. At the end of the kernel, each individual i is represented by a matrix 
                        
                           
                              
                                 O
                              
                              
                                 i
                              
                           
                        
                      (
                        
                           
                              
                                 N
                              
                              
                                 max
                              
                           
                           ×
                           2
                        
                     ) that describes the instructions and the arguments for i.

The third kernel performs the evaluation of all individuals in the same way as GMGP. The population is evaluated with a single kernel execution. Each thread block evaluates a single individual. All the threads inside the block execute the same instruction of the individual for different fitness cases (input data).

The fourth kernel selects the individuals on the basis of their evaluations. The evaluations of M individuals from a previous population and M individuals from the current population are sorted (in an increasing order of evaluation). Using a single block with 
                        
                           2
                           ×
                           M
                        
                      threads, the fourth kernel selects the M best-evaluated individuals from the previous and current populations. The parallel sort is executed using the insertion sort algorithm, where each thread compares two evaluations and replaces their positions when needed. The process is repeated iteratively until the evaluations are sorted.

The fifth kernel updates the qudits of the quantum individuals. The update is accomplished by applying operator P to all qudits. Each thread inside a block updates an instruction qudit or a terminal qudit of the quantum individual that corresponds to the classical individuals selected in the previous kernel. The number of threads inside a block is 
                        
                           
                              
                                 N
                              
                              
                                 max
                              
                           
                        
                     , and the number of blocks is the number of individuals multiplied by two.

One important aspect of GMGP-gpu is that the selection of the best individuals to update the qudits significantly influences the creation of the next population of individuals. It can have a considerable impact on the quality of the results of the evolutionary algorithm.

The sorting mechanism implemented in GMGP-gpu is a parallel version of the GMGP algorithm. The mechanism attempts to find the best individuals from the current and the previous populations. The goal is to maintain only the best-fitted individuals from the two populations. Although this idea makes sense in terms of advancing the search with the best individuals, it can fail in providing more diversity to the search. After some generations, the new populations tend to be inherited from a few good individuals at the beginning.

In order to increase the search diversity and to select the best evaluation efficiently, we propose a selection mechanism that provides a more efficient comparison of the previous population evaluations with the current population evaluations. The idea is to compare each current individual only with its counterpart, i.e., the individual in the previous population that created it. This implies that the selection tests whether the new individual improved the search.

This mechanism is implemented in a new approach, called GMGP-gpu+, that is similar to GMGP-gpu except for the fourth kernel. This kernel is modified to perform the selection in the GPU in the following manner: Using a single block with M threads, each thread compares a different pair of individuals, one from the current population and one from the previous population. The comparison considers their evaluations. If the evaluations are different, the best evaluation is selected. If the evaluations are equivalent, the smaller individual is selected.

@&#EXPERIMENTAL RESULTS@&#

GMGP-gpu and GMGP-gpu+ were implemented in C and CUDA 5.5 by using gcc 4.4.7, nvcc release 5.5, and V5.5.0, and by using 256 threads per block in the evaluation step. The experiments were conducted on a GeForce GTX TITAN GPU, with 2688 CUDA cores (at 837MHz) and 6GB of RAM (no ECC) with a memory bandwidth of 288.4GB/s through a 384-bit data bus. GMGP-gpu and GMGP-gpu+ were compared with a CPU-only parallel processing version, where the Intel x86 machine code was synthesized. For this experiment, we used the six cores of an Intel Xeon CPU X5690 processor, with 32KB of L1 data cache, 1.5MB of L2 cache, 12MB of L3 cache, and 24GB of RAM, running at 3.46GHz.

We used three different applications to evaluate our GP systems: symbolic regression (Salutowicz), time-series forecasting (Mackey–Glass), and binary classification (intrusion detection system – IDS). The symbolic regression application represents a synthetic benchmark, and we can evaluate the behavior of GMGP-gpu and GMGP-gpu+ with varying input data sizes. The time-series prediction represents a real-world application, typical in data mining. Intrusion detection also represents a real-world application that examines user activities in a computer system and detects anomalies and misuse. Intrusion detection is a large-scale problem; its input data consist of a dataset with more than 7,000,000 entries.

Each experiment is executed 10 times, and the timing results are the average of these executions. The input data used to evaluate the individuals are divided into three parts called training, validation, and test data. The training data are used for performing the evaluation of the individuals. The test data are used for confirming the performance of the algorithm on unseen data. The validation data are used for providing generalization and avoiding the overfitting of the individuals to the training data. The validation data are used whenever the training data evolution finds a better fitness.

The symbolic regression problem is also called function finding. It can be stated as finding an expression that fits well (within a certain error) to all the points of the given data (i.e., data fitting). The task of GP is to rebuild a surface based on the given points. Here, we focus on the bidimensional Salutowicz surface [24] that is represented by Eq. (1).
                           
                              (1)
                              
                                 f
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    
                                       
                                          y
                                          -
                                          5
                                       
                                    
                                 
                                 ×
                                 
                                    
                                       e
                                    
                                    
                                       -
                                       x
                                    
                                 
                                 ×
                                 
                                    
                                       x
                                    
                                    
                                       3
                                    
                                 
                                 ×
                                 cos
                                 
                                    
                                       
                                          x
                                       
                                    
                                 
                                 ×
                                 sin
                                 
                                    
                                       
                                          x
                                       
                                    
                                 
                                 ×
                                 
                                    
                                       
                                          cos
                                          
                                             
                                                
                                                   x
                                                
                                             
                                          
                                          ×
                                          sin
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                
                                             
                                             
                                                2
                                             
                                          
                                          -
                                          1
                                       
                                    
                                 
                                 .
                              
                           
                        The x and y variables are uniformly sampled in the range 
                           
                              [
                              0
                              ,
                              10
                              ]
                           
                        . This sampling generates the training, validation, and testing datasets. The validation and testing datasets represent 15% of the data each and are generated with a different sampling interval from that used in the training. In our experiments, we varied the number of input points to investigate the execution behavior under different sizes of input data. The variables x and y are sampled in 
                           
                              [
                              0
                              ,
                              10
                              ]
                           
                         according to different numbers of subdivisions: 16, 32, 64, 128, and 256. The subdivisions define the number of samples. Each time, both variables use the same number of samples, producing a grid. Accordingly, the number of input points varies in the set 
                           
                              S
                              =
                              {
                              256
                              ,
                              1024
                              ,
                              4096
                              ,
                              16
                              
                              K,
                              64
                              
                              K
                              }
                           
                        . The fitness value of an individual is its mean absolute error (MAE).

The parameters used in the execution of the symbolic regression are listed in Table 1
                        . As can be observed, we used a considerably large number of generations and a small population size. This is a typical characteristic of quantum-inspired algorithms. The evolution status is represented by a probability distribution, and there is no need to include a large number of individuals. The superposition of states provides a good global search ability.


                           Fig. 3
                            shows the performance of GMGP, GMGP-gpu, and GMGP-gpu+ in the Salutowicz evolution for the following different input dataset sizes: S
                           ={256, 1024, 4096, 16K, 64K}. The total execution times are shown in the left figure, and the speedups obtained by GMGP-gpu and GMGP-gpu+ as compared to GMGP are shown in the right figure.

The execution times of GMGP, GMGP-gpu, and GMGP-gpu+ increase almost linearly as the size of the input dataset increases from 256 to 64K. GMGP-gpu+ outperforms the other approaches for all the input dataset sizes, and GMGP-gpu also outperforms GMGP for all the input dataset sizes. GMGP-gpu+ executes from 1.7 to 2.6 times faster than GMGP, and GMGP-gpu executes from 1.3 to 2.1 times faster than GMGP. The most prominent gains were observed for the smaller datasets. This confirms that the massive parallelism of the GPU can be exploited to accelerate all the GP steps, not only the evaluation step as previously proposed. GMGP-gpu has been shown to be very effective when dealing with large datasets. In addition, the use of a more efficient selection mechanism reduces the execution time of the GP steps by about 23%.

To completely analyze the performance improvements the massive GPU parallelism brings to the GP evaluation process, we compare GMGP, GMGP-gpu, and GMGP-gpu+ to the CPU-only approach. The CPU-only approach is a parallel implementation of the original QILGP algorithm that synthesizes Intel x86 machine code. The parallel implementation of QILGP takes advantage of the multiple cores present in modern CPUs, and their ability to execute different instructions over multiple data in an MIMD (multiple instruction, multiple data) fashion. The CPU parallel implementation executes distinct evolutions on each core.


                           Table 2
                            shows the speedups of GMGP, GMGP-gpu, and GMGP-gpu+ as compared to the CPU execution in the Salutowicz evolution as the size of the input dataset varies: S
                           ={256, 1024, 4096, 16K, 64K}. For the smallest problem size (256), GMGP, GMGP-gpu, and GMGP-gpu+ are 1.7, 3.7, and 4.7 times faster than the CPU-only approach, respectively. For the largest dataset (64K), GMGP, GMGP-gpu, and GMGP-gpu+ are 190.5, 242.6, and 325.5 times faster than the CPU approach, respectively. Increasing the problem size and the amount of data to be processed improves the GPU occupancy.

Time-series forecasting is an important application commonly present in data mining and decision support systems. The forecasting aims at predicting the series behavior based on the historical values. The idea is to find a function 
                           
                              f
                              (
                              x
                              (
                              t
                              )
                              )
                           
                         that predicts the values of 
                           
                              x
                              (
                              t
                              )
                           
                        , based on the past values of 
                           
                              x
                              :
                              x
                              (
                              t
                              -
                              1
                              )
                              ,
                              x
                              (
                              t
                              -
                              2
                              )
                           
                        ,…, 
                           
                              x
                              (
                              t
                              -
                              i
                              )
                           
                        . Here, we are interested in the time series generated by the Mackey–Glass equation presented in (2). The task of an evolved program is to predict the next value of x.
                           
                              (2)
                              
                                 
                                    
                                       dx
                                       (
                                       t
                                       )
                                    
                                    
                                       dt
                                    
                                 
                                 =
                                 
                                    
                                       0.2
                                       x
                                       (
                                       t
                                       -
                                       τ
                                       )
                                    
                                    
                                       1
                                       +
                                       
                                          
                                             x
                                          
                                          
                                             10
                                          
                                       
                                       (
                                       t
                                       -
                                       τ
                                       )
                                    
                                 
                                 -
                                 0.1
                                 x
                                 (
                                 t
                                 )
                              
                           
                        In our experiments, the time series consists of 1200 data points. The GP inputs are the eight earlier values from the series at time steps 1, 2, 4, 8, 16, 32, 64, and 128. The fitness value of an individual is its root mean square error (RMS). Table 3
                         shows the parameters used in the Mackey–Glass evolution.


                           Table 4
                            presents the partial and total execution times (in seconds) of the Mackey–Glass forecasting for GMGP, GMGP-gpu, and GMGP-gpu+. We present the time spent in evaluating the individuals, the time spent in loading the individuals’ code into the GPU memory, the time spent in all other GP steps, except for the evaluation step, and the total execution time.

The evaluation time, which is usually the most expensive component of GP, becomes considerably less expensive when the entire evaluation process runs on GPU. This suggests that the GPU parallelism has been adequately exploited. The difference in the evaluation times of GMGP, GMGP-gpu, and GMGP-gpu+ is explained by the different evolutionary paths that they follow.

The loading of the individual codes into the GPU memory is performed by the CPU for all three approaches. Although GMGP-gpu and GMGP-gpu+ were conceived to fully exploit the GPU, current GPUs restrain the loading of machine code programs on the GPU program memory. This loading has to be done through the CUDA module cuModuleLoadDataEx, which requires the code to be allocated in the CPU memory. This limitation forces GMGP-gpu and GMGP-gpu+ to have some CPU computation and to transfer the codes to the GPU memory. Besides the difference in the evolutionary paths taken, the individuals generated by GMGP-gpu+ are about 49% smaller than the GMGP individuals. This difference is attributed to the new selection mechanism. This difference explains the shorter loading time for GMGP-gpu+. GMGP, which generates individuals with greater length, spends 90.5s in loading the codes, while GMGP-gpu+ spends only 29.58s.

The time spent on the other GP steps is 1.6 times greater for GMGP than for GMGP-gpu, which confirms that GMGP-gpu efficiently exploits the high degree of parallelism offered by the GPU architecture. The total execution time includes the GPU and CPU computation and the overhead of invoking the kernels (individuals). The total execution time increases by about 75% as compared to the GP time. Again, the time taken by GMGP-gpu+ is the shortest. GMGP-gpu+ executes 1.3 times faster than GMGP-gpu and 2 times faster than GMGP.

An intrusion is defined as a set of actions that endanger the integrity, confidentiality, or availability of a resource. Intrusion detection systems (IDS) are fundamental for computer security. Given the significance of this problem, the Classifier Learning Contest, KDD Cup 1999 [25], provides the only publicly available labeled dataset that recorded network traffic from the DARPA 98 Lincoln Lab. The KDD Cup 1999 dataset describes connections, and the training data encompass four categories of attacks: (1) Probing: surveillance and other probing, (2) DoS: denial of service, (3) U2R: unauthorized access to root privileges, and (4) R2L: unauthorized access from a remote machine.

The entire dataset consists of approximately 4,898,431 connection records. Previous works on GP that use the KDD Cup 1999 data, usually sample these data to make the evolution feasible. We address the entire dataset in our experiments. In addition, since the distribution of the attacks varies significantly, we balanced the dataset. This balancing increased the attack records, as shown in Table 5
                        . Table 6
                         shows the parameters used in the intrusion detection evolution.


                           Table 7
                            presents the partial and total execution times (in seconds) of the intrusion detection evolution for GMGP, GMGP-gpu, and GMGP-gpu+. For such a huge problem, the evaluation time is not the less-expensive step of GMGP, GMGP-gpu, and GMGP-gpu+. There are 2,000,000 generations over more than 5,000,000 input data items to be evaluated. GMGP-gpu requires considerably more evaluation time than GMGP because both these algorithms follow different evolution paths and GMGP-gpu has more instructions to evaluate. On the other hand, GMGP-gpu+ evolves smaller individuals and provides the shortest evaluation times. The time spent on loading the individual codes into the GPU memory is negligible as compared to the other timings.

Although the number of samples is different for the five classes, the times for computing the other GP steps are almost constant for both GMGP-gpu and GMGP-gpu+. This is an expected result since this timing does not depend on the problem size. The new selection mechanism of GMGP-gpu+ slightly improved its performance over GMGP-gpu. GMGP, which executes the other GP steps on the CPU, performed these steps about 1.4 times slower than GMGP-gpu and 1.5 times slower than GMGP-gpu+ for all the classes, except for DoS that consists of a smaller number of records.

@&#CONCLUSIONS@&#

The synthesis of programs based on user-defined requirements is an exciting field of evolutionary computation. At present, however, this ambition is bounded by the computational power needed to perform the evaluation of billions of programs. This work focuses on accelerating genetic programming (GP) to support the synthesis of large problems. We worked on exploiting the highly parallel environment of the GPU to tackle the huge computational effort required by GP.

We provide a new perspective on the implementation of GP on the GPU. We worked on a quantum-inspired linear GP system that generates programs using the GPU machine code. Our approach implements all the GP steps in the GPU and offers the following advantages over the previous approach: (1) the parallelization of all the GP steps, improving the GP performance; (2) the elimination of the overhead associated with copying the fitness results from the GPU to the CPU through the PCIe bus; and (3) the incorporation of a new selection mechanism to recognize the programs with the best evaluations.

Completely exploiting the power of the GPU to synthesize programs produces impressive accelerations. Our approach outperformed the previous approach for synthetic and real-world problems and provided remarkable speedups over the CPU execution. We obtained speedups ranging from 1.3 to 2.6 as compared to the previous approach, and speedups of up to 325.5 as compared to the CPU execution. We were the first to evolve the intrusion detection problem by using all of the input data during the evolution, rather than samples.

This work opens up the possibility of applying GP to a variety of relevant problems. Evolutionary machine learning techniques present a considerable potential for big data learning tasks owing to their flexibility in knowledge representations, learning paradigms, and their innate parallelism.

@&#REFERENCES@&#

