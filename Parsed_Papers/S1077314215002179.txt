@&#MAIN-TITLE@&#A novel double-layer sparse representation approach for unsupervised dictionary learning

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a DLSR approach for dictionary learning.


                        
                        
                           
                           The DLSR formulation enhances reconstructive and discriminative abilities of dictionary.


                        
                        
                           
                           A DLSR-OMP algorithm is developed to solve the DLSR formulation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sparse representation

Unsupervised learning

Texture segmentation

@&#ABSTRACT@&#


               
               
                  This paper presents a novel double-layer sparse representation (DLSR) approach, for improving both reconstructive and discriminative capabilities of unsupervised dictionary learning. In supervised/unsupervised discriminative dictionary learning, classical approaches usually develop a discriminative term for learning multiple sub-dictionaries, each of which corresponds to one-class training image patches. As such, the image patches for different classes can be discriminated by coefficients of sparse representation, with respect to different sub-dictionaries. However, in the unsupervised scenario, some of the training patches for learning the sub-dictionaries of different clusters are related to more than one cluster. Thus, we propose a DLSR formulation in this paper to impose the first-layer sparsity on the coefficients and the second-layer sparsity on the clusters for each training patch, embedding both the reconstructive (via the first-layer) and discriminative (via the second-layer) capabilities in the learned dictionary. To address the proposed DLSR formulation, a simple yet effective algorithm, called DLSR-OMP, is developed on the basis of the conventional OMP algorithm. Finally, the experiments verify that our approach can improve reconstruction and clustering performance of the learned dictionaries of the conventional approaches. More importantly, the experimental results on texture segmentation show that our approach outperforms other state-of-the-art discriminative dictionary learning approaches in the clustering task.
               
            

@&#INTRODUCTION@&#

Recently, there has been a growing interest in the use of sparse representation in both image processing and computer vision communities [1,2]. The key idea of sparse representation is to approximate a natural signal (e.g., an image patch) via a linear combination of a few elements (or called atoms) from an over-complete dictionary. That is, a natural signal can be reconstructed by aggregating the elements of the over-complete dictionary with their corresponding sparse coefficients, obtained via ℓ1 relaxation [3,4] or matching pursuit algorithms [5,6]. The earlier works of sparse representation [7] mainly concentrated on the predefined parametric over-complete dictionary. Afterwards, Wright et al. [8] found out that the dictionary can be directly chosen from training examples and it is competent for face recognition. More commonly, the dictionary learned from training examples has shown more effectiveness in modeling images [9]. The past decade has witnessed the explosion of approaches on learning the non-parametric dictionary from a set of training examples. At the beginning, several approaches [10,11] were proposed for learning reconstructive dictionary, aiming at faithfully reconstructing signals. A representative approach is K-SVD [10]. In exploring the reconstructive capability of the learned dictionary, K-SVD approach has been successfully applied for many image reconstruction tasks, such as image denoising [12] and image compression [13].

In reality, the image patches with similar patterns may be sparsely represented by similar elements in a same sub-dictionary. Therefore, discriminative dictionary learning [14] was proposed for image classification in the coefficient space, where the term “discriminative” refers to making the sparse coefficients of an image patch discriminant from patches of other classes. Most recently, the algorithms on learning discriminative dictionary have emerged as promising and effective approaches to deal with the tasks of classification or clustering, in computer vision area [14–18]. For supervised classification, the discriminative terms have been developed to embed the discriminative ability in the learned dictionary. Mairal et al. [14] proposed a softmax discriminative cost term in K-SVD approach for learning multiple sub-dictionaries, each of which corresponds to one-class training examples. Then, the classification, such as scene analysis and texture segmentation, can be achieved via seeking the minimal reconstruction error of these sub-dictionaries on the test image patches. Besides, Yang et al. [18] proposed the Fisher discrimination dictionary learning (FDDL) method to impose the Fisher discrimination criterion on both structured dictionary and sparse coefficients, via introducing a discriminative fidelity term for the dictionary and developing a discriminative coefficient term for the sparse coefficients of image patches.

Dictionary learning is more challenging in the unsupervised scenario [19–22]. Gowreesunker et al. [19] proposed a subspace clustering approach for dictionary learning. Such an approach utilizes Orthogonal Subspace Pursuit (OSP) decomposition to identify the subspaces, followed by clustering the observed data that fall into the same subspace. Sprechmann and Sapiro [21] proposed to learn the sub-dictionaries, with each one fitting best to a cluster of data, upon sparse representation. Later, in order to increase the independence between the sub-dictionaries of each cluster, an incoherence promoting term [22] was developed, as the discriminative term, for unsupervised dictionary learning in [21]. However, most existing unsupervised dictionary learning approaches
                        2
                     
                     
                        2
                        In this paper, unsupervised dictionary learning refers to learning discriminative dictionaries in unsupervised scenarios for clustering tasks, whereas some papers such as [23] define the reconstructive dictionary learning as unsupervised dictionary learning.
                      assume that there is only one cluster assigned to each training image patch. It is intuitive that some of the image patches used for learning dictionary, in unsupervised cases, belong to more than one cluster (or class). Fig. 1 reveals the possibility of an image patch belonging to one or more classes. In [24], Gramfort et al. have also shown that exploiting sparse pattern, over a few subsets of dictionaries, is able to improve the reconstructive performance of sparse representation, for functional brain imaging. However, their approach simply utilizes the fixed dictionaries, rather than learning dictionaries.

Therefore, the basic philosophy of this paper is to enforce the sparsity not only on the coefficients of training patches but also on the clusters of their related sub-dictionaries. That is a training patch can be reconstructed by a few elements from a handful of (one or more) sub-dictionaries, each of which is associated with a specific cluster. Towards this philosophy, we propose a double-layer sparse representation (DLSR) approach for unsupervised dictionary learning, where the first layer follows the traditional way to impose sparsity on coefficients with minimized reconstruction error, and the second layer is dedicated to sparsity on clusters of sub-dictionaries used for sparse representation. This way, both reconstructive (via the first layer) and discriminative (via the second layer) capabilities can be ensured in the learned dictionary. In our approach, the discriminative capability of the dictionary means that the image patches for different clusters can be discriminated in the sparse coefficient space, according to sparse representation with respect to the sub-dictionaries of different clusters. By integrating these two layers together, our approach has the potential to handle both reconstruction and clustering tasks in computer vision. The experimental results show that our approach is able to slightly improve the reconstruction performance of the conventional K-SVD approach, in terms of denosing accuracy. Although the denoising accuracy of our approach is less than other state-of-the-art approaches, it at least verifies that the discriminative ability is useful in image reconstruction. More importantly, our approach is much superior to other state-of-the-art discriminative dictionary learning approaches in texture segmentation, revealing the significant improvement of clustering performance by our approach.

From the perspective of reconstructive dictionary, both our approach and Learned Simultaneous Sparse Coding (LSSC) [25] are exploiting the self-similarities of natural images. However, our DLSR approach integrates reconstructive and discriminative powers in a unified formulation, whereas LSSC only focuses on reconstructive power of the learned dictionary. Hence, LSSC cannot be used for image classification/clustering tasks. Moreover, the training stage of our DLSR approach only depends on the local image, rather than the extensive training images required by LSSC. Beyond, another benefit of our approach is that it can easily shift from reconstructive learning to discriminative learning via simply setting the parameter of between-cluster scatter.

From the perspective of discriminative dictionary, the closest literature to our work is that of hierarchical dictionary learning or group dictionary learning [26–28], in which the dictionary is learned in a hierarchy or in a group structure. For example, elements learned by [26] are hierarchically organized with a tree-structure. Thus, they have to construct a complicate structure before learning the dictionary, and develop sophisticated algorithm for learning such a dictionary. In group dictionary learning [28], group structure for sub-dictionaries and their weight vectors have to be given before dictionary learning. Therefore, the advantage of our approach is that it requires no prior knowledge on dictionary structure, as needed in [26–28]. In addition, our approach enjoys the simplicity of only modifying the sparse representation step of conventional reconstructive dictionary learning approaches, with no change in the dictionary update step.

Now, we briefly review the basic ideas of reconstructive dictionary learning (Section 2.1) and discriminative dictionary learning (Section 2.2).

Due to the large dimension of images, sparse representation for modeling an image normally works with the image patches, extracted from a whole image. Let x be one of such image patches
                           3
                        
                        
                           3
                           As in other sparse representation approaches, each image patch is preprocessed by subtracting their mean intensity values.
                        , in the form of column vector with n pixels, extracted from input image X. Then, the problem of sparse representation for each image patch can be modeled to find the corresponding sparse coefficients w:

                           
                              (1)
                              
                                 
                                    
                                       min
                                       w
                                    
                                    
                                       
                                          ∥
                                          x
                                          −
                                          D
                                          w
                                          ∥
                                       
                                       2
                                       2
                                    
                                    
                                    s
                                    .
                                    t
                                    .
                                    
                                    
                                       
                                          ∥
                                          w
                                          ∥
                                       
                                       0
                                    
                                    ≤
                                    L
                                    ,
                                 
                              
                           
                        where L is the sparsity level of coefficients, and 
                           
                              D
                              ∈
                              
                                 R
                                 
                                    n
                                    ×
                                    m
                                 
                              
                           
                         (n ≪ m) is the over-complete dictionary (with m elements) to be learned. Note that each column of D is normalized to be 1, in order to eliminate the scaling ambiguity of coefficients. Also, note that || · ||0 is ℓ0 norm, indicating the number of nonzero values in the vector. However, the minimization problem of (1) is NP-hard. Thus, an extensive literature on solving (1) has been proposed, divided into two main categories: greedy approaches, e.g., orthogonal matching pursuit (OMP) [5], and ℓ1 norm relaxation, e.g., LASSO [3]. As greedy approaches are simple yet effective, this paper mainly focuses on OMP algorithm, summarized in Table 1.

Then, dictionary D can be learned in general from training image patches 
                           
                              
                                 X
                                 *
                              
                              =
                              
                                 {
                                 
                                    x
                                    1
                                    *
                                 
                                 ,
                                 …
                                 ,
                                 
                                    x
                                    t
                                    *
                                 
                                 ,
                                 …
                                 ,
                                 
                                    x
                                    T
                                    *
                                 
                                 }
                              
                           
                         with their corresponding sparse coefficients being 
                           
                              
                                 W
                                 *
                              
                              =
                              
                                 {
                                 
                                    w
                                    1
                                    *
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    t
                                    *
                                 
                                 ,
                                 …
                                 ,
                                 
                                    w
                                    T
                                    *
                                 
                                 }
                              
                           
                        . According to (1), learning D can be formulated as

                           
                              (2)
                              
                                 
                                    
                                       min
                                       
                                          
                                             W
                                             *
                                          
                                          ,
                                          D
                                       
                                    
                                    
                                       ∑
                                       t
                                    
                                    R
                                    
                                       (
                                       
                                          x
                                          t
                                          *
                                       
                                       ,
                                       
                                          w
                                          t
                                          *
                                       
                                       ,
                                       D
                                       )
                                    
                                    
                                    s
                                    .
                                    t
                                    .
                                    
                                    
                                       
                                          ∥
                                          
                                             w
                                             t
                                             *
                                          
                                          ∥
                                       
                                       0
                                    
                                    ≤
                                    L
                                    .
                                 
                              
                           
                        Here, we define by 
                           
                              R
                              
                                 (
                                 
                                    x
                                    t
                                    *
                                 
                                 ,
                                 
                                    w
                                    t
                                    *
                                 
                                 ,
                                 D
                                 )
                              
                              =
                              
                                 
                                    ∥
                                    
                                       x
                                       t
                                       *
                                    
                                    −
                                    D
                                    
                                       w
                                       t
                                       *
                                    
                                    ∥
                                 
                                 2
                                 2
                              
                           
                         the reconstructive term representing the squared error of reconstructing 
                           
                              x
                              t
                              *
                           
                         with dictionary D and sparse coefficients 
                           
                              w
                              t
                              *
                           
                        . Fortunately, (2) is able to be relaxed to convex in terms of either D or W*
                         with the other being fixed. As such, dictionary can be learned by iteratively updating D and W*
                        , with the following two steps:

                           
                              (a)
                              
                                 Sparse representation step: 
                                 Eq. (1) is addressed to find sparse coefficients W*
                                  of all training examples in X*
                                 , with dictionary D being fixed.


                                 Dictionary update step: 
                                 D is updated upon (2) using the sparse coefficients, obtained at the previous step, such that D is more suitable to represent the training examples.

To be more specific, we overview the skeleton of K-SVD [10], a well-known reconstructive dictionary learning algorithm. At the first step, K-SVD applies OMP to solve (1), finding the sparse coefficients of all training examples. At the second step, each element of D is updated sequentially with other elements being fixed. For updating each element d
                        
                           i
                         in D, only the training examples using d
                        
                           i
                         at the first step are utilized to estimate d
                        
                           i
                         and its corresponding coefficients, with the least overall reconstruction error addressed by singular value decomposition (SVD). The updating of each d
                        
                           i
                         using the relevant training examples is similar to K-means algorithm. So, such an algorithm is called K-SVD.

Instead of concentrating on the reconstructive capability of dictionary, discriminative dictionary learning approaches embed the discriminative power in dictionary D, which is composed of multiple sub-dictionaries 
                           
                              D
                              =
                              [
                              
                                 D
                                 1
                              
                              ,
                              …
                              
                                 D
                                 c
                              
                              ,
                              …
                              ,
                              
                                 D
                                 C
                              
                              ]
                           
                        . Here, D
                        
                           c
                         is the class-specified sub-dictionary corresponding to class c, and C is the class number in total. Then, input patch x can be categorized according to the minimum reconstruction error among all sub-dictionaries:

                           
                              (3)
                              
                                 
                                    
                                       min
                                       
                                          w
                                          ,
                                          c
                                          ∈
                                          {
                                          1
                                          ,
                                          …
                                          ,
                                          C
                                          }
                                       
                                    
                                    
                                       ∥
                                       x
                                       −
                                    
                                    
                                       D
                                       c
                                    
                                    
                                       
                                          w
                                          ∥
                                       
                                       2
                                       2
                                    
                                    
                                    s
                                    .
                                    t
                                    .
                                    
                                    
                                       
                                          ∥
                                          w
                                          ∥
                                       
                                       0
                                    
                                    ≤
                                    L
                                    .
                                 
                              
                           
                        From (3), one may see that it is rather important to make D discriminative such that D
                        
                           c
                         is capable of reconstructing the patches of class c with small errors, whereas reconstructing the patches of other classes with large errors. Towards this end, a discriminative term is introduced in the following for either supervised or unsupervised learning approaches on discriminative dictionary.


                        Supervised dictionary learning:Assume that Λc
                         is the subset of indices 
                           
                              {
                              1
                              ,
                              …
                              ,
                              t
                              ,
                              …
                              ,
                              T
                              }
                              ,
                           
                         corresponding to the training examples of class c. Then, the discriminative dictionary can be learned by adding discriminative term 
                           
                              S
                              (
                              
                                 x
                                 k
                                 *
                              
                              ,
                              
                                 w
                                 k
                                 *
                              
                              ,
                              
                                 D
                                 c
                              
                              )
                           
                         in (2) for each k ∈ Λc
                        :

                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             
                                                min
                                                
                                                   
                                                      W
                                                      *
                                                   
                                                   ,
                                                   
                                                      
                                                         {
                                                         
                                                            D
                                                            c
                                                         
                                                         }
                                                      
                                                      
                                                         c
                                                         =
                                                         1
                                                      
                                                      C
                                                   
                                                
                                             
                                             
                                                ∑
                                                c
                                             
                                             
                                                ∑
                                                
                                                   k
                                                   ∈
                                                   
                                                      Λ
                                                      c
                                                   
                                                
                                             
                                             R
                                             
                                                (
                                                
                                                   x
                                                   k
                                                   *
                                                
                                                ,
                                                
                                                   w
                                                   k
                                                   *
                                                
                                                ,
                                                
                                                   D
                                                   c
                                                
                                                )
                                             
                                             +
                                             β
                                             S
                                             
                                                (
                                                
                                                   x
                                                   k
                                                   *
                                                
                                                ,
                                                
                                                   w
                                                   k
                                                   *
                                                
                                                ,
                                                
                                                   D
                                                   c
                                                
                                                )
                                             
                                             
                                             
                                             s
                                             .
                                             t
                                             .
                                             
                                             
                                             
                                                
                                                   ∥
                                                   
                                                      w
                                                      k
                                                      *
                                                   
                                                   ∥
                                                
                                                0
                                             
                                             ≤
                                             L
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where β indicates the tradeoff between the reconstructive and discriminative capabilities of dictionary. Similar to learning reconstructive dictionary, the dictionary can be obtained via iterating between the sparse representation and dictionary update steps until convergence.

There are many variations of discriminative term 
                           
                              S
                              (
                              
                                 x
                                 k
                                 *
                              
                              ,
                              
                                 w
                                 k
                                 *
                              
                              ,
                              
                                 D
                                 c
                              
                              )
                              ,
                           
                         and a classical one is proposed in [14]. Assuming that 
                           
                              w
                              
                                 k
                                 c
                              
                              *
                           
                         is the sparse coefficients of 
                           
                              x
                              k
                              *
                           
                         with respect to sub-dictionary D
                        
                           c
                        , [14] defines discriminative term 
                           
                              S
                              (
                              
                                 x
                                 k
                                 *
                              
                              ,
                              
                                 w
                                 k
                                 *
                              
                              ,
                              
                                 D
                                 c
                              
                              )
                           
                         by

                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             S
                                             
                                                (
                                                
                                                   x
                                                   k
                                                   *
                                                
                                                ,
                                                
                                                   w
                                                   k
                                                   *
                                                
                                                ,
                                                
                                                   D
                                                   c
                                                
                                                )
                                             
                                             ≡
                                             log
                                             
                                                (
                                                
                                                   ∑
                                                   
                                                      
                                                         c
                                                         ′
                                                      
                                                      =
                                                      1
                                                   
                                                   C
                                                
                                                
                                                   e
                                                   
                                                      −
                                                      μ
                                                      (
                                                      R
                                                      
                                                         (
                                                         
                                                            x
                                                            k
                                                            *
                                                         
                                                         ,
                                                         
                                                            w
                                                            
                                                               k
                                                               
                                                                  c
                                                                  ′
                                                               
                                                            
                                                            *
                                                         
                                                         ,
                                                         
                                                            D
                                                            
                                                               c
                                                               ′
                                                            
                                                         
                                                         )
                                                      
                                                      −
                                                      R
                                                      
                                                         (
                                                         
                                                            x
                                                            k
                                                            *
                                                         
                                                         ,
                                                         
                                                            w
                                                            
                                                               k
                                                               c
                                                            
                                                            *
                                                         
                                                         ,
                                                         
                                                            D
                                                            c
                                                         
                                                         )
                                                      
                                                      )
                                                   
                                                
                                                )
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where μ > 0 is the parameter controlling the discriminative capability of dictionary. Obviously, the minimum value of 
                           
                              S
                              (
                              
                                 x
                                 k
                                 *
                              
                              ,
                              
                                 w
                                 k
                                 *
                              
                              ,
                              
                                 D
                                 c
                              
                              )
                           
                         can be achieved, once 
                           
                              R
                              (
                              
                                 x
                                 k
                                 *
                              
                              ,
                              
                                 w
                                 
                                    k
                                    c
                                 
                                 *
                              
                              ,
                              
                                 D
                                 c
                              
                              )
                           
                         is smallest value among all 
                           
                              R
                              (
                              
                                 x
                                 k
                                 *
                              
                              ,
                              
                                 w
                                 
                                    k
                                    
                                       c
                                       ′
                                    
                                 
                                 *
                              
                              ,
                              
                                 D
                                 
                                    c
                                    ′
                                 
                              
                              )
                              ,
                           
                        
                        
                           
                              
                                 c
                                 ′
                              
                              =
                              1
                              ,
                              …
                              ,
                              C
                           
                        . In addition, (5) encourages the difference in the reconstruction errors of each sub-dictionary via minimizing 
                           
                              S
                              (
                              
                                 x
                                 k
                                 *
                              
                              ,
                              
                                 w
                                 k
                                 *
                              
                              ,
                              
                                 D
                                 c
                              
                              )
                           
                         in (4). Finally, given (5) the dictionary can be obtained based on the discriminative K-SVD algorithm [14].


                        Unsupervised dictionary learning:Since Λc
                         of the training examples is unknown in unsupervised scenario, an additional example assignment step 
                        [22] has to be included for classifying the training examples via

                           
                              (6)
                              
                                 
                                    
                                       
                                          
                                             Λ
                                             c
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             {
                                             k
                                             |
                                             1
                                             ≤
                                             k
                                             ≤
                                             T
                                             ,
                                             R
                                             
                                                (
                                                
                                                   x
                                                   k
                                                   *
                                                
                                                ,
                                                
                                                   w
                                                   
                                                      k
                                                      c
                                                   
                                                   *
                                                
                                                ,
                                                
                                                   D
                                                   c
                                                
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          ≤
                                       
                                       
                                          
                                             R
                                             
                                                (
                                                
                                                   x
                                                   k
                                                   *
                                                
                                                ,
                                                
                                                   w
                                                   
                                                      k
                                                      
                                                         c
                                                         ′
                                                      
                                                   
                                                   *
                                                
                                                ,
                                                
                                                   D
                                                   
                                                      c
                                                      ′
                                                   
                                                
                                                )
                                             
                                             ,
                                             ∀
                                             
                                                c
                                                ′
                                             
                                             
                                                =
                                                1
                                                ,
                                                …
                                                ,
                                                C
                                                }
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              w
                              
                                 k
                                 c
                              
                              *
                           
                         and 
                           
                              
                                 w
                                 
                                    k
                                    
                                       c
                                       ′
                                    
                                 
                                 *
                              
                              ,
                           
                         as the sparse coefficients of 
                           
                              x
                              k
                              *
                           
                         regarding D
                        
                           c
                         and 
                           
                              
                                 D
                                 
                                    c
                                    ′
                                 
                              
                              ,
                           
                         can be obtained at the sparse representation step. 
                           
                              D
                              =
                              [
                              
                                 D
                                 1
                              
                              ,
                              …
                              
                                 D
                                 c
                              
                              ,
                              …
                              ,
                              
                                 D
                                 C
                              
                              ]
                           
                         is from the dictionary update step of the last iteration. Afterwards, the current iteration updates the discriminative dictionary via (4), with 
                           
                              
                                 ∑
                                 
                                    c
                                    ′
                                 
                              
                              
                                 ∥
                                 
                                    D
                                    c
                                    T
                                 
                                 
                                    D
                                    
                                       c
                                       ′
                                    
                                 
                                 ∥
                              
                           
                         
                        [22] being the discriminative term to promote the incoherence between sub-dictionaries. In a word, we can learn the discriminative dictionary in unsupervised manner by iterating over sparse representation, example assignment and dictionary update steps.

From now on, we provide the details of our DLSR approach on unsupervised dictionary learning. First, Section 3.1 proposes a DLSR formulation, in which the first-layer sparsity on coefficients minimizes the reconstruction error and the second-layer sparsity on the clusters of each patch yields more discriminative dictionary. Second, Section 3.2 presents a solution to the proposed DLSR formulation. Third, as the key part of our solution, the DLSR-OMP algorithm, for the sparse representation step of dictionary learning, is developed in Section 3.3. Finally, given the dictionary learned with our DLSR approach, reconstruction and clustering can be achieved by (1) and (3), respectively.

As presented above, (2) has formulated the first-layer sparsity on coefficients for dictionary learning. To make the dictionary discriminative, the corresponding sub-dictionaries of nonzero coefficients from the first layer have to fall into a small number of clusters. As such, the sparsity on the clusters of each image patch can be imposed. We therefore develop the diagram of DLSR formulation in Fig. 2
                        , in which the objective of the second-layer sparsity is to enforce nonzero coefficients (via their indices) belonging to the sub-dictionaries of “few” clusters. Consequently, only these sub-dictionaries with their corresponding nonzero coefficients are updated, given the training patches that correspond to those “few” clusters. As such, the discriminative capability is encoded in the learned dictionary.

We start to describe the proposed DLSR formulation by introducing 
                           
                              f
                              
                                 (
                                 
                                    w
                                    
                                       t
                                    
                                    *
                                 
                                 )
                              
                              =
                              
                                 [
                                 ∥
                              
                              
                                 w
                                 
                                    t
                                    1
                                 
                                 *
                              
                              
                                 
                                    ∥
                                 
                                 1
                              
                              ,
                              …
                              ,
                              
                                 
                                    ∥
                                    
                                       w
                                       
                                          t
                                          c
                                       
                                       *
                                    
                                    ∥
                                 
                                 1
                              
                              ,
                           
                        
                        
                           
                              
                                 …
                                 ,
                                 ∥
                              
                              
                                 w
                                 
                                    t
                                    C
                                 
                                 *
                              
                              
                                 
                                    ∥
                                    1
                                 
                                 ]
                              
                           
                         as the importance weights of sub-dictionaries 
                           
                              [
                              
                                 D
                                 1
                              
                              ,
                              …
                              
                                 D
                                 c
                              
                              ,
                           
                        
                        
                           
                              …
                              
                                 D
                                 C
                              
                              
                                 ]
                              
                           
                         to training image patch 
                           
                              x
                              t
                              *
                           
                        . 
                           
                              w
                              
                                 t
                                 c
                              
                              *
                           
                         is the sub-vector of 
                           
                              
                                 w
                                 
                                    t
                                 
                                 *
                              
                              ,
                           
                         corresponding to sub-dictionary D
                        
                           c
                        . Since nonzero values of 
                           
                              
                                 ∥
                              
                              
                                 w
                                 
                                    t
                                    c
                                 
                                 *
                              
                              
                                 
                                    ∥
                                 
                                 1
                              
                           
                         indicate that 
                           
                              x
                              t
                              *
                           
                         falls into cluster c, 
                           
                              
                                 ∥
                                 f
                              
                              
                                 (
                                 
                                    w
                                    
                                       t
                                    
                                    *
                                 
                                 )
                              
                              
                                 
                                    ∥
                                 
                                 0
                              
                           
                         means the cluster number, to which 
                           
                              x
                              t
                              *
                           
                         is related. Then, we make discriminative term 
                           
                              S
                              
                                 (
                                 
                                    x
                                    t
                                    *
                                 
                                 ,
                                 
                                    w
                                    t
                                    *
                                 
                                 ,
                                 D
                                 )
                              
                              =
                              
                                 
                                    ∥
                                    f
                                    
                                       (
                                       
                                          w
                                          
                                             t
                                          
                                          *
                                       
                                       )
                                    
                                    ∥
                                 
                                 0
                              
                           
                         as sparsity constraint on the clusters of each image patch, thus having the following DLSR formulation via rewriting (4):

                           
                              (7)
                              
                                 
                                    
                                       
                                          
                                             
                                                min
                                                
                                                   
                                                      W
                                                      *
                                                   
                                                   ,
                                                   D
                                                
                                             
                                             
                                                ∑
                                                t
                                             
                                             R
                                             
                                                (
                                                
                                                   x
                                                   t
                                                   *
                                                
                                                ,
                                                
                                                   w
                                                   t
                                                   *
                                                
                                                ,
                                                D
                                                )
                                             
                                             
                                                +
                                                β
                                                ∥
                                                f
                                             
                                             
                                                (
                                                
                                                   w
                                                   
                                                      t
                                                   
                                                   *
                                                
                                                )
                                             
                                             
                                                
                                                   ∥
                                                
                                                0
                                             
                                             
                                             s
                                             .
                                             t
                                             .
                                             
                                             
                                                
                                                   ∥
                                                   
                                                      w
                                                      t
                                                      *
                                                   
                                                   ∥
                                                
                                                0
                                             
                                             ≤
                                             L
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where parameter β ≥ 0 is the penalty on between-cluster scatter, implying the tradeoff between the discriminative and reconstructive capabilities of dictionary. Note that in (7), we have 
                           
                              D
                              =
                              [
                              
                                 D
                                 1
                              
                              ,
                              …
                              
                                 D
                                 c
                              
                              ,
                              …
                              ,
                              
                                 D
                                 C
                              
                              ]
                           
                        .

Our DLSR formulation, as presented in (7), has advantages over the conventional approaches in three aspects. First, the DLSR formulation does not require any prior of Λc
                         that indicates the classification on training examples. Our approach thereby enjoys the simplicity of prescinding from the additional step of example assignment step for obtaining Λc
                        , which is included in conventional unsupervised approaches [21,22]. Second, the prior on the dictionary structure is not required in our DLSR formulation. As such, our approach can be easily applied without any sophisticated design for dictionary structure, which is required in [26–28]. Third, in our DLSR formulation the discriminative term is only dependent on sparse coefficients, while others, e.g., [14,18], take into account dictionary in their discriminative terms. Thus, with sparse coefficients estimated for the specific clusters, our DLSR formulation can be easily implemented by directly utilizing the same dictionary update step as the existing reconstructive dictionary learning approaches.

Now, we consider the solution to the DLSR formulation. Recall that for reconstructive dictionary learning in (2), a dictionary update step needs to be run with sparse coefficients 
                           
                              w
                              t
                              *
                           
                         being fixed. It is important to recognize that when 
                           
                              w
                              t
                              *
                           
                         is fixed, the derivative formulation of Lagrange multiplier for (7), in terms of D, is

                           
                              (8)
                              
                                 
                                    
                                       ∂
                                       
                                          ∂
                                          D
                                       
                                    
                                    
                                       ∑
                                       t
                                    
                                    R
                                    
                                       (
                                       
                                          x
                                          t
                                          *
                                       
                                       ,
                                       
                                          w
                                          t
                                          *
                                       
                                       ,
                                       D
                                       )
                                    
                                    ,
                                 
                              
                           
                        which is the same as that of Lagrange multiplier for (2). Therefore, the dictionary update step of reconstructive dictionary learning algorithms, e.g., K-SVD, can be directly utilized for learning dictionary with the DLSR formulation.

In fact, the sparse representation step of our approach imposes discrimination on the dictionary, through the ℓ0 norm constraint on 
                           
                              f
                              (
                              
                                 w
                                 
                                    t
                                 
                                 *
                              
                              )
                           
                         (i.e., the second-layer sparsity). This results in updating the sub-dictionaries of the relevant clusters at the dictionary update step, since their corresponding nonzero-valued coefficients only fall into these clusters. For each individual 
                           
                              x
                              t
                              *
                           
                         and 
                           
                              
                                 w
                                 t
                                 *
                              
                              ,
                           
                         the coefficients of the sparse representation step can be determined by

                           
                              (9)
                              
                                 
                                    
                                       min
                                       
                                          w
                                          t
                                          *
                                       
                                    
                                    R
                                    
                                       (
                                       
                                          x
                                          t
                                          *
                                       
                                       ,
                                       
                                          w
                                          t
                                          *
                                       
                                       ,
                                       D
                                       )
                                    
                                    
                                       +
                                       β
                                       ∥
                                       f
                                    
                                    
                                       (
                                       
                                          w
                                          t
                                          *
                                       
                                       )
                                    
                                    
                                       
                                          ∥
                                       
                                       0
                                    
                                    
                                    s
                                    .
                                    t
                                    .
                                    
                                    
                                       
                                          ∥
                                          
                                             w
                                             t
                                             *
                                          
                                          ∥
                                       
                                       0
                                    
                                    ≤
                                    L
                                    ,
                                 
                              
                           
                        with all sub-dictionaries being fixed.

Generally speaking, the dictionary can be achieved by addressing the DLSR formulation of (7) through alternating between sparse representation step and traditional dictionary update step. However, (9) for sparse representation step is a non-convex problem, similar to (1). Since OMP is an effective algorithm to solve (1), the DLSR-OMP algorithm is developed in the next subsection for solving (9), on the basis of OMP. Here, our DLSR-OMP algorithm extends OMP, to pose a possible solution to DLSR formulation of (7). The optimal solution to (7), such as LASSO-based algorithm, is beyond the scope of this paper, and it is worthy to be investigated in future work.

For fast convergence, the dictionary has to be initialized to contain several sub-dictionaries. Following [22], this paper refers to the similarity graph G of dictionary elements by {D, W
                        *
                        W
                        *T
                        }, in which the elements of D are the vertex set and W
                        *
                        W
                        *T
                         is the matrix of edge weights. Given G, we use the spectral clustering algorithm [30] to partition D into several clusters, as the initial structured dictionary. Note that despite sharing the same initial structured dictionary as [22], our DLSR approach has better convergence performance as validated in the experimental results of Section 4.2.

With the dictionary learned by the proposed DLSR formulation, conventional sparse representation approaches can be directly used to compute sparse coefficients and their corresponding reconstruction errors, for each image patch. Then, reconstruction of each image patch can be obtained given the sparse coefficients yielded by (1). Besides, Eq. (3) can be directly applied for clustering image patches.

This subsection concentrates on solving (9) by proposing the DLSR-OMP algorithm. Table 2 summarizes the proposed DLSR-OMP algorithm. More details about steps for one iteration in the DLSR-OMP algorithm (Table 2) are presented in the following.


                        Steps 1–4.These four steps mainly deal with the classification of nonzero coefficients, as 
                           
                              
                                 ∥
                                 f
                              
                              
                                 (
                                 
                                    w
                                    t
                                    *
                                 
                                 )
                              
                              
                                 
                                    ∥
                                 
                                 0
                              
                           
                         needs to be estimated and minimized in (9). As summarized in Table 1, each iteration of OMP selects a new element from the dictionary to estimate the sparse coefficients, thus yielding one more nonzero coefficient at each iteration. To solve (9), the selection of the new element in each iteration of OMP needs to take into account the minimization on 
                           
                              
                                 ∥
                                 f
                              
                              
                                 (
                                 
                                    w
                                    t
                                    *
                                 
                                 )
                              
                              
                                 
                                    ∥
                                 
                                 0
                              
                           
                        . In fact, the value of 
                           
                              
                                 ∥
                                 f
                              
                              
                                 (
                                 
                                    w
                                    t
                                    *
                                 
                                 )
                              
                              
                                 
                                    ∥
                                 
                                 0
                              
                           
                         can be counted by judging whether the selected element belongs to the same or different clusters, compared to the previous iterations. Therefore, given residual r from the previous iteration and sub-dictionaries D
                        
                           c
                         of each cluster, the newly yielded nonzero coefficient at iteration l of the DLSR-OMP algorithm can be classified and estimated, according to the following two cases.


                        Case 1: Compared to the previous iterations, the newly yielded nonzero coefficient stays in the same cluster. Then, the cluster of the new nonzero coefficient at iteration l can be obtained with

                           
                              (10)
                              
                                 
                                    
                                       
                                          c
                                          ^
                                       
                                       1
                                       s
                                    
                                    =
                                    
                                       argmax
                                       
                                          c
                                          ∈
                                          L
                                       
                                    
                                    
                                       
                                          ∥
                                          
                                             D
                                             c
                                             T
                                          
                                          r
                                          ∥
                                       
                                       ∞
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              L
                              ⊆
                              {
                              1
                              ,
                              …
                              ,
                              c
                              ,
                              …
                              ,
                              C
                              }
                           
                         is the cluster subset, to which the nonzero coefficients of the previous iterations belong. Infinity norm || · ||∞ is the maximum absolute value of vector. Afterwards, similar to OMP, the largest coordinate λ
                        1 of 
                           
                              
                                 D
                                 
                                    
                                       
                                          c
                                          ^
                                       
                                       1
                                       s
                                    
                                 
                                 T
                              
                              r
                           
                         in absolute value identifies the index of the new nonzero coefficient at iteration l, with respect to 
                           
                              D
                              
                                 
                                    c
                                    ^
                                 
                                 1
                                 s
                              
                           
                         denoted as the sub-dictionary of cluster 
                           
                              
                                 c
                                 ^
                              
                              1
                              s
                           
                        . Let I
                        1 ← I ∪ {Υ(λ
                        1)} be the index set of this case at the current iteration. Here, I is the index set from the previous iterations and Υ(·) maps coordinates of 
                           
                              
                                 D
                                 
                                    
                                       
                                          c
                                          ^
                                       
                                       1
                                       s
                                    
                                 
                                 T
                              
                              r
                           
                         into coordinates of D
                        
                           T
                        
                        r. Finally, the same as OMP, we compute the sparse coefficients of this case by 
                           
                              
                                 w
                                 1
                              
                              =
                              
                                 D
                                 
                                    I
                                    1
                                 
                                 †
                              
                              x
                           
                        . 
                           
                              D
                              
                                 I
                                 1
                              
                              †
                           
                         is the pseudoinverse matrix of 
                           
                              
                                 D
                                 
                                    I
                                    1
                                 
                              
                              ,
                           
                         which only keeps the columns of D corresponding to I
                        1 and sets others to zero vector.


                        Case 2: Compared to the previous iterations, the newly yielded nonzero coefficient belongs to a different cluster. Cluster 
                           
                              
                                 c
                                 ^
                              
                              2
                              s
                           
                         of the new nonzero coefficient can be selected from subset 
                           
                              H
                              =
                              {
                              1
                              ,
                              …
                              ,
                              c
                              ,
                              …
                              ,
                              C
                              }
                              ∖
                              L
                           
                        :

                           
                              (11)
                              
                                 
                                    
                                       
                                          c
                                          ^
                                       
                                       2
                                       s
                                    
                                    =
                                    
                                       argmax
                                       
                                          c
                                          ∈
                                          H
                                       
                                    
                                    
                                       
                                          ∥
                                          
                                             D
                                             c
                                             T
                                          
                                          r
                                          ∥
                                       
                                       ∞
                                    
                                    .
                                 
                              
                           
                        Given sub-dictionary
                        
                           
                              D
                              
                                 
                                    c
                                    ^
                                 
                                 2
                                 s
                              
                           
                         of cluster 
                           
                              
                                 
                                    c
                                    ^
                                 
                                 2
                                 s
                              
                              ,
                           
                         the index of the new nonzero coefficient at iteration l can be obtained with the largest absolute coordinate λ
                        2 of 
                           
                              
                                 D
                                 
                                    
                                       
                                          c
                                          ^
                                       
                                       2
                                       s
                                    
                                 
                                 T
                              
                              r
                           
                        . Similar to Case 1, we denote by I
                        2 ← I ∪ {Υ(λ
                        2)} the index set of Case 2. At last, sparse coefficients of this case, denoted by w
                        2, can be reached via 
                           
                              
                                 D
                                 
                                    I
                                    2
                                 
                                 †
                              
                              x
                              ,
                           
                         where 
                           
                              D
                              
                                 I
                                 2
                              
                              †
                           
                         is the pseudoinverse matrix of 
                           
                              D
                              
                                 I
                                 2
                              
                           
                        . In 
                           
                              
                                 D
                                 
                                    I
                                    2
                                 
                              
                              ,
                           
                         only the columns of D corresponding to I
                        2 are maintained and others are set to 0.


                        Steps 5–6. These two steps aim at choosing between w
                        1 and w
                        2, to find the minimum of (9). Clearly, since there is one more cluster involved in Case 2 than Case 1, the above two cases deliver 
                           
                              
                                 ∥
                                 f
                              
                              
                                 (
                                 
                                    w
                                    2
                                 
                                 )
                              
                              
                                 
                                    ∥
                                 
                                 0
                              
                              −
                              
                                 
                                    ∥
                                    f
                                    
                                       (
                                       
                                          w
                                          1
                                       
                                       )
                                    
                                    ∥
                                 
                                 0
                              
                              =
                              1
                           
                        . This yields

                           
                              (12)
                              
                                 
                                    
                                       
                                          w
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                argmin
                                                
                                                   
                                                      w
                                                      1
                                                   
                                                   ,
                                                   
                                                      w
                                                      2
                                                   
                                                
                                             
                                             
                                                {
                                                R
                                             
                                             
                                                (
                                                x
                                                ,
                                                
                                                   w
                                                   1
                                                
                                                ,
                                                
                                                   D
                                                   
                                                      I
                                                      1
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             
                                                +
                                                β
                                                ∥
                                                f
                                             
                                             
                                                (
                                                
                                                   w
                                                   1
                                                
                                                )
                                             
                                             
                                                
                                                   ∥
                                                
                                                0
                                             
                                             ,
                                             R
                                             
                                                (
                                                x
                                                ,
                                                
                                                   w
                                                   2
                                                
                                                ,
                                                
                                                   D
                                                   
                                                      I
                                                      2
                                                   
                                                
                                                )
                                             
                                             
                                                +
                                                β
                                                ∥
                                                f
                                             
                                             
                                                (
                                                
                                                   w
                                                   2
                                                
                                                )
                                             
                                             
                                                
                                                   ∥
                                                   0
                                                
                                                }
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                argmin
                                                
                                                   
                                                      w
                                                      1
                                                   
                                                   ,
                                                   
                                                      w
                                                      2
                                                   
                                                
                                             
                                             
                                                {
                                                R
                                                
                                                   (
                                                   x
                                                   ,
                                                   
                                                      w
                                                      1
                                                   
                                                   ,
                                                   
                                                      D
                                                      
                                                         I
                                                         1
                                                      
                                                   
                                                   )
                                                
                                                ,
                                                R
                                                
                                                   (
                                                   x
                                                   ,
                                                   
                                                      w
                                                      2
                                                   
                                                   ,
                                                   
                                                      D
                                                      
                                                         I
                                                         2
                                                      
                                                   
                                                   )
                                                
                                                +
                                                β
                                                }
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        for finding the optimal solution to (9) at each iteration. As seen from (12), the selection of w
                        1 and w
                        2 relies on the comparison between 
                           
                              R
                              (
                              x
                              ,
                              
                                 w
                                 1
                              
                              ,
                              
                                 D
                                 
                                    I
                                    1
                                 
                              
                              )
                           
                         and 
                           
                              R
                              (
                              x
                              ,
                              
                                 w
                                 2
                              
                              ,
                              
                                 D
                                 
                                    I
                                    2
                                 
                              
                              )
                              +
                              β
                           
                        . For the convenience of such comparison, β can be represented by 
                           
                              η
                              R
                              (
                              x
                              ,
                              
                                 w
                                 2
                              
                              ,
                              
                                 D
                                 
                                    I
                                    2
                                 
                              
                              )
                              ,
                           
                         where the value of η is to be discussed in the next section. After obtaining w at each iteration, residual r can be updated: 
                           
                              r
                              =
                              x
                              −
                              D
                              w
                              ,
                           
                         for the next iteration.


                        Step 7. In this step, cluster sets L, H and index set I have to be updated in the end of each iteration. As such, the information on the number of clusters already selected for sparse representation can be simply stored in L and H. If w is equivalent to w
                        2, then sub-dictionary 
                           
                              D
                              
                                 
                                    c
                                    ^
                                 
                                 2
                                 s
                              
                           
                         is picked up to estimate w, so that L, H, and I need to be updated with 
                           
                              L
                              ∪
                              {
                              
                                 
                                    c
                                    ^
                                 
                                 2
                                 s
                              
                              }
                              ,
                           
                        
                        
                           
                              H
                              ∖
                              {
                              
                                 
                                    c
                                    ^
                                 
                                 2
                                 s
                              
                              }
                              ,
                           
                         and I
                        2, for the next iteration. Otherwise, we have I ← I
                        1 without any change in L and H.

At last, let us look at the initialization and convergence of the DLSR-OMP algorithm. It simply initializes 
                           
                              I
                              =
                              ⌀
                           
                         and 
                           
                              r
                              =
                              x
                              ,
                           
                         identical to the conventional OMP algorithm. Besides, w
                        2 of Case 2 has to be chosen as the output coefficients w at the first iteration, since no cluster is available before the first iteration. Meanwhile, H and L are initialized to 
                           
                              {
                              1
                              ,
                              …
                              ,
                              c
                              ,
                              …
                              ,
                              C
                              }
                           
                         and 
                           
                              ⌀
                              ,
                           
                         respectively. This implies that no cluster is selected before the first iteration. For the convergence of the DLSR-OMP algorithm, see [5], since our algorithm is similar to the conventional OMP in the way of computing residual r.

@&#EXPERIMENTAL RESULTS@&#

In this section, our approach was tested on several experiments on image denoising and texture segmentation, to evaluate the reconstructive and discriminative capabilities of learned dictionaries. Recall that Section 3.2 presents that our DLSR approach shares the same dictionary update step as other reconstructive dictionary learning approaches. So, in our experiments, the dictionary update step of K-SVD [12] was applied in combination with the proposed DLSR-OMP algorithm for our DLSR approach. Although K-SVD is not a state-of-the-art image denoising approach, its denoising results are compared with those of our DLSR approach in Section 4.1, to find out the improvement of reconstructive capability of our approach for image reconstruction task. Note that the conventional K-SVD approach can be seen as the relaxation of our approach to the case of only using single layer sparsity. Also note that our DLSR approach can be integrated into other reconstructive dictionary learning approach by only modifying sparse representation step. For the clustering task, we show in Section 4.2 that the DLSR approach is able to segment the texture with the better results than discriminative dictionary learning approaches [14] and [22], which are also based on K-SVD. Thus, it reveals the enhancement of our DLSR approach in the discriminative capability of learned dictionaries.

To validate the reconstructive ability of the DLSR approach, image denoising was conducted on several images, corrupted by additive Gaussian noise with zero mean and various standard deviations. The conventional K-SVD approach [12] removes the noise by finding the sparse representation
                           4
                        
                        
                           4
                           The threshold of the reconstruction error by sparse representation is the standard deviation of noise multiplied by a constant, empirically chosen to be 1.15 [12].
                         of each noisy image patch, regarding the dictionary learned from local image patches by OMP (for sparse representation step) and K-SVD (for dictionary update step). Different from [12], our approach used the DLSR-OMP algorithm of Table 2, instead of OMP, to learn the dictionary upon (7), for image denoising. Then, we compared the denoising results between our DLSR and K-SVD [12] approaches. For fair comparison, the same parameter settings were chosen in the K-SVD and our approaches. For example, 8 × 8 image patches were extracted from the local noisy image for dictionary learning and the sub-dictionary number for our DLSR-OMP approach was simply set to 4.

For initial dictionary, our DLSR approach first ran K-SVD with OMP 5 iterations. Then, following the spectral clustering [30], our approach ran 5 iterations of K-SVD with the DLSR-OMP algorithm, to obtain the learned dictionaries. By contrast, K-SVD with the OMP algorithm was directly run 10 iterations for the conventional K-SVD approach
                           5
                        
                        
                           5
                           The spectral clustering groups the dictionary elements belonging to the same cluster together, by modifying their indices without any adjustment in their values. Therefore, the spectral clustering is not useful in initializing dictionary of the conventional K-SVD approach, whereas it is quite important to our DLSR approach which relies on element indices.
                        . After obtaining the learned dictionary, the OMP algorithm was conducted to solve (1), for reconstructing the image patches. The image denoising was finally worked out by aligning the reconstructed image patches. It is worth pointing out that η of Table 2 was tuned to be 
                           
                              
                                 1
                                 40
                              
                              ,
                           
                         to make the overall reconstruction error in our image denoising experiment smallest.

First, Table 3 reports the PSNR results of our DLSR, K-SVD [12], LSSC [25] and NCSR [31] approaches on denoising five standard test images, with the standard deviations σ of Gaussian noise being 10, 15, 20 and 25. It can be observed from this table that our approach is slightly better than K-SVD in most cases, with up to 0.25 dB improvement. Such a slight improvement verifies the intuition that reconstructing each kind of objects in an image, with dictionary elements belonging to their relevant cluster, can achieve better reconstruction results. Fig. 3
                        
                        
                         further shows the denoising results of the image “House” (
                           
                              σ
                              =
                              15
                           
                        ) by our and K-SVD approaches. However, the denoising accuracy of our approach is lower than those of the latest LSSC and NCSR approaches. It is mainly because only a discriminative term is introduced in the K-SVD approach, which is far from the state-of-the-art LSSC and NCSR approaches. At least, it verifies that the discriminative term of (7) has positive effect on image reconstruction.

Second, five images were randomly selected from Berkeley Segmentation Data Set (BSDS), for the test. These images are shown in Fig. 4. Table 4 demonstrates the denoising results on these images with additive Gaussian noises. Again, our DLSR approach is superior to the conventional K-SVD approach, and is inferior to LSSC and NCSR approaches. Although image denoising results of our approach cannot be comparable to the state-of-the-art results, the discriminative term of our approach is able to improve the reconstructive ability of K-SVD. Besides, it is obvious that the improvement of our DLSR approach over K-SVD in Table 4 is much greater than that in Table 3. This phenomenon is probably caused by the fact that the discriminative term of (7) is more helpful in reconstructing the images which have the distinct texture patterns and clear boundaries. Since the discriminative term in dictionary learning is normally used for image classification, we mainly concentrate on the following texture segmentation experiments,
                         which show the superior performance of our approach in the clustering task.

In this subsection, the clustering performance of the proposed DLSR approach is evaluated with the experiments on unsupervised texture segmentation. The overlapping 16 × 16 patches [22] were extracted from the original texture images, as the input to our DLSR approach. Since the dictionary is over-complete, it included 800 · C ( ≫ 256) elements in our experiments, where C is the class number of texture in an image. Next, given the input patches, our DLSR approach performed 10 iterations of K-SVD with OMP for initial dictionary. Afterwards, following the spectral clustering [30], K-SVD with DLSR-OMP was run 20 iterations for training discriminative dictionary. After the training stage, the reconstruction errors, corresponding to different sub-dictionaries, were obtained for each patch. Based on (3), an image patch was assigned to a cluster once its sub-dictionary minimized the reconstruction error. Finally, following the postprocessing scheme of majority voting
                           6
                        
                        
                           6
                           For a pixel, all the patches containing this pixel vote for its class.
                         for pixel-wise classification, we applied the hierarchical method [29] to smooth and obtain the output segmented regions.

It is worth pointing out that we tuned the parameters to minimize the averaged segmenting errors over all 8 texture images of Table 5. Specifically, the possible values of L and η were traversed from 2 to 40 and from 1/40 to 1/2, respectively. The results have shown that when 
                           
                              L
                              =
                              15
                           
                         and 
                           
                              η
                              =
                              1
                              /
                              10
                              ,
                           
                         our DLSR-OMP algorithm of Table 2 achieved the minimal segmenting errors (i.e., only 3.09% pixels were wrongly segmented for all 8 test images). Thus, we here set L and η to be 15 and 1/10. For highlighting the discriminative ability of dictionary, a large value of β (
                           
                              
                                 =
                                 η
                                 ∥
                                 x
                                 −
                              
                              
                                 D
                                 
                                    I
                                    2
                                 
                              
                              
                                 w
                                 2
                              
                              
                                 
                                    ∥
                                 
                                 2
                                 2
                              
                           
                        ) should be set to impose high priority on minimization of ||f(w
                        1)||0 in (9). Here, we empirically increased η from 1/40 to 1/2 with the step being 1/40. We found out that when η was increased to 1/10, Case 1 existed for most coefficients, such that the sub-dictionaries of only few clusters were used to reconstruct the image patches. As we have shown in Fig. 1 that most image patches belong to one or two classes, this setting is suitable to the clustering task for the smallest segmenting error.

Then, our DLSR approach was compared with D2 approach
                           7
                        
                        
                           7
                           As reported by [14], D2 can achieve the best texture segmentation results among all approaches.
                         of [14], which is a state-of-the-art texture segmentation approach based on discriminative dictionary learning. Note that [14] is a supervised dictionary learning approach with other annotated texture images available for training, whereas our DLSR, as an unsupervised approach, only relies on the test images. However, unsupervised approaches cannot perform well on segmenting texture belonging to a large number of classes [22]. Thereby, 8 images, which are comprised by five or less texture classes, were chosen (texture images # 1–5 and 10–12 of [14]) as the test images. Table 5 presents the resulting segmentation error rates, referring to these images. Roughly speaking, the segmentation results of our approach, even in the unsupervised scenario, are better than those obtained by [14]. Such an improvement of our DLSR approach may be due to the more precise sparse representation of some patches related to more than one cluster, normally along with the edges of texture regions. Also, we show in Fig. 5 some of the visual segmentation results by our approach.

Moreover, we tabulate the quantitative segmentation results of our approach in Table 6
                        , compared with the state-of-the-art results of unsupervised texture segmentation reported in [22]. Note that [22] is also based on discriminative dictionary learning. Here, our test images were chosen from Fig. 5 of [22]. As seen in Table 6, our DLSR approach outperforms [22]. Probably, it is because that the sparsity of DLSR on the clusters can avoid the limitation of [22] that only the sub-dictionary of one cluster is updated, when the patches are related to more than one cluster. Fig. 5 shows the visual result of our DLSR approach in segmenting one texture image of [22]. In addition, the sub-dictionaries of two clusters, learned by our approach, are shown in Fig. 6
                        , and they reveal the various texture patterns of different classes.

In general, the above results show the advantage of our unsupervised DLSR approach in reducing the misclassified pixels of texture segmentation, over the state-of-the-art supervised and unsupervised approaches. This also verifies the enhanced discriminative capability of our DLSR approach.

@&#CONCLUSION@&#

In this paper, we have proposed double-layer sparse representation (DLSR) approach for unsupervised dictionary learning. The motivation of this paper is that an image patch can be represented by a small number of elements, from one or more sub-dictionaries, each of which is related to a specific cluster. Therefore, central to our approach in dictionary learning is the proposed DLSR formulation, imposing the sparsity on both coefficients and their corresponding clusters, for each image patch. Then, an efficient DLSR-OMP algorithm, based on OMP, is developed to solve the proposed DLSR formulation. By applying the DLSR-OMP algorithm at the sparse representation step, our approach permits to combine the reconstructive and discriminative abilities together in a unified dictionary, and enjoys the simplicity of sharing the same dictionary update step as other reconstructive dictionary learning approaches. Finally, we have shown that our approach slightly increases the denosing accuracy over the conventional K-SVD approach. Such an increase implies that the discriminative term introduced by our dictionary learning approach is able to assist in reconstructing an image, since the second layer sparsity on cluster numbers is added into the conventional K-SVD. Moreover, we have also validated that our approach is “good at” the clustering task through the experiments on texture segmentation.

@&#ACKNOWLEDGMENTS@&#

This work was supported by the NSFC projects under Grants 61573037, 61202139, and 61471022, and the China 973 program under Grant 2013CB329006.

@&#REFERENCES@&#

