@&#MAIN-TITLE@&#An iterated multi-stage selection hyper-heuristic

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We introduce a new general “multi-stage” hyper-heuristic framework.


                        
                        
                           
                           The framework enables the use of multiple hyper-heuristics at different stages.


                        
                        
                           
                           We propose a multi-stage hyper-heuristic based on the designed framework.


                        
                        
                           
                           The approach outperformed the state-of-the-art approach from CHeSC2011.


                        
                        
                           
                           The overall performance outperformed performance of each constituent hyper-heuristic.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Heuristics

Combinatorial optimisation

Hyper-heuristic

Meta-heuristic

Hybrid approach

@&#ABSTRACT@&#


               
               
                  There is a growing interest towards the design of reusable general purpose search methods that are applicable to different problems instead of tailored solutions to a single particular problem. Hyper-heuristics have emerged as such high level methods that explore the space formed by a set of heuristics (move operators) or heuristic components for solving computationally hard problems. A selection hyper-heuristic mixes and controls a predefined set of low level heuristics with the goal of improving an initially generated solution by choosing and applying an appropriate heuristic to a solution in hand and deciding whether to accept or reject the new solution at each step under an iterative framework. Designing an adaptive control mechanism for the heuristic selection and combining it with a suitable acceptance method is a major challenge, because both components can influence the overall performance of a selection hyper-heuristic. In this study, we describe a novel iterated multi-stage hyper-heuristic approach which cycles through two interacting hyper-heuristics and operates based on the principle that not all low level heuristics for a problem domain would be useful at any point of the search process. The empirical results on a hyper-heuristic benchmark indicate the success of the proposed selection hyper-heuristic across six problem domains beating the state-of-the-art approach.
               
            

@&#INTRODUCTION@&#

Most of the current decision support systems tend to use expert knowledge at their core, and are often custom tailored to a specific application domain. As a result, they cannot be reused for solving a problem from another domain. On the other hand, there has been some significant scientific progress in developing automated general purpose systems that can learn, adapt and improve their behaviour on the fly while solving a given problem. Hyper-heuristics are such methodologies which perform search over the space formed by a set of low level heuristics (move operators) which operate on solutions (Burke et al., 2013). An aim in hyper-heuristic research is to increase the level of generality of solution methodologies by selecting and/or generating heuristics automatically during the search process (Burke et al., 2010b). One of the key features of a hyper-heuristic is that there is a logical separation between the problem domain and the high level hyper-heuristic methodology that operates at the top level. This study focuses on selection hyper-heuristics which combine heuristic selection and move acceptance methods as their components under a single point iterative search framework which processes a single complete solution at each step (Burke et al., 2003; Burke, Kendall, Misir, & Özcan, 2012; Özcan, Bilgin, & Korkmaz, 2008). A selection hyper-heuristic chooses a heuristic from a predefined set of low level heuristics and applies it to a candidate solution. The new solution is then considered and a decision is made whether it will be accepted or not. If accepted, the new solution replaces the current solution and the search continues iteratively. There is a growing number of hyper-heuristics (Burke et al., 2013; Chakhlevitch & Cowling, 2008) and they have been successfully applied to different hard computational problems, including examination timetabling (Pillay & Banzhaf, 2009; Rahman et al., 2014), course timetabling (Burke, McCollum, Meisels, Petrovic, & Qu, 2007; Soria-Alcaraz et al., 2014), response time variability (García-Villoria, Salhi, Corominas, & Pastor, 2011) and water distribution problems (Kheiri, Keedwell, Gibson, & Savic, 2015).

It has been observed that a selection hyper-heuristic performs differently at different stages of the search process (Asta, Karapetyan, Kheiri, Özcan, & Parkes, 2013a; Kheiri, Özcan, & Parkes, in press). Moreover, there is a strong empirical evidence indicating that the choice of heuristic selection and move acceptance combination influences the overall performance of a hyper-heuristic (Bilgin, Özcan, & Korkmaz, 2007; Özcan et al., 2008). Lehre and Özcan (2013) conducted a theoretical study using a selection hyper-heuristic on a benchmark function showing that an improved run-time complexity can be obtained by mixing simple move acceptance criteria rather than using each move acceptance method on its own. In that study, random choice is used as a heuristic selection and the algorithmic framework could be viewed as a multi-stage framework in which two hyper-heuristics with different move acceptance is employed. This situation points out the potential of a modified single point-based search framework for selection hyper-heuristics enabling multi-stage operation of multiple hyper-heuristics. The design of multi-stage selection hyper-heuristics then would require another hyper-heuristic level for managing those multiple hyper-heuristics. One of the frameworks proposed in (Özcan, Bilgin, & Korkmaz, 2006) that handles mutational and hill climbing heuristics separately by invoking a mutational heuristic first followed by a hill climber, actually performs a multi-stage search. The higher level in this framework employs two prefixed hyper-heuristics in which each hyper-heuristic controls diversification (exploration of the search space) managing mutational heuristics and intensification (exploitation the accumulated search experience) managing hill climbing heuristics only. In another study, Özcan and Burke (2009) conceptualised a multilevel search framework for selection hyper-heuristics with three levels. The proposed method combines multiple hyper-heuristics, each performing search over a set of low level heuristics. Considering the recursive nature of the hyper-heuristic definition of ‘heuristics to choose heuristics’ by Cowling, Kendall, and Soubeiga (2001), multiple hyper-heuristics managing low level hyper-heuristics is also possible requiring another level and so on, causing a hierarchical growth in the hyper-heuristic levels like a tree. The usefulness of such a structure is still under debate. An initial attempt to flatten the hierarchical growth in the hyper-heuristic levels was made by Özcan, Misir, and Kheiri (2013), assuming a particular situation in which there are multiple move acceptance methods for use. Instead of employing each move acceptance method individually or utilising them all in a multi-stage manner, the authors proposed an approach merging all move acceptance methods into a single method via a group decision making strategy.

To the best of our knowledge, the effectiveness of a general multilevel search framework which combines multiple selection hyper-heuristics based on the ideas in Özcan and Burke (2009) has not been investigated further. This work describes an iterated multilevel search framework which allows the use of multiple interacting hyper-heuristics cyclically during the search process. Given that one of the selection hyper-heuristics would be employed at each stage during the search process, we will refer to the overall approach as multi-stage (selection) hyper-heuristic. The additional level on top of the multiple selection hyper-heuristics will be referred to as multi-stage level. The proposed multi-stage hyper-heuristic framework is general, reusable and useful in relieving the difficulty of choosing a hyper-heuristic method for solving a problem, by automating the process of selecting a hyper-heuristic at different point of the search process.

There are already some software tools supporting the rapid development of (meta-)hyper-heuristics, such as HyFlex (Hyper-heuristics Flexible framework) (Ochoa et al., 2012a) and Hyperion (Swan, Özcan, & Kendall, 2011). The Java implementation of the HyFlex interface was used in a cross-domain heuristic search challenge, referred to as CHeSC 2011. The results form this competition and six HyFlex problem domain implementations became a benchmark in selection hyper-heuristic studies (see Section 4 for more details). The winning state-of-the-art approach of CHeSC 2011 is an elaborate but complicated algorithm which makes use of machine learning techniques (Misir, Verbeeck, De Causmaecker, & Vanden Berghe, 2011). In this study, a novel iterated multi-stage selection hyper-heuristic based on the proposed framework is designed, implemented and analysed. The empirical results using the CHeSC 2011 benchmark across six problem domains indicate the success of our hyper-heuristic beating the state-of-the-art approach.

The sections are organised as follows. Section 2 covers some selection hyper-heuristics relevant to this study and introduces the concept of multi-stage selection hyper-heuristics summarising some recent studies. Section 3 describes the components of the proposed multi-stage selection hyper-heuristic framework. An overview of HyFlex and CHeSC 2011 is provided in Section 4. Section 5 presents the empirical results. Finally, Section 6 concludes the study.

@&#RELATED WORK@&#

A variety of simple selection hyper-heuristic components were presented in Cowling et al. (2001), including the following heuristic selection methods. Simple Random chooses a random heuristic at each time. Random Gradient selects a random heuristic and then applies it to the candidate solution as long as the solution is improved. Random Permutation applies low level heuristics in sequence based on a random permutation. Random Permutation Gradient combines the Random Permutation and Random Gradient strategies applying a heuristic from the sequence in a random permutation until that heuristic makes no improvement. Greedy chooses the best solution after applying all actively used low level heuristics to the current solution. Choice Function is a learning heuristic selection method which gives a score to each low level heuristic based on their utility value. Cowling and Chakhlevitch (2003) suggested the use of a Tabu search based hyper-heuristic that provides a list that disallows heuristics with poor performance.

The idea of applying different hyper-heuristics at different stages has been studied previously. Hyper-heuristics often employ a learning mechanism (Burke et al., 2013). Online learning hyper-heuristics receive feedback during the search process potentially influencing their decision in heuristic selection and move acceptance. Offline learning hyper-heuristics generally operate in a train and test fashion in two fixed successive stages. Mostly, latter type of hyper-heuristics are used to generate heuristics (Burke et al., 2009). There are studies on offline learning selection hyper-heuristics as well. For instance, Chakhlevitch and Cowling (2005) proposed a two-stage hyper-heuristic using a different hyper-heuristic at each stage. The first stage hyper-heuristic employs a Greedy approach which is used to reduce the number of low level heuristics. In the following stage, a simple random hyper-heuristic accepting non-worsening moves is used. The authors reported that using both greedy and Tabu search in combination with the aim of linearly reducing the number of the best performing low level heuristics, is promising.

The studies particularly on iterated multi-stage selection hyper-heuristics which enable the cyclic use of multiple selection hyper-heuristics in a staged manner are limited. Asta and Özcan (2015) used a data science technique to partition the low level heuristics which perform well together under a hyper-heuristic using a certain move acceptance method. Then each partition is associated with a relevant move acceptance method. Hence learning takes place during this phase. In the following phase, the approach employs an iterated multi-stage search with two hyper-heuristics, each combining the simple random heuristic selection and a different move acceptance method with the associated low level heuristics. There is no learning mechanism used during this phase.


                     Kalender, Kheiri, Özcan, and Burke (2012) 
                     2013) applied an iterated online learning multi-stage greedy gradient (GGHH) hyper-heuristic to the curriculum-based university course timetabling and high-school timetabling problems. This approach performs search iteratively going through two successive stages, which contain Greedy and Reinforcement Learning based hyper-heuristics, both using simulated annealing move acceptance. Both hyper-heuristics embed an online learning mechanism. Reinforcement Learning oriented hyper-heuristic uses the cost change as the score for each low level heuristic choosing the one with the highest score at each step. Since Greedy stage is costly, this stage gets invoked dynamically if all heuristics start generating poor performance.

An iterated multi-stage hyper-heuristic which combines a Dominance-based heuristic selection method and a Random Descent hyper-heuristic with Naïve Move Acceptance (DRD) is proposed in (Özcan & Kheiri, 2012). The dominance-based selection method employs a greedy approach aiming to determine an active subset of low level heuristics taking into account the trade-off between the change in the objective value and the number of steps taken to achieve that result. The second stage applies random descent hyper-heuristic using that active subset of low level heuristics from the first stage to improve the quality of a solution in hand. If the second stage hyper-heuristic stagnates, then the first stage restarts with a given probability for detecting a new active subset of low level heuristics. This hyper-heuristic is tested using HyFlex producing the best performance when compared to the ‘default’ hyper-heuristics provided with HyFlex.

An iterated multi-stage hyper-heuristic known as Robinhood hyper-heuristic (RHH) combining three hyper-heuristics is proposed in (Kheiri & Özcan, 2013). The three hyper-heuristics use the same heuristic selection method but they differ in the move acceptance component. The selection heuristic component employs round-robin strategy-based neighbourhood method and applies the mutational and ruin and re-create heuristics on the candidate solution, then crossover heuristics, and then hill climbing heuristics and assigns equal time for each low level heuristic. Three move acceptance criteria (one per each hyper-heuristic) including only improving, improving or equal, and an adaptive acceptance methods are used in this approach. In the adaptive acceptance method, a move that improves the quality of the current solution is always accepted. Deteriorating moves are accepted according to a probability that is adaptively modified at different stages throughout the search. Each hyper-heuristic in RHH is applied for a fixed duration of time and therefore The transition between the three stages is static. This approach outperforms the ‘default’ hyper-heuristics of HyFlex, and took the fourth place with respect to the twenty approaches from CHeSC 2011.

An iterated multi-stage hyper-heuristic, referred to as Hyper-heuristic Search Strategies and Timetabling (HySST) with two stages, each using a different hyper-heuristic is designed to solve a variety of high school timetabling problems from different countries (Kheiri et al., in press). The Simple Random combined with Adaptive Threshold move acceptance and Simple Random combined with Accept All Moves hyper-heuristics are utilised only with mutational and hill-climbing operators, respectively. The transition between two stages of HySST is deterministic. The following stage starts if the input solution cannot be improved at all in a given stage after a certain duration. This solver is tested on a set of real-world instances and competed at the 3 rounds of the third International Timetabling Competition (ITC 2011).
                        1
                     
                     
                        1
                        ITC2011 website: http://www.utwente.nl/ctit/hstt/.
                      In round 1, HySST generated the best solutions for three instances, and took the second place in rounds 2 and 3 (Kheiri et al., in press).


                     Asta et al. (2013a) proposed another iterated multi-stage hyper-heuristic combining two hyper-heuristics, namely; Dominance-based hyper-heuristic and Roulette Wheel selection with Adaptive Threshold move acceptance (DRW). The dominance-based hyper-heuristic reduces the set of low level heuristics using a greedy-like approach in a given stage. This learning approach considers the trade-off between number of steps vs. achieved solution quality, discovering the most useful low level heuristics in improvement and at the same time generates their selection probabilities. Then the other hyper-heuristic gets invoked using the reduced set of low level heuristics and associated selection probabilities to improve a given solution at a stage. Similar to HySST, the transition between stages in DRW is deterministic. This multi-stage hyper-heuristic is used to improve pool of solutions as a local search. The approach won the MISTA 2013 challenge
                        2
                     
                     
                        2
                        MISTA2013 challenge website: http://allserv.kahosl.be/mista2013challenge/.
                      at which the purpose was to solve a multi-mode resource-constrained multi-project scheduling problem (MRCMPSP).

In this study, our approach extends the previous multi-stage hyper-heuristics and makes use of the relay hybridisation (Kheiri & Keedwell, 2015; Misir et al., 2011) technique which applies a low level heuristic to a solution generated by applying a preceding heuristic. We propose an online learning iterated multi-stage hyper-heuristic which is evaluated on six HyFlex problem domains. Its performance is compared to the other multi-stage hyper-heuristics from the scientific literature and also to the competing hyper-heuristics from CHeSC 2011.

@&#METHODOLOGY@&#

The traditional single-stage selection hyper-heuristic framework employs a single heuristic selection and a single move acceptance method. If a different hyper-heuristic component is used during the search process, this constitutes a different stage enabling the design of multi-stage hyper-heuristics as illustrated in Fig. 1
                        . Allowing the use of multiple hyper-heuristic components interchangeably under a multi-stage framework opens up richer design options, such as the possibility of having several hyper-heuristics controlling different sets of low level heuristics cooperatively. As discussed in Section 2, there is empirical evidence that multi-stage hyper-heuristics has the potential to deliver better performance than the traditional (meta)hyper-heuristics (Asta & Özcan, 2015; Kalender et al., 2012, 2013; Özcan & Kheiri, 2012) (Asta et al., 2013a; Kheiri et al., in press). A multi-stage framework requires inclusion of an additional upper level which will be referred to as multi-stage level within the selection hyper-heuristic framework as shown in Fig. 1. The multi-stage level allows the transition between available hyper-heuristics and their automated control at different points during the search process. Algorithm 1
                         provides the pseudocode of a multi-stage hyper-heuristic algorithm based on the framework in Fig. 1.

This study introduces an iterated multi-stage selection hyper-heuristic utilising two interacting hyper-heuristics cyclically and controlled by the multi-stage level as provided in Algorithms 2
                        –4
                        
                        . In one stage, a subset of “useful” low level heuristics, each associated with a score is determined by a hyper-heuristic embedding a greedy heuristic selection method (Algorithm 2, lines 15–28). Only that subset of low level heuristics is then used in the other stage (Algorithm 2, lines 9–11) and at each step, a heuristic is selected using a roulette wheel strategy based on those scores. As a move acceptance component of the multi-stage hyper-heuristic, a threshold move acceptance method is used (Asta et al., 2013a; Kheiri et al., in press) in both stages (Algorithm 3, line 13 and Algorithm 4, line 10), however the threshold values are treated in a different way in each stage as explained in this section.

In this study, we assume that the number of low level heuristics for a given problem domain is already provided. We form “new” heuristics by pairing up each low level heuristic and invoking them successively. The technique of combining two heuristics is known as relay hybridisation which applies the second low level heuristic to the solution generated by the preceding low level heuristic. The motivation behind relay hybridisation is that although a low level heuristic that does not generate any improvement can still be useful when employed in combination with another low level heuristic. There is indeed empirical evidence that this technique is useful when embedded into a selection hyper-heuristic, considering that relay hybridisation is part of the winning selection hyper-heuristic of CHeSC 2011 (Misir et al., 2011). The proposed selection hyper-heuristic method employing relay hybridisation ignores the nature of the low level heuristics and does not make explicit use of the heuristic type information. Consequently, given n low level heuristics ({LLH
                        1
                        
                           
                              ,
                              …
                              ,
                           
                         
                        LLHn
                        }), we end up with (
                           
                              n
                              +
                              
                                 n
                                 2
                              
                           
                        ) low level heuristics in the overall ({
                           
                              L
                              L
                              
                                 H
                                 1
                              
                              ,
                              …
                              ,
                              L
                              L
                              
                                 H
                                 n
                              
                              ,
                              L
                              L
                              
                                 H
                                 1
                              
                              +
                              L
                              L
                              
                                 H
                                 1
                              
                              ,
                              L
                              L
                              
                                 H
                                 1
                              
                              +
                              L
                              L
                              
                                 H
                                 2
                              
                              ,
                              l
                              d
                              o
                              t
                              s
                              ,
                              L
                              L
                              
                                 H
                                 n
                              
                              +
                              L
                              L
                              
                                 H
                                 n
                              
                           
                        }), where 
                           
                              L
                              L
                              
                                 H
                                 i
                              
                              +
                              L
                              L
                              
                                 H
                                 j
                              
                           
                         denotes the combined heuristic of the pair LLHi
                         and LLHj
                        . Iterated Local Search (ILS) is an iterative metaheuristic seeking an improved (optimal) solution through two main steps of perturbation and local search (Lourenço, Martin, & Stützle, 2010). ILS explicitly balances diversification/exploration (capability of jumping to other promising regions of the search space) and intensification/exploitation (capability of performing a thorough search within a restricted promising region) employing those steps, respectively. The relay hybridisation technique has the potential to combine a mutational and a hill climber as a new low level heuristics and if chosen, such a low level heuristic would yield an ILS-like behaviour in an iteration.

Following the previous work in (Kheiri & Özcan, 2013), any selected low level heuristic is executed for a certain duration, τ (Algorithm 3, lines 9–21 and Algorithm 4, lines 6–12).

Combining multiple hyper-heuristics always requires a decision on when to switch between selection hyper-heuristics. The proposed method handles this decision adaptively. If there is no improvement in consecutive stages while the first hyper-heuristic (denoted as S1HH) is used then the second hyper-heuristic (denoted as S2HH) may kick in with a pre-defined transition probability (P
                        
                           S2HH
                        ), otherwise, with a probability of (1-P
                        
                           S2HH
                        ), S1HH is re-invoked again. The transition from S2HH to S1HH is not probabilistic, meaning that S1HH is invoked for certain after applying S2HH. This is explained in more details in the following subsections.

In stage one (Algorithm 3), the roulette wheel selection based hyper-heuristic chooses and applies randomly a low level heuristic based on a score associated with each low level heuristic (Algorithm 3, lines 3 and 10). Assuming that the ith low level heuristic LLHi
                            has a score of scorei
                           , then the probability of that heuristic being selected is scorei
                           /∑∀k
                           (scorek
                           ). Initially, all single heuristics are assigned a score of 1, while the rest of the paired heuristics are assigned a score of 0. The stage one hyper-heuristic always maintains the best solution found during search process, denoted as Sbeststage
                            and keeps track of the time since the last improvement. The move acceptance approach directly accepts improving moves, while non-improving moves are accepted if the objective value of the candidate solution is better than 
                              
                                 (
                                 1
                                 +
                                 ϵ
                                 )
                              
                            of the objective value of the best solution obtained in the relevant stage. Whenever the best solution during a stage can no longer be improved for a duration of d, ϵ gets updated according to Eq. (1).

                              
                                 (1)
                                 
                                    
                                       ϵ
                                       =
                                       
                                          
                                             
                                                ⌊
                                                log
                                                
                                                   (
                                                   f
                                                   
                                                      (
                                                      
                                                         S
                                                         
                                                            b
                                                            e
                                                            s
                                                            t
                                                            s
                                                            t
                                                            a
                                                            g
                                                            e
                                                         
                                                      
                                                      )
                                                   
                                                   )
                                                
                                                ⌋
                                             
                                             +
                                             
                                                c
                                                i
                                             
                                          
                                          
                                             f
                                             (
                                             
                                                S
                                                
                                                   b
                                                   e
                                                   s
                                                   t
                                                   s
                                                   t
                                                   a
                                                   g
                                                   e
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           where f(Sbeststage
                           ) is the objective value of the best solution obtained during the stage and ci
                            is an integer value in C={
                              
                                 
                                    c
                                    0
                                 
                                 ,
                                 …
                                 ,
                                 
                                    c
                                    i
                                 
                                 ,
                                 …
                                 ,
                                 
                                    c
                                    
                                       (
                                       k
                                       −
                                       1
                                       )
                                    
                                 
                              
                           }, where 
                              
                                 
                                    c
                                    
                                       (
                                       i
                                       −
                                       1
                                       )
                                    
                                 
                                 <
                                 
                                    c
                                    i
                                 
                              
                            for 0 < i < k and 
                              
                                 k
                                 =
                                 |
                                 C
                                 |
                              
                           . If f(Sbeststage
                           ) is less than 1, ϵ takes a small value ∼ 0.

The value of ci
                            never changes in this stage but it might get updated in stage two as explained in the following section. In the first execution of stage one, c
                           0 is used by default.

If the overall given time limit (timeLimit) is exceeded, or there is no improvement in the quality of the best solution obtained during the stage for a duration of s
                           1, then the stage one hyper-heuristic terminates.

The aim of this stage (Algorithm 4) is to reduce the set of low level heuristics and adjust their scores according to their “performance” using the idea of the dominance-based heuristic selection (Asta et al., 2013a; Özcan & Kheiri, 2012). A score of 0 indicates that the corresponding heuristic will not be used in the following stage. The reduced set of low level heuristics along with the associated score are fed into the stage one hyper-heuristic.

Firstly, ϵ is set using Eq. (1) for once at the start of this stage. Having a sorted circular list of values C={
                              
                                 
                                    c
                                    0
                                 
                                 ,
                                 …
                                 
                                 ,
                                 
                                    c
                                    i
                                 
                                 ,
                                 …
                                 
                                 ,
                                 
                                    c
                                    
                                       (
                                       k
                                       −
                                       1
                                       )
                                    
                                 
                              
                           } enables adaptive control of the level of diversification and gives flexibility of relaxing the threshold further whenever necessary allowing larger worsening moves. Initially, ci
                            takes the value of c
                           0. If the best solution obtained after applying stage one does not improve for a stage and stage two hyper-heuristic is applied, the parameter takes the next value on the list, that is, for example, c
                           1, allowing a larger worsening move to be accepted, and so on. If stage one hyper-heuristic manages to improve the solution and then stage two hyper-heuristic is applied, the parameter is reset to c
                           0. By default, Sbeststage
                            is fed as an input to the next stage. There might be a case when even the 
                              
                                 (
                                 
                                    c
                                    
                                       (
                                       k
                                       −
                                       1
                                       )
                                    
                                 
                                 )
                              
                            value is not sufficient to escape from a local optimum and the current (candidate) solution is worse than Sbeststage
                           . If the second stage gets executed at this point of the search process, then the current solution is fed into the next stage as input to allow further diversification. It is possible for a given problem domain that Eq. (1) could return a value of 0, then ci
                            is assigned to one of the values in C at random. After ci
                            is updated, it does not get changed during the execution of the remaining steps of this stage.

A greedy hyper-heuristic is applied using LLHall
                            (Algorithm 4, lines 4–14) for a fixed number of steps s
                           2. At each step, all the objective values obtained by applying all the low level heuristics are recorded only if they generate solutions different in quality to the input solution. If all heuristics cannot generate a new solution, then they considered all to have the worst possible objective value. The greedy approach takes the best generated solution obtained at a step and feeds it as an input solution to the next step.

At the end of the stage, the non-dominated solutions each associated with the low level heuristic that generated it are determined from the archive (Algorithm 2, line 27). Then the score of each “non-dominated” low level heuristic is increased by 1. It is potentially possible that a low level heuristic could produce a non-dominated solution more than once and so get a higher score indicating the frequency of such success. In the case of a tie where multiple low level heuristics produce the same non-dominated solution for a given step, their scores are all incremented.

Due to the costly run time that the second stage hyper-heuristic introduces, taking 
                              
                                 
                                    s
                                    2
                                 
                                 ×
                                 
                                    (
                                    n
                                    +
                                    
                                       n
                                       2
                                    
                                    )
                                 
                              
                            steps, a relatively low value for s
                           2 is preferred. Additionally, we have introduced a probability parameter (P
                           
                              S2HH
                           ) (Algorithm 2, line 15) in order to limit the use of this stage often. The stage terminates if s
                           2 steps are fully executed or overall given time limit (timeLimit) is exceeded (Algorithm 2, line 23).


                           Fig. 2
                            provides an example of how stage two hyper-heuristic executes with 
                              
                                 n
                                 =
                                 2
                                 ,
                              
                            i.e., combining two heuristics {LLH
                           1, LLH
                           2} via relay hybridisation yielding a set of six low level heuristics, LLHall
                            = {LLH
                           1, LLH
                           2, LLH
                           3 = 
                              
                                 L
                                 L
                                 
                                    H
                                    1
                                 
                                 +
                                 L
                                 L
                                 
                                    H
                                    1
                                 
                                 ,
                              
                            
                           LLH
                           4 = 
                              
                                 L
                                 L
                                 
                                    H
                                    1
                                 
                                 +
                                 L
                                 L
                                 
                                    H
                                    2
                                 
                                 ,
                              
                            
                           LLH
                           5 = 
                              
                                 L
                                 L
                                 
                                    H
                                    2
                                 
                                 +
                                 L
                                 L
                                 
                                    H
                                    1
                                 
                                 ,
                              
                            
                           LLH
                           6 = 
                              
                                 L
                                 L
                                 
                                    H
                                    2
                                 
                                 +
                                 L
                                 L
                                 
                                    H
                                    2
                                 
                              
                           } and where 
                              
                                 
                                    s
                                    2
                                 
                                 =
                                 4
                              
                           . After running all low level heuristics in a greedy fashion for 4 steps, the number of low level heuristics is automatically reduced to three using the Pareto front which considers the trade-off between number of steps a heuristics executes and improvement in the quality of the resultant solution. Also, the selection probability of each heuristic for the use in the next stage is determined based on the information derived from the Pareto front. The low level heuristics on the Pareto front are {LLH
                           1, LLH
                           2}, {LLH
                           1} and {LLH
                           3}. Hence, the scores of the fourth, fifth and sixth low level heuristics are zeros; and scores of LLH
                           1, LLH
                           2 and LLH
                           3 are assigned to 2, 1 and 1, respectively. Given that the probability of a heuristic in S1HH being selected is scorei
                           /∑∀k
                           (scorek
                           ), therefore, in this example the probability of selecting LLH
                           1 becomes 50 percent, while it is 25 percent for LLH
                           2 and LLH
                           3. This means that LLH
                           4, LLH
                           5 and LLH
                           6 are not expected to perform well and hence disabled. LLH
                           1 is expected to perform better than LLH
                           2 and LLH
                           3 and hence its selection probability is higher than LLH
                           2 and LLH
                           3.

HyFlex
                        3
                     
                     
                        3
                        
                           http://www.hyflex.org/.
                      is an interface which supports the selection hyper-heuristic development. This interface including six problem domains was implemented in Java as version v1.0 for a competition, referred to as Cross-Domain Heuristic Search Challenge, CHeSC 2011.
                        4
                     
                     
                        4
                        CHeSC website: http://www.asap.cs.nott.ac.uk/chesc2011/.
                      The aim of CHeSC 2011 was to determine the state-of-the-art hyper-heuristic that generalises well across a set of problem instances from six different problem domains. Throughout this paper, “HyFlex” refers to that version. There is a new version of HyFlex which is version v1.1 supporting the batch mode operation of hyper-heuristics (Asta, Özcan, & Parkes, 2013b), which is not within the scope of this study. HyFlex currently provides implementation of six minimisation problem domains: boolean satisfiability (SAT), one-dimensional bin-packing (BP), personnel scheduling (PS), permutation flow-shop (PFS), travelling salesman problem (TSP) and vehicle routing problem (VRP). The software package includes a set of low level heuristics (LLHs) and a number of instances associated with each domain. The code for SAT, BP, PS and PFS was released first along with some public instances, while the code and instances for TSP and VRP were released after the competition.

In HyFlex, the low level heuristics are perturbative heuristics processing and returning complete solutions after their application. Heuristics are categorised as mutational (MU) which modifies a solution in some way with no guarantee of improvement, ruin and re-create heuristic (RR) which destructs a given complete solution generating a partial solution and then reconstructs a complete solution, hill climbing (HC) which performs local search returning a solution which has the same or better quality of the input solution, and crossover (XO) which creates a new solution by combining some parts from two given solutions. HyFlex provides functionality for controlling the intensity of the mutation and ruin and re-create operators, as well as, the depth of the search in local search operators. The setting of a parameter is allowed to range from 0.0 to 1.0. Table 1
                      provides an overview of the low level heuristics from each heuristic category for each HyFlex domain. A low level heuristic from a domain in HyFlex is given a unique ID. For example, low level heuristics from LLH
                     0 to LLH
                     5 in the SAT domain are all mutational heuristics.

Before CHeSC 2011, the results of eight mock hyper-heuristics from literature over four of the HyFlex problem domains were put on the competition website. The description of the mock hyper-heuristics were not provided on the competition website, but it is reported in Burke et al. (2010a) that the iterated local search which applies a sequence of heuristics in a predefined order has the best performance.

The ranking method used at CHeSC 2011 is inspired from the Formula 1 points scoring system. The top eight hyper-heuristics are determined after comparison of the median objective values that all hyper-heuristics achieve over 31 trials for each instance. Each algorithm is then awarded a score according to its ranking. The winner receives 10 points, the runner up gets 8 and then 6, 5, 4, 3, 2 and 1, respectively. In the case of a tie for a given instance, the corresponding points are added together and shared equally between each algorithm. In CHeSC 2011, five instances from each domain is used. The winner is the one which scores the maximum points over the thirty instances across all six problem domains.

The results of twenty participants in the competition along with the description of their algorithms were available from the website of the competition. Those results were based on five instances of all HyFlex problem domains. The winner, Mustafa Misir, developed an approach, denoted as AdapHH, which is a solver that applies an adaptive heuristic selection combined with adaptive iteration limited list-based threshold move accepting method (Misir et al., 2011). The second place was taken by a hyper-heuristic based on Variable Neighbourhood Search (VNS-TW) which applies shaking heuristics then hill-climber heuristics (Hsiao, Chiang, & Fu, 2012).

There has been a growing number of studies on selection hyper-heuristics which are evaluated on HyFlex problem domains since CHeSC 2011. We provide a brief overview of some of those single stage generic selection hyper-heuristics here. An improved choice function hyper-heuristics is proposed in Drake, Özcan, and Burke (2012) showing that this approach is more successful than the traditional choice function heuristic selection. Drake, Özcan, and Burke (2015) extended this previous study introducing crossover operators into the set of low level heuristics and a mechanism to control the input those binary operators, which slightly improved the overall performance of the hyper-heuristic. An adaptive iterated local search approach is proposed and applied on HyFlex problem domains in Burke, Gendreau, Ochoa, and Walker (2011) 
                     Ochoa, Walker, Hyde, and Curtois (2012b). Jackson, Özcan, and Drake (2013) evaluated variants of late acceptance-based selection hyper-heuristics. The authors point out the best configuration for the late acceptance strategy which accepts a solution if its quality is better than the quality of a solution obtained from a certain number of prior steps. None of those previously proposed selection hyper-heuristics perform better than AdapHH.

@&#EXPERIMENTAL RESULTS@&#

We have evaluated the performance of the proposed dominance-based and relay hybridisation multi-stage hyper-heuristic, denoted as MSHH, across six problem domains of HyFlex. During our experimentation, crossover operators are ignored as low level heuristics, considering that the multi-stage hyper-heuristics operate under a single point based search framework. The Mann–Whitney–Wilcoxon test (Fagerland & Sandvik, 2009; Kruskal, 1957) is used as a statistical test for pairwise average performance of two given algorithms. We have used the following notation: Given two algorithms; A vs. B, > ( < ) denotes that A (B) is better than B (A) and this performance difference is statistically significant within a confidence interval of 95 percent and A ≥ B (A ≤ B) indicates that A (B) performs better on average than B (A) but no statistical significance.

In CHeSC 2011, the competing algorithms are run for 31 trials. Therefore, each experiment is repeated for 31 times unless it is mentioned otherwise. A benchmarking software tool provided at the CHeSC 2011 website is used to obtain the equivalent time value (timeLimit) on the used machines that correspond to 600 nominal seconds according to the competition rule.

We have fixed the parameter values based on our previous work (Asta et al., 2013a; Kheiri & Özcan, 2013; Kheiri et al., in press; Özcan & Kheiri, 2012): 
                        
                           τ
                           =
                           15
                           
                           millisecond
                        
                     s, 
                        
                           d
                           =
                           9
                           
                           second
                        
                     s, 
                        
                           
                              s
                              1
                           
                           =
                           20
                           
                           second
                        
                     s, 
                        
                           
                              s
                              2
                           
                           =
                           5
                           ,
                        
                     
                     
                        
                           
                              P
                              
                                 S
                                 2
                                 H
                                 H
                              
                           
                           =
                           0.3
                           ,
                        
                     
                     
                        
                           C
                           =
                           {
                           0
                           ,
                           3
                           ,
                           6
                           ,
                           9
                           }
                        
                     . The experiments are performed using those settings as “regular” settings on all thirty instances from all domains used at CHeSC 2011. We compare the performance of our approach to each individual hyper-heuristic used in a stage, previously proposed multi-stage hyper-heuristics and competing hyper-heuristics of CHeSC 2011 including the state-of-the-art hyper-heuristic which won the competition, respectively.

A set of experiments is performed on four arbitrarily chosen (first) instances of four public problem domains to observe the performance of the proposed algorithm under different parameter settings:

                           
                              •
                              
                                 
                                    
                                       τ
                                       =
                                       {
                                       10
                                       ,
                                       15
                                       ,
                                       20
                                       ,
                                       30
                                       }
                                    
                                  (in milliseconds)


                                 
                                    
                                       d
                                       =
                                       {
                                       7
                                       ,
                                       9
                                       ,
                                       10
                                       ,
                                       12
                                       }
                                    
                                  (in seconds)


                                 
                                    
                                       
                                          s
                                          1
                                       
                                       =
                                       
                                          {
                                          10
                                          ,
                                          15
                                          ,
                                          20
                                          ,
                                          25
                                          }
                                       
                                    
                                  (in seconds)


                                 
                                    
                                       
                                          s
                                          2
                                       
                                       =
                                       
                                          {
                                          3
                                          ,
                                          5
                                          ,
                                          10
                                          ,
                                          15
                                          }
                                       
                                    
                                  (in steps/iterations)


                                 
                                    
                                       
                                          P
                                          
                                             S
                                             2
                                             H
                                             H
                                          
                                       
                                       =
                                       
                                          {
                                          0.1
                                          ,
                                          0
                                          .
                                          3
                                          ,
                                          0.6
                                          ,
                                          0.9
                                          ,
                                          1.0
                                          }
                                       
                                    
                                 
                              


                                 
                                    
                                       C
                                       =
                                       {
                                       {
                                       0
                                       }
                                       ,
                                       {
                                       3
                                       }
                                       ,
                                       {
                                       6
                                       }
                                       ,
                                       {
                                       9
                                       }
                                       ,
                                       {
                                       0
                                       ,
                                       3
                                       ,
                                       6
                                       ,
                                       9
                                       }
                                       }
                                    
                                 
                              

While testing a different setting for a given parameter, the remaining parameters are fixed with the values marked in bold which are our initial settings. MSHH is run with each setting for 10 trials on the selected instance from each public domain. Table 2
                         summarises the results based on the average performance of MSHH with various parameter settings when a given parameter value deviates from its regular setting. In all cases, MSHH with the “regular” parameter setting wins against another setting, however, mostly, this performance difference is not statistically significant. There are a few cases for which MSHH with a setting other than the proposed one yields a slightly better average performance on the BP and PS instances. For example, 
                           
                              τ
                              =
                              10
                           
                         performs slightly better than 
                           
                              τ
                              =
                              15
                           
                         on the BP instance, and 
                           
                              
                                 P
                                 
                                    S
                                    2
                                    H
                                    H
                                 
                              
                              =
                              0.6
                           
                         is a slightly better choice than 
                           
                              
                                 P
                                 
                                    S
                                    2
                                    H
                                    H
                                 
                              
                              =
                              0.3
                           
                         for the PS instance. MSHH with the “regular” parameter setting always performs better than another setting on the PFS and SAT instances. The overall performance of a setting across all domains is important in cross domain search. Hence, MSHH with the “regular” parameter setting turns out to be indeed a good choice and so the same setting is used during the remaining experiments.

Recall that each low level heuristic in HyFlex has a parameter (intensity or depth of the search) that can take any value in the range from 0.0 to 1.0. Initially, we assigned the parameter value of 0.0 to each low level heuristic. The relevant parameter setting of a selected low level heuristic gets updated to a random value in case the move does not improve the candidate solution, otherwise the same setting is kept.

We have experimented with the hyper-heuristics used at each stage, denoted as S1HH and S2HH, respectively, run on their own and compare their performances to the performance of the proposed multi-stage hyper-heuristic. Tables 3
                         presents the results. MSHH obtains the best solution in 31 trials for 27 out of 30 of the CHeSC 2011 instances, which include all instances from the SAT, BP and TSP domains and exclude one instance from the remaining domains. On average, MSHH still performs better than the constituent hyper-heuristics of S1HH and S2HH run on their own on the 22 instances across all six problem domains. The standard deviation associated with the average objective value from MSHH is the lowest in all cases on the SAT and TSP problem domains.

On average, MSHH outperforms S2HH and this performance is statistically significant for all instances, except for Inst2, Inst3 and Inst4 from the PS domain and Inst3 from the VRP domain. On the SAT and TSP domains, MSHH performs still significantly better than S1HH on all instances. On PS, MSHH is better than S1HH in four instances, but this performance variation is significant for two out of the four instances. MSHH performs slightly better than S1HH on the BP, PFS and VRP domains. However, S1HH performs better than MSHH only on two instances, Inst1 from BP and Inst2 from PFS for which the performance difference is statistically significant.

Our study empirically confirms that combining hyper-heuristics under a multi-stage framework can potentially lead to an improved overall performance.

The performance of the proposed dominance-based and relay hybridisation multi-stage hyper-heuristic is compared to the performance of some previously proposed elaborate and successful multi-stage hyper-heuristics which are described in Section 2: greedy, random gradient and simulated annealing hyper-heuristic (also known as greedy-gradient) (GGHH) (Kalender et al., 2012), dominance-based and random descent hyper-heuristic (DRD) (Özcan & Kheiri, 2012), Robinhood selection hyper-heuristic (RHH) (Kheiri & Özcan, 2013), hyper-heuristic search strategies and timetabling approach (HySST) (Kheiri et al., in press), dominance-based and roulette wheel hyper-heuristic (DRW) (Asta et al., 2013a). Table 4
                         presents the results achieved after the application of all those multi-stage hyper-heuristics to the CHeSC 2011 domains under the same setting.

In the overall, MSHH turns out to be a viable general methodology outperforming the other multi-stage hyper-heuristic approaches in most of the HyFlex problem domains. The MSHH consistently performs the best in SAT, BP and TSP problem domains based on the average and minimum objective values obtained over 31 runs for each instance. Only for Inst1 from BP, DRD performs better in terms of average and minimum objective values. MSHH achieves the best average results on three instances on the PS and PFS problem domains. MSHH performs the best on average only on the Inst1 VRP instance, while RHH and GGHH perform better on three instances and one VRP instance, respectively. This appears to be an indication that application of all low level heuristics and performing local search and accepting solutions which is the best at any given time is potentially a better approach on the VRP domain. DRD performs the worst on the SAT problem domain, but delivers a good average performance on the BP problem domain. GGHH and DRW manage to provide the best average results on a single instance of VRP and PFS, respectively. MSHH is better than HySST on all problem instances across all domains and this performance difference is statistically significant.

MSHH and the twenty competing hyper-heuristics from CHeSC 2011 are ranked under the same criteria used at the time of the competition. Table 5 presents the scores for each algorithm based on the Formula 1 scoring system with respect to the median objective values obtained during the 31 trials over all instances across the six domains. Although MSHH delivers a relatively poor “median” performance in the PS and VRP problem domains, the overall results reveal that MSHH is the winner with a total score of 163.60.

Another performance evaluation method was suggested by Di Gaspero and Urli (2012) to illustrate the relative performance variation of each hyper-heuristic in a given bunch. Considering a set of hyper-heuristics, denoted as P and the resultant solutions associated with their objective values obtained from running a hyper-heuristic, denoted as j ∈ P for 31 trials on a given problem instance i, the median objective value, denoted as medj
                        (i) is normalised to a value, Nmedj
                        (i) in [0,1] using Eq. (2):

                           
                              (2)
                              
                                 
                                    N
                                    m
                                    e
                                    
                                       d
                                       j
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                    =
                                    
                                       
                                          m
                                          e
                                          
                                             d
                                             j
                                          
                                          
                                             (
                                             i
                                             )
                                          
                                          −
                                          m
                                          i
                                          
                                             n
                                             P
                                          
                                          
                                             (
                                             i
                                             )
                                          
                                       
                                       
                                          m
                                          a
                                          
                                             x
                                             P
                                          
                                          
                                             (
                                             i
                                             )
                                          
                                          −
                                          m
                                          i
                                          
                                             n
                                             P
                                          
                                          
                                             (
                                             i
                                             )
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where minP
                        (i) and maxP
                        (i) are the minimum and maximum median objective values achieved by running all hyper-heuristics in P on i, respectively. Normalising the median objective values also acts as a unification method for all hyper-heuristics on a given domain as well as all problem domains enabling visualisation of relative performance of different hyper-heuristics on the same scale. Figs. 3 and 4 provide the box plots of the normalised median objective values for the MSHH and the competitors’ hyper-heuristics for each domain and overall, respectively. It is observed that the MSHH outperforms the other approaches overall and in SAT, PFS, and TSP problem domains, taking the second place in BP problem domain. However, the proposed hyper-heuristic delivers a relatively poor performance on the PS and VRP problem domains.

We have repeated some experiments in order to track and interpret the behaviour of the proposed multi-stage hyper-heuristic. Each trial is repeated for 10 times during this set of experiments. The percentage utilisation is the ratio of the number of improvements that a low level heuristic generates over the best solution found so far to the total number of such improvements. Fig. 5 shows the average percentage utilisation of the single and combined low level heuristics while an arbitrarily chosen representative instance from each problem domain is solved. As one would expect, not all the low level heuristics can generate improvement over the best solution found so far during the search process. For example, in PS, surprisingly, LLH
                        0 and LLH
                        1 heuristics which are provided as hill climbers do not yield any improvement neither themselves individually nor in combination with another low level heuristic on the tested instance. On the other hand, LLH
                        3 and LLH
                        5 are not able to make any improvement on the best solutions while BP instance is being solved. This is not surprising, though, as those low level heuristics are mutational heuristics.

With the exception on SAT problem domain, most of the improving moves are due to hill climbers rather than mutational heuristics. The use of the combination of a mutational heuristic followed by a hill climbing heuristic, like the basic steps of iterated local search (Burke et al., 2010a), is automatically favoured by our hyper-heuristic in the PFS and TSP domains (Fig. 5(d) and (e)). Similarly, ruin and re-create followed by a hill climber is another favourite automatically detected pairing in the TSP problem domain. In TSP, LLH
                        1 does not seem to be that useful at the first glance, but considering the relay hybridisation technique, it seems to serve as a ‘good’ diversification component, improving the performance of the hill climbing heuristic (LLH
                        8) employed afterwards. In BP, MSHH favours the pairing of a mutational low level heuristic followed by a ruin and re-create heuristic. The relay hybridisation of low level heuristics seem to be useful, except for the PS and VRP domains, in which it has been observed that no generated heuristic pairs contributes towards the improvement of the best solutions. The proposed hyper-heuristic looses time by testing all pairs of given low level heuristics which could have been used in the search process. This could be one of the reasons why the proposed hyper-heuristic performs relatively poor on those domains.

The behaviour of MSHH considering the average threshold value of the move acceptance method and average objective values of the current solution in time is illustrated in Figure 6 for an arbitrarily selected instance from each problem domain. In some cases, MSHH improves the quality of the initially generated solution at the beginning of the search process rapidly. Then the improvement slows down, but still continues as in the BP problem domain (Fig. 6(b)).
                        
                        
                        
                         While MSHH solves a given instance, it enters into what seems to be a “neutral” region getting stuck at a local optimum. Due to the employment of the adaptive move acceptance method, the MSHH managed to escape from the local optimum and further improvements to the candidate solutions are obtained. MSHH seems to require partial restarts while solving problem instances from the SAT and PFS problem domains more than the others which definitely works and this could be one of the reasons for the success of MSHH on those problem domains.


                        Fig. 7
                         depicts the average number of low level heuristics including individual and paired low level heuristics vs. time, over 10 trials on a selected instance from each problem domain. Interestingly, on average, the number of low level heuristics are reduced to approximately less than 10 percent of the total in all six problem domains. This reduction occurs due to the employment of S2HH which aims to disable the poor performing heuristics. In PS problem domain, it is observed that all the single low level heuristics are used during the entire search process. The fluctuations in the number of used low level heuristics during the search process are very frequent in all the other problem domains. It has been observed that the number of low level heuristics never decreases to a single low level heuristic at any time in none of the domains. Fig. 7 illustrates that different set of low level heuristics are useful at different parts of the overall search process. For example, at the start of the search process, the number of the low level heuristics stays the same for BP, then it starts decreasing towards the midst of the given time.

@&#CONCLUSION@&#

A selection hyper-heuristic is a general-purpose search methodology that mixes and controls a given set of heuristics for solving a computationally hard problem. Such high level methods do not require any modification while being applied to a new/unseen problem domain. Moreover, the component-based design of selection hyper-heuristics enables re-usability of those components as well.

Up to this date, most of the selection hyper-heuristics performing single point based search contains two key components: heuristic selection and move acceptance. This work is one of the initial studies that explicitly addresses whether it is useful to combine multiple hyper-heuristics or not and how. We present a general multi-stage hyper-heuristic framework and describe its main components. The proposed multi-stage hyper-heuristic framework is reusable and useful in relieving the difficulty of choosing a hyper-heuristic method for solving a problem. This framework is used as a basis to implement an iterated multi-stage hyper-heuristic with synergistic components embedding two hyper-heuristics with an adaptive threshold move acceptance method. One of the hyper-heuristics aims to reduce the number of low level heuristics discovering the “potentially” useful ones at a given stage during the search process and adjust the probability of each low level heuristic being selected in the following stages. The hyper-heuristic extends the low level heuristic set first by creating “new” heuristics through relay hybridisation, and then a dominance based learning strategy is employed reducing the number of heuristics. The strategy captures the trade-off between the extent of improvement that a heuristic can generate and the number of steps it takes to achieve that improvement. Moreover, each chosen low level heuristic in the “reduced” set is associated with an adaptively decided selection probability to be used in the following stages. The second hyper-heuristic mixes the “reduced” set of low level heuristics with the given probabilities during the search process.

The proposed learning multi-stage selection hyper-heuristic with adaptive move acceptance is tested on a benchmark of problem domains. The results confirm its success when compared to each constituent hyper-heuristics, previously proposed other multi-stage hyper-heuristics as well as the state-of-the-art hyper-heuristic which won the CHeSC 2011 competition. Our hyper-heuristic is a relatively simple approach which is easy-to-implement and easy-to-maintain as compared to some previously proposed hyper-heuristics including the previous state-of-the-art hyper-heuristic, yet, it is extremely effective in cross-domain search delivering a superior performance. The success of the proposed multi-stage hyper-heuristic approach based on the proposed “simple” framework across a variety of domains indeed indicates the potential of utilising and mixing the existing or new selection hyper-heuristics.

@&#REFERENCES@&#

