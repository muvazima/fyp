@&#MAIN-TITLE@&#Generalized discriminant analysis for congestive heart failure risk assessment based on long-term heart rate variability

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Investigating the discrimination power of long-term HRV for risk assessment in CHF patients.


                        
                        
                           
                           Introducing the significant features of HRV for risk assessment of CHF patients.


                        
                        
                           
                           To examine the influence of GDA in order to achieve desired accuracy of the classification.


                        
                        
                           
                           We achieved sensitivity and specificity of 100% having the least number of features.


                        
                        
                           
                           The results are far better than any other previously reported ones.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Heart rate variability (HRV)

Congestive heartfailure (CHF)

Generalized discriminant analysis (GDA)

k-Nearest neighbor

@&#ABSTRACT@&#


               
               
                  The aims of this study are summarized in the following items: first, to investigate the class discrimination power of long-term heart rate variability (HRV) features for risk assessment in patients suffering from congestive heart failure (CHF); second, to introduce the most discriminative features of HRV to discriminate low risk patients (LRPs) and high risk patients (HRPs), and third, to examine the influence of feature dimension reduction in order to achieve desired accuracy of the classification. We analyzed two public Holter databases: 12 data of patients suffering from mild CHF (NYHA class I and II), labeled as LRPs and 32 data of patients suffering from severe CHF (NYHA class III and IV), labeled as HRPs. A K-nearest neighbor classifier was used to evaluate the performance of feature set in the classification. Moreover, to reduce the number of features as well as the overlap of the samples of two classes in feature space, we used generalized discriminant analysis (GDA) as a feature extraction method. By applying GDA to the discriminative nonlinear features, we achieved sensitivity and specificity of 100% having the least number of features. Finally, the results were compared with other similar conducted studies regarding the performance of feature selection procedure and classifier besides the number of features used in training.
               
            

@&#INTRODUCTION@&#

By definition, heart failure is the inability of the heart to maintain or increase cardiac output at a rate commensurate with somatic aerobic requirements [1]. This may happen when the heart muscle itself is weaker than normal or when there is a defect in the heart that prevents blood from getting out into the circulation [2,3]. When the heart does not circulate blood normally, the kidneys receive less blood and filter less fluid out of the circulation into the urine. The extra fluid in the circulation builds up in the lungs, the liver, around the eyes, and sometimes in the legs. This is called fluid “congestion” and for this reason doctors call this “congestive heart failure” (CHF) [2]. CHF is chronic, degenerative and age related [4]. Therefore, the growing number of elderly people in western countries could be one of the reasons that the number of patients with CHF is increasing [5]. It is said that CHF is asymptomatic in its first stages. A decreased heart functioning may be diagnosed by several tests, including the “echocardiogram”, “heart catheterization”, “chest X-ray”, “chest CT scan”, “cardiac MRI”, “nuclear heart scans (MUGA, RNV)”, and “ECG” [6]. Obviously, detecting patients’ conditions at the starting phases of disease would lead to a reduction in medical cost as well as prevention of the disease progress. However, patients with heart failure can have an enjoyable life by controlling the disease with medicine besides changing their life style. CHF severity can be measured by symptomatic classification scale developed by New York Heart Association (NYHA) [1]. Classification via NYHA scale has been proved to be a risk factor for mortality [7]. According to the estimation via NYHA scale [1], patients suffering from mild CHF (NYHA classes I and II) are considered as low risk patients (LRPs) while the ones suffering from severe CHF (NYHA classes III and IV) are considered as high risk patients (HRPs).

Heart rate variability (HRV) is the variation over time of the period between consecutive heartbeats (RR intervals) [8]. It is calculated using ECG signals and commonly used to assess the influence of the autonomic nervous system (ANS) on the heart [9]. Many studies used HRV analysis in order to classify different arrhythmias [5,6,9–26]. Some of these studies used long-term [21–23] and short-term [5,6] HRV analysis for distinguishing CHF patients from normal subjects. Also the effect of HRV analysis in order to assess risk and mortality of CHF patients is mentioned in some studies [27,28].

Recent clinical studies have demonstrated that HRV analysis may be a useful tool to assess the balance of cardiac autonomic nervous system. HRV indices are closely related and reflect parasympathetic, mixed sympathetic, and parasympathetic and circadian rhythms [29]. On the other hand, it has been recognized that, in heart failure, the sympathetic nervous system (SNS) is activated and the imbalance of the activity of the SNS and vagal activity interaction occurs. The abnormal activation of the SNS leads to further worsening of heart failure [30,31]. So, based on this rational relationship, in this study, we used HRV analysis to investigate the power of linear and nonlinear HRV measures to individuate low risk (LR) and high risk (HR) CHF patients. In 2011, a classifier was presented by Mellilo [28] which took advantage of short-term HRV measures to individuate severity of CHF. He also used long-term HRV analysis for risk assessment in patients suffering from CHF [27]. He used linear HRV analysis and classification tree to classify patients in LR and HR classes and achieved sensitivity and specificity rates of 93.3% and 63.6%, respectively. The method we used for the aforementioned purpose is based on the use of discriminative nonlinear features of HRV. Some studies on the CHF detection such as [6,22,23] have used SD1/SD2 with other features extracted from HRV time series. Voss [32] showed that besides clinical indices, non-clinical parameters, especially nonlinear ones such as DFA revealed significant differences between LR and HR groups of CHF patients. He also showed that α
                     1 from DFA is a powerful independent predictor of mortality in CHF. In 1991 Pincus [33] evaluated and quantified nonlinear dynamic changes based on chaos theory for analysis of heart failure subjects using FD, DFA, and ApEn. Krstacic et al. [34] studied the nonlinear dynamics (DFA, FD, and ApEn) in 250 patients with heart failure during 12 months and found that the patients with heart failure had lower ApEn while higher FD.

In the next step of our algorithm, we proposed a new method which enabled us to individuate LR and HR patients. The proposed algorithm uses the generalized discriminant analysis (GDA) which is a sort of generalization to the widely accepted linear discriminant analysis (LDA) algorithm. Recently, Asl [35] used GDA and support vector machine (SVM) methods to classify cardiac arrhythmia. Yaghouby [26] used GDA as a feature dimension reduction method besides MLP as a classifier to classify four different types of cardiac arrhythmias. Soleymani [25] presented an algorithm for classification of seven heart arrhythmias by neural networks using chaotic features of HRV signal and GDA. In GDA method, the input data is mapped into a convenient higher dimensional feature space F and instead of the original input space, the LDA algorithm is performed on the obtained space F. Therefore, by using GDA we achieved dimensionality reduction of the input feature space as well as selection of the most useful discriminating features at the same time.

In the last part of our study we classified CHF patients in two classes of LRP and HRP by using k-nearest neighbor (KNN) as a classifier which was previously used by Isler [6] for diagnosing CHF patients. In the following sections, the details of the proposed algorithm are explained.

In Section 2, our method is described and some information about the linear and nonlinear features extracted from HRV, feature dimension reduction and classifier are presented. The results of the proposed algorithm are presented in Section 3. Finally, we compared this work to the previous relevant studies and the arguments are presented in Section 4.

@&#METHODS@&#

The dataset we used herein was retrieved from two databases, both of which are available on the PhysioNet. It includes 29 data obtained from Congestive Heart Failure RR intervals Database [36] with patients suffering from CHF (NYHA classes I–III) and 15 data obtained from BIDMC Congestive Heart Failure Database [36] with patients suffering from severe CHF (NYHA class III and IV). The first database includes RR Intervals extracted from 24-h ECG Holter recordings of 8 men, 2 women and 19 unknown-gender subjects with the age of 34–79, and the original ECG recordings of this RR interval database digitized at 128samples per second. The latter database includes long-term ECG recordings of 11 men and 4 women subjects with the age of 22–71, and the original ECG recordings of this database digitized at 250samples per second. These 44 nominal 24-h recordings were divided into two sets based on the NYHA criterion: low risk patients (LRPs) and high risk patients (HRPs). The LRP subdataset includes 12 data of patients suffering from mild CHF (NYHA classes I and II) and the HRP subdataset includes 32 data of patients suffering from severe CHF (NYHA classes III and IV). We just selected the patients with a fraction of total heartbeats intervals (RR) classified as normal-to-normal (NN) intervals (NN/RR) higher than 80% as explained in Section 2.2.

In order to compute HRV measures, standard long-term HRV analysis on nominal 24-h recordings was used according to international guidelines [9].

The NN/RR ratio was used as a fraction of all the RR intervals classified as Normal to Normal intervals in a recording. To compute this ratio, we obtained the series of Normal to Normal beat intervals from the beat annotation files of the available databases. This ratio is used in previous studies as a measure of data reliability.

Choosing 80% as a threshold [36], we excluded data with the ratio less than it. This threshold was a satisfactory trade off between numbers of included subjects and quality of NN signals [27]. As a result of using this technique, 2 LRPs data and 3 HRPs data were excluded. So, the final database for analysis included 29 HRPs and 10 LRPs. In the next step, linear and nonlinear features were computed. Physionet's HRV Toolkit [36] was used for linear analysis and Kubios [37] was used for nonlinear analysis. These softwares were used mainly because they are open source and rigorously validated softwares.

Many standard basic time- and frequency domain HRV measures were used in previous studies as discussed in [8,9]. Some standard statistical time-domain HRV measures were calculated as shown in Table 1
                           .

The frequency-domain HRV measures are based on the estimation of power spectral density (PSD). Different methods to estimate PSD of the RR intervals series were suggested in studies [8,9,38,39]. We used FFT-based method in order to estimate PSD. After that, the reported standard frequency-domain HRV measures in Table 2
                            were calculated.

A number of nonlinear features were calculated and used as follows: Approximate entropy (ApEn); Sample entropy (SampEn); Poincare plot (SD1/SD2); Detrended fluctuation analysis (DFA); Correlation dimension (CD); and Recurrence plot (RP). The studies on the CHF detection [6,22,23] have only used SD1/SD2 feature together with other linear features. Voss [32] showed that in addition to clinical indices, non-clinical parameters, especially nonlinear parameters such as DFA revealed significant differences between LR and HR groups of CHF patients. He also showed that α
                           1 from DFA is a powerful independent predictor of mortality in CHF. Pincus [33], evaluated and quantified nonlinear dynamic changes based on chaos theory for analysis of heart failure subjects using FD, DFA, and ApEn and showed that ApEn decreases and FD increases in CHF patients. Krstacic et al. [34] studied the nonlinear dynamics (DFA, FD, and ApEn) in 250 patients with heart failure during 12 months and found that the patients with heart failure had lower ApEn and higher FD.

The p-values of the mentioned features are reported in Table 3
                            to show the significance of the features in discriminating LR from HR in CHF patients. Here, we assumed that the features with p-values less than 0.05 are the significant features for CHF risk assessment.

Reducing the number of features is considerably important due to the major reasons: calculation cost and classification accuracy. Dimensionality reduction usually takes place through manipulating feature selection and feature extraction methods. Feature selection uses an algorithm to select the best subset of the input feature sets and feature extraction develops an algorithm to create new features based on transformations or combinations of the original feature sets [40].

After calculating the aforementioned linear and nonlinear features, as shown in Fig. 1
                            a considerable overlap is observed between some samples of the two classes leading to a difficulty in distinguishing them. Under such circumstances, it could be helpful to have a feature transformation mechanism to minimize the within-class scatter while maximizing the between-class scatter. Because of the above-mentioned characteristics we found GDA beneficial to be used in this study as a transform method.

GDA, which was first proposed by Baudat [41] is a nonlinear extension to the ordinary linear discriminant analysis (LDA) [42]. LDA itself is a traditional statistical method which has proved to be successful for classification purposes [40]. However, it normally fails to classify nonlinear problems [41]. The primary purpose of GDA is to maximize the inter-classes inertia and at the same time minimize the intra-classes inertia [41]. In this method, the input training data is mapped into a feature space of higher dimension with linear properties by a kernel function where different classes are linearly separable. In other words, we mapped the input training data into a feature space in which samples are nonlinearly related to the input space and then LDA is applied to the mapped data. In fact, LDA scheme tries to find a W matrix as a transformed matrix which maps vectors of the input data from N classes to N
                           −1 dimension space and maximizes the ratio of the between-class inertia to the within-class inertia. Furthermore, given a number of independent features which describe the data, LDA creates a linear combination of the features that yields the largest mean differences of the desired classes [35].

Let X be the training data set with M feature vectors and N classes. If X
                           
                              pq
                            be the qth HRV feature vector in the pth class, then n
                           
                              p
                            denotes the class size of the pth class. Say that the space X is mapped into a higher dimensional feature space F using Φ as a nonlinear mapping function. So,


                           
                              
                                 (1)
                                 
                                    
                                       X
                                       i
                                    
                                    ∈
                                    
                                       R
                                       f
                                    
                                    ⟶
                                    Φ
                                    (
                                    
                                       X
                                       i
                                    
                                    )
                                    ∈
                                    
                                       R
                                       F
                                    
                                    
                                    F
                                    ≫
                                    f
                                    .
                                 
                              
                           
                        

Considering the observations Φ(X
                           
                              i
                           ) to be centered in F, we defined two matrices of the within-class inertia V and the between-class inertia B in space F before projecting the training data set X into a new set Y using GDA:


                           
                              
                                 (2)
                                 
                                    V
                                    =
                                    
                                       1
                                       M
                                    
                                    
                                       ∑
                                       
                                          p
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       ∑
                                       
                                          q
                                          =
                                          1
                                       
                                       
                                          
                                             n
                                             p
                                          
                                       
                                    
                                    Φ
                                    (
                                    
                                       X
                                       pq
                                    
                                    )
                                    
                                       Φ
                                       T
                                    
                                    (
                                    
                                       X
                                       pq
                                    
                                    )
                                 
                              
                           
                           
                              
                                 (3)
                                 
                                    B
                                    =
                                    
                                       1
                                       M
                                    
                                    
                                       ∑
                                       
                                          p
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       n
                                       p
                                    
                                    
                                       ∑
                                       
                                          q
                                          =
                                          1
                                       
                                       
                                          
                                             n
                                             p
                                          
                                       
                                    
                                    (
                                    
                                       X
                                       pq
                                    
                                    )
                                    
                                       
                                          (
                                          
                                             ∑
                                             
                                                r
                                                =
                                                1
                                             
                                             
                                                
                                                   n
                                                   p
                                                
                                             
                                          
                                          (
                                          
                                             X
                                             pr
                                          
                                          )
                                          )
                                       
                                       T
                                    
                                    .
                                 
                              
                           
                        

GDA tries to find the projection vector 
                              v
                            in a way that maximizes the inter-class inertia and minimizes the intra-class inertia at the same time. For this purpose, it should solve the following maximization problem:


                           
                              
                                 (4)
                                 
                                    v
                                    =
                                    
                                       argmax
                                       v
                                    
                                    
                                       
                                          
                                             v
                                             T
                                          
                                          Bv
                                       
                                       
                                          
                                             v
                                             T
                                          
                                          Vv
                                       
                                    
                                    .
                                 
                              
                           
                        

Actually, solving the above problem is equivalent to finding the eigenvector of the matrix V
                           −1
                           B associated with eigenvalue 
                              λ
                              =
                              
                                 
                                    
                                       v
                                       T
                                    
                                    Bv
                                 
                                 
                                    
                                       v
                                       T
                                    
                                    Vv
                                 
                              
                           . All solutions of 
                              v
                            lie in the span of Φ(X).


                           
                              
                                 (5)
                                 
                                    v
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       M
                                    
                                    
                                       a
                                       i
                                    
                                    Φ
                                    (
                                    
                                       X
                                       i
                                    
                                    )
                                 
                              
                           where a
                           
                              i
                            are expansion coefficients. Considering the kernel function k(x
                           
                              i
                           , x
                           
                              j
                           )=
                           k
                           
                              ij
                           
                           =
                           f(x
                           
                              i
                           )f(x
                           
                              j
                           ) and performing the eigenvectors decomposition on the kernel matrix K
                           =(k
                           
                              ij
                           )
                           for
                           
                           i
                           =1, …, M
                           
                           and
                           
                           j
                           =1, …, M, M normalized expansion coefficients for each projection vector 
                              a
                              =
                              (
                              a
                              )
                              /
                              (
                              
                                 
                                    
                                       (
                                       
                                          a
                                          T
                                       
                                       ka
                                       )
                                    
                                    
                                       1
                                       /
                                       2
                                    
                                 
                              
                              )
                            are obtained. Now, for a feature vector x from the test HRV dataset, the projection on the ith eigenvector 
                              
                                 v
                                 i
                              
                            can be calculated according to Eq. (6) in which the jth expansion coefficient of the ith eigenvector is shown by 
                              
                                 a
                                 j
                                 i
                              
                           :


                           
                              
                                 (6)
                                 
                                    
                                       y
                                       i
                                    
                                    =
                                    
                                       v
                                       iT
                                    
                                    Φ
                                    (
                                    X
                                    )
                                    =
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       M
                                    
                                    
                                       a
                                       j
                                       i
                                    
                                    Φ
                                    (
                                    
                                       X
                                       j
                                    
                                    )
                                    Φ
                                    (
                                    X
                                    )
                                    =
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       M
                                    
                                    
                                       a
                                       j
                                       i
                                    
                                    k
                                    (
                                    
                                       X
                                       j
                                    
                                    ,
                                    X
                                    )
                                    .
                                 
                              
                           Finally, in order to form the transformation matrix 
                              
                                 W
                                 T
                              
                              =
                              (
                              
                                 v
                                 1
                              
                              ,
                              …
                              ,
                              
                                 v
                                 
                                    N
                                    −
                                    1
                                 
                              
                              )
                            as the main purpose of GDA to reduce the number of features, N
                           −1 eigenvectors associated with the first largest nonzero N
                           −1 eigenvalues are selected. Next, each HRV feature vector is projected into a new coordinates using the N
                           −1 projection vectors.

It is worth mentioning that the optimal number of eigenvectors for the data transformation is generally equal to N
                           −1 [35]. In this paper, we have defined two classes in such a manner that the number of input features would be reduced to 1 with the help of GDA in order to guarantee better performance compared to our previous results obtained without GDA.

In this work, a radial basis function (RBF) is used as the kernel function and the only parameter – kernel width σ – was empirically chosen as σ
                           =1 to achieve the best classification result. Figs. 2 and 3
                           
                            show the feature space plot and the boxplot of the new feature created by GDA, respectively.

In order to evaluate the feature set created by the GDA, we used a K-nearest-neighbor (KNN) classifier. The KNN is one of the most fundamental and simple classification methods. The KNN algorithm is commonly based on the Euclidean distance between a test sample and the specified training samples.

In this method, training samples, created by GDA, are plotted in a d-dimensional feature space. Here, d represents the number of features for classification considered to be 1. Then these samples are allocated according to their defined classifications. Then, the test sample, an unknown sample, is plotted in the feature space. The test sample is then typically classified according to the majority class of its KNN [43]. The Euclidean distance between two samples is calculated and based on these calculated distances, the test sample is classified.

In order to validate our method, the dataset was randomly divided into training and test sets. Test set which was used as the validation set, was not used in all procedures of training GDA and KNN.

An alternative method is the leave-one-out cross-validation [6,44]. This method is widely used especially when the available dataset is small. In this method, one piece of data is randomly excluded as test set and the whole dataset except the excluded data is used as the training set. This process is repeated for all the samples in the dataset.

In this study, we used the leave-one-out method in which the classifier and GDA were trained using the whole dataset except one that was excluded. Then, they were tested on this excluded data. We repeated this process for all 39 samples in the dataset to give the same chance to all the samples to be in training and test sets.

Confusion matrices were used [45] to measure the performance of our method by computing commonly used measures: sensitivity (true positive ratio) explained as the ability of test to correctly identify high risk patients, specificity (true negative ratio) explained as the ability of test to correctly identify low risk patients, accuracy, and precision [6] according to Table 4
                        .

@&#RESULTS@&#

The proposed method for CHF risk assesment were applied to long-term HRV data in two different steps. First, the algorithm was run using linear, nonlinear, and combination of linear and nonlinear HRV measures with p-values less than 0.05. Second, the algorithm was run using features created by GDA trained by linear, nonlinear, and combination of linear and nonlinear HRV measures with p-values less than 0.05. A 5-nearest neighbors classifier is used in these two steps.


                     Table 5
                      shows the results of the first step in which linear and nonlinear features with p-values less than 0.05 and combination of them without any feature dimension reduction were used. The results reveal that the four significant nonlinear features (
                        
                           
                              SD
                              1
                           
                           
                              SD
                              2
                           
                        
                     , Alpha1, CD, and Lmax) are more powerful compared to the other feature sets for risk assessment in CHF patients. The box-plots of these discriminative nonlinear features for LRPs and HRPs classes are presented in Fig. 4
                     . According to the results of Table 5, although nonlinear features can classify all the HRP subjects in the right class, as we know, diagnosing of the patients in the early steps of disease (LRPs) is vitally important to control the disease by prescribing medicine.

In the second step of our algorithm we used GDA as a feature extraction method to reduce the number of features as well as to improve the performance of the classifier. The results of this step are presented in Table 6
                     .

The confusion matrices of the classifier enhanced with the nonlinear features with and without GDA are reported in Tables 7 and 8
                     
                     , respectively.

Comparing Tables 5 and 6 indicates that using GDA in order to reduce the number of features as well as the overlap of the samples of two classes in feature space enables us to classify all the LRP and HRP subjects in the right class and achieve 100 % for both sensitivity and specificity.

The performance of the proposed method and Mellilo's methods are compared in Table 9
                     . It can be seen that the results of the proposed method are better than those of Mellilo's. Moreover, we achieved the ability to distinguish all the subjects without any error by using only one single feature created by GDA.

@&#DISCUSSION AND CONCLUSION@&#

In this study, we applied HRV data from an online and widely-used database to investigate the class discrimination power of linear and nonlinear long-term HRV features for risk assessment in patients suffering from CHF. A GDA feature extraction and a KNN classifier were used to evaluate the performance of linear and nonlinear features. The KNN classification method used in this study does not depend on whether or not the data follow any particular distribution. It is concluded that discrimination of CHF patients according to their severity by means of long term HRV measures is an acceptable procedure.

The results seen in Table 5 show that using only nonlinear features with p-values less than 0.05 can be more powerful than using linear features and combination of linear and nonlinear features in order to perform CHF risk assessment. So, we used a feature set of nonlinear features with p-values less than 0.05 which improved the performance of the HRV analysis to diagnose LRPs and HRPs.

In the next step, after calculating the features and selecting the more discriminative features, for reducing the number of features and achieving the desired performance of the classifier according to Tables 6 and 8, GDA was used as a feature extraction method. GDA proposed by Baudat [41] is a feature dimension reduction method to reduce the number of features as well as remove the overlap between the samples of two classes. So, the number of features reduced to one and finally, CHF patients were effectively classified in the LRPs and HRPs classes.

Compared to other studies, here we investigated linear and nonlinear features of HRV and combination of them for CHF risk assesment. Voss in his study [32] showed that besides clinical indices, non-clinical parameters, especially nonlinear ones such as DFA revealed significant differences between LR and HR groups of CHF patients. He also proved that α
                     1 from DFA is a powerful independent predictor of mortality in CHF patients. Here, we proved that nonlinear features with p-values less than 0.05 including α
                     1, CD, 
                        
                           
                              SD
                              1
                           
                           
                              SD
                              2
                           
                        
                      and Lmax are important features for risk assessment of CHF patients. On the other hand, as seen in Fig. 3, using the GDA for feature extraction leads to an efficient new feature which improved the classification performance compared to the KNN model even though it uses all the features. As a result, in our method compared to Mellilo's [27,28] in Table 9, a feature set of powerful nonlinear features is used for training GDA as a feature dimension reduction. So, we achieved higher accuracy, precision, sensitivity, and specificity with the lowest possible number of features (Table 9).

The limitations we faced here using 24-h Holter databases were somehow the same limitations reported by [5,27]. First, the dataset is small and unbalanced and therefore we used leave-one-out cross-validation performance estimates. As long as our dataset is small and unbalanced, it is impossible to confirm the generalization of our results unless a larger public dataset is available. Second, the sampling frequency of ECG recordings are not equal. However, the lowest sampling rate used in this study is 128Hz which is logical and acceptable in order to calculate HRV signal according to adjacent RR intervals [9]. Finally, the procedures of extracting NN intervals are not the same. So, we used the ratio of NN/RR as a measure of data reliability to exclude unreliable results.

To sum up, the long-term HRV measures enabled us to distinguish higher risk patients from lower risk ones. The 5-nearest neighbor classifier used in this study, helped us to achieve excellent accuracy, sensitivity, and specificity of 100% using the feature set created by GDA, that gives far better results in comparison with the Mellilo's results.

The authors declare that there are no conflicts of interest.

@&#REFERENCES@&#

