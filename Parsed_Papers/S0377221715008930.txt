@&#MAIN-TITLE@&#Integer programming models for feature selection: New extensions and a randomized solution algorithm

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Feature Selection (FS) is modelled as a (mixed) integer optimization problem.


                        
                        
                           
                           To solve this problem, a new FS algorithm (FSA) with short memory is proposed.


                        
                        
                           
                           This algorithm has been already successfully applied to life science data.


                        
                        
                           
                           New experiments on randomly generated and real biological datasets are reported.


                        
                        
                           
                           The results are compared w.r.t. other FSA confirming the validity of our approach.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Data mining

Heuristics

Integer programming

@&#ABSTRACT@&#


               
               
                  Feature selection methods are used in machine learning and data analysis to select a subset of features that may be successfully used in the construction of a model for the data. These methods are applied under the assumption that often many of the available features are redundant for the purpose of the analysis. In this paper, we focus on a particular method for feature selection in supervised learning problems, based on a linear programming model with integer variables. For the solution of the optimization problem associated with this approach, we propose a novel robust metaheuristics algorithm that relies on a Greedy Randomized Adaptive Search Procedure, extended with the adoption of short memory and a local search strategy. The performances of our heuristic algorithm are successfully compared with those of well-established feature selection methods, both on simulated and real data from biological applications. The obtained results suggest that our method is particularly suited for problems with a very large number of binary or categorical features.
               
            

@&#INTRODUCTION@&#


                     Feature Selection (FS) addresses a class of methods used to extract relevant information from data. FS has always been a central topic in Multivariate Statistics and Data Analysis, but has received important contributions also from mathematicians and computer scientists; at the same time, the ever increasing amount of data that are being collected in many real world applications, jointly with the evolution of technology, poses new challenges for FS methods. An example of such challenges can be found in the study of biological and genomic data, where interesting data sets may be composed of a few hundreds of tissue samples on which the activity of tens of thousands of genes is measured. The analysis of such data requires to identify a limited number of genes (i.e., features) able to identify an interesting model. Similarly, new data collection techniques based on cheap sensors and on internet activity are creating very large repositories where precious information may be hidden and needs to be mined out.

In the general setting, FS can be described as follows: given a data matrix defined by a finite set of features measured on a finite number of objects, select a subset of the feature set that is particularly relevant, with respect to all the other possible subsets, for the analysis that is to be conducted on the data under study. In this work, we focus on supervised learning (i.e., classification), where data are analysed to identify a model able to predict if an observation belongs to one of two or more classes, based on the values of its features. In supervised learning, FS operates to select a relevant – and possibly small – subset of features to be used by the classifier.

We propose a method designed to treat directly integer or binary features, keeping in mind that discretization methods are often used to transform continuous measures into discrete or binary ones; such a process is adopted in many settings to control noise, to ease the interpretation of the classification model, and, last but not least, to apply specific logic-based classifiers also in the presence of continuous features.

The proposed approach is based on an optimization problem derived from the data matrix, where each feature is associated with a binary variable. Such an approach is not new in the literature; it stems from the minimum test collection originally described in Garey and Johnson (1979). We show that such optimization problems are still not tractable – even with state-of-the-art mixed integer solvers – and propose a new heuristic algorithm for their solution.

The performances of our method are tested on different data sets, and compared with other established FS methods, in combination with classifiers of different nature. The tests are run both on simulated data sets, composed of binary features, and on two real genomic data sets, composed of continuous variables. For the latter, we adopt a simple discretization procedure. The results appear to be very satisfactory both from the standpoint of solution quality and of solution time, particularly when applied to data sets of large dimension.

The paper is organized as follows. Section 2 provides a brief introduction to the different approaches to FS and the related literature. Integer programming models for FS are treated in Section 3. In Section 4 we describe the new Greedy Randomized Adaptive Search Procedure with memory proposed for the solution of the optimization problem associated with FS. The comparison of all the FS methods and their performances on real and simulated data sets are treated in Section 5 and its subsections, jointly with the description of the classifiers that we use to compare the different FS methods, and the main motivations of our experiments. Conclusions and future lines of work are drawn in Section 6.

FS can be looked at from different angles. One may simply evaluate the features according to their individual merit, order them accordingly, and then select the desired number of them, possibly controlling the quality of the solution when the number of selected features increases. Such an approach is the one adopted by Ranker methods (Kira & Rendell, 1992a). Conversely, one may want to evaluate a subset of the features according to their integrated contribution, and thus is faced with a more complex subset selection problem, which has an intrinsic combinatorial nature and is recognized to be a complex problem. In the latter case, some methods are designed to construct a solution set by adding features iteratively, paying attention to evaluate the feature to be added conditionally to those that are already in the set; such a forward selection approach is paired with a backward approach, where features are, iteratively, eliminated from the current set.

Another way of looking at FS methods is to distinguish them according to how the feature sets are evaluated and used in data analysis. This defines Filter, Wrapper, and Embedded methods (Bolón-Canedo, Sánchez-Maroño, & Alonso-Betanzos, 2013; Forman, 2003). Methods of the first group select features according to a score function; methods of the second group iteratively test feature sets performing data analysis, until a satisfactory result is obtained; to the third group belong those methods that automatically select the features that appear to be good for the purpose of their analysis. As recognized in Bolón-Canedo et al. (2013), filters are often to be preferred for their stand-alone nature and their speed when compared to wrappers. Indeed, analysing the performances of different methods on several synthetic data sets, the authors of Bolón-Canedo et al. (2013) conclude that filter methods seem to perform better. Also in Forman (2003), where the analysis is restricted to text classification problems, filter methods stand out – in particular, the Bi-Normal Separation proposed by the authors.

FS problems of large size can be solved efficiently also with embedded methods; among the most successful ones are Support Vector Machines (SVM; Cristianini and Shawe-Taylor, 2000), where some proper modifications of the underlying optimization model can efficiently combine the choice of the separating hyperplane with the selection of good features (see, among others, Carrizosa, Martin-Barragan, & Morales, 2008; Maldonado, Pérez, Weber, & Labbé, 2014). For an additional overview of FS, the interested reader may refer to Guyon and Elisseeff (2003), John, Kohavi, and Pfleger (1994), Kira and Rendell (1992b), Liu and Motoda (2000), Liu, Li, and Wong (2002) and Swiniarski and Skowron (2003); a more specific analysis of FS methods for data mining is presented in Piramuthu (2004). As far as FS applications are concerned, a very actual battlefield is to be found in medical and bioinformatics data analysis, where supervised learning problems with very large number of features abound; here, data mining applications strongly rely on FS methods – some examples are in Dagliyan, Uney-Yuksektepe, Kavakli, and Turkay (2011), Lan and Vucetic (2013) and Peter and Somasundaram (2012).

Particularly relevant for the scope of this paper are the methods that adopt a mathematical formulation of the FS problem based on integer variables, able to exploit its combinatorial nature. The most representative and seminal work in this line of research is the minimum test collection problem, stated in Garey and Johnson (1979), based on a Set Covering formulation where binary variables are associated with the features, and a covering constraint is defined for each pair of elements that belong to different classes. In these constraints the feature variable is present only if it exhibits a different value in the two addressed elements.

Also in embedded methods, mathematical optimization is largely used. In Rubin (1990), the solution to a linear program is used to find a separating hyperplane between two sets of points; the linear program is then augmented with binary variables associated with features, resulting in a difficult problem for which several heuristics have been proposed. Similarly, in Bradley and Mangasarian (1998) linear separating hyperplanes are derived via linear programming, and then developed into the well-established theory of the already mentioned SVM (Cristianini & Shawe-Taylor, 2000). Iannarilli and Rubin (2003) adopt an optimization model, where additional packing constraints on binary variables control the dimension of the feature set, while the objective function takes care of maximizing a quality measure of the features based on the Kullback–Leiber divergence.
                  

In this paper, we propose a method based on some variants of the minimum test collection problem, that is guaranteed to provide a separation between the classes, but does not rely on the choice of a specific classification method. A similar approach is used, among others, in Boros, Ibaraki, and Makino (1999) and in previous applications to biological and genomic data (Bertolazzi, Felici, Festa, & Lancia, 2008; Weitschek et al., 2012; Weitschek, Velzen, Felici, & Bertolazzi, 2013). Such an approach is substantially different from methods based on the search of separating hyperplanes such as Bradley and Mangasarian (1998), Carrizosa et al. (2008), Iannarilli and Rubin (2003), Maldonado et al. (2014) and Rubin (1990).

The adoption of a model where integer variables are associated with the choice of a feature sets results in computationally challenging problems, that become intractable for general purpose solvers when the dimensions of the problem increase. We thus propose a properly designed greedy randomized adaptive heuristic, usually referred to as GRASP (Feo & Resende, 1989; 1995), as a viable strategy to obtain good solutions for large FS problems that arise in supervised learning. As already mentioned above, the adoption of properly designed heuristics is frequent in FS problems: a similar GRASP approach is proposed, in a different framework, in Bermejo, Gámez, and Puerta (2011) to control the choice of the feature sets evaluated by a wrapper method; in Unler and Murat (2010) the importance of good heuristics for large sized FS problems is acknowledged, proposing a particle swarm optimization algorithm, while in Meiri and Zahavi (2006) simulated annealing is used to deal with FS problems arising in marketing applications.

According to the distinction of FS into filter, wrapper, and embedded approaches, the method proposed in this work can be considered as a filter method, and therefore the main filter FS algorithms will be taken into account for a computational assessment of the quality of the results of our method. A more detailed description of these methods – namely, Relief (Kira & Rendell, 1992a), Las Vegas Filter (LVF) (Liu & Setiono, 1996), FOCUS (Almuallim & Dietterich, 1994), Correlation-based Feature Selection (CFS) (Hall, 1999), Sequential Forward (Backward) Selection (Elimination) SFS (SBE) (Devijver & Kittler, 1982), and Information Gain (InfoGain) (Hall & Smith, 1998) – is provided in Section 5.1.

Following, we describe the integer programming models (Section 3) and the algorithm designed for their solution 4.

In this section, we provide a general framework that motivates the use of Integer Programming (IP) models for FS. As a starting point, we assume to deal with continuous features; then, we specify our model for supervised learning and for integer or discrete features.

Assume that a real-valued matrix A is available, composed of m rows and n columns, where the rows are associated with objects or samples drawn from one or more populations, and each column is associated with a measure of a feature. Therefore, aik
                      would represent the value of feature k on sample i. By denoting with M (resp. N) the index set of the rows (resp. columns) of A, it results that:

                        
                           
                              
                                 
                                    
                                       
                                          m
                                          =
                                          
                                             |
                                             M
                                             |
                                             ,
                                             
                                             n
                                          
                                          =
                                          
                                             |
                                             N
                                             |
                                             ,
                                             
                                             A
                                          
                                          ∈
                                          
                                             
                                                R
                                             
                                             
                                                m
                                                ×
                                                n
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     
                  

We are interested in maximizing the amount of information withheld by a small number of features (columns of A). The literature proposes alternative quantitative definitions of information, usually related to the extent to which the characteristics – or features – of the observed objects vary among each other, starting from the universally shared assumption that a measure that does not vary among the observations contains no information. Since a complete discussion of the definition of information is out of the scope of the paper, we point the interested reader to Hastie, Tibshirani, Friedman, and Franklin (2005), Phua, Lee, Smith-Miles, and Gayler (2010) and Witten and Frank (2005). Here, we consider a measure of information for real data, based on the Euclidean distance:

                        
                           (1)
                           
                              
                                 
                                    
                                       
                                          I
                                          
                                             (
                                             A
                                             )
                                          
                                          =
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                m
                                             
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   i
                                                   +
                                                   1
                                                
                                                m
                                             
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   (
                                                   
                                                      a
                                                      
                                                         i
                                                         k
                                                      
                                                   
                                                   −
                                                   
                                                      a
                                                      
                                                         j
                                                         k
                                                      
                                                   
                                                   )
                                                
                                                2
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     
                  


                     I(A) is directly related to the variance expressed by A, a widely used measure in statistics and data analysis, or else to a measure of the entropy (Shannon, 1948) when the features can be described by qualitative classes.

As anticipated, the task of FS is to reduce the dimension of the original matrix. Assume that we fix a target dimension β and consider the projection of A on a subset of columns N′ with 
                        
                           
                              |
                           
                           
                              N
                              ′
                           
                           
                              |
                              =
                              β
                              <
                              n
                           
                        
                     . Such a problem can be easily formalized by introducing a binary variable xk
                      defined as follows:

                        
                           (2)
                           
                              
                                 
                                    
                                       
                                          
                                             x
                                             k
                                          
                                          =
                                          
                                             {
                                             
                                                
                                                   
                                                      
                                                         1
                                                         ,
                                                      
                                                   
                                                   
                                                      
                                                         if
                                                         
                                                         k
                                                         ∈
                                                         
                                                            N
                                                            ′
                                                         
                                                         ;
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         0
                                                         ,
                                                      
                                                   
                                                   
                                                      
                                                         otherwise
                                                         .
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     Then, we have that

                        
                           (3)
                           
                              
                                 
                                    
                                       
                                          
                                             I
                                             x
                                          
                                          
                                             (
                                             A
                                             )
                                          
                                          =
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                m
                                             
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   i
                                                   +
                                                   1
                                                
                                                m
                                             
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   (
                                                   
                                                      a
                                                      
                                                         i
                                                         k
                                                      
                                                   
                                                   −
                                                   
                                                      a
                                                      
                                                         j
                                                         k
                                                      
                                                   
                                                   )
                                                
                                                2
                                             
                                             
                                                x
                                                k
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     represents the portion of information preserved by the projection of the points of A on β columns, projection represented by the values of the binary vector x. The FS problem may then be formulated as follows:

                        
                           (4)
                           
                              
                                 
                                    
                                    
                                    
                                       
                                          max
                                          
                                             I
                                             x
                                          
                                          
                                             (
                                             A
                                             )
                                          
                                          =
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                m
                                             
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   i
                                                   +
                                                   1
                                                
                                                m
                                             
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   (
                                                   
                                                      a
                                                      
                                                         i
                                                         k
                                                      
                                                   
                                                   −
                                                   
                                                      a
                                                      
                                                         j
                                                         k
                                                      
                                                   
                                                   )
                                                
                                                2
                                             
                                             
                                                x
                                                k
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                x
                                                k
                                             
                                          
                                          ≤
                                          β
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          x
                                          ∈
                                          
                                             
                                                {
                                                0
                                                ,
                                                1
                                                }
                                             
                                             n
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     
                  

An optimal solution for problem (4) can be found by applying a greedy strategy, by ordering the n features in decreasing order of contribution to the function to be maximized, and then selecting the first β features, similarly to the approach adopted by the Rankers described in Section 2.

Problem (4) is therefore easy to solve, but presents a major limitation: its objective function is the sum (or, equivalently, the average) of the distances between all pairs, where the contributions of all pairs are mixed together to obtain the final value. In this way, few distant pairs of points may largely influence the projection, while the distance among many closer points may not be given the proper attention in the solution.

Such a scenario is not infrequent in data analysis, mainly motivated by the presence of measurement errors, outliers, or different measures scales among the features. A trivial example is depicted in Fig. 1. Fig. 1(a) represents a very simple data set composed of 4 samples described by 2 real valued features x and y. In such a simple case, if 
                        
                           β
                           =
                           1
                        
                      the only projections that may be considered are the one on the x axis and the one on the y axis. The first one, represented in Fig. 1(b), has the property of maximizing the objective function of problem (4), but is not able to separate the first 3 samples on the left from each other. On the other hand, the projection on the y axis (Fig. 1(c)) would not represent an optimal solution for (4) but is able to maintain a good spacing among all points. Although one could not argue that one projection is better than the other without knowing the final purpose of the analysis, this simple example brings to our attention one main point: the simple model (4) may result in projections that do not identify important information for separating points in the target space.

An alternative approach to the problem consists in requiring – directly within the optimization model – that all pairs of points are distinct by at least a minimal threshold quantity α > 0, and then requiring such quantity to be as large as possible:

                        
                           (5)
                           
                              
                                 
                                    
                                    
                                    
                                       
                                          max
                                          α
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   (
                                                   
                                                      a
                                                      
                                                         i
                                                         k
                                                      
                                                   
                                                   −
                                                   
                                                      a
                                                      
                                                         j
                                                         k
                                                      
                                                   
                                                   )
                                                
                                                2
                                             
                                             
                                                x
                                                k
                                             
                                          
                                          −
                                          α
                                          ≥
                                          0
                                          
                                          ∀
                                          
                                          i
                                          ,
                                          j
                                          ,
                                          i
                                          <
                                          j
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                x
                                                k
                                             
                                             ≤
                                             β
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          x
                                          ∈
                                          
                                             
                                                {
                                                0
                                                ,
                                                1
                                                }
                                             
                                             n
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          α
                                          ∈
                                          
                                             
                                                R
                                             
                                             +
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     
                  

We now turn the focus on supervised learning problems, where each object belongs to one out of two or more classes and the selected features should support models that are able to tell the correct class of an object. In this case, the row vectors of A are partitioned into two or more different groups and the purpose is to project the objects onto a space where the classes are easily separable. In this case, a simple relaxation of model (5) would serve the purpose: if constraints associated with objects that belong to the same class are eliminated, then we only require points of different classes to be distant from each other. Model (6) reported below represents this problem. Here, the class of a sample is indicated by a mapping c from the set of the objects M into the index set of the p classes 
                        
                           {
                           1
                           ,
                           …
                           
                           p
                           }
                           ,
                           p
                           ≤
                           M
                        
                      and we write 
                        
                           c
                           (
                           i
                           )
                           =
                           c
                           (
                           j
                           )
                        
                      if and only if objects i and j belong to the same class.

                        
                           (6)
                           
                              
                                 
                                    
                                    
                                    
                                       
                                          max
                                          α
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   (
                                                   
                                                      a
                                                      
                                                         i
                                                         k
                                                      
                                                   
                                                   −
                                                   
                                                      a
                                                      
                                                         j
                                                         k
                                                      
                                                   
                                                   )
                                                
                                                2
                                             
                                             
                                                x
                                                k
                                             
                                          
                                          −
                                          α
                                          ≥
                                          0
                                          ,
                                          
                                          ∀
                                          
                                          i
                                          ,
                                          j
                                          ,
                                          c
                                          
                                             (
                                             i
                                             )
                                          
                                          ≠
                                          c
                                          
                                             (
                                             j
                                             )
                                          
                                          ,
                                          i
                                          <
                                          j
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                x
                                                k
                                             
                                             ≤
                                             β
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          x
                                          ∈
                                          
                                             
                                                {
                                                0
                                                ,
                                                1
                                                }
                                             
                                             n
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          α
                                          ∈
                                          
                                             
                                                R
                                             
                                             +
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     
                  

Note that the relaxation above reduces only linearly the number of constraints of the model, without affecting its computational complexity.

An additional variant is obtained in the case of objects represented by logic or categorical data (corresponding to binary or integer features) – a quite common case in many data mining problems. In this case, we do not define a continuous distance function among samples, but simply check if two samples are equal or different on a given feature. Therefore, we define the generic element of the constraint matrix 
                        
                           d
                           
                              i
                              j
                           
                           k
                        
                      as:

                        
                           (7)
                           
                              
                                 
                                    
                                       
                                          
                                             d
                                             
                                                i
                                                j
                                             
                                             k
                                          
                                          =
                                          
                                             {
                                             
                                                
                                                   
                                                      
                                                         1
                                                         ,
                                                      
                                                   
                                                   
                                                      
                                                         if
                                                         
                                                         
                                                            a
                                                            
                                                               i
                                                               k
                                                            
                                                         
                                                         ≠
                                                         
                                                            a
                                                            
                                                               j
                                                               k
                                                            
                                                         
                                                         ;
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         0
                                                         ,
                                                      
                                                   
                                                   
                                                      otherwise
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     and problem (6) becomes:

                        
                           (8)
                           
                              
                                 
                                    
                                    
                                    
                                       
                                          max
                                          α
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                d
                                                
                                                   i
                                                   j
                                                
                                                k
                                             
                                             
                                                x
                                                k
                                             
                                          
                                          −
                                          α
                                          ≥
                                          0
                                          ,
                                          
                                          ∀
                                          
                                          i
                                          ,
                                          j
                                          ,
                                          c
                                          
                                             (
                                             i
                                             )
                                          
                                          ≠
                                          c
                                          
                                             (
                                             j
                                             )
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                x
                                                k
                                             
                                             ≤
                                             β
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          x
                                          ∈
                                          
                                             
                                                {
                                                0
                                                ,
                                                1
                                                }
                                             
                                             n
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          
                                          α
                                          ∈
                                          
                                             
                                                R
                                             
                                             +
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     
                  

The particular interest of model (8) resides in the fully binary nature of its constraints matrix. We finally note that model (8) has a strong connection with the minimum test collection of Garey and Johnson (1979): formally, the latter would be obtained from (8) simply by eliminating α from the constraints of the model, substituting their right hand side with 1, dropping the constraint involving β, and adopting the sum of the 
                        
                           
                              x
                              k
                           
                           ,
                           k
                           =
                           1
                           ,
                           …
                           ,
                           n
                        
                      as the objective function to be minimized. While (8) fixes the dimension of the target space (i.e., β) and then maximizes the separating information withheld by the features, the minimum test collection problem identifies the smallest set of features with sufficient separating information.

As a concluding remark, it is worth noting that model (8) has solution with α > 0 (or equivalently, the minimum test collection problem has feasible solution) only if the classes in which data is divided do not overlap, i.e., there are no observations of different classes that are exactly equal one another. In the case of overlapping classes, a natural strategy adopted in many methods is to eliminate the overlapping elements from the data sets, or else expand the feature set to obtain separation.

Problems of the type described in (5)–(8) turn out to be very hard to solve (actually, they can be easily proved to be NP-hard). Moreover, real instances may have large size and, as confirmed by the experiments reported later, a fast heuristics needs to be used. Among different strategies, we propose to use a Greedy Randomized Adaptive Search Procedure (GRASP) (Feo & Resende, 1989; 1995) with some original modifications (among which, the use of a short memory). A GRASP meta-heuristic approach has been used also in Bermejo et al. (2011) in a FS context, to speed up the feature selection process in a wrapper approach. GRASP is a multi-start or iterative method, in which each iteration consists of two phases: construction of a solution and local search. The construction phase builds a solution x. Once x is obtained, its neighbourhood is explored by the local search until a local minimum is found. The best overall local optimum solution is kept as the result. The pseudo-code in Fig. 2 illustrates the main blocks of a generic GRASP procedure for minimization, in which MaxIt iterations are performed and Seed is used as the initial seed for the pseudo-random number generator.

An extensive survey of the literature on this class of meta-heuristics is given in Festa and Resende (2002, 2011).

Starting from an empty solution, the construction phase iteratively constructs a complete solution, one element at a time. At each iteration, the choice of the next element to be added is determined by ordering all the elements that can be added to the solution in a candidate list C with respect to a greedy function 
                        
                           g
                           :
                           C
                           →
                           R
                        
                      that measures the (myopic) benefit of selecting each element. The heuristic is adaptive because the benefits associated with every element are updated at each iteration of the construction phase, and thus reflects the changes brought on by the selection of the previous elements. The probabilistic component of GRASP is characterized by randomly choosing one of the best candidates in the list, but not necessarily the top candidate. The list of best candidates is called the restricted candidate list (RCL). In this context, a column covers a row of a binary matrix when that row exhibits a 1 in correspondence of that column. Since the selection involves candidate columns, the greedy function for a given column (say, column j) is based on the number of rows that have not yet been covered that are covered by column j.

As a countermeasure, we introduce 2 elements of short memory in the construction process:

                        
                           •
                           At each iteration, we memorize when each row has been covered (a row is cover at order h if it has been covered by the column selected at the h-th step of the solution construction process). Then, in each step of each iteration, we use the order derived in the previous iteration as follows: we consider all rows that have not been covered and have largest order. Among them one is picked at random; then, all columns that cover that row are placed in the RCL.

The selection of a column c from the RCL is done sampling from a weight distribution. Such weights are higher for columns that have appeared in one of the (so far) best solutions; namely, they are proportional to the ratio between the number of current best solutions in which a column appears and the total number of current best solutions.

As it is the case for many deterministic methods, the solutions generated by a GRASP algorithm are not guaranteed to be locally optimal with respect to simple neighbourhood definitions. Hence, it is always beneficial to apply a local search to attempt to improve a solution. A local search algorithm works in an iterative fashion by successively replacing the current solution by a better one in its neighbourhood. It terminates when no better solution is found in the neighbourhood. The neighbourhood structure
                     
                        N
                      for a problem relates a solution s of the problem to a subset of solutions 
                        
                           N
                           (
                           s
                           )
                        
                     . A solution s is said to be locally optimal if in 
                        
                           N
                           (
                           s
                           )
                        
                      there is no better solution in terms of objective function value. The key to success for a local search algorithm consists of the suitable choice of a neighbourhood structure, efficient neighbourhood search techniques, and the starting solution.

In this case, we adopt a local search that, starting from a feasible solution, tries to find a better one, i.e., a new set of columns with lower cardinality (removal of redundant columns) and/or corresponding to a higher coverage.

It is difficult to formally analyse the quality of solution values found by the GRASP methodology. However, there is an intuitive justification that views GRASP as a repetitive sampling technique. Each GRASP iteration produces a sample solution from an unknown distribution of all possible results. The mean and variance of the distribution are functions of the restrictive nature of the candidate list. For example, if the cardinality of the restricted candidate list is limited to one, then only one solution will be produced and the variance of the distribution will be zero. Given an effective greedy function, the mean solution value in this case should be of good quality, but probably suboptimal. If a less restrictive cardinality limit is imposed, many different solutions will be produced implying a larger variance. Since the greedy function is more compromised in this case, the mean solution value should degrade. However, by order statistics and the fact that the samples are randomly produced, the best value found should outperform the mean value. Indeed, often the best solutions sampled are optimal (see more details in Festa & Resende, 2002, 2011).

@&#RESULTS AND DISCUSSION@&#

In this section, we report the results of the experiments to assess the performances of our method with respect to other well-established Feature Selection Algorithms (FSAs) in combination with different state-of-the-art classifiers. In the experiments we test our algorithm on two-class simulated (Section 5.3) as well as real data sets from the life science domain (Section 5.4). Two-class problems have been adopted to better focus on the separation performances of the methods, bearing in mind that for solving multiclass problems we can adopt a simple iterative scheme.

We test the performances of IP-GRASP with respect to the other FSAs (i.e., Relief, CFS, SFS, SBE, and InfoGain, see Section 5.1) in terms of correct classification rate and maximum coverage when used in a cross-validation scheme by Nearest Neighbour (NN), Support vector Machine (SVM), and Decision Tree (J48) classifiers (outlined in Section 5.2).

In this section, we identify some established FSAs for which a fair direct comparison with the method proposed in this paper is possible: filter methods, such as Relief, Las Vegas Filter (LVF), FOCUS, Correlation-based Feature Selection (CFS), Sequential Forward Selection (SFS), Sequential Backward Elimination (SBE), and Information Gain (InfoGain). The FS methods used for comparisons have been tested using the Weka environment Ver. 3.6.9 (Hall et al., 2009).

As anticipated (see Section 2), these methods look at the intrinsic properties of the data set to evaluate the feature relevance, and are not based on the performances of a specific classifier to evaluate the features. Furthermore, several FSAs integrate also a classification step: these methods are called embedded. These methods are out of the scope of our work, but the reader may refer to (Carrizosa et al., 2008; Iannarilli & Rubin, 2003; Maldonado et al., 2014; Rubin, 1990) for an in-depth analysis. Similar studies and reviews have been performed in Bolón-Canedo et al. (2013) and Forman (2003), where the authors tested several wrapper, filter, and embedded methods on simulated and real data sets. The results are statistically validated and confirm that filters have good generalization abilities, as well as being faster than embedded and wrapper methods. The FSAs selected for comparison are briefly described in the following.


                        RELIEF is a randomized algorithm developed for two-class and multiple-class problems in Kira and Rendell (1992a). It relies on a statistical method that employs a set of features through few heuristics and assesses the feature relevance by weighting each feature according to a distance measure.


                        Las Vegas Filter (LVF) is a probabilistic approach developed by Liu and Setiono (1996). It randomly generates subsets of features and evaluates their relevance by a consistency-based metric. Specifically, two features are defined inconsistent if they have identical value on two or more samples that belong to different classes.


                        FOCUS (Almuallim & Dietterich, 1994) relies on an exhaustive search into the hypothesis space combined with a consistency measure to evaluate all features. Specifically, it comprehensively assesses all subsets of features (i.e., starting with the singleton set until all features are covered) and halts when a sufficiently consistent subset is found.


                        Correlation-Based Feature Selection (CFS) (Hall, 1999) looks for the best subset of features based on the redundancy value among the features. In particular, it evaluates the features according to their correlation degree within the same class and with different ones.


                        Sequential Forward Selection (SFS) (Devijver & Kittler, 1982) iteratively adds features to an initial empty set in order to improve, step by step, a given evaluation measure.


                        Sequential Backward Elimination (SBE) (Devijver & Kittler, 1982) works in a specular fashion compared with SFS. It sequentially removes features to an initial full set in order to improve, step by step, a given evaluation measure (i.e., in our case, the consistency).


                        Information Gain (InfoGain) (Hall & Smith, 1998) is an entropy-based feature evaluation method that assesses the information quantity retained by each feature.


                        Table 1
                        
                         summarizes the considered algorithms and highlights the type of search organization, the generation of successors, and the evaluation function for each one.

The quality of the features selected by IP-GRASP has been tested using the Weka implementations of three different supervised learning methods, whose correct recognition rates have been computed using a 10-folds cross-validation sampling scheme. The three classifiers and the parameters used are described below.

                           
                              (1)
                              
                                 Nearest Neighbour (NN) (Dasarathy, 1991) is the simplest possible classifier based on the metric structure of the feature space, and is thus adopted in order to minimize the bias introduced by the classification method in evaluating the FS algorithms. Given a training set of objects whose class is known, a new input object is classified by looking at its closest neighbours of the training set (e.g., using the Euclidean distance). We used the IBk NN implementation (Aha & Kibler, 1991), with the following settings: number of neighbours equal to 1, linear NN search algorithm, Euclidean distance.


                                 Support Vector Machines (SVM) (Cristianini & Shawe-Taylor, 2000) map the data in n dimensional vectors and build a separating hyperplane which maximizes the margin (i.e., the minimum distance between the hyperplane and the closest point in each class) between the data. Their strength relies in the ability to impose very complex non-linear transformation on the data, lifting the data in a new space where linear separation is easier to achieve. The experiments have been performed with Platt’s Sequential Minimal Optimization algorithm described in Keerthi, Shevade, Bhattacharyya, and Murthy (2001); Platt (1998), with the following settings: complexity parameter equal to 1 (soft margin), round-off error epsilon equal to 
                                    
                                       1.0
                                       ×
                                       
                                          10
                                          
                                             −
                                             12
                                          
                                       
                                       ,
                                    
                                  random seed equal to 1, tolerance equal to 
                                    
                                       1.0
                                       ×
                                       
                                          10
                                          
                                             −
                                             3
                                          
                                       
                                       ,
                                    
                                  quadratic kernel with cache size 250007.


                                 Decision Trees (J48) (Quinlan, 1993) are trees whose nodes are associated to a predicate representing the features of the objects in the data set. The values of these predicates are used to iteratively generate new nodes, while the leaves of the tree represent the class labels. Typically, decision tree classifiers rely on rules that create new nodes with the local objective of minimizing the class entropy. We used the Decision Tree J48 (Quinlan, 1996), with the following settings: minimum number of objects per leaf equal to 2, reduced error pruning with 3 folds enabled (1 for pruning and the rest for growing the tree), random seed equal to 1.

We have first considered a number of randomly generated test problems defined by binary features. The generation procedure has been driven by a number of parameters, listed in Table 2. We have considered 5 types of problems (referred to as A, B, C, D, and S), for each of which 10 random repetitions have been generated. Each problem is characterized by the number of elements in each of two classes (columns 2 and 3 of the table), by the number of binary features (column 4), and by the size of the logic DNF formula that decides whether an element belongs to Class 1 or Class 2. Such a formula is defined by the number of conjunctive clauses (column 5) and by the number of literals in each clause (column 6). In addition, we specify the support of the formula, that is, the dimension of a subset of the features from which the features are selected to define the literals in the clauses (column 8). Each generation is obtained with the following procedure. First, a subset of the features is selected at random to compose the support set. Then, from this set the features are selected at random to compose the required number of clauses of the required dimension, and the disjoint of such clauses forms the classification formula. Finally, elements are generated as random binary vectors of dimension equal to the original number of features, each feature having independent and uniform probability, and classified in Class 1 or Class 2 according to their evaluation over the classification formula. The random generation of the elements is iterated until both classes reach the required size.

Problems of types A and B are of manageable size, and differ only in the dimension of their support sets and of the classification formulas. Problems of types C and D are of larger size and also differ for the classification formulas. Some of the FS methods, in particular LVF and FOCUS, are not able to deal even with problems of types A and B. In these cases, a smaller test problem S was generated to complete the design of the experimental comparisons, where only 30 elements and 20 features are used.

Additional tests are run on two large-scale microarray data sets downloaded from ArrayExpress and Gene Expression Omnibus (GEO) repositories: Psoriasis and Multiple Sclerosis Diagnostic data. The data set of Psoriasis is released from the National Psoriasis Foundation and contains 54,613 gene expression profiles of 176 experimental samples (i.e., 85 healthy control samples and 91 diseased ones). The Multiple Sclerosis Diagnostic data set is provided by the National Institute of Neurological Disorders and Stroke (NINDS) and is composed of 22,215 gene expression profiles of 178 experimental samples (i.e., 44 healthy control samples and 134 diseased ones). All gene expression profile values have been normalized using the Affymetrix Expression Console software (ver 1.2) by the MAS5 algorithm. Both data sets are characterized by a very large number of features (the genes) and, comparatively, a small number of observations, a typical case in genomic data sets.

To obtain the binary matrix of model (8), we adopt a simple rule. Given two samples belonging to different classes (say, sample i and sample j), the coefficient 
                           
                              d
                              
                                 i
                                 j
                              
                              k
                           
                         is equal to 1 if the difference between the values of feature k in samples i and j is above a given percentage threshold. For the described experiments, a value of 20 per cent was adopted.

The experiments have been designed to test two hypotheses: first, that the model that we propose is meaningful and the features it extracts work well – and often better than other established FS methods; second, that the hard integer programming problems associated to the formulations of Section 3 can be solved efficiently by the IP-GRASP algorithm, where commercial solvers fail to find feasible solutions. Experiments were run on a 4-Core 3 gigahertz Intel I-7 processor with 24 gigabytes RAM and Linux Debian Kernel Version 2.6.26-2-amd64. We test the performances of the different methods by referring to two metrics. First, we evaluate how the selected features perform, in terms of correct classification rates (CC), when used in a 10-folds cross-validation scheme by Nearest Neighbour (NN), Support vector Machine (SVM), and Decision Tree (J48) classifiers (see Section 5.2).

Second, we verify how many features have been hit by the algorithm among those that compose the original model used to generate the data sets (maximum coverage). Although these two metrics should – and will – exhibit a good level of correlation, it is of interest to analyse both of them because the random generation of the data does not guarantees that the model used in generation is the only possible good model. In the tables, we refer as CCNN, CCSVM
                        , and CC
                        
                           J48 to indicate Correct Classification (or accuracy) of NN, SVM, and J48, respectively. The Maximum Coverage is indicated as MC.

As far as MC is concerned, it can be computed only on the simulated data sets. For this reason, the analysis of the results has a slightly different structure between simulated and real data sets.

An additional issue that needs to be addressed is related with the type of solution provided by the FS algorithm. In general, a FS method can provide two types of answers: either say what are the best β features for a fixed value of β, or identify a small list of features considered to be ideal and to list them. We consider preferable the first approach (model (8)), for the reasons discussed in Section 3. Nevertheless, one may also be interested in knowing the minimum number of features considered to be ideal for solving the feature selection problem without fixing a threshold (answer of the second type). The latter problem consists in the straight-forward adoption of the minimum test collection problem (see Section 3), that can be easily accommodated in the framework of IP-GRASP. Such a variant is implemented with the specific purpose of comparing the performances of our proposed approach also with respect to those FSAs (e.g., CFS) that do not use a fixed number of features.

Therefore, the experiments are organized as follows: we compare (i) the FS model described in Eq. (8) with methods that allow to specify the number of features, and (ii) the above-mentioned variant of the model (α fixed to 1) with those that do not. Methods that are able to provide both types of answers are compared in both settings.

For a fixed value of β we select the value 25. The choice stems from a battery of tests performed with increasing values of β (i.e., 5, 10, 20, 25, 35, and 50), which prove the value 25 as a good compromise among model size, execution time, and human readable classification models (extracted by supervised rule-based algorithms).

We adopt the Mixed Integer Programming Solver ILOG Cplex Version 12.1.0, referred to as MIP solver in the following, and report the results obtained for problems of types A and B in Table 3
                        . For these problems, we can directly compare our IP-GRASP with the commercial MIP Solver, which produces feasible solutions within reasonable time bounds. Considering the limited dimensions of the problems, we set a bound on the execution times of 300 seconds for both algorithms. Regarding IP-GRASP, we report the best solution obtained, represented by the value of α (αG
                        ); regarding the MIP solver, we indicate the best solution in terms of α (αMIP
                        ) and the associated gap. We see that the best solution is slightly better for the MIP solver, and the LP gap obtained by MIP solver is always very large (from 80 per cent to 50 per cent).

Problems of types C and D are larger in size and they are intractable for the MIP solver, while they are solved by the IP-GRASP. We impose a time bound of 1800 seconds, in which the MIP solver manages to determine only the initial LP relaxed solution and no feasible integer solution. For that reason, with types C and D we do not report detailed results.


                        Table 4
                         reports the result on the small size problem indicated with S (recall that the parameters describing the different test problems are available in Table 2). Such a problem is the only one for which results by LVF and FOCUS have been obtained within the 300 seconds time bound using the standard parameters setting of Weka. The table reports all methods where the number of features is minimized. The results presented are the average over 10 different instances of type S, obtained varying the random seed in the problem generation procedure.

IP-GRASP outperforms the other methods in terms of CCNN
                         and CCSVM
                        , exhibiting an average value of 0.90 and 0.85 with a very small standard deviation, respectively. Even in terms of CC
                        
                           J48, IP-GRASP reaches a comparable value with respect to the other FSAs (0.74, with the highest one being the 0.77 of SBE). As for the MC value, we see that the best value (0.62) is obtained by IP-GRASP and LVF. The performances all other methods are lower.


                        Tables 5
                         and 6
                         report on experiments where the four different types of problems were solved with the requirement of identifying the best feature set of dimension 25. The algorithms tested are IP-GRASP, Relief, SBE, SFS, and InfoGain. SBE is not able to find feasible solutions to the IP problem within the maximum running time for large problems (C, D), and thus the corresponding cells of the Table 6 are empty. The performances of the three classifiers (i.e., CCNN
                         for NN, CCSVM
                         for SVM, and CC
                        
                           J48 for J48) and MC are averaged over 10 different instances, and the corresponding standard deviation is reported, separately for medium sized problems (A,B) (Table 5) and large ones (C,D) (Table 6).

Again, we verify that IP-GRASP outperforms the other methods in terms of CCNN, CCSVM
                        , and CC
                        
                           J48 for medium size problems (0.82 of both CCNN
                         and CCSVM
                        , with the closest ones being the 0.77 of SFS and the 0.80 of the SBE, respectively). For what concerns the large size problems, IP-GRASP outperforms the other ones in terms of CCNN
                         (0.82, followed by 0.77 of SFS), while it is the closest one to the 0.79 of the InfoGain in terms of CCSVM
                         (0.77), and it follows the 0.78 of SFS in term of CC
                        
                           J48 (reaching 0.75). As far as the value of MC is concerned, we see that IP-GRASP outperforms the other ones on medium sized problems (0.75, with the closest one being the 0.62 of Relief), while its performance is worse than that of InfoGain.

In Tables 7
                         and 8
                        , we report the results of the FS methods that produce a solution where the number of features is kept small or minimized. Besides the IP model solved by the IP-GRASP, we have the three variants of CFS (BD, BW, FW), SBE, and SFS. The maximum running time was set to 300 seconds for problems A and B (Table 7) and to 1800 for problems C and D (Table 8). Results report the average and the standard deviations over 10 different instances generated at random. As mentioned, the SBE and also the CFS (with the BW search option) are not able to find feasible solutions to the IP problem within the maximum running time for large problems, and thus the corresponding cells of Table 8 are empty.

Again, IP-GRASP exhibits the best performances both in terms of CCNN
                         and MC for medium sized problems (average correct recognition of 0.79, and average maximum coverage 0.40); when turning to large problems, IP-GRASP still dominates the other methods in terms of CCNN
                        , and shows comparable performances with respect to SBE and SFS both in terms of CCSVM
                         and CC
                        
                           J48, while it is outperformed in MC by CFS (when using the BD and FW search options).

The results on the Psoriasis data set are listed in Table 9
                        , when the best β features have been identified for a fixed value of β (set to 25, top side of the Table 9), and when the number of features is kept small or minimized (bottom side of Table 9). We note that IP-GRASP performs best (100 per cent of accuracy) in both cases (
                           
                              β
                              =
                              25
                              ,
                           
                        
                        
                           
                              α
                              =
                              1
                           
                        ), when the NN classifier is used. With SVM and J48 classifiers, it has comparable performances with respect to the other FSAs.

The results on Multiple Sclerosis data set are listed in Table 10
                        , when the best β features has been identified for a fixed value of β (set to 25, top side of the Table 10), and when the number of features is kept small or minimized (bottom side of the Table 10). In both cases (β = 25, α = 1), IP-GRASP shows comparable performances w.r.t. the other FSAs. In particular, it outperforms the other ones in terms of CCSVM
                         by fixing β to 25.

The analysed methods have different levels of complexity, and their running time on the same instance may indeed vary significantly. We thus fix a solution time that we believe reasonable and evaluate the solutions that the different methods produce within that time, regardless of the portion used.

The main focus of the experiments here presented is not on the speed of the algorithms but rather on their ability to find good solutions within a time limit that appears to be reasonable for the type of applications considered. Nevertheless, we note that IP-GRASP is always fast in finding its best solution, and spends most of the available time in trying to improve it in finding equivalent ones. Similarly, most of the other methods are also fast and produce their best solution in small portions of the available time.

In Table 11
                         we report the maximum and the minimum number of iterations required by the IP-GRASP algorithm to reach its best solution, over the 10 different instances randomly generated for the four types of problems; in the same table we report the maximum time in seconds required to obtain the best solution. Table 12
                         symmetrically reports the max and min solution times of the other tested algorithms over the 10 random repetitions of the instances.

When turning to the experiments on real data (Section 5.10), we observe that for both data sets (i.e., Psoriasis and Multiple Sclerosis), SBE and CFS (with BW search option) algorithms are not able to find feasible solutions within the allowed maximum running time, and thus the corresponding cells of Tables 9 and 10 are empty. Furthermore, all the other FSAs require a very large amount of memory with respect to our IP-GRASP (i.e., approx. 8 times more than IP-GRASP). In conclusion, for validating IP-GRASP scalability, we can state that it is able to process data sets composed of hundreds of elements (in this work up to 400) and thousands (more than 50,000) of features providing a feasible solution in less than 30 minutes on a standalone workstation by using low amount of memory with respect to other feature selection algorithms.

@&#CONCLUSIONS@&#

This paper presents an approach to solve feature selection problems in supervised learning. The contribution of the paper is to be found in the use of integer programming models, and in a new randomized heuristics to solve them. Using a set of randomly generated test problems, we provide experimental evidence of the practical challenge of solving our models at optimality, showing that even with state-of-the-art commercial solvers and significant computing resources, we fail in finding solutions that can be proven to be optimal. On the other hand, we show that the feasible solutions identified with the ad-hoc GRASP here proposed are good in quality. The performances of the method are compared with those of several well-known FS algorithms in their established implementations, on the simulated instances and on two real data sets. Our method always provides solution of comparable quality, and often outperforms the others. In particular, it provides good solutions for the simulated instances with binary features and when the bias introduced by the classifier is kept at a minimum (i.e., the case of Nearest Neighbour classifier).

A distinguishing feature of the proposed model is to be found in its capability of incorporating, as linear constraints, additional knowledge to drive the selection of the features, e.g., requiring the covering of certain subsets of features in the solution, adding extra penalties for certain features, or controlling the error in the data by requiring larger separation thresholds for outliers.

The extension of the proposed randomized heuristic to accommodate the management of additional constraints in the model is the object of currently ongoing research; an interesting direction is indeed related to the search for multiple feasible feature subsets, which could be modelled with additional constraints that cut out the already found solutions.

@&#ACKNOWLEDGEMENTS@&#

The authors are extremely grateful to the editor and to the anonymous reviewers who have largely contributed to the final quality of the paper. The research described in this paper has been partially supported by the Flagship InterOmics project (PB.P05), the Epigenomics Flagship Project EPIGEN (PB.P01), the Italian Project PRIN 2012 (2012L783TW_006), and the CNR Premiale 2012 project MATHTECH (ICT.P11.005.001).

@&#REFERENCES@&#

