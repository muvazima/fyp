@&#MAIN-TITLE@&#Mid-level image representations for real-time heart view plane classification of echocardiograms

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Proposal of new mid-level representations for real-time heart view plane classification of 2D echocardiograms.


                        
                        
                           
                           Approach relies on bags of visual words with image sampling using large regions.


                        
                        
                           
                           Extensive set of experiments comparing the proposed method with existing descriptors.


                        
                        
                           
                           Evaluation considering real-time constraints, noise filtering, and different machine learning classifiers.


                        
                        
                           
                           Proposed approach is very fast to compute and consistently achieves accuracy above 90%.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Echocardiography

Feature extraction

Real-time systems

Image classification

Pattern analysis

@&#ABSTRACT@&#


               
               
                  In this paper, we explore mid-level image representations for real-time heart view plane classification of 2D echocardiogram ultrasound images. The proposed representations rely on bags of visual words, successfully used by the computer vision community in visual recognition problems. An important element of the proposed representations is the image sampling with large regions, drastically reducing the execution time of the image characterization procedure. Throughout an extensive set of experiments, we evaluate the proposed approach against different image descriptors for classifying four heart view planes. The results show that our approach is effective and efficient for the target problem, making it suitable for use in real-time setups. The proposed representations are also robust to different image transformations, e.g., downsampling, noise filtering, and different machine learning classifiers, keeping classification accuracy above 90%. Feature extraction can be performed in 30fps or 60fps in some cases. This paper also includes an in-depth review of the literature in the area of automatic echocardiogram view classification giving the reader a through comprehension of this field of study.
               
            

@&#INTRODUCTION@&#

Echocardiography plays an important role aiding cardiologists in heart analysis. It relies on the use of ultrasonic techniques that can capture information about the heart of a patient. The heart ultrasound images provide information about different anatomical aspects of the heart structures such as the position, size, and shape of the atrium and ventricles, and how they move. In an echocardiogram examination, the operator of an ultrasound device uses a probe to capture the heart images of a patient. Ultrasound devices capture “slices” of the heart, which are commonly named heart views. Those views depend on the position of the probe in the patient and the most common views are the parasternal long axis, parasternal short axis, and apical views. In each view, different heart structures can be observed and analyzed.

Automatic classification of echocardiogram ultrasound images has been studied recently in several aspects [1–10]. The most common task is the automatic classification of echo videos into the different heart views. The automatic classification has several applications. During an ongoing examination, automatically classifying the heart views under analysis makes it possible to label the images/videos as they are recorded, providing a facility for organization and management of echocardiogram videos. It can also help the operator for better probe positioning and even for training of new specialists. Knowing the heart view plane, even after the examination, can make it possible the retrieval and analysis of examinations according to the heart view [11,12]. Other possible use is when taking heart measures [13], like blood volume and size of cavities, which usually requires a previous manual indication of the heart view. Therefore, there are two main scenarios where the automatic recognition of heart views can be used: the first includes the categorization of pre-stored echo videos while the second aims at the real-time view classification, whereby the view categorization is performed during an examination. Efficiency constraints are not as important for the former as they are for the latter.

The main approaches used for automatic view plane classification of echocardiograms are based on extracting features from heart images (echo video frames) and using a machine learning scheme for learning and then predicting the view of a test echo video or image [1,3,4,6–9]. For feature extraction, some works point out that the direct use of traditional image descriptors usually employed for object and scene recognition may fail in the ultrasound scenario [6]. However, in the literature review that we present in the paper, we could notice a trend for using generic features for heart view classification, like GIST [8] and HOG [7]. In the current work, we show that despite the noise and contrast issues of ultrasound images, some traditional image representation approaches can be effectively used. Our proposed approach is based on the use of bags of visual words (mid-level features), which are widely used in the computer vision community for visual recognition [14–16].

We show experimentally on a dataset of more than 7500 frames (in 52 echo videos, captured by a device used in multiple configurations) how different descriptors perform. Considering the real-time requirement, we also evaluate the descriptors in resized versions of the dataset. An additional evaluation is also performed considering the use of noise filtering procedures. On top of that, we also show how the proposed mid-level representations perform with different machine learning classifiers (Support Vector Machines and Random Forests). We show that the proposed approach is robust to any of those transformations and to the different classifiers, being suitable for use under several different conditions.

The main contribution of this paper is the proposal of an efficient and effective approach for heart view classification that can be used both for pre-stored echo videos and for real-time applications. Another differential aspects of the paper are an evaluation of several image representation schemes for automatic classification of echocardiogram images/videos and an in-depth review of the literature detailing the main advances in the heart view classification task and contrasting the pros and cons of each approach.


                     Section 2 discusses approaches employed in the literature for automatic heart view classification, as well as existing image descriptors and the machine learning classifiers used in this paper. Section 3 introduces our proposed approach while Section 4 shows the experiments and the obtained results. Finally, Section 5 concludes the paper and delineates possible future work.

@&#RELATED WORK@&#

This section presents the advances in the literature of automatic view classification of echocardiograms. We also show a review of image descriptors in Section 2.2, which we used as baselines for our proposed approach in the experimental section. For details about echocardiography and the clinical heart view categorization, please refer to [17]. In Section 2.3, we also show a brief review of machine learning classifiers.


                        Table 1
                         summarizes the related work analyzed in greater details throughout this section. We show their pros and cons and present a summarized description of the approaches, the datasets and devices used, and the obtained results. In Table 1, we show only the information that was available by analyzing the papers where each approach was proposed. For instance, if we do not show the device used for capturing images or the time required for the method to run, it is because such information was not available.

Ebadollahi et al. [1] are among the first works to deal with view classification of echocardiograms. They point out that the spatial arrangement of the heart cavities is unique to each view and propose the use of constellation models for differentiating views. For classifying an echo video, energy vectors in relation to the models of each view are used with a multiclass Support Vector Machines (SVM) classifier. In a leave-one-out protocol, abnormal cases were used only for testing while normal ones were used also for training. If the chamber detector fails, their performance drops significantly.

Aschkenasy et al. [2] used multi-resolution spline filtering, where each image was classified independently by minimizing the mean absolute deviation (MAD) between two images. Elastic deformation and the deformation energy were used with linear discriminant analysis (LDA). Their dataset is composed of consecutive echocardiographic images recorded during daily clinical works (with different sonographers).

Otey et al. [3] used a hierarchical approach for classifying four heart views. They first differentiate between parasternal and apical views. For parasternal, they then classify as long or short axis. For apical, they further classify as two or four chambers. For feature extraction, they consider only pixels inside a mask (learned on training images) covering the fan area.

Zhou et al. [18] presented an approach based on multiple object detection. They manually defined templates based on the left ventricle (LV) orientation and size, which were used to align the data and reduce appearance variation. The end diastolic (ED) frame and its LV annotation were used to crop the template region. Classification is determined combining the results of all scanned subwindows on the ED frame. Their approach is almost real-time, taking about 1.5s to classify a sequence containing a full cardiac cycle (about 30 frames).

Park et al. [4] trained a LV detector for each of the four views considered. Their classification system performs: LV detection, global view classification using four multi-view classifiers, and final view classification by integrating the classification results. Their approach has the advantage of computing measures about the LV, providing feedback to the sonographer for probe adjustment. However, the system can fail if no LV is detected.

Roy et al. [19] classified echocardiogram videos in different levels of precision: views, states, and substates. Only a region of interest (ROI) automatically marked by their system is considered in each frame. Given a view sequence, they randomly selected five frames and classify each of them. Majority voting is used to classify the sequence. Their system is also able to classify heart states (systolic, diastolic) and substates (isovolumetric contraction, ejection, isovolumetric relaxation, rapid inflow/diastasis, fully expanded).

Snare et al. [5] used non-uniform rational B-spline (NURBS) and an extended Kalman filter to classify three apical views. They created models based on the heart structures present in each of the desired views. Classification considered a score measure based on the detection of each structure. Their system fails if the heart structure is not detected or if it is falsely detected.

Kumar et al. [6] used a spatiotemporal feature (fusing motion and intensity information) for classifying four and eight heart views. Videos are initially aligned, then motion information is extracted, and finally scale-invariant features are obtained from the motion images. Videos are classified according to a majority voting scheme based on frames.

Agarwal et al. [7] used Histogram of Oriented Gradients (HOG) [20] for classifying two heart views. They converted images onto polar coordinates and resized them to 124×64 pixels. HOG features were extracted from four non-overlapping blocks of each image, quantized into 18 orientation bins for each block, and concatenated to form a 72-d vector for each image. SVM was then used in cross-validation protocols.

Wu et al. [8] presented an incremental classification scheme for differentiating eight heart views. They used GIST [21] for feature extraction in images divided into 4×4 blocks, creating a 384-d vector for each image. Multiclass SVM is used incrementally: if the class probability is above a threshold, classification is finished, otherwise, the next frame is used to construct a new feature as the convex sum of the kernels.

Qian et al. [9] employed bag of visual words (BoVW) based on spatiotemporal features. 3D SIFT is extracted in regions detected by a cuboid detector. Sparse coding is used in a codebook of 4000 visual words. The echocardiogram volume is split into 12 regions and, for each region, max pooling is used to compute the final video feature vector of 48,000 dimensions (4000×12).

We have used several texture and shape descriptors as baselines which have shown good results for texture representation [22] or which have already been used for ultrasound image representation [7,8] in the literature. Many of the descriptors below were never used for heart view classification.


                        SASI: Statistical Analysis of Structural Information (SASI) [23] is based on a set of sliding windows, which are covered in different ways. SASI was chosen due to its good ability for texture discrimination in [22].


                        LAS: Local Activity Spectrum (LAS) [24] captures the spatial activity of a texture in the horizontal, vertical, diagonal, and anti-diagonal directions separately. It presented good results in the experiments of [22] in terms of both effectiveness and efficiency.


                        Unser: Unser [25] extracts information similarly to a gray-level co-occurrence matrix. It computes histograms of sums and differences between neighboring pixels. We chose it because of its efficiency and compact representation [25].


                        GIST: GIST [21] provides a global holistic description representing the dominant spatial structure of a scene. GIST is popularly used for scene representation [26] and was successfully used by Wu et al. [8] for heart view classification.


                        HOG: Histogram of Oriented Gradients (HOG) [20] computes histograms of gradient orientations in each position of a sliding window. HOG was used by Agarwal et al. [7] for heart view classification. The most usual window size for HOG is of 8 × 8 pixels. Here, however, we have used a window of size 80×80 pixels in order to control the size of the final feature vector to about 2000 dimensions. Different sizes were considered but without significant difference.


                        BoVW: Bag-of-Visual-Words (BoVW) descriptors compute statistics about the occurrences of texture patterns, based on quantized local features. BoVW descriptors are the basis for our proposed approach (see Section 3), however, the ones used as baselines are based on sparse sampling. The proposed approach uses dense sampling with large regions.

Sparse sampling refers to the use of interest-point detectors such as the Harris–Laplace detector [27]. Those kinds of detectors analyze the image for finding regions with high differences of contrast (e.g., edges and corners). As low contrast and noise are usually problems of ultrasound images, those detectors could provide poor performances for heart view classification. However, in the experiments, we show that some configurations of BoVW descriptors based on sparse sampling are very accurate (obtain classification accuracy above 90%).

BoVW descriptors are used in some related works [9], but not in the same way we are using here. In [9], their BoVW descriptors consider motion information.

Our implementation of the BoVW descriptors used as baselines follows most of the configurations evaluated for the proposed approach. However, for pooling, besides testing average and max pooling [15], combined or not with spatial pyramids (SPM) [28], we also tested WSA (word spatial arrangement) [29], a spatial pooling approach which was proposed for sparse-sampling cases. WSA encodes the relative spatial position of visual words in the image space, not encoding the frequency of occurrence of visual words. Thus, only spatial information is taken into account by WSA.

In Table 2
                        , we show the dimensionality of each descriptor.

In a typical classification setting, we receive a set of training vectors 
                           
                              
                                 x
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 x
                              
                              
                                 n
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                        , each belonging to one of two classes, denoted by the respective labels 
                           
                              
                                 y
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 y
                              
                              
                                 n
                              
                           
                           ∈
                           {
                           −
                           1
                           ,
                           +
                           1
                           }
                        . The task is then to find a function 
                           f
                           :
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                           →
                           {
                           −
                           1
                           ,
                           +
                           1
                           }
                         that accurately predicts the label when presented with a new sample 
                           
                              
                                 x
                              
                              
                                 t
                              
                           
                         
                        [30].

In the classification context, Support Vector Machines (SVMs) have been used in many different problems including in some previous work related for heart view classification of echocardiograms [1,3,6–9]. SVM's idea is to find the maximum-margin hyperplane (
                           w
                           ,
                           b
                        ) in a high-dimensional space 
                           H
                         that accurately separates the positive instances from the negative ones. Given a separating hyperplane (
                           w
                           ,
                           b
                        ), the support vector classifier is given by 
                           
                              
                                 
                                    
                                       f
                                    
                                    
                                       w
                                       ,
                                       b
                                    
                                 
                                 (
                                 x
                                 )
                                 =
                                 
                                    sgn
                                 
                                 (
                                 〈
                                 w
                                 ,
                                 Ψ
                                 (
                                 x
                                 )
                                 〉
                                 +
                                 b
                                 )
                                 ,
                              
                           
                        where 
                           Ψ
                           :
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                           →
                           H
                         is a kernel function that transforms the input data onto a high-dimensional feature space, and b is a parameter that indicates the offset of w with respect to the origin of 
                           H
                        . The transformation Ψ is implicitly defined by a kernel function, so that 
                           〈
                           Ψ
                           (
                           a
                           )
                           ,
                           Ψ
                           (
                           b
                           )
                           〉
                           =
                           K
                           (
                           a
                           ,
                           b
                           )
                        .

Although there are different formulations for SVM, here we consider the standard formulation (C-SVM). This algorithm finds w and b by solving the following quadratic problem:
                           
                              (1)
                              
                                 
                                    
                                       
                                          minimize
                                       
                                       
                                          w
                                       
                                    
                                 
                                 
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 ∥
                                 w
                                 
                                    
                                       ∥
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       C
                                    
                                    
                                       n
                                    
                                 
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                                 
                                    
                                       ξ
                                    
                                    
                                       i
                                    
                                 
                                 .
                                 subject
                                 
                                 to
                                 
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 〈
                                 Ψ
                                 (
                                 
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                 
                                 )
                                 ,
                                 w
                                 〉
                                 +
                                 b
                                 )
                                 ≥
                                 1
                                 −
                                 
                                    
                                       ξ
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       ξ
                                    
                                    
                                       i
                                    
                                 
                                 ≥
                                 0
                                 ,
                              
                           
                        where ξ
                        
                           i
                         with i=1,…,n, are slack variables and 
                           C
                           ≥
                           0
                         is a parameter that balances the amount of slack (misclassifications) and the size of the margin.

For multiclass classification, multiple binary SVM classifiers are used considering the one-vs-one (OVO), one-vs-all (OVA) or different combination approaches. In our work, we use the SVM implementation of libSVM [31] in its basic form, which consists in a one-vs-one approach by training a linear binary SVM classifier for each pair of training classes. Then, in prediction phase, a voting scheme is used and the predicted class is the one which receives the majority of votes.

Although SVMs have presented good results for different applications thus far, recent studies point that Random Forest classifiers are most likely to perform equally well or even better for many situations [32].

Random forest is a machine learning classifier that relies upon an ensemble of simple decision tree classifiers assuring that each Decision Tree does not overfit the training set. Its two most important features are the use of the out-of-bag error as an estimate of the generalization error and the measuring of variable importance through permutation. The random forest training procedure uses bootstrap aggregation (bagging) to generate the different learners (trees). We start with a sample of training vectors 
                           
                              
                                 x
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 x
                              
                              
                                 n
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                         with responses 
                           
                              
                                 y
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 y
                              
                              
                                 n
                              
                           
                           ∈
                           N
                        , and repeatedly select a random sample with replacement of the training (referred to as 
                           
                              
                                 
                                    X
                                 
                                 
                                    b
                                 
                              
                           
                           ⊂
                           X
                           ,
                           
                              
                                 
                                    Y
                                 
                                 
                                    b
                                 
                              
                           
                           ⊂
                           Y
                        ). Afterwards, we fit K trees to these samples and perform majority voting in the end for pointing out the most likely class of an input example 
                           
                              
                                 x
                              
                              
                                 t
                              
                           
                        . The number of trees K is a free parameter.

A random forest slightly differs from this original bagging formulation by one aspect: it uses a modified tree learning algorithm that selects, at each tree creation procedure in the learning process, a random subset of the features. We refer to this process as “feature bagging.” Typically, we create each tree by sampling 
                           
                              
                                 d
                              
                           
                         features. In our work, we use the random forest implementation of R, which is also recommended by [32].

Given their historical good performance for different problems, here we decided to evaluate the SVM and Random Forest classifiers with the proposed mid-level representations. As they rely on different rationales (SVMs are margin-based classifiers while Random Forests are based on bootstrap aggregation and random sampling), the classification performance when using the proposed descriptors may vary depending on the classifier.

This section describes the proposed approach for real-time heart view classification of echocardiogram ultrasound images.
                        1
                     
                     
                        1
                        The method proposed herein is patent pending under the application number BPO BR 10 2014 011059 3 filed on May 7th, 2014: “Método para Classificação Automática de Visões do Coração a Partir de Ecocardiogramas”.
                      The approach comprises mid-level representations based on the widely used visual dictionary model, describing images by statistical information of visual word occurrences (bags of visual words – BoVW).


                     Fig. 1
                      shows the proposed approach's scheme. More specifically, Fig. 1(b) shows BoVW vector computation, which constitutes of dense sampling with large regions, region description with a local invariant descriptor, coding, and pooling. Next, we describe each of these steps.

Dense sampling is an approach for detecting regions of interest in images without looking at their content. Fig. 2
                         shows two common ways for dense sampling an image. We decided to use dense sampling specially because of its simplicity and its capability of detecting interest points in every region of an image. Even in cases of low contrast, an issue that potentially occurs in ultrasound images and directly affects interest-point detectors [33], dense sampling detects regions to be characterized.

As we show in Section 4.3, we tested different scales for the sampled regions and the best results were obtained by large representative regions (low dense), resulting in images being sampled by very few regions. That is an important solution for real-time applications: the fewer the regions, the shorter the processing time. The use of large regions is better probably because the heart views considered herein are different globally (see Fig. 3
                         and Section 4.1). Another interesting aspect of using large regions refers to the fact that those regions sometimes comprise whole heart structures, e.g., atrium and ventricles.

As our dense sampling implementation relies on the software of van de Sande et al. [14], the selected regions during sampling are overlapped Gaussian circles (more importance for central pixels, less for peripherals). According to the documentation of the referred software, the scale parameter for the circles corresponds to the Gaussian filter sigma. The dense sampling obtains N regions from an input image.

Given the N regions obtained by dense sampling, we use a local invariant image descriptor to characterize each of them capturing the most important cues they have. This results in a set of feature vectors 
                           X
                           =
                           {
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                           }
                         per image, where 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                        , 
                           i
                           ∈
                           {
                           1
                           ..
                           N
                           }
                        , and d is the feature vector dimensionality.

In our approach, we have used Scale Invariant Features Transform (SIFT) [34], as it is the most popular descriptor used in similar cases nowadays. Although SIFT was used, we believe that the impact of using similar descriptors, like Speeded Up Robust Features (SURF) [35] or others alike, is minimum.

When creating the visual dictionary, we quantize the 
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                         feature space, usually, using a subset of the training feature vectors. The visual dictionary can be seen as a set of image regions which represent important elements of the heart, which will be important for distinguishing the views. More formally, a visual dictionary can be defined as 
                           C
                           =
                           {
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                              
                           
                           }
                         where w
                        
                           i
                         is the feature vector of visual word i, 
                           i
                           ∈
                           {
                           1
                           ..
                           k
                           }
                        , and k is the dictionary size.

An effect of the quantization of the 
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                         feature space is the reduction of the specificity of the feature vectors. The more quantized the space, the more generic the description. This is related to the dictionary size: larger dictionaries mean less quantization, while smaller ones, more quantization.

By analyzing the ultrasound images visually, we could observe that even in their global aspect, they differ among views. We can see this by looking at the average images of each view in Fig. 3, Section 4.1, for example. Therefore, more quantized spaces (smaller dictionaries) should be more promising, as they provide a more general representation.

For implementing the feature space quantization, clustering techniques are usually employed, then each cluster represents a visual word. k-means is commonly used, however, given the curse of dimensionality, a simple random selection of vectors can provide dictionaries of similar quality [36,37] at much lower cost. On one hand, for high-dimensional feature spaces, k-means is not recommended as it is more expensive. On the other hand, in cases of small dictionaries (less than 500 visual words), the random selection of points can be deficient, as there is a greater chance of selecting points only from one specific area of the feature space. For larger dictionaries, this chance is smaller. Thus, to avoid this effect, it is recommended the use of k-means for small dictionaries. In our implementation, we have used a simple random selection of points in the feature space, even for small dictionaries, as it is much more efficient. In those cases (small dictionaries), different random dictionaries may provide different representation qualities.

After creating the dictionary 
                           C
                        , the description set 
                           X
                         of the regions of interest of an image must be encoded appropriately in the quantized space. One can simply assign to each feature vector the id of the visual word (cluster) where it falls in the quantized feature space (hard assignment). However, in high-dimensional spaces, points tend to be in the frontier of several clusters (codeword uncertainty [16]), thus, ignoring the neighboring clusters of a point discards information about the region description. Soft assignment is usually used in such cases [16,38,39]. This coding scheme considers neighboring clusters of a given feature vector in the quantized feature space and is more robust to the effects of poor quantization steps and to large dictionaries. We implemented the codeword uncertainty scheme proposed in [16] to obtain the coding vector 
                           
                              
                                 α
                              
                              
                                 i
                                 ,
                                 j
                              
                           
                         for a region 
                           i
                           ∈
                           {
                           1
                           ..
                           N
                           }
                        :
                           
                              (2)
                              
                                 
                                    
                                       α
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             K
                                          
                                          
                                             σ
                                          
                                       
                                       (
                                       D
                                       (
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             w
                                          
                                          
                                             j
                                          
                                       
                                       )
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             l
                                             =
                                             1
                                          
                                          
                                             k
                                          
                                       
                                       
                                          
                                             K
                                          
                                          
                                             σ
                                          
                                       
                                       (
                                       D
                                       (
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             w
                                          
                                          
                                             l
                                          
                                       
                                       )
                                       )
                                    
                                 
                                 ,
                              
                           
                        where 
                           j
                           ∈
                           {
                           1
                           ..
                           k
                           }
                        , v
                        
                           i
                         is the feature vector of the i-th region, w
                        
                           j
                         is the vector corresponding to the j-th visual word, 
                           
                              
                                 K
                              
                              
                                 σ
                              
                           
                           (
                           x
                           )
                           =
                           
                              
                                 
                                    1
                                 
                                 
                                    
                                       
                                          2
                                          π
                                       
                                    
                                    ×
                                    σ
                                 
                              
                           
                           ×
                           
                           
                              exp
                           
                           (
                           −
                           
                              
                                 
                                    1
                                 
                                 
                                    2
                                 
                              
                           
                           
                              
                                 
                                    
                                       
                                          x
                                       
                                       
                                          2
                                       
                                    
                                 
                                 
                                    
                                       
                                          σ
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           
                           )
                        , and 
                           D
                           (
                           a
                           ,
                           b
                           )
                         is the distance between vectors a and b. The σ parameter indicates the variance of the Gaussian function: the higher the value, the larger the number of neighboring clusters considered. In our experiments, we have used σ=60 and the Euclidean distance for 
                           D
                           (
                           a
                           ,
                           b
                           )
                        .

The i-th image region is represented by a k-dimensional coding vector 
                           
                              
                                 α
                              
                              
                                 i
                                 ,
                                 j
                              
                           
                        , 
                           j
                           ∈
                           {
                           1
                           ..
                           k
                           }
                        . Thus each image has N coding vectors.

The coding vectors are finally pooled into a single feature vector h representing the image [15]. One can pool by summing all the visual word activations in the image and normalizing by the number of points in the image (average pooling). Another alternative, with better results in the literature of image classification, is max pooling [15]. Max pooling considers only the maximum activation of each visual word in the image and can be defined as [15]
                        
                           
                              (3)
                              
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 =
                                 
                                    
                                       
                                          max
                                       
                                       
                                          i
                                          ∈
                                          N
                                       
                                    
                                 
                                 
                                 
                                    
                                       α
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                              
                           
                        where 
                           
                              
                                 α
                              
                              
                                 i
                                 ,
                                 j
                              
                           
                         is obtained in the coding step (by Eq. (2)), N is the number of regions in the image, and 
                           j
                           ∈
                           {
                           1
                           ..
                           k
                           }
                        .

Therefore, the final image feature vector h has dimensionality k and has statistical information about the visual word occurrences in the image.

For instance, if h is generated by max pooling, h has the maximum activation of each visual word in the image.

Considering that we are using large regions in the dense sampling and so in the visual codebook, our final feature vector h approximately corresponds to the activations of heart structures in each image. This can give us a “higher-level” representation of the echo frames.

The use of spatial pooling approaches is also interesting for enriching the representation [29,28]. Spatial Pyramids (SPM) [28] are commonly used for that. They are based on hierarchically splitting the image into rectangular regions and by computing one BoVW for each region. At the end, BoVW are weighted and concatenated to form the image feature vector h. Spatial Pyramids are very simple to compute and they can be used with other pooling strategies, like average and max pooling. However, the feature vector is significantly larger than the ones computed by non-spatial pooling approaches. For instance, for a pyramid level of 2, the feature vector is 21 times larger than a vector resulting from a simple max pooling. The impact of larger feature vectors (higher dimensional spaces) is an increase in learning and classification times.

In our approach, as we use large regions in the dense sampling, the impact of Spatial Pyramids is small. However, for denser sampling, Spatial Pyramids are crucial for higher accuracies, specially when used with max pooling.

@&#EXPERIMENTS@&#

In this section, we evaluate the proposed approach in terms of effectiveness and efficiency, comparing it with existing image descriptors. We start by presenting the dataset and the classification protocol used. Then, we present the evaluation of two important elements of the proposed approach: the dense sampling region size and the codebook size. Next, we show the comparison of our mid-level representations with the baselines presented in Section 2.2 using the images as they were acquired by the ultrasound device. Additional experiments were performed in resized versions of the dataset, aiming at reducing the feature extraction time and evaluating the robustness of the methods to such transformations. We then show experiments considering the use of noise filtering aiming to explore whether or not noise significantly influences the classification process. And finally, we show experiments evaluating different machine learning classifiers.

The dataset used in our experiments is composed of 52 transthoracic (TTE) echo videos comprising 7527 frames in BMP format with resolution of 832×540 pixels (mostly healthy adult hearts). The following heart views are used: parasternal long axis (PLA), parasternal short axis mid-left ventricle (PSA_MID), apical two-chamber (A2C), apical four-chamber (A4C). Each video refers to only one view. The images in the dataset were captured by a Samsung Medison EKO 7 device in different configurations using a phased array transducer in B-mode (no dopler). Most of the images were obtained using the Cardiology configuration while others use the Emergency Room (ER) configuration. ER images are usually inferior to Cardiology but there is no fundamental difference between them. We observed the following differences among the echo videos:
                           
                              •
                              misalignment of the fan area (wider or narrower areas and small rotation),

differences in contrast and in noise patterns,

and differences in color tone (grayish and yellowish aspects).


                        Fig. 3 presents the number of videos and images of each view, as well as their average images. We can see that the views are visually different, even considering their global aspect. We have also analyzed the differences among the videos of each view. Fig. 4
                         shows the average images of the seven videos from view A2C. We can see that, although there is a common visual pattern in all images, the edges and other structures have a large variation.

All the frames of one echo video per view are used for testing (i.e., one video per view for testing). For the remaining frames (i.e., the frames of the training videos), we randomly selected nTrain frames per view for training (independent of the video). This guarantee a balanced training set. As we are evaluating four views, we will always have 
                           4
                           ×
                           nTrain
                         frames for training. We varied nTrain from 5 to 1000 frames. Given the random parts of the protocol, everything is run 100 times and the average classification accuracies are considered, as well as the confidence intervals (95% of confidence) based on the 100 runs. In each run i, we compute the accuracy per class c as 
                           
                              
                                 acc
                              
                              
                                 i
                              
                              
                                 c
                              
                           
                           =
                           
                              
                                 X
                              
                              
                                 Y
                              
                           
                        , where X is the number of correctly classified samples of class c and Y is the total number of samples of class c in the test set. The average accuracy for run i is then computed as 
                           
                              
                                 acc
                              
                              
                                 i
                              
                           
                           =
                           
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          c
                                          =
                                          1
                                       
                                       
                                          
                                             
                                                N
                                             
                                             
                                                c
                                             
                                          
                                       
                                    
                                    
                                       
                                          acc
                                       
                                       
                                          i
                                       
                                       
                                          c
                                       
                                    
                                 
                                 
                                    
                                       
                                          N
                                       
                                       
                                          c
                                       
                                    
                                 
                              
                           
                        , where N
                        
                           c
                         is the number of classes. Then, the average accuracy among the 100 runs is computed 
                           
                              
                                 Acc
                              
                              
                                 avg
                              
                           
                           =
                           
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          100
                                       
                                    
                                    
                                       
                                          acc
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    100
                                 
                              
                           
                        .

We used Support Vector Machines (SVMs) with the linear kernel (C=1.0). The times were measured in a desktop computer with Intel i7-3770 CPU@3.40GHz with 8GB of memory. For low-level feature extraction of BoVW descriptors, we used the software from van de Sande et al. [14] version 4.0, which uses parallelization but we did not use the GPU implementation. Other steps of the BoVW computation were implemented in C. The global descriptors SASI, LAS, and Unser were implemented in C according to [22]. GIST implementation is the one used in [26] with the parameters discussed therein.
                           2
                        
                        
                           2
                           
                              http://lear.inrialpes.fr/software (as of October 22th, 2014).
                         HOG implementation came from VLFeat [40].

We decided to classify images instead of videos, because, in a real-time scenario, we should be able to classify an ongoing examination on-the-fly, that is, we cannot wait to have the complete video for performing the classification. We know that even in that case, we could use motion information to help classification, but we decided to work only with static information from isolated frames.

One important parameter of the proposed approach is the size of the dense sampling region. As we explained in Section 3.1, the use of large regions obtained the best results. To show more precisely the impact of the region size in dense sampling, we evaluated regions varying in 6, 12, 25, 50, 80, 100, 120, and 150 pixels of radius. For the smaller regions, we had also to be worried about the regions completely out of the fan area in the ultrasound images. In such cases, we had to remove the black regions after dense sampling. To also avoid cross effects related to dictionary size and pooling, we considered dictionaries of 100 and 1000 visual words as well as avg and max pooling.

Another important aspect related to the region size is the feature extraction time. Therefore, we also measured the extraction time per image.

Results are presented in Fig. 5
                        . We can see in Fig. 5(a) that the highest accuracies are obtained for the size of 120 pixels, which is very large in comparison to the image size, resulting in very few regions detected per image. In Fig. 5(b), we note that as the region size increases, the extraction time decreases very fast. For regions larger than 100 pixels, the extraction time is below 0.2s per image.

As a conclusion, the best region size for dense sampling in the proposed approach is 120 pixels, resulting in very few regions per image. Such large regions may comprise whole heart structures, as we show in Fig. 6
                        . In Section 4.5.2, we show how to define the region size based on the resolution of the input image.

Choosing the appropriate visual dictionary size is a key challenge for BoVW-based approaches. We evaluate this factor both for our proposed BoVW configuration based on low dense sampling and for the BoVW based on sparse sampling.


                        Fig. 7
                         presents the average classification accuracies of 100 runs of the classification protocol comparing the results for each pooling method when several different sizes are used for the codebook. Fig. 7(a) has the results for our proposed BoVW descriptors. We can see that the best codebook size has around 100 and 200 visual words, independently of the pooling method, and the differences are statistically insignificant or very small comparing to the other sizes (except for 1000 visual words, which is worse). The analysis considered the intersection or not of confidence intervals. This is a good behavior, because we can keep the representation more compact without significant loss of accuracy.

In Fig. 7(b), contrasting to the behavior of our proposed BoVW, we can see that the BoVW descriptors based on sparse sampling have different behaviors depending on the pooling strategy used. Average pooling and its version with spatial pyramids (avgSPM), for instance, are better with smaller codebooks. AvgSPM, in fact gets worse as the codebook increases. This is the opposite behavior of max pooling, which gets better with more visual words. MaxSPM, however, stabilizes with more than 100 visual words. WSA has similar results, independently of the codebook size.

Considering efficiency, we decided to not evaluate the BoVW descriptors based on sparse sampling in larger dictionaries, as this impacts in the classification time.

The results presented in the following sections consider our proposed method using a codebook of 100 visual words. BoVW descriptors based on sparse sampling are used with both 100 and 1000 visual words, depending on the pooling method used: avg, avgSPM, and WSA with 100 visual words, and max and maxSPM with 1000.

@&#RESULTS@&#

We first show the results of the descriptors in the original dataset (images as they were acquired by the ultrasound device). Next, we show the results after downsampling and after noise filtering. And then, we show how the proposed descriptors perform with different machine learning classifiers. We selected only the best training set size (nTrain) for each descriptor to show here. In many cases, increasing the training set size (
                           nTrain
                           >
                           100
                        ) does not represent considerable increase in accuracy.

To clarify the differences between the several parameters of the BoVW descriptors that were evaluated, we use the following acronyms for them: P
                        
                           s
                        
                        
                           k
                        , where P refers to the pooling strategy (average [avg], max, average or max with spatial pyramids [avgSPM, maxSPM], and WSA), s is the sampling scheme (sparse [S] or dense [D]), and k is the codebook size. For example, 
                           
                              
                                 maxSPM
                              
                              
                                 100
                              
                              
                                 D
                                 60
                              
                           
                         refers to a BoVW based on max pooling with spatial pyramids on a codebook of 100 visual words which were obtained from quantized dense features (60 pixels of radius for each region).

The results presented in Fig. 8
                            show that our proposed approach (represented by 
                              ⋆
                           ) is at the same time effective and efficient. Feature extraction of an image can be performed in 0.17s, and average accuracy is above 92%. Sparse sampling BoVW descriptors (
                              ⋄
                            and ) are also very effective, but they are computationally slower (more than 5s). Some global descriptors (
                              ○
                           ) are very fast (Unser 0.04s, LAS 0.05s, HOG 0.05s), but their accuracy is low. Results here consider SVM as classifiers.

We also performed a statistical analysis to verify the differences in classification accuracy of all the descriptors tested. For the statistical tests, we used the Pairwise Wilcoxon Rank Sum Test, which calculates comparisons between group levels with corrections of p-values for multiple testing. We used the Bonferroni correction of p-values. Each comparison of two methods considers 100 runs (executions) with different training/testing sets. In Table 3
                           , an arrow indicates a p-value lower than 0.05 (95% confidence level) and it points in the direction of the best method when comparing two methods (e.g., SASI outperforms LAS with statistical difference, p-value 
                              <
                              0.05
                           ).

The tests mainly show that: (1) the global-wise methods are worse than local ones as LAS, Unser, GIST, and HOG methods are outperformed by the other methods and (2) the proposed mid-level representations (BoVW low dense) are really effective as they outperform many counterparts (i.e., most of the arrows are pointing to our BoVW methods).


                           Fig. 9
                            shows the average confusion matrices for the four pooling methods tested with our method. We can see, for instance, that view A2C is rarely confused with other views. View A4C is sometimes confused with PLA or PSA_MID. PSA_MID was the most difficult (confusion varies depending on the pooling method), although its accuracy was close or above 90%. Spatial Pyramids increase the rate for view PSA_MID in relation to the pooling versions without them. The method 
                              
                                 
                                    maxSPM
                                 
                                 
                                    100
                                 
                                 
                                    D
                                    120
                                 
                              
                           , for instance, has accuracy per class above or equal to 94%. A small confusion of around 3% happens between classes A4C and PLA; and around 4% between classes A2C and PSA_MID.

We also computed the receiver operating characteristic (ROC) curves for a random run of our approach (not the average of 100 runs). The ROC curves can help understanding the errors when the approach is applied in a real situation. As ROC curves are usually employed for binary problems, we computed one ROC for each binary classifier (i.e., each combination of two classes at a time of the four-class problem we deal with in this paper). This is possible to accomplish when using SVMs, for instance, which naturally builds its multi-class predictions based on combinations of two class problems known in the literature as class binarization [41]. The SVM implementation of libSVM we are using deploys such class binarization by means of the one-vs-one approach, resulting in a binary classifier for each pair of training classes. As our problem has four classes, we end up with six binary classifiers. Fig. 10
                            shows the ROC curves for each pooling method along with the corresponding confusion matrices. In each case, we also computed the mean ROC curve of the six classifiers (black line) with its area under the curve (AUC). We can see that, in some cases of the selected run, the errors are higher than the average case, such as in the confusion matrices of 
                              
                                 
                                    avg
                                 
                                 
                                    100
                                 
                                 
                                    D
                                    120
                                 
                              
                            and 
                              
                                 
                                    avgSPM
                                 
                                 
                                    100
                                 
                                 
                                    D
                                    120
                                 
                              
                            (classes PLA and A4C) or 
                              
                                 
                                    maxSPM
                                 
                                 
                                    100
                                 
                                 
                                    D
                                    120
                                 
                              
                            (classes A2C and PSA_MID). For instance, in the case of the large confusion between classes PLA and A4C of 
                              
                                 
                                    avgSPM
                                 
                                 
                                    100
                                 
                                 
                                    D
                                    120
                                 
                              
                           , we believe that the reason is that the testing video has many frames with high presence of noise, compromising the viewable structures that differentiate such views. However, the ROC curves still have a high area under the curve showing the high effectiveness of the proposed classification approach independent of any operation point chosen in the curve. The final classification may not be directly viewable from the ROC curves of the intermediate binary classifiers, because the final classifier decision depends on the majority voting of the individual binary classifiers. This is also interesting as the OVO approach used in SVM also serves as an error correcting scheme for small mistakes done by individual classifiers. For example, the binary classifier A4C-vs-PSA_MID may confuse the samples of these two classes, but when the samples of such classes are confronted with other classes in the other binary classifiers, they are correctly classified. Thus, in the majority voting scheme of libSVM, the final classification is not affected by some bad intermediate binary classifiers.

Given that our dataset has images with relatively high resolution (832×540 pixels), one could argue that we could perform some adjustments in the images before processing them. Therefore, we have applied downsampling aiming at reducing the extraction times.

Considering a video in 30 frames per second (fps) and a real-time classification system, we would need to process 1 frame at each 0.033s. For 60fps videos, 1 frame should be processed at each 0.017s. Aiming at reducing the extraction time for helping the descriptors to achieve real-time performance, we performed image downsampling. It is worth noting that even the global descriptors were not able to process one image in less than 0.033s. BoVW based on sparse sampling, specially, were very far from this real-time constraint.

For large-scale classification experiments, Perronnin et al. [42] suggested to resize images to have at most 100 thousand (100k) pixels. Additionally to this image resolution, we also resized our images to 50k 25k, 5k, and 1k pixels. Table 4
                            shows the downsampling schemes used and their corresponding image resolutions. Table 4 also shows the size of the sampling region in our low-dense sampling scheme, as it is adjusted according to the image resolution for keeping very few regions per image. We can observe that the region size has around 15% of image width and around 23% of image height, resulting in at most 15 regions per image in our experiments. Therefore, if a dataset with different image resolutions is used and resizing is not possible, one can check Table 4 to adjust the size of the sampling region according to the image resolution.

We evaluated the image resize factor regarding both efficiency and effectiveness. Fig. 11
                            shows the average accuracies for BoVW descriptors in the resized versions of the dataset. We can see that the variation in accuracy is small when using the proposed BoVW descriptors based on low dense sampling. However, there is more variation for BoVW descriptors based on sparse sampling. In the size of 5k, for instance, where the images are very small, our BoVW method has average accuracy around 95% (for both avg and max pooling). As we showed in Fig. 3 in Section 4.1, the heart views differ globally, therefore, even when we resize the images and loose some details, their global aspects remain similar. The removal of some details can also remove noise artifacts. In the case of BoVW based on sparse sampling, avg pooling has a drop in accuracy from the size of 100k pixels, while max pooling presents a drop in accuracy in the 1k size.

A remark about the results with the very tiny images (1k version): the Harris–Laplace detector failed to detect points in 45 images. Such images were all from the PSA_MID view and they had no final representation. This is a problem for descriptors based on interest-point detectors. Our low dense sampling scheme does not suffer from that. For the 1k version of the dataset, 13 regions were used per image and, as we can see, they have a high discriminative power.


                           Table 5
                            shows the extraction times per image for each method. We are showing only the times for low-level feature extraction, which considers only the time for image sampling and local description (it does not include the time for creating the bag of visual words, which depends on coding and pooling). The time for low-level feature extraction corresponds to most of the time for BoVW computation. In the original images (450k), the time for low-level feature extraction corresponds to more than 97% with any of the pooling approaches when using our proposed method. For the BoVW based on sparse sampling, this time corresponds to more than 94% of the whole BoVW computation time. In Fig. 8, however, we show the times for computing the whole BoVW vector.

We can see that low dense sampling is much faster than sparse sampling. It is more than 33 times faster in the original image resolution and, in the tiny images, it is still 3 times faster. Our proposed low dense sampling would be able to process a 30fps video in real-time almost since the first downsampling size (100k pixels). Real-time 60fps could be reached since the 5k size for the proposed method. With sparse sampling, we would be able to process 30fps videos in real time only for the 1k pixel resolution.

Downsampling could be an effective way for reducing extraction time, while keeping good accuracy when using the proposed BoVW descriptors based on low dense sampling. However, in some heart views, the difference between the heart structures may be on the details and they can disappear after downsampling. Hence, downsampling must be used carefully.

Ultrasound images are well known to contain noise or speckle. The speckle itself can also be considered as diagnostic information [43,44]. Therefore, we performed a set of experiments evaluating the impact of noise/speckle in the accuracy of the proposed method.

We used four different filters and compared how each descriptor performed before and after filtering. The filters used are: median, Frost [45], Kuan [46], Lee [47]. Frost, Kuan, and Lee are common filters for ultrasound images, while the median filter is the very popular in the computer vision community. All the filters were used with a window size of 7×7 pixels.


                           Table 6
                            shows the results for the proposed mid-level representations as well as for the global descriptors. We can see that the classification accuracies of most of the global descriptors change. For some of them, the filtering may have also removed details that were important for their extraction algorithms, so their accuracy scores decreased (e.g., SASI, LAS). On the other hand, for some others, noise was harming the representation and the results improved after noise filtering (e.g., Unser, HOG). We highlight the increase in accuracy of HOG, which reached +90% accuracy.

Considering the proposed approach, we can see again its robustness to image transformations, which reinforces its applicability on a variety of scenarios. Its classification accuracy consistently remained above 90% for all filters tested. With the median filter, we achieve a remarkable result of ∼98%.

For the proposed method (BoVW low dense sampling), we performed a statistical analysis to verify if there is significant difference in the results with and without noise filtering for all the filters tested. The statistical tests also considered the different pooling techniques used (avg, max, avgSPM, and maxSPM). We used the Pairwise Wilcoxon Rank Sum Test, which calculates comparisons between group levels with corrections for multiple testing, with the Bonferroni correction of p-values. The tests showed that there is no significant change when applying any of the considered filtering methods, although small variations are present in the results. We do not show the table with the statistical tests herein because none of the filters showed statistical significance.

In this section, we considered the use of Random Forest as an alternative classifier to linear SVMs to verify the robustness of our mid-level representations to different classifiers. SVM and Random Forest rely on different rationales: SVM is a margin-based classifier, while Random Forest is based on bootstrap aggregation and random sampling. We tested two different values for the parameter related to the number of trees (ntree): 100 and 500. The difference in results for both values was not statistically significant for all the descriptors evaluated and we decided to show only the results for ntree=500.


                           Table 7
                            shows the results. We can see that our approach obtains the highest accuracy rates. We can also note that some of the global descriptors have variation in performance when changing the classifier. Our mid-level representations, however, are robust to the different classifiers and again keep accuracy above 90%, highlighting their robustness to many conditions.

In this section, we contrast the related work presented in Section 2.1 and our proposed methodology. Most of the methods presented in Section 2.1 have peculiarities which can create constraints or extra costs in their use. For instance, some approaches [1,18,4] only deal with the end diastolic (ED) frame, which could limit their use in the real-time scenario (heart view shown during the examination). Waiting for the ED frame to be displayed may delay the system response. In addition, it is not clear if those methods also work with the other frames. In our experiments, we have worked with all frames, even knowing that this may create a more difficult scenario.

Many methods [3,18,19,6,7] also apply pre-processing steps to normalize images/videos. Contrast/brightness normalization, noise reduction, alignment, and so on, usually introduce extra costs. We show that our method works well even without any image pre-processing.

Some methods [3,18,4,19,5] also depend on training detectors, models or regions of interest that are specific for each view. This is not a major problem, but can represent an additional cost if many views are used or many different acquisition equipments are employed. Our method does not assume any prior knowledge about the existing views nor the acquisition equipment.

Some methods [18] also depend on human intervention, limiting their scalability. Our method is completely automatic.

An interesting phenomenon observed by studying the related work is that there is a trend in using general features for heart view classification of echocardiograms. The most recent works [7–9] employed features that are popularly used for general visual recognition problems. This shows that the approach we present in this paper also follows this trend.

We could also note that in this field, there is no standard dataset. Therefore, given the specificities of each dataset, like the devices used, it is almost impossible to compare the results among the works. Different devices can create easier or more difficult scenarios. We should use the works for analyzing how each research group approached the problem, specially in terms of feature extraction and machine learning. We could note, however, that the works analyzed are based on 2D echo images or videos (i.e., not 3D [10]).

Another issue is that authors of related work often do not specify carefully the views used. For instance, there are views composed of sub-views, such as the short axis views (e.g., aortic valve, mitral valve, mid left ventricle, and apex). Only some authors specify which short axis views they used.

@&#CONCLUSIONS@&#

This paper presents mid-level representations for real-time heart view classification of echocardiograms. The paper also presents a thorough experimental evaluation of different image descriptors and an in-depth literature review of the existing solutions to this problem.

In the in-depth literature review presented, we could note that the existing solutions usually present constraints, such as being evaluated only with the end diastolic frame, requiring the training of specific detectors or regions of interest and, in some cases, requiring manual intervention. On top of that, we could also note a trend in more recent works of using generic feature descriptors for heart view classification.

Our real-time solution to this problem is based on the use of a bag-of-visual-words (BoVW) methodology, following the trend observed in the literature. The main novelty herein relies on low dense sampling for image characterization, i.e., large and representative image regions are used (instead of a very dense grid) resulting in few (
                        <
                        20
                     ) highly discriminative regions per image. The small number of regions drastically reduces the extraction time, making our approach suitable for real-time systems. Another effect of using large regions is that those regions may sometimes correspond to whole heart structures. Hence, the final BoVW descriptor can roughly correspond to an activation vector of heart structures. The proposed approach does not depend on performing any pre- or post-processing in the images or in the detected regions.

We compared the proposed approach with several existing image descriptors, both global and based on visual codebooks. Our approach is the only one to present, at the same time, high accuracy and fast feature extraction. We have also evaluated the methods in transformed versions of the image dataset (downsampling and noise/speckle filtering) and the proposed approach was robust to the transformations. Experiments comparing two different classifiers (linear SVMs and Random Forests) also show the quality and robustness of the proposed mid-level representations. In terms of effectiveness, our results were consistently above 90% of average accuracy. Specifically after noise filtering with the median filter, the proposed descriptors achieved very high accuracy (∼98%). In terms of efficiency, in some cases, we could process 30fps or 60fps videos in real-time. Therefore, we can rely on the proposed classification system regardless of the image resolution and acquisition conditions (e.g., presence or absence of noise).

As future work, we mention the possibility of creating a supervised codebook, aiming at selecting image regions containing whole heart structures. This would open the opportunity to create a bag of heart structures. Also, as most of the image descriptors herein explore different properties for image characterization, it is likely that some of them encompass complementary information which can be an opportunity for feature and classifier fusion.

We also would like to evaluate the method with more diseased hearts. Adding training examples of this kind, we could evaluate the generalization power of the approach. We also envision the applicability of the proposed characterization to other problems outside the realm of echocardiography.

Another important evaluation for real-time systems would be in the use of open-set classifiers, for correctly discarding videos/frames of unknown views. While searching for the correct probe position in the patient, the ultrasound device shows images that are not related to any view of interest. A real-time classification system should be able to ignore such images instead of classifying them as one of the existing views.

None declared.

@&#ACKNOWLEDGEMENTS@&#

Part of the results presented in this paper were obtained through the project “Pattern recognition and classification by feature engineering, ⁎-fusion, open-set recognition, and meta-recognition”, sponsored by Samsung Eletrônica da Amazônia Ltda., in the framework of law No. 8,248/91. We also thank Fapesp, CNPq, Capes, and Microsoft Research.

@&#REFERENCES@&#

