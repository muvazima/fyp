@&#MAIN-TITLE@&#Permutation entropy analysis of vital signs data for outcome prediction of patients with severe traumatic brain injury

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Permutation entropy can measure complexity of clinical time series.


                        
                        
                           
                           Features derived from early vital signs have prediction power for long-term outcomes.


                        
                        
                           
                           Ordinal patterns from vital signs could provide clinical meaningful interpretation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Permutation entropy

Ordinal pattern

Traumatic brain injury

Vital signs

Prediction

@&#ABSTRACT@&#


               
               
                  Permutation entropy is computationally efficient, robust to outliers, and effective to measure complexity of time series. We used this technique to quantify the complexity of continuous vital signs recorded from patients with traumatic brain injury (TBI). Using permutation entropy calculated from early vital signs (initial 10–20% of patient hospital stay time), we built classifiers to predict in-hospital mortality and mobility, measured by 3-month Extended Glasgow Outcome Score (GOSE). Sixty patients with severe TBI produced a skewed dataset that we evaluated for accuracy, sensitivity and specificity. The overall prediction accuracy achieved 91.67% for mortality, and 76.67% for 3-month GOSE in testing datasets, using the leave-one-out cross validation. We also applied Receiver Operating Characteristic analysis to compare classifiers built from different learning methods. Those results support the applicability of permutation entropy in analyzing the dynamic behavior of TBI vital signs for early prediction of mortality and long-term patient outcomes.
               
            

@&#INTRODUCTION@&#

Traumatic brain injury (TBI) is the most common cause of admission to emergency care and trauma-related death in the U.S. civilian population and is a major cause of death and disability in combat causalities [1,2]. In most modern intensive care units (ICUs), vital signs (VS), such as heart rate (HR), blood pressure (BP), and oxygen saturation (SpO2), among others, are collected in high-quality, automated, continuous electronic data streams, as sequential assessments of important physiological functions, providing basic evidence of patients’ status. Because VS are an early warning system of physiologic perturbation, they are usually recorded hourly in the ICU setting. However, in most modern ICUs, the massive quantities of high-quality data produced create both a challenge to store, analyze, and interpret and an opportunity to explore novel advanced analytic methods for predicting outcomes. Such predictive algorithms can support advanced instrumentation and decision-assist tools that have the potential to significantly improve clinical outcome for these very ill patients.

To discover the intrinsic patterns that characterize continuous, multivariate, clinical time series, a variety of methods can be used, such as entropy, auto-correlation, autoregressive models, and structure models [3]. One strategy is to embed the time series into higher dimensional space and then compute various entropies for the elements of the embedded time series. Conventional entropies such as Shannon entropy, Rényi entropy and Tsallis entropy can be calculated given the distribution of elements of the embedded time series. The Rényi entropy of a time series has been used to detect spatially varying multivariate relationships [4] and to study brain injuries [5] and heart rate variability [6,7]. The Tsallis entropy of the elements of a time series has been used to monitor brain injuries after cardiac arrests [8], and to improve the accuracy of gene regulatory networks inference [9].

Bandt and Pompe [10] introduced permutation entropy as a new measure of complexity of non-linear time series. Zanin et al. [11] provide an extensive review of various biomedical applications of permutation entropy. Permutation entropy has been used to predict the onset of epileptic episodes from EEG data by considering changes in the permutation entropy of the EEG time series over time [12,13]. Veisi et al. [14] find that permutation entropy can be used to effectively classify EEG signals into normal vs. epileptic with an accuracy of 85% even for highly noisy EEG. Physiologically, epileptic episodes/symptoms are manifested with deterministic behavior of the EEG signals, while healthy states are characterized by higher non-chaotic state variability [7]. Permutation entropy has also been used to study sleep using EEGs [15,16], to identify motifs in the EEG signals of patients given fast acting anesthetic drugs [17,18], and to identify temporal gene expression profiles [19]. Bian et al. [20] use permutation entropy to identify heart rate variability under different physiological conditions. Berg et al. [21] use ordinal patterns of beat-to-beat heart rate variability from an EKG signals of 40 patients who suffered from myocardial infraction, and try to classify them based on whether they survived for more than two years or not. They achieve a classification accuracy of 85%. Permutation entropy can also highlight forbidden patterns: state-space patterns/permutations appear very infrequently or not at all [22]. It can also be used to quantify non-linear interactions among time series by considering the relative entropy of the joint Takens embeddings of such time series versus the product of independent Takens embeddings [23].

In this research effort, we use permutation entropy to derive features from continuous, multivariate, time series for outcome prediction of patients with severe TBI. The remainder of this paper is organized as follows. In Section 2, we briefly introduce the permutation entropy and the entropy map that we used for quantifying the characteristics of the dynamic system. We use different independence tests to assist in variable selection. In Section 3, we describe the dataset and experiment design. We apply the permutation entropy to predict mortality and 3-month Extended Glasgow Outcomes Scale (GOSE), and present experiment results, evaluated by accuracy and the area under the receiver operating characteristic (ROC) curve. We conduct preliminary interpretation of ordinal patterns derived from the VS. Finally, in Section 4, we discuss and summarize the results.

@&#METHOD@&#

We assume that the physiological status of living things is dynamic but has identifiable and repeated patterns. Likewise, we assume that these patterns will be different in the healthy, injured, and/or ill individuals and that the patterns will be discernibly different from each other. For instance, if the patient is also losing blood, blood pressure (BP) will fall. Heart rate (HR) increases to compensate for the decreased BP to ensure adequate circulation and oxygenation of the brain, and the increase in HR usually increases the BP, at least temporarily. If blood loss continues, BP falls, and clinicians will usually give fluid, including blood, to raise the BP and ensure adequate oxygenation. These changing patterns of HR and BP are accompanied by changes in intracranial pressure (ICP), cerebral perfusion pressure (CPP), and so on.

Bandt and Pompe [10,24] suggested an approach to time series analysis in which they embedded a continuous time series as a symbolic sequence into another space, a process which they called “permutation entropy”. One major ingredient of permutation entropy is the ordinal pattern. The ordinal pattern of a sequence of elements 
                           
                              
                                 x
                              
                              
                                 1
                              
                           
                           ,
                           
                           …
                           ,
                           
                           
                              
                                 x
                              
                              
                                 n
                              
                           
                         is the permutation (re-arrangement) 
                           π
                           =
                           (
                           
                              
                                 i
                              
                              
                                 1
                              
                           
                           ,
                           
                           
                              
                                 i
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 i
                              
                              
                                 n
                              
                           
                           )
                         that sorts the amplitude values in ascending order so that 
                           
                              
                                 x
                              
                              
                                 
                                    
                                       i
                                    
                                    
                                       1
                                    
                                 
                              
                           
                           ≤
                           
                              
                                 x
                              
                              
                                 
                                    
                                       i
                                    
                                    
                                       2
                                    
                                 
                              
                           
                           ≤
                           ,
                           …
                           ,
                           ≤
                           
                              
                                 x
                              
                              
                                 
                                    
                                       i
                                    
                                    
                                       n
                                    
                                 
                              
                           
                        .

The order 
                           L
                         permutation entropy of a time series 
                           
                              
                                 x
                              
                              
                                 1
                                 ,
                                 …
                                 ,
                                 N
                              
                           
                         is calculated as follows. Let 
                           
                              
                                 π
                              
                              
                                 t
                              
                           
                         be the ordinal pattern (i.e. the sorting permutation) for the segment of the time series under the sliding window of length 
                           L
                         that ends at 
                           
                              
                                 x
                              
                              
                                 t
                              
                           
                        , i.e. the subsequence 
                           
                              
                                 x
                              
                              
                                 t
                                 −
                                 L
                                 +
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                           
                              
                                 x
                              
                              
                                 t
                              
                           
                        . Let 
                           
                              
                                 S
                              
                              
                                 L
                              
                           
                           =
                           {
                           
                              
                                 π
                              
                              
                                 t
                              
                           
                           }
                         be the set of all those unique (alphabet) ordinal patterns 
                           
                              
                                 π
                              
                              
                                 t
                              
                           
                        . The time series 
                           
                              
                                 x
                              
                              
                                 1
                                 ,
                                 …
                                 ,
                                 N
                              
                           
                         corresponds to the sequence 
                           〈
                           
                              
                                 
                                    π
                                 
                                 
                                    t
                                 
                              
                              :
                              t
                              =
                              L
                              ,
                              …
                              ,
                              N
                           
                           〉
                         of 
                           N
                           −
                           L
                           +
                           1
                         ordinal patterns from the alphabet 
                           
                              
                                 S
                              
                              
                                 L
                              
                           
                        . The entropy of this sequence of ordinal patterns is the permutation entropy of the time series 
                           
                              
                                 x
                              
                              
                                 1
                                 ,
                                 …
                                 ,
                                 N
                              
                           
                        . For example, the Shannon permutation entropy is defined in (1),
                           
                              (1)
                              
                                 
                                    
                                       H
                                    
                                    
                                       L
                                    
                                 
                                 =
                                 −
                                 
                                    ∑
                                    
                                       
                                          
                                             π
                                          
                                          
                                             k
                                          
                                       
                                       ∈
                                       
                                          
                                             S
                                          
                                          
                                             L
                                          
                                       
                                    
                                 
                                 P
                                 
                                    (
                                    
                                       
                                          
                                             π
                                          
                                          
                                             k
                                          
                                       
                                    
                                    )
                                 
                                 log
                                 
                                    (
                                    
                                       P
                                       
                                          (
                                          
                                             
                                                
                                                   π
                                                
                                                
                                                   k
                                                
                                             
                                          
                                          )
                                       
                                    
                                    )
                                 
                                 ,
                                 
                              
                           
                        where 
                           P
                           (
                           
                              
                                 π
                              
                              
                                 k
                              
                           
                           )
                         is the frequency of 
                           
                              
                                 π
                              
                              
                                 k
                              
                           
                         in the sequence 
                           〈
                           
                              
                                 
                                    π
                                 
                                 
                                    t
                                 
                              
                           
                           〉
                        . In the work presented here, we use the Rényi entropy with parameter 
                           α
                         of the sequence 
                           〈
                           
                              
                                 
                                    π
                                 
                                 
                                    t
                                 
                              
                           
                           〉
                         defined as
                           
                              (2)
                              
                                 
                                    
                                       R
                                    
                                    
                                       L
                                    
                                    
                                       α
                                    
                                 
                                 =
                                 
                                    1
                                    
                                       1
                                       −
                                       α
                                    
                                 
                                 
                                 log
                                 
                                    (
                                    
                                       
                                          ∑
                                          
                                             
                                                
                                                   π
                                                
                                                
                                                   k
                                                
                                             
                                             ∈
                                             
                                                
                                                   S
                                                
                                                
                                                   L
                                                
                                             
                                          
                                       
                                       P
                                       
                                          
                                             (
                                             
                                                
                                                   
                                                      π
                                                   
                                                   
                                                      k
                                                   
                                                
                                                
                                             
                                             )
                                          
                                          α
                                       
                                    
                                    )
                                 
                                 .
                                 
                              
                           
                        
                     

The parameter 
                           α
                         in the Rényi entropy acts as a selector of probabilities. It assigns almost equal weight to each possible probability when 
                           α
                         is sufficiently close to zero. When 
                           α
                         is larger, it puts more weight on higher probabilities. We can use this parameter to assign different weights on events of different probabilities.

The idea of permutation entropy, introduced by Bandt and Pompe [10,24], relies on a large body of previous work on using information theory to study the phase space (state-space) of dynamical systems [23]. For example, the Kolmogorov–Sinai (KS) entropy is used extensively to characterize the probability distributions (random processes) induced by finite partitions of the state-space of dynamical systems [23,25].

The underlying distribution of the states is an invariant measure of a dynamical system (invariant under smooth transformations of the state space), while entropy functions provide us with a way to compare such distributions. Due to the intractability in deriving explicit analytic expressions of the state distributions, researchers have resorted to numerical estimates from the data. To this end, of particular importance is the Takens–Whitney delay embedding and reconstruction theorem [23] that relates the dimension 
                              d
                            of the system’s attractor and the dimension (
                              2
                              d
                              +
                              1
                           ) of the embedding space that is sufficient to reconstruct those properties of the system’s attractor that are invariant under smooth transformations. Characterizing the attractor of a dynamical system enables us to predict the system’s long-term behavior (since the attractor contains all states that are mapped by the system back into a state in the attractor). The Takens–Whitney theorem provides an effective way to estimate the dimension of the attractor by estimating the Kolmogorov–Sinai entropy of an embedding of a system’s state-space.

One particular partition of the space of a Takens delay embedding is obtained via permutations as follows. Consider for simplicity a univariate discrete-time time series 
                              
                                 
                                    x
                                 
                                 
                                    t
                                 
                              
                              ,
                            and its Takens delay embedding of order 
                              m
                            with delay lag τ: 
                              
                                 
                                    X
                                 
                                 
                                    t
                                 
                              
                              =
                              
                              
                                 (
                                 
                                    
                                       
                                          x
                                       
                                       
                                          t
                                          −
                                          (
                                          m
                                          +
                                          1
                                          )
                                          τ
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          t
                                          −
                                          
                                             
                                                τ
                                             
                                             
                                                t
                                             
                                          
                                       
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          t
                                       
                                    
                                 
                                 )
                              
                              ∈
                              
                                 
                                    R
                                 
                                 m
                              
                           . Partition the space 
                              
                                 
                                    R
                                 
                                 m
                              
                            into 
                              m
                              !
                            subsets, each labeled by a unique permutation π of 
                              [
                              1
                              ,
                              …
                              ,
                              m
                              ]
                           , with each subset containing all points in 
                              
                                 
                                    R
                                 
                                 m
                              
                            that can be sorted by the subset’s labeling permutation.

Permutation-based partitions are more robust to noise and other non-linear distortions and artifacts than value-based fixed-size partitions of the state space, since they depend on the relative order rather than the exact values of the time series. Furthermore, in order to obtain reliable entropy estimates with fixed-size partitions, one needs long time series (in the order of 
                              
                                 
                                    2
                                 
                                 m
                              
                            in order to cover all blocks of such fixed-size partitions); permutation-based entropy estimates do not require long time series. The robustness of permutation entropy makes it particularly attractive for mining vital signs collected in real clinical settings, without expensive pre-processing and cleaning of such signals.

Recall that the uniform distribution has maximum entropy among discrete distributions of bounded support. Large values of permutation entropies correspond to dynamical systems with substantial uncertainty/randomness (divergence in time of initially nearby system states) and small values indicate rather deterministic behavior (fixed points or simple limit cycles).

Given 
                           M
                         variables and a window size 
                           M
                        , vital signs within that window are viewed as one slice of size 
                           M
                           ×
                           L
                        . 
                        Fig. 1 demonstrates one example of finding ordinal patterns from a finite sequence of time series. Suppose that there are three vital signs (
                           M
                           =
                           3
                        ) available for inclusion: ICP, HR, and SBP. Let the window size be 
                           L
                           =
                           2
                        . Therefore, one slice constitutes 6 points, which means that we embed VS in a window of size 2 into a higher dimension 6. There are two choices to permute in a slice. The first one considers one slice as one bag. All values in this bag are sorted in ascending order. For example, in Fig. 1, slice a can be written linearly as the sequence: (ICP)12.4, 14.4; (HR)59.5, 59.4; (SBP)142, 138. Labeling each value 1–6, the values of this sequence can then be sorted into ascending order by applying a permutation 
                           〈
                           
                              1
                              
                              2
                              
                              4
                              
                              3
                              
                              6
                              
                              5
                           
                           〉
                        . Another choice is to sort within each variable, then concatenate them. For the same example, if we sort ICP, HR, SBP in slice a separately, and concatenate their local permutation index, we obtain the pattern 
                           〈
                           
                              1
                              
                              2
                              
                              2
                              
                              1
                              
                              2
                              
                              1
                           
                           〉
                        . The second method would help keep each variable isolated even if the variables may have similar range, and hence maintain the ordinal patterns from each variable.

More than 50 different types of VS are recorded in the hospital. To make the prediction model simple, we selected a group of VS that are frequently used in clinical diagnosis.

We use tests of statistical independence to explore relations between VS and to select a set of VS for calculating entropy values to sketch the change in patients’ physiological status. If two variables have strong linear correlation, then including both variables is not expected to be helpful in capturing more permutation patterns. On the other hand, if two variables are almost independent or have a non-linear relationship, using both may favor including more information.

Recall that two random variables 
                           X
                         and 
                           Y
                        , with probability density functions 
                           
                              
                                 p
                              
                              
                                 X
                              
                           
                           (
                           x
                           )
                         and 
                           
                              
                                 p
                              
                              
                                 Y
                              
                           
                           (
                           y
                           )
                        , are independent if and only if their joint density factors as 
                           
                              
                                 p
                              
                              
                                 X
                                 Y
                              
                           
                           
                              (
                              
                                 x
                                 ,
                                 y
                              
                              )
                           
                           =
                           
                              
                                 p
                              
                              
                                 X
                              
                           
                           
                              (
                              x
                              )
                           
                           
                              
                                 p
                              
                              
                                 Y
                              
                           
                           (
                           y
                           )
                        . To evaluate how far away from independence two random variables are, the mutual information (MI) can be used to quantify the difference between 
                           
                              
                                 p
                              
                              
                                 X
                                 Y
                              
                           
                           (
                           x
                           ,
                           y
                           )
                         and 
                           
                              
                                 p
                              
                              
                                 X
                              
                           
                           
                              (
                              x
                              )
                           
                           
                              
                                 p
                              
                              
                                 Y
                              
                           
                           (
                           y
                           )
                         
                        [26]. Recently, Reshef et al. [27] proposed a new measure, the maximal information coefficient (MIC), to assess a wide range of correlations between two variables. MIC takes real values between 0 and 1, representing the two ends of no relationship and noise-free relationship of linear or nonlinear form respectively.

Although MIC is claimed to be powerful to discover a wide range of relationships between two variables, Tibshirani et al. [28] argue that the MIC is inferior to distance correlation [29,30] in many independence tests. In light of this, we compare the MIC and distance correlation coefficients for all pairs of variables.

In 
                        Fig. 2a, MIC scores are plotted against 
                           ρ
                         to show the strength of linear and nonlinear relations. There are some interesting observations that can be made from this plot. First, most of the VS pairs show similar form of relationship despite their mortality outcomes, i.e. the same color dots in the plot cluster in closed regions. Second, most pairs of variables show weak correlations, with MIC and 
                           ρ
                         both being close to 0. Third, both MIC and 
                           ρ
                         suggest that CPP versus HR and CPP versus SpO2 have relatively stronger linear relationships. We further use the distance correlation to explore the relationships for all pairs of selected vital signs. In Fig. 2b, the MIC scores mainly agree with the relationship discovered by the distance correlation coefficients.


                        Fig. 2 indicates that most pairs of variables are weakly dependent or related to each other. Such weak dependency can be further confirmed by the scatter plots of VS pairs. In 
                        Fig. 3, the scatter plots for some pairs of vital signs show that there are no clear linear nor other functional relationships between those variables. For example, in clinical practice, oxygen is provided essentially constantly to severely injured patients. Most of the time SpO2 stays close to 100 without being strongly related to SBP or other variable. In Fig. 3c, we see most points cling to the vertical line SpO2=100%.

To evaluate results, not only the accuracy, but also the sensitivity, specificity and ROC analysis are utilized to compare performance of different classifiers. The ROC is a tool to depict the tradeoff between sensitivity and specificity. One major reason we adopt the ROC AUC for classifier comparison is that the dataset is skewed, and the ROC AUC is insensitive to the skewness of datasets [31]. This property of ROC curves provides us a way to evaluate the classifiers without worrying about the datasets from which they were trained. Instead of using one single point, we can use the instance statistics to produce a full ROC curve by calculating the class label scores [31]. Provost et al. [32] described a method of calculating the ROC by assigning a score to each instance that reaches the leaf of the decision tree. That score is equal to the ration of positive class labels assigned to that leaf during training. Platt [33] suggested a way of estimating posterior probability from the output of a support vector machine by fitting a sigmoid function.

@&#EXPERIMENTS AND RESULTS@&#

With the approval from the institutional review board (IRB) of University of Maryland School of Medicine, continuous, automated electronic VS data collected over the course of hospitalization from patients with severe TBI were analyzed using permutation entropy to predict in-hospital mortality and 3-month GOSE outcomes. Our dataset was collected during 2008 and 2009 from 60 sequentially admitted individuals, 9 female and 51 male, 8 of whom died while in hospital. The average duration of stay in hospital was 16 days (ranging from 1.5 to 53 days); 52 patients remained in the hospital longer than 1 week; and 27 patients stayed longer than 2 weeks. Among the 52 patients discharged from the hospital alive, follow-up interviews were carried out at 3 months post-discharge to assess functional outcomes of patients in terms of an 8-category scale [34]: dead, vegetative state, lower severe disability, upper severe disability, lower moderate disability, upper moderate disability, lower good recovery, and upper good recovery. Categories 1 to 4 are defined as “unfavorable” (value 1) and categories 5 to 8 as “favorable” outcomes (value 0). For 3-month GOSE in our dataset, 25 individuals had “favorable” outcome and 35 had “unfavorable” outcome, which, for our purposes, give a relatively balanced dataset.

The raw, every-6s data were collected by BedMaster® server (Excel Medical Electronics, Jupiter, FL), and were preprocessed to deal with noise due to unstable attachment of sensors, patients’ movement and missing values. All data processing and predictive model learning were implemented in Matlab® (2012b, MathWorks, Boston, MA). To reduce the negative effect of noise, VS data were smoothed into a 5-min tumbling window, as previously [35]. 
                        Table 1 shows the percentage of missing points of six selected VS. To utilize all information, we used the k-nearest neighbors’ average as surrogate values to fill in missing points.

Determining the optimal selection of VS with which to set up the experiment parameters can be difficult, that is, which values are optimal for the window size and the 
                           α
                         range of the Rényi entropy. Therefore, our parameters were selected based on the following considerations. First, a group of VS that are frequently used in clinical diagnosis was chosen, such as ICP, CPP, SBP, SpO2, etc. Those VS with the lowest percentage of missing points and missing data were selected to increase the chances of preserving more patterns, therefore more accurately characterizing the changing physiologic dynamics. A dataset was also tested for change of accuracy with and without removing a given vital sign. Correlated or dependent variables may be included in the dataset for ordinal pattern finding. However, it will not be redundant to include those variables when the relationship among those correlated variables is not order-preserving.

Using the above criteria and tests, a group of five VS were selected (see Table 1) and tested iteratively. The window sizes were equivalent to VS collection duration of 15, of 30 and 60min. In addition, the range for the Rényi entropy parameter 
                           α
                         was selected as 0.1 to 2.0 with step size 0.01.

The way to select VS described above is simple and effective but may also raise the question of whether one must include correlated variables in calculating the ordinal patterns. If two variables are not simply linearly correlated, including both may introduce extra ordinal patterns than only using one of them. In fact, the shock index (SI=HR/SBP) is strongly correlated to HR and SBP, since it is the ratio of the latter two. Because of its nonlinear functional relationship to HR and SBP, adding SI in the ordinal patterns unveils more interesting patterns that portray the interaction between HR and SBP. In general, with the presence of unknown correlation among the variables, the rule of thumb is a straightforward way to help us select a set of VS for study. Using tests of statistical independence, such as MIC or distance correlation coefficient, we also discover and exclude those variable pairs with strong linear relationships. For example, CPP shows relatively strong linear relationships to HR and SpO2 in the independence tests. Also it has very limited contribution to accuracy improvement when it is used with other VS. Hence, CPP was not used in the experiment, although it is an important physiological status indicator.

With the above setting, experiments were conducted to predict in-hospital mortality and 3-month GOSE. Since the sample size of 60 instances does not form a very large dataset, the leave-one-out cross validation method was used to test the prediction accuracy of our approach to new data. ). The leave-one-out cross-validation method is widely used in machine learning studies involving small datasets with 50 to 100 samples [36] pp. 203. 
                        
                        Figs. 4(b) and 5(b) show the average ROC curves from the leave-one-out cross-validation.

For each individual patient, a collection of features based on entropy is built as follows. First, selected VS of a certain length (e.g. 3 days VS) [37] are aligned by time and filled in for missing values with the $k$-nearest neighbor imputation method. Next, given a slice window size 
                           L
                        , the VS within a moving window of length 
                           L
                         are sorted in bag and are represented by permutations. Such collection of permutations makes an alphabet, where the frequency of each “word” (permutation pattern) is calculated. With a vector of instantiation of parameter 
                           α
                         in (2), a set of entropy values is calculated for the window size 
                           L
                        . Then the second step is repeated for different parameter values for 
                           L
                        . In this manner, a personalized dataset of features is created for each individual patient that encompasses the complexity of the patient’s physiological status. With those features, various kinds of classification methods are applied to predict outcomes of clinical interest.


                        
                        Table 2a and b shows confusion matrices and overall accuracy for predicting mortality and 3-month GOSE. The a priori knowledge is that 13.3% died in hospital, and 58.3% have unfavorable 3-month GOSE. Using early VS as defined above, a classification tree built upon permutation entropy achieved 62.50% in true positive rate (91.67% in overall accuracy) in predicting death, and 82.86% in true positive rate (76.67% in overall accuracy) in predicting unfavorable cases for 3-month GOSE, which are all higher than the a priori. This suggests that the permutation entropy is capable of classifying patients of different physiological status and can handle imbalanced class distribution. On the other hand, the permutation entropy also demonstrates good performance of prediction using early VS. This has potential clinical importance in providing medical care providers with timely prognostic information.

We then applied two other different learning methods, the support vector machine with linear kernel and the quadratic discriminant analysis. The ROC AUC is employed to assess the performance of different classifiers. As noted above, ROC graphs depict the tradeoff between sensitivity and specificity for each classifier in both training and testing datasets, and the AUC measures the probability of the classifier assigning a higher score to the positive than to the negative case, if one positive and one negative case were to be randomly drawn. Fig. 4a and b show the in-hospital mortality prediction on the training and testing sets, using the first 3 days’ VS. Fig. 5a and b compare prediction power of three classifiers for 3-month GOSE using the last 3 days’ VS. Note that the classifier built by the classification tree has the best discrimination for mortality prediction on both the training and the testing sets. The classification tree also has good discrimination capability on the 3-month GOSE outcomes.

In this section, we compare our results with other models created from clinical experience to demonstrate that the permutation entropy method has stable and comparable performance.

Many empirical models have been studied and reported to estimate patients’ current and future status. With computer assistance, more statistical metrics can be calculated from long duration vital signs records. Previous work by our group [36,38] on this same dataset studied cumulated dose of ICP >20mmHg, CPP <60mmHg and brain trauma index (BTI=CPP/ICP) as features to predict functional outcomes for patients with severe BTI, using ROC analysis and observed good predictive power for 3-month GOSE 1–4 (AUC=0.65–0.75, p<0.05) [39].

To compare with features built from the permutation entropy, up to five features from the five VS in Table 1 were selected. Mean values of HR, SpO2, SBP, SI, and ICP were calculated using the first 3 days’ data for the in-hospital mortality prediction, and the last 3 days for the 3-month GOSE. 
                        Table 3 compares the performance of the classification tree built on features from the permutation entropy and the top three classification trees built on subsets of features out of total 
                           
                              ∑
                              
                                 k
                                 =
                                 1
                              
                              5
                           
                           
                              
                                 C
                              
                              
                                 5
                              
                              
                                 k
                              
                           
                           =
                           31
                         combinations from Table 1.

It can be observed that the classification tree built upon features created by the permutation entropy demonstrated better performance in terms of overall accuracy and values of AUC for both in-hospital mortality and 3-month GOSE prediction.

As entropy calculations are not familiar to clinicians, we also consider ordinal patterns with high frequency that are associated with the “dead” outcome.

Since we use five VS, and each vital sign has three points (when using a 15-min window, with each point corresponding to a 5-min average), we encode the index of each point of each vital sign in a slice with a two-digit integer 01–15. So, ICP samples have indices 01-06-11, SI samples have indices 02-07-12, SpO2 samples have indices 03-08-13, SBP samples have indices 04-09-14, and HR samples have indices 05-10-15 as shown in 
                        Table 4. This encoding allows us to represent the ordinal patterns as a string (“word”) of two-digit numbers. Note that an ordinal pattern is a permutation of the 15 two-digit numbers 01-15.

We are interested in the difference of ordinal patterns associated with the “alive” and “dead” outcomes. Let 
                           
                              
                                 f
                              
                              
                                 a
                              
                           
                           (
                           
                              
                                 P
                              
                              
                                 i
                              
                           
                           )
                         and 
                           
                              
                                 f
                              
                              
                                 d
                              
                           
                           (
                           
                              
                                 P
                              
                              
                                 i
                              
                           
                           )
                         be the frequency of pattern 
                           
                              
                                 P
                              
                              
                                 i
                              
                           
                         associated with the “alive” and “dead” outcome respectively. The ratio 
                           r
                           
                              (
                              
                                 
                                    
                                       P
                                    
                                    
                                       i
                                    
                                 
                              
                              )
                           
                           =
                           
                              
                                 f
                              
                              
                                 d
                              
                           
                           (
                           
                              
                                 P
                              
                              
                                 i
                              
                           
                           )
                           /
                           
                              
                                 f
                              
                              
                                 a
                              
                           
                           (
                           
                              
                                 P
                              
                              
                                 i
                              
                           
                           )
                         measures the degree that 
                           
                              
                                 P
                              
                              
                                 i
                              
                           
                         is associated with the “dead” outcome. Since the dataset is imbalanced in mortality outcomes, we rescale 
                           r
                         by the ratio of the outcome frequencies 52/8. The rescaled ratio is used to rank all the patterns, so that higher ranked patterns are mostly associated with the “dead” outcome. The 10 highest ranked patterns are shown in 
                        Table 5.

Moreover, we can visualize these ordinal patterns with drawing the changing trends for each vital sign (recall that each ordinal pattern is a permutation of 01-15). Fig. 6a–c, show the top three ordinal patterns as a trend of each vital signs in each 15-min window. The overall trend of the top 10 patterns is shown in 
                        Fig.6d. Such visual patterns together with their VS values may be useful to clinicians for further interpretation.

@&#CONCLUSION@&#

@&#SUMMARY@&#

Using a large collection of continuous, automated, electronic patient VS data, we derived features to quantify the complexity of this dynamic system using permutation entropy and found that VS features can predict in-hospital mortality and 3-month GOSE, despite a skewed dataset from relatively few instances. These features created by permutation entropy demonstrated promising results. Among 13.3% deaths (58.3% unfavorable cases), we observed 91.67% overall accuracy (62.5% for deaths) for in-hospital mortality prediction, and 76.67% in 3-month GOSE prediction (82.86% for bad outcomes). In comparison with other classifiers on the same dataset, permutation entropy predicted in-hospital mortality and 3-month GOSE with greater accuracy and area under the receiver operating characteristic curves (ROC AUC=0.84, p<0.001 for mortality, and ROC AUC=0.71, p=0.001 for 3-month GOSE on testing sets).

Permutation entropy is capable of capturing the essentials of dynamic systems described by time series, which can then be used to create interpretable decision rules. Our study demonstrates the capacity of this method to identify, within the first 3 days of care, changes in VS associated with long-term outcome and offers clinicians a potential window for early interventions that may improve patient outcome.

@&#FUTURE WORK@&#

In this study, we used features created by permutation entropy to compare the capabilities of this technique with AUC in prediction of outcome. The accuracy of the prediction models can be improved by including extra descriptive features, such as those features studied in comparison. Furthermore, patients can be categorized into refined subgroups, for which more specific models can be built by categorizing by age or types of injury.

Higher frequency data can be used to enhance early prediction. Optimal calculation of entropy requires time series of sufficient length for a reasonable estimation of ordinal pattern distribution. Using higher frequency data, such as waveform data, permutation entropy may be able to create features to describe the system complexity in earlier time series, such as the first 12h in the hospital.

Access to valid clinical prognosis is important in the first 72h of care among a group of patients typically hospitalized for several weeks. However, the overall mean time to death for people who died of TBI in our system is 24h [2]. Our long-term goal in this work is to provide the critical care team with access to valid clinical prognosis in the first 12h after hospital admission and even, if possible, during pre-hospital care and transport, maximizing the potential for timely therapeutic interventions that can save lives and, more importantly, improve long-term clinical outcome.

None of the authors have any relevant financial interests to disclose.

@&#ACKNOWLEDGMENTS@&#

This research was funded in part by the grants W81XWH-07-2-0118 (Early Support of Intracranial Perfusion) and FA8650-11-2-6D01 (Continuing Non-Invasive Monitoring and the Development of Predictive Triage Indices for Outcomes Following Trauma). The authors thank Karen Murdoch, RPT; Melissa Binder, BS; Betsy Kramer RN, MS; and the ONPOINT investigators group for their support.

@&#REFERENCES@&#

