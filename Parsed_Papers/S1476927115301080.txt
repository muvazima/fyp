@&#MAIN-TITLE@&#Computational identification of circular RNAs based on conformational and thermodynamic properties in the flanking introns

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A computational method was proposed to distinguish circRNAs from non-circularized, expressed exons.


                        
                        
                           
                           Thermodynamic and conformational properties were extracted for model training.


                        
                        
                           
                           Two feature selection methods were used to construct the optimized feature subset.


                        
                        
                           
                           Our method received a high performance using 10-fold cross-validation on the training dataset.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Circular RNA

Competing endogenous RNA

Support vector machine

10-Fold cross-validation

@&#ABSTRACT@&#


               
               
                  Circular RNAs (circRNAs) were found more than 30 years ago, but have been treated as molecular flukes in a long time. Combining deep sequencing studies with bioinformatics technique, thousands of endogenous circRNAs have been found in mammalian cells, and some researchers have proved that several circRNAs act as competing endogenous RNAs (ceRNAs) to regulate gene expression. However, the mechanism by which the precursor mRNA to be transformed into a circular RNA or a linear mRNA is largely unknown. In this paper, we attempted to bioinformatically identify shared genomic features that might further elucidate the mechanism of formation and proposed a SVM-based model to distinguish circRNAs from non-circularized, expressed exons. Firstly, conformational and thermodynamic dinucleotide properties in the flanking introns were extracted as potential features. Secondly, two feature selection methods were applied to gain the optimal feature subset. Our 10-fold cross-validation results showed that the model can be used to distinguish circRNAs from non-circularized, expressed exons with an Sn of 0.884, Sp of 0.900, ACC of 0.892, MCC of 0.784, respectively. The identification results suggest that conformational and thermodynamic properties in the flanking introns are closely related to the formation of circRNAs. Datasets and the tool involved in this paper are all available at https://sourceforge.net/projects/predicircrnatool/files/.
               
            

@&#INTRODUCTION@&#

Circular RNAs (circRNAs) were first discovered in the cytoplasm of HeLa cells (Hsu and Coca-Prados, 1979). Since then, circRNA expressions at low levels have been found in a few expressed mammalian genes, such as ETS-1 in human cells and cytochrome P450 2C24 (CYPIIC24) in mouse cells (Cocquerelle et al., 1993 
                     Zaphiropoulos, 1993). As only a small number of circRNAs were found, circRNAs were considered as molecular flukes with uncertain importance before the era of massively parallel sequencing. However, with the development of next-generation sequencing, thousands of circRNAs were found in different species and tissues (Glazar et al., 2014). Most of the circRNAs contain almost exclusively exonic sequences, located in the cytoplasm with high conservation and are more stable than linear RNAs (Salzman et al., 2012; Jeck et al., 2013). Besides, the expression levels of circRNAs are almost the same as linear RNAs and some can even exceed 10-fold of those associated linear transcripts (Salzman et al., 2012).

Although the functions of circRNAs in biological processes are largely unknown, researchers have found that the transcript antisence to the cerebellar degeneration-related protein 1 (CDR1as) is highly expressed in the cytoplasmic of nerve tissues, which contains about 70 miRNAs response elements (MRE) and can be interacted with miR-7 as miRNA sponges (Memczak et al., 2013; Hansen et al., 2013). What’s more, circRNA expressions in human blood were also observed in the latest research, and the results suggest that circRNAs could be used as biomarker molecules in standard clinical blood samples (Memczak et al., 2015).

In order to elucidate the mechanism of formation, Jeck et al. (2013) investigated cis-sequence elements in the flanking introns and found that circRNAs are more likely to be generated by longer than average exons, flanked by longer than average introns that contain complementary ALU repeats, and bounded by a GT-AG pair of canonical splice sites. By using extensive mutagenesis of expression plasmids, Liang and Wilusz (2014) found that the intronic repeats and exonic sequences must collaborate with one another, and a functional 3′ end processing signal should be required . However, the CisFinder program (Sharov and Ko, 2009) used by Jeck et al. (2013) to identify enriched motifs in the flanking introns needs a motif clustering step to remove redundant motifs, and cannot be restricted to find short, core motifs. And only three genes, ZKSCAN1, HIPK3, and EPHB4, were analyzed in the research of Liang and Wilusz (2014), which is hard to build a common model by using three genes only. Thus, the mechanism of formation is still largely unclear.

In this paper, we attempted to bioinformatically identify shared genomic features that might further elucidate the mechanism of formation and proposed a machine learning method to distinguish circRNAs from non-circularized, expressed exons. Firstly, we extracted different length of sequences in the flanking introns and calculated the thermodynamic and conformational dinucleotide properties as the original features. And then two feature selection methods, Minimum Redundancy and Maximum Relevance (mRMR) and Random Forest (RF), were used to construct the optimized feature subset. For predictions, the performance of three algorithms, Support Vector Machine (SVM), Artificial Neural Network (ANN) and RF were compared on the training dataset and the independent dataset, respectively. Our results suggest that conformational and thermodynamic properties in the flanking introns are closely related to the formation of circRNAs.

The circRNA dataset proposed in PredcircRNA (Pan and Xiong, 2015) was used as the positive dataset. This dataset contains 14,084 circRNAs, all of which are longer than 200nt and non-overlapped from the same gene. The HEXEvent (Busch and Hertel, 2013) provides all the splice events based on EST information from the UCSC Genome Browser. In this research, only constitutive exons, showing non-circularized, were selected from HEXEvent as negative samples. In this way, 139,180 constitutive exons, showing non-circularized, were collected as negative samples. We randomly selected 19,022 exons from the negative samples and removed the overlapped samples with the circRNA dataset. Thus, the whole datasets contain 14,084 circRNAs and 18,970 non-circularized exons. To avoid overfitting, we split the whole datasets into three parts: 3000 circRNAs and 3000 non-circularized exons were randomly selected for feature selection; 7000 circRNAs and 7000 non-circularized exons were randomly selected for model training; the remaining 4084 circRNAs and 8970 non-circularized exons were used to construct the independent testing dataset.

Researchers have found that specific elements, such as ALU elements in the flanking introns, may be required for circularization (Jeck et al., 2013). And in order to allow sufficient time for a backsplice to occur, the intron immediately preceding the circularizing exons likely must be spliced slower than the average intron (Liang and Wilusz, 2014). These results suggest that nucleic acid properties in the flanking introns may be very important for the formation of circRNAs. In this research, the DiProDB database, collects conformational and thermodynamic dinucleotide properties, was used to calculate the nucleic acid properties in the flanking introns (Friedel et al., 2009). As sequences in the flanking introns might collaborate with each other for the formation of circRNAs, we extracted different length of sequences in the flanking introns and combined each of them to comprehensive analysis the corresponding nucleic acid properties. Thus, a total of 125 features were extracted in the original feature space.

Feature selection can remove irrelevant or redundant characteristics, so as to reduce the running time of training process and to improve the accuracy of model. On the other hand, the selection of true relevant features will simplify the model, which makes it is easier for researchers to clearly understand the research objects. Recently, there exists a large number of feature selection methods, such as RF (Strobl et al., 2007; Ganz et al., 2015; Janitza et al., 2016), Principal Component Analysis (PCA) and minimum redundancy maximum relevance (mRMR) algorithm (Peng et al., 2005). As each method have its own advantages and disadvantages, a comparative study is required. In this research, two feature selection methods, mRMR and RF, were applied to the feature selection dataset to obtain the optimal feature subset. We selected different number of features into the optimal feature subset and tested the performance of each methods on the training dataset.

Support vector machine (SVM), proposed by Vapnik, is developed based on the VC dimension theory and the structural risk minimization principle. It shows many special advantages in solving small sample, nonlinear and high dimensional pattern recognition, such as the identification of lincRNAs (Wang et al., 2014), and the identification of transmembrane protein topology (Nugent and Jones, 2009). In this paper, the LIBSVM program (Chang and Lin, 2011) was adopted, which can be downloaded from http://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/.

In order to test the performance of our model, we compared the performance of three different models, SVM, RF and artificial neural network (ANN), on the training dataset and the independent testing dataset, respectively. Here RF implementation from http://cran.r-project.org/web/packages/randomForest/ and ANN implementation in MATLAB were used. Standard indices, such as sensitivity 
                           
                              (
                              
                                 
                                    S
                                 
                                 n
                              
                              )
                           
                        , specificity 
                           
                              (
                              
                                 
                                    S
                                 
                                 p
                              
                              )
                           
                        , accuracy 
                           
                              (
                              A
                              C
                              C
                              )
                           
                         and Matthews correlation coefficient 
                           
                              (
                              M
                              C
                              C
                              )
                           
                        , were used to evaluate the performance of our model.
                           
                              (1)
                              
                                 
                                    
                                       S
                                       n
                                    
                                    =
                                    
                                       
                                          T
                                          P
                                       
                                       
                                          T
                                          P
                                          +
                                          F
                                          N
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       S
                                       p
                                    
                                    =
                                    
                                       
                                          T
                                          N
                                       
                                       
                                          T
                                          N
                                          +
                                          F
                                          P
                                       
                                    
                                 
                              
                           
                        where, Ture Positive 
                           
                              (
                              T
                              P
                              )
                           
                         and False Negative 
                           
                              (
                              F
                              N
                              )
                           
                         denote the numbers of positive samples that are predicted to be positive and negative, respectively. Analogously, True Negative 
                           
                              (
                              T
                              N
                              )
                           
                         and False Positive 
                           
                              (
                              F
                              P
                              )
                           
                         denote the number of negative samples that are predicted as negative and positive, respectively. 
                           
                              
                                 S
                                 n
                              
                           
                         represents the ability to identify the positive samples, while 
                           
                              
                                 
                                    S
                                 
                                 p
                              
                           
                         represents the ability to identify the negative samples. The ACC is defined as:
                           
                              (3)
                              
                                 
                                    A
                                    C
                                    C
                                    =
                                    
                                       
                                          T
                                          P
                                          +
                                          T
                                          N
                                       
                                       
                                          T
                                          P
                                          +
                                          F
                                          N
                                          +
                                          T
                                          N
                                          +
                                          F
                                          P
                                       
                                    
                                 
                              
                           
                        and the MCC is defined as:
                           
                              (4)
                              
                                 
                                    M
                                    C
                                    C
                                    =
                                    
                                       
                                          T
                                          P
                                          ×
                                          T
                                          N
                                          −
                                          F
                                          P
                                          ×
                                          F
                                          N
                                       
                                       
                                          
                                             
                                                (
                                                
                                                   
                                                      T
                                                      P
                                                      +
                                                      F
                                                      P
                                                   
                                                
                                                )
                                                (
                                                T
                                                P
                                                +
                                                F
                                                N
                                                )
                                                (
                                                T
                                                N
                                                +
                                                F
                                                P
                                                )
                                                (
                                                T
                                                N
                                                +
                                                F
                                                N
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

@&#RESULTS@&#

As an oversized or undersized window may cause the model overfitting or underfitting, it is very important to obtain an optimal window size for the sequence extraction process. Thus, in this research, in order to obtain an optimal window size, we extracted sequences in the flanking introns with different window sizes and compared the performance of the SVM-based model using 10-fold cross-validation on the feature selection dataset. Here all 125 features in the original feature space were used without feature selection. As shown in Table 1
                        , when the window size was set to 100nt, the model achieved the best S
                        p of 0.822, ACC of 0.792, MCC of 0.586 and the S
                        n of 0.763, which was slight lower than that of 0.774 with a 200nt window. When the window size was larger than 100nt, the performance decreased with the increase of window size. Thus, based on an overall consideration of various factors, the optimal window size of our model was set to 100nt.

In order to remove irrelevant and redundant features in the original dataset, two feature selection methods, mRMR and RF, were applied on the feature selection dataset to get the optimal feature subset. We compared the performance of the SVM-based model with different feature subsets using 10-fold cross-validation on the training dataset. As the results shown in Table 2
                        , the model achieved the best S
                        n of 0.876, S
                        p of 0.895, ACC of 0.886, MCC of 0.772, when 40 features ranked by RF were selected into the optimal feature subset. Meanwhile, the model only achieved the best S
                        n of 0.809, S
                        p of 0.850, ACC of 0.830, MCC of 0.660, when 100 features ranked by mRMR were selected. What’s more, RF performed much better than mRMR, when less than 40 features were selected. The detailed information of the top 40 features ranked by RF is listed in the Supplementary Table S1 (uploaded together with our online tool). The results show that the top 40 features included 29 conformational features which suggest that conformational features have very powerful discriminative ability. Besides, 8 physicochemical features and 3 letter based features, such as Thymine content, Guanine content, Adenine content, were also ranked in the top 40 features among the extracted 125 features. The important scores of the top 40 features are shown in Fig. 1
                        .

The performance of the three classifiers, SVM, RF, ANN, were compared using 10-fold cross-validation on the training dataset. Here the optimal parameters, window size of 100 nt and 40 features ranked by RF, were selected into each classifiers. For SVM, grid search method was used to optimize the punishment parameter c and the Gaussian kernel width parameter g using 10-fold cross-validation. With this method, the best c of 1.41, g of 1, were obtained. For RF, the parameter number of trees was set to 100 and the other parameters were set to their default values. For ANN, a 3-layer neural network was trained with back propagation (BP) algorithm. The number of hidden nodes was set to 40 and the other parameters were set to their default values. As the results shown in Table 3
                        , the BP network received the worst S
                        n of 0.871, S
                        p of 0.875, ACC of 0.873, MCC of 0.747 using 10-fold cross-validation on the training dataset. Compared to the RF algorithm, SVM performed better and received the higher S
                        n of 0.884, S
                        p of 0.900, ACC of 0.892, MCC of 0.784. But RF still have some advantages on some aspects, such as needed a shorter running time and had fewer parameters to tune.

To further demonstrate the robustness of our proposed method, we applied our trained model to the independent testing dataset. As the results shown in Table 4
                        , the SVM classifier received the best S
                        n of 0.907, S
                        p of 0.910, ACC of 0.909, MCC of 0.797. Meanwhile, we also noticed that the ANN classifier received the worst S
                        p of 0.857, ACC of 0.867, MCC of 0.715, which suggest that the robustness of ANN is poorer than the other two methods. Another interesting thing here is that the performance of the SVM classifier and RF classifier on the independent testing dataset are higher than those on the training dataset. One reason is that the training dataset size is larger than the dataset used in doing 10-fold cross-validation. The receiver operating characteristic (ROC) curves of the three classifiers are also shown in Fig. 2
                        . As the results show that the SVM classifier received the best performance, but there were only a few differences between the three classifiers.

@&#DISCUSSION@&#

The idea for this research came from the earlier observations that circRNAs are more likely to be flanked by longer than average introns, most of which contain complementary ALU repeats (Jeck et al., 2013), and the intron immediately preceding the circularizing exons likely must be spliced slower than the average intron (Liang and Wilusz, 2014). These results suggest that nucleic acid properties in the flanking introns may be very important for the formation of circRNAs. Thus, in this research, the thermodynamic and conformational dinucleotide properties in the flanking introns were used to train a SVM based model, which can be used to distinguish circRNAs from non-circularized, expressed exons. We carried out strict experiments on each step, including obtaining the best window size, selecting the optimal feature subset, and testing the performance of our model on the training dataset and the independent dataset, respectively. Our model achieved S
                     n of 0.884, S
                     p of 0.900, ACC of 0.892, MCC of 0.784 using 10-fold cross-validation on the training dataset, and S
                     n of 0.907, S
                     p of 0.910, ACC of 0.909, MCC of 0.797 on the independent dataset, which suggest that conformational and thermodynamic properties in the flanking introns are closely related to the formation of circRNAs.

As more and more circRNAs are being found in different organisms and species, elucidating the mechanism of formation and providing reliable prediction tools are urgently to be solved. Different sources of features, such as graph features, conservation information and ALU elements had been used to distinguish circRNAs from other lncRNAs (Pan and Xiong, 2015). And our research suggests that conformational and thermodynamic properties in the flanking introns should also be considered in the prediction of circRNAs, which may provide assistance to the future research.

@&#ACKNOWLEDGMENTS@&#

This work is supported in part by grants from National Natural Science Foundation of China (No. 61105021), Ph.D. Program Foundation of the Ministry of Education of China (No. 20110201110010), China Postdoctoral Science Foundation (No. 2011M501442), Natural Science Foundation of Shanxi Province (No. 2012JQ042) and the Fundamental Research Funds for the Central Universities.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.compbiolchem.2016.02.003.

The following are Supplementary data to this article:
                        
                           
                        
                     
                  

@&#REFERENCES@&#

