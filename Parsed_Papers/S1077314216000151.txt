@&#MAIN-TITLE@&#CURL: Image Classification using co-training and Unsupervised Representation Learning

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new classification method via co-training and unsupervised representation learning.


                        
                        
                           
                           Representations are obtained by early and late fusion schemes.


                        
                        
                           
                           Co-training is performed on the representations to enlarge the training set.


                        
                        
                           
                           Three scenarios have been tested: inductive, transductive and self-taught learning.


                        
                        
                           
                           The method outperforms other supervised and semi-supervised learning methods.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Image classification

Machine learning algorithms

Pattern analysis

Semi-supervised learning

@&#ABSTRACT@&#


               
               
                  In this paper we propose a strategy for semi-supervised image classification that leverages unsupervised representation learning and co-training. The strategy, that is called CURL from co-training and unsupervised representation learning, iteratively builds two classifiers on two different views of the data. The two views correspond to different representations learned from both labeled and unlabeled data and differ in the fusion scheme used to combine the image features.
                  To assess the performance of our proposal, we conducted several experiments on widely used data sets for scene and object recognition. We considered three scenarios (inductive, transductive and self-taught learning) that differ in the strategy followed to exploit the unlabeled data. As image features we considered a combination of GIST, PHOG, and LBP as well as features extracted from a Convolutional Neural Network. Moreover, two embodiments of CURL are investigated: one using Ensemble Projection as unsupervised representation learning coupled with Logistic Regression, and one based on LapSVM. The results show that CURL clearly outperforms other supervised and semi-supervised learning methods in the state of the art.
               
            

@&#INTRODUCTION@&#

Semi-supervised learning [1] consists in taking into account both labeled and unlabeled data when training machine learning models. It is particularly effective when there is plenty of training data, but only a few instances are labeled. In the last years, many semi-supervised learning approaches have been proposed [2] including generative methods [3,4], graph-based methods [5,6], and methods based on Support Vector Machines [7,8]. Co-training is another example of semi-supervised technique [9]. It consists in training two classifiers independently which, on the basis of their level of confidence on unlabeled data, co-train each other through the identification of good additional training examples. The difference between the two classifiers is that they work on different views of the training data, often corresponding to two feature vectors. Pioneering works on co-training identified the conditional independence between the views as the main reason of its success. More recently, it has been observed that conditional independence is a sufficient, but not necessary condition, and that even a single view can be considered, provided that different classification techniques are used [10].

In this work we propose a semi-supervised image classification strategy which exploits unlabeled data in two different ways: first two images representations are obtained by unsupervised representation learning (URL) on a set of image features computed on all the available training data; then co-training is used to enlarge the labeled training set of the corresponding co-trained classifiers (C). The difference between the two image representations is that one is built on the combination of all the image features (early fusion), while the other is the combination of sub-representations separately built on each feature (late fusion). This strategy has the advantages that only a set of features must be defined and in the co-training the different fusion schemes are used as different views. Moreover, by exploiting both representation learning and co-training, classification can be effectively performed when very few labeled data are available. We call the proposed strategy CURL from the combination of C and URL components. The schema of CURL is illustrated in Fig. 1
                     .

In standard co-training each classifier is built on a single view, often corresponding to a single feature. However, the combination of multiple features is often required to recognize complex visual concepts [11–13]. Both the classifiers built by CURL exploit all the available image features in such a way that these concepts can be accurately recognized. We argue that the use of two different fusion schemes together with the non-linear transformation produced by the unsupervised learning procedure, makes the two image representations uncorrelated enough to allow an effective co-training of the classifiers.

The proposed strategy is built on two base components: the unsupervised representation learning, and the classifier used in co-training. By changing these two components we can have different embodiments of CURL that can be experimented and evaluated.

To assess the merits of our proposal we conducted several experiments on widely used data sets: the 15-scene data set, the Caltech-101 object classification data set, and the ILSVCR 2012 data set which contains 1000 different classes. We considered a variety of scenarios including transductive learning (i.e. unlabeled test data available during training), inductive learning (i.e. test data not available during training), and self-taught learning (i.e. test and training data coming from two different data sets). In order to verify the efficacy of the CURL classification strategy, we also tested two embodiments: one that uses Ensemble Projection unsupervised representation coupled with Logistic Regression classification, and one based on LapSVM semi-supervised classification. Moreover different variants of the embodiments are evaluated as well. The results show that CURL clearly outperforms other semi-supervised learning methods in the state of the art.

Summarizing, the contributions of this work are: the proposal of a new classification strategy based on unsupervised representation learning; the use of a single set of visual features to create two new image representations; the use of early and late feature fusion schemes in a co-training workflow to allow effective classification in presence of few labeled data; extensive experiments to demonstrate the effectiveness of the proposed classification strategy under inductive, transductive and self-taught scenarios on different image datasets of heterogeneous contents and cardinalities.

@&#RELATED WORK@&#

There is a large literature on semi-supervised learning. For the sake of brevity, we discuss only the paradigms involved in the proposed strategy. More information about these and other approaches to semi-supervised learning can be found in the book by Chapelle et al. [1].

Blum and Mitchell proposed co-training in 1998 [9] and verified its effectiveness for the classification of web pages. The basic idea is that two classifiers are trained on separate views (features) and then used to train each other. More precisely, when one of the classifiers is very confident in making a prediction for unlabeled data, the predicted labels are used to augment the training set of the other classifier. The concept has been generalized to three [14] or more views [15,16]. Co-training has been used in several computer vision applications including video annotation [17], action recognition [18], traffic analysis [19], speech and gesture recognition [20], image annotation [21], biometric recognition [22], image retrieval [23], image classification [24,25], object detection [19,26], and object tracking [27].

According to Blum and Mitchell, a sufficient condition for the effectiveness of co-training is that, besides being individually accurate, the two classifiers are conditionally independent given the class label. However, conditional independence is not a necessary condition. In fact, Whang and Zhou [28] showed that co-training can be effective when the diversity between the two classifiers is larger than their errors; their results provided a theoretical support to the success of single-view co-training variants [29–31] (the reader may refer to an updated study from the same authors [32] for more details about necessary and sufficient conditions for co-training).

New image representations can be efficiently learned if a large amount of labeled data is available [33–35]. For example, the features extracted from the ConvNets [34] are widely used in different application scenarios. In the last years, as a consequence of the success of deep learning frameworks we observed an increased interest in methods that make use of unlabeled data to automatically learn new representations. In fact, these has been demonstrated to be very effective for the pre-training of large neural networks [36,37]. Restricted Boltzmann machines [38] and auto-encoder networks [39] are notable examples of this kind of methods. The tutorial by Bengio covers in detail this family of approaches [40].

A conceptually simpler approach consists in using clustering algorithms to identify frequently occurring patterns in unlabeled data that can be used to define effective representations. The K-means algorithm has been widely used for this purpose [41]. In computer vision this approach is very popular and lead to the many variants of bag-of-visual-words representations [42–45]. Briefly, clustering on unlabeled data is used to build a vocabulary of visual words. Given an image, multiple local features are extracted and for each of them the most similar visual word is searched. The final representation is a histogram counting the occurrences of the visual words. Sparse coding can be seen as an extension of this approach, where each local feature is described as a sparse combination of multiple words of the vocabulary [46–48]. Spectral clustering can build a more discriminating dissimilarity measure between data points by using kernel functions e.g. [49–51].

Another strategy for unsupervised feature learning is represented by Ensemble Projection (EP) [52]. From all the available data (labeled and unlabeled) Ensemble Projection samples a set of prototypes. Discriminative learning is then used to learn projection functions tuned to the prototypes. Since a single set of projections could be too noisy, multiple sets of prototypes are sampled to build an ensemble of projection functions. The values computed according to these functions represent the components of the learned representations.

LapSVM [8] can be seen as an unsupervised representation learning method as well. In this case the learned representation is not explicit but it is implicitly embedded in a kernel learned from unlabeled data.

Combining multimodal information is an important issue in pattern recognition. The fusion of multimodal inputs can bring complementary information from various sources, useful for improving the quality of the image retrieval and classification performance [53]. The problem arises in defining how these modalities are to be combined or fused. In general, the existing fusion approaches can be categorized as early and late fusion approaches, which refers to their relative position from the feature comparison or learning step in the whole processing chain. Early fusion usually refers to the combination of the features into a single representation before comparison/learning. Late fusion refers to the combination, at the last stage, of the responses obtained after individual features comparison or learning [54,55]. There is no universal conclusion as to which strategy is the preferred method for a given task. For example, Snoek et al. [54] found that late fusion is better than early fusion in the TRECVID 2004 semantic indexing task, while Ayache et al. [56] stated that early fusion gets better results than late fusion on the TRECVID 2006 semantic indexing task. A combination of these approaches can also be exploited as hybrid fusion approach [57].

Another form of data fusion is Multiple Kernel Learning (MKL). MKL has been introduced by Lanckriet et al. [58] as extension of the support vector machines (SVMs). Instead of using a single kernel computed on the image representation as in standard SVMs, MKL learns distinct kernels. The kernels are combined with a linear or nonlinear function and the function’s parameters can be determined during the learning process. MKL can be used to learn different kernels on the same image representation or by learning different kernels each one on a different image representation [59]. The former corresponds to have different notion of similarity, and to choose the most suitable one for the problem and representation at hand. The latter corresponds to have multiple representations each with a, possibly, different definition of similarity that must be combined together. This kind of data fusion, in [55], is termed intermediate fusion.

In the semi-supervised image classification setup the training data consists of both labeled examples 
                        
                           
                              {
                              
                                 X
                                 l
                              
                              ,
                              Y
                              }
                           
                           =
                           
                              
                                 {
                                 
                                    (
                                    
                                       x
                                       i
                                    
                                    ,
                                    
                                       y
                                       i
                                    
                                    )
                                 
                                 }
                              
                              
                                 
                                    i
                                    =
                                    1
                                 
                              
                              
                                 L
                              
                           
                        
                      and unlabeled ones 
                        
                           
                              X
                              u
                           
                           =
                           
                              
                                 {
                                 
                                    x
                                    i
                                 
                                 }
                              
                              
                                 
                                    i
                                    =
                                    L
                                    +
                                    1
                                 
                              
                              
                                 
                                    L
                                    +
                                    U
                                 
                              
                           
                           ,
                        
                      where x
                     
                        i
                      denotes the feature vector of image i, 
                        
                           
                              y
                              i
                           
                           ∈
                           
                              {
                              1
                              ,
                              …
                              ,
                              K
                              }
                           
                        
                      is its label, and K is the number of classes.

In this work, for each image i a set of S different image features 
                        
                           
                              x
                              i
                              
                                 (
                                 s
                                 )
                              
                           
                           ,
                        
                     
                     
                        
                           s
                           =
                           1
                           ,
                           …
                           ,
                           S
                        
                      is considered. Two views are then generated by using two different fusion strategies: early and late fusion. In case of Early Fusion (EF), the image features are concatenated and then used to learn a new representation 
                        
                           
                              x
                              i
                              
                                 
                                    E
                                    F
                                 
                              
                           
                           =
                           φ
                           
                              (
                              
                                 [
                                 
                                    x
                                    i
                                    
                                       (
                                       1
                                       )
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    x
                                    i
                                    
                                       (
                                       S
                                       )
                                    
                                 
                                 ]
                              
                              )
                           
                        
                      in an unsupervised way, where 
                        
                           φ
                           
                              (
                              ·
                              )
                           
                           :
                           
                              R
                              n
                           
                           →
                           
                              R
                              m
                           
                        
                      is a projection function. In case of Late Fusion (LF), an unsupervised representation 
                        
                           
                              φ
                              s
                           
                           
                              (
                              
                                 x
                                 i
                                 
                                    (
                                    s
                                    )
                                 
                              
                              )
                           
                        
                      is independently learned for each image feature and then the representations are concatenated to obtain 
                        
                           
                              x
                              i
                              
                                 
                                    L
                                    F
                                 
                              
                           
                           =
                           
                              [
                              
                                 φ
                                 1
                              
                              
                                 (
                                 
                                    x
                                    i
                                    
                                       (
                                       1
                                       )
                                    
                                 
                                 )
                              
                              ,
                              …
                              ,
                              
                                 φ
                                 S
                              
                              
                                 (
                                 
                                    x
                                    i
                                    
                                       (
                                       S
                                       )
                                    
                                 
                                 )
                              
                              ]
                           
                        
                     .

Using the learned EF and LF unsupervised representations, the two views are built: 
                        
                           
                              X
                              l
                              
                                 
                                    E
                                    F
                                 
                              
                           
                           =
                           
                              
                                 {
                                 
                                    x
                                    i
                                    
                                       
                                          E
                                          F
                                       
                                    
                                 
                                 }
                              
                              
                                 
                                    i
                                    =
                                    1
                                 
                              
                              
                                 L
                              
                           
                           ,
                        
                     
                     
                        
                           
                              X
                              u
                              
                                 
                                    E
                                    F
                                 
                              
                           
                           =
                           
                              
                                 {
                                 
                                    x
                                    i
                                    
                                       
                                          E
                                          F
                                       
                                    
                                 
                                 }
                              
                              
                                 
                                    i
                                    =
                                    L
                                    +
                                    1
                                 
                              
                              
                                 
                                    L
                                    +
                                    U
                                 
                              
                           
                        
                      and 
                        
                           
                              X
                              l
                              
                                 
                                    L
                                    F
                                 
                              
                           
                           =
                           
                              
                                 {
                                 
                                    x
                                    i
                                    
                                       
                                          L
                                          F
                                       
                                    
                                 
                                 }
                              
                              
                                 
                                    i
                                    =
                                    1
                                 
                              
                              
                                 L
                              
                           
                           ,
                        
                     
                     
                        
                           
                              X
                              u
                              
                                 
                                    L
                                    F
                                 
                              
                           
                           =
                           
                              
                                 {
                                 
                                    x
                                    i
                                    
                                       
                                          L
                                          F
                                       
                                    
                                 
                                 }
                              
                              
                                 
                                    i
                                    =
                                    L
                                    +
                                    1
                                 
                              
                              
                                 
                                    L
                                    +
                                    U
                                 
                              
                           
                        
                     . Furthermore, two label sets 
                        
                           Y
                           
                              
                                 E
                                 F
                              
                           
                        
                      and 
                        
                           Y
                           
                              
                                 L
                                 F
                              
                           
                        
                      are initialized equal to 
                        Y
                     .

Once the two views are generated, our method iteratively co-trains two classifiers 
                        
                           ϕ
                           
                              
                                 E
                                 F
                              
                           
                        
                      and 
                        
                           ϕ
                           
                              
                                 L
                                 F
                              
                           
                        
                      on them [9]. SVMs, logistic regressions, or any other similar technique can be used to obtain them. The idea of iterative co-training is that one can use a small labeled sample to train the initial classifiers over the respective views (i.e. 
                        
                           
                              ϕ
                              
                                 
                                    E
                                    F
                                 
                              
                           
                           :
                           
                              X
                              l
                              
                                 
                                    E
                                    F
                                 
                              
                           
                           ↦
                           
                              
                                 Y
                              
                              
                                 
                                    E
                                    F
                                 
                              
                           
                        
                      and 
                        
                           
                              ϕ
                              
                                 
                                    L
                                    F
                                 
                              
                           
                           :
                           
                              X
                              l
                              
                                 
                                    L
                                    F
                                 
                              
                           
                           ↦
                           
                              
                                 Y
                              
                              
                                 
                                    L
                                    F
                                 
                              
                           
                        
                     ), and then iteratively bootstrap by taking unlabeled examples for which at least one of the classifiers is very confident. The confident classifier determines pseudo-labels [60] that are then used as if they were true labels to improve the other classifier [61]. The confidence scores depend on the classification framework; for instance posterior probabilities can be used for this purpose if the classifiers provide them.

Given the classifier confidence scores 
                        
                           
                              w
                              i
                              
                                 
                                    E
                                    F
                                 
                              
                           
                           =
                           
                              ϕ
                              
                                 
                                    E
                                    F
                                 
                              
                           
                           
                              (
                              
                                 x
                                 i
                                 
                                    
                                       E
                                       F
                                    
                                 
                              
                              )
                           
                        
                      and 
                        
                           
                              w
                              i
                              
                                 
                                    L
                                    F
                                 
                              
                           
                           =
                           
                              ϕ
                              
                                 
                                    L
                                    F
                                 
                              
                           
                           
                              (
                              
                                 x
                                 i
                                 
                                    
                                       L
                                       F
                                    
                                 
                              
                              )
                           
                           ,
                        
                      the pseudo-labels 
                        
                           
                              y
                              ^
                           
                           i
                           
                              
                                 E
                                 F
                              
                           
                        
                      and 
                        
                           
                              y
                              ^
                           
                           i
                           
                              
                                 L
                                 F
                              
                           
                        
                      are respectively obtained as:

                        
                           (1)
                           
                              
                                 
                                    
                                       y
                                       ^
                                    
                                    i
                                    
                                       
                                          E
                                          F
                                       
                                    
                                 
                                 =
                                 arg
                                 
                                 
                                 
                                 
                                    max
                                    
                                       j
                                       =
                                       1
                                       ,
                                       …
                                       ,
                                       K
                                    
                                 
                                 
                                    w
                                    i
                                    
                                       
                                          E
                                          F
                                       
                                    
                                 
                                 
                                    [
                                    j
                                    ]
                                 
                              
                           
                        
                     
                     
                        
                           (2)
                           
                              
                                 
                                    
                                       y
                                       ^
                                    
                                    i
                                    
                                       
                                          L
                                          F
                                       
                                    
                                 
                                 =
                                 arg
                                 
                                 
                                 
                                 
                                    max
                                    
                                       j
                                       =
                                       1
                                       ,
                                       …
                                       ,
                                       K
                                    
                                 
                                 
                                    w
                                    i
                                    
                                       
                                          L
                                          F
                                       
                                    
                                 
                                 
                                    [
                                    j
                                    ]
                                 
                              
                           
                        
                     
                  

In each round of co-training, the classifier 
                        
                           ϕ
                           
                              
                                 L
                                 F
                              
                           
                        
                      chooses some examples in 
                        
                           X
                           u
                           
                              
                                 E
                                 F
                              
                           
                        
                      to pseudo-label for 
                        
                           
                              ϕ
                              
                                 
                                    E
                                    F
                                 
                              
                           
                           ,
                        
                      and vice versa. For each class k, let us call 
                        
                           X
                           ☆
                        
                      the set of candidate unlabeled examples to be pseudo-labeled for 
                        
                           ϕ
                           
                              
                                 E
                                 F
                              
                           
                        
                     . Each 
                        
                           
                              x
                              ☆
                           
                           ∈
                           
                              X
                              ☆
                           
                        
                      must belong to the unlabeled set, i.e. 
                        
                           
                              x
                              ☆
                           
                           ∈
                           
                              X
                              u
                              
                                 
                                    E
                                    F
                                 
                              
                           
                           ,
                        
                      has not to be already used for training, i.e. 
                        
                           
                              x
                              ☆
                           
                           ∉
                           
                              X
                              l
                              
                                 
                                    E
                                    F
                                 
                              
                           
                           ,
                        
                      and its pseudo-label has to be 
                        
                           
                              
                                 y
                                 ^
                              
                              ☆
                              
                                 
                                    L
                                    F
                                 
                              
                           
                           =
                           k
                        
                     . Furthermore, 
                        
                           ϕ
                           
                              
                                 L
                                 F
                              
                           
                        
                      should be more confident on the classification of x
                     ⋆ than 
                        
                           
                              ϕ
                              
                                 
                                    E
                                    F
                                 
                              
                           
                           ,
                        
                      and its confidence should be higher than a fixed threshold t
                     1:

                        
                           (3)
                           
                              
                                 ∀
                                 
                                    x
                                    ☆
                                 
                                 ∈
                                 
                                    X
                                    ☆
                                 
                                 :
                                 
                                    w
                                    ☆
                                    
                                       
                                          E
                                          F
                                       
                                    
                                 
                                 
                                    [
                                    k
                                    ]
                                 
                                 <
                                 
                                    w
                                    ☆
                                    
                                       
                                          L
                                          F
                                       
                                    
                                 
                                 
                                    [
                                    k
                                    ]
                                 
                                 ,
                                 
                                 
                                    w
                                    ☆
                                    
                                       
                                          L
                                          F
                                       
                                    
                                 
                                 
                                    [
                                    k
                                    ]
                                 
                                 >
                                 
                                 
                                    t
                                    1
                                 
                              
                           
                        
                     
                  

If no x
                     ⋆ satisfying Eq. (3) are found, then the constraints are relaxed:

                        
                           (4)
                           
                              
                                 ∀
                                 
                                    x
                                    ☆
                                 
                                 ∈
                                 
                                    X
                                    ☆
                                 
                                 :
                                 
                                    w
                                    ☆
                                    
                                       
                                          L
                                          F
                                       
                                    
                                 
                                 
                                    [
                                    k
                                    ]
                                 
                                 >
                                 
                                    t
                                    2
                                 
                                 ,
                                 
                                 
                                 with
                                 
                                 
                                 
                                 
                                    t
                                    2
                                 
                                 <
                                 
                                    t
                                    1
                                 
                              
                           
                        
                     
                  

Non-maximum suppression is applied to add one single pseudo-labeled example for each class by extracting the most confident 
                        
                           
                              x
                              ☆
                           
                           ∈
                           
                              X
                              ☆
                           
                        
                     :

                        
                           (5)
                           
                              
                                 
                                    find
                                    
                                    
                                       x
                                       ☆
                                    
                                    ∈
                                    
                                       X
                                       ☆
                                    
                                    :
                                    
                                       w
                                       ☆
                                       
                                          
                                             L
                                             F
                                          
                                       
                                    
                                    
                                       [
                                       k
                                       ]
                                    
                                    =
                                    arg
                                    
                                    
                                       max
                                       j
                                    
                                    
                                       w
                                       j
                                       
                                          
                                             L
                                             F
                                          
                                       
                                    
                                    
                                       [
                                       k
                                       ]
                                    
                                 
                              
                           
                        
                     The selected x
                     ⋆ and its corresponding pseudo-label 
                        
                           
                              y
                              ^
                           
                           ☆
                        
                      are added to 
                        
                           X
                           l
                           
                              
                                 L
                                 F
                              
                           
                        
                      and 
                        
                           Y
                           
                              
                                 L
                                 F
                              
                           
                        
                      respectively. If no x
                     ⋆ satisfying Eq. (4) are found, then nothing is added to 
                        
                           X
                           l
                           
                              
                                 L
                                 F
                              
                           
                        
                      and 
                        
                           Y
                           
                              
                                 L
                                 F
                              
                           
                        
                     .

Similarly, the classifier 
                        
                           ϕ
                           
                              
                                 E
                                 F
                              
                           
                        
                      chooses some examples in 
                        
                           X
                           u
                           
                              
                                 L
                                 F
                              
                           
                        
                      to pseudo-label for 
                        
                           ϕ
                           
                              
                                 L
                                 F
                              
                           
                        
                     . At the next co-training round, two new classifiers 
                        
                           ϕ
                           
                              
                                 E
                                 F
                              
                           
                        
                      and 
                        
                           ϕ
                           
                              
                                 L
                                 F
                              
                           
                        
                      are trained on the respective views, that now contain both labeled and pseudo-labeled examples. The complete procedure of the CURL method is outlined in Algorithms 1–3.
                     
                     
                  

The unsupervised representation learning is done once, and its computational cost is dominated by that of co-training which, in turn, mostly depends on the cost of the supervised training steps. The number of these steps is linear in the number of co-training rounds, and their individual cost increases with the number of pseudo-labels assigned.

@&#EXPERIMENTS@&#

CURL is parametric with respect to the projection function φ used in the unsupervised representation learning URL, and the supervised classification technique C used during to co-train 
                        
                           ϕ
                           
                              
                                 E
                                 F
                              
                           
                        
                      and 
                        
                           ϕ
                           
                              
                                 L
                                 F
                              
                           
                        
                     . As first embodiment of CURL, we used Ensemble Projection [52] for the former and logistic regression for the latter. Another embodiment, based on LapSVM [8] is presented in Section 5.4.

We evaluated our method on two data sets: Scene-15 (S-15) [43], and Caltech-101 (C-101) [62]. Scene-15 data set contains 4485 images divided into 15 scene categories with both indoor and outdoor environments. Each category has 200 to 400 images. Caltech-101 contains 8677 images divided into 101 object categories, each having 31 to 800 images. Furthermore, we collected a set of random images by sampling 20,000 images from the ImageNet data set [63] to evaluate our method on the task of self-taught image classification. Since the current version of ImageNet has 21,841 synsets (i.e. categories) and a total of more than 14 million images, there is a small probability that the random images and images in the two considered data sets come from the same distribution.

In our experiments we used the following three features: GIST [64], Pyramid of Histogram of Oriented Gradients (PHOG) [65], and Local Binary Patterns (LBP) [66]. GIST was computed on the rescaled images of 256 × 256 pixels, at 3 scales with 4, 8 and 8 orientations, respectively. PHOG was computed with a 2-layer pyramid and in 8 directions. Uniform LBP with radius equal to 1, and 8 neighbors was used.

In Section 5.3 we also investigate the use of features extracted from a CNN [67] in combination with the previous ones.

Differently from others semi-supervised methods that train a classifier from labeled data with a regularization term learned from unlabeled data, Ensemble Projection [52] learns a new image representation from all known data (i.e. labeled and unlabeled data), and then trains a plain classifier on it.

Ensemble Projection learns knowledge from T different prototype sets 
                           
                              
                                 P
                                 t
                              
                              =
                              
                                 
                                    {
                                    
                                       (
                                       
                                          s
                                          i
                                          t
                                       
                                       ,
                                       
                                          c
                                          i
                                          t
                                       
                                       )
                                    
                                    }
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    r
                                    n
                                 
                              
                              ,
                           
                         with 
                           
                              t
                              ∈
                              {
                              1
                              ,
                              …
                              ,
                              T
                              }
                           
                         where 
                           
                              
                                 s
                                 i
                                 t
                              
                              ∈
                              
                                 {
                                 1
                                 ,
                                 …
                                 ,
                                 L
                                 +
                                 U
                                 }
                              
                           
                         is the index of the ith chosen image, 
                           
                              
                                 c
                                 i
                                 t
                              
                              ∈
                              
                                 {
                                 1
                                 ,
                                 …
                                 ,
                                 r
                                 }
                              
                           
                         is the pseudo-label indicating to which prototype 
                           
                              s
                              i
                              t
                           
                         belong to. r is the number of prototypes in 
                           
                              
                                 P
                                 t
                              
                              ,
                           
                         and n is the number of images sampled for each prototype. For each prototype set, m hypotheses are randomly sampled, and the one containing images having the largest mutual distance is kept.

A set of discriminative classifiers ϕt
                        (·) is learned on 
                           
                              
                                 P
                                 t
                              
                              ,
                           
                         one for each prototype set, and the projected vectors ϕt
                        (x
                        
                           i
                        ) are obtained. The final feature vector is obtained by concatenating these projected vectors.

Following [52] we set 
                           
                              T
                              =
                              300
                              ,
                           
                        
                        
                           
                              r
                              =
                              30
                              ,
                           
                        
                        
                           
                              n
                              =
                              6
                              ,
                           
                        
                        
                           
                              m
                              =
                              50
                              ,
                           
                         using Logistic Regression (LR) as discriminative classifier ϕt
                        (·) with 
                           
                              C
                              =
                              15
                           
                        .

Within CURL, Ensemble Projection is used to learn both Early Fusion and Late Fusion unsupervised representations. In the case of Early Fusion (EF), the feature vector x
                        
                           i
                         is obtained concatenating the S different features available 
                           
                              
                                 x
                                 i
                              
                              =
                              
                                 [
                                 
                                    x
                                    i
                                    
                                       (
                                       1
                                       )
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    x
                                    i
                                    
                                       (
                                       S
                                       )
                                    
                                 
                                 ]
                              
                              ,
                           
                        
                        
                           
                              s
                              =
                              1
                              ,
                              …
                              ,
                              S
                           
                        . In the case of Late Fusion (LF), the feature vector x
                        
                           i
                         is made by considering just one single feature at time 
                           
                              
                                 x
                                 i
                              
                              =
                              
                                 x
                                 i
                                 
                                    (
                                    s
                                    )
                                 
                              
                           
                        . For both EF and LF, the same number T of prototypes is used in order to assure that the unsupervised representations have the same size. The posterior probabilities computed by the LR classifier on the feature vector x
                        
                           i
                         are used as the confidence scores w
                        
                           i
                        .

We conducted two kinds of experiments: (1) comparison of our strategy with competing methods for semi-supervised image classification; (2) evaluation of our method at different number of co-training rounds. We considered three scenarios corresponding to three different ways of using unlabeled data. In the inductive learning scenario 25% of the unlabeled data is used together with the labeled data for the semi-supervised training of the classifier; the remaining 75% is used as an independent test set. In the transductive learning scenario all the unlabeled data is used during both training and test. In the self-taught learning scenario the set of unlabeled data is taken from an additional data set featuring a different distribution of image content (i.e. the 20,000 images from ImageNet); all the unlabeled data from the original data set is used as an independent test set.

As evaluation measure we followed [52] and used the multi-class average precision (MAP), computed as the average precision over all recall values and over all classes. Different numbers of training images per class were tested for both Scene-15 and Caltech-101 (i.e. 1, 2, 3, 5, 10, and 20). All the reported results represent the average performance over ten runs with random labeled-unlabeled splits.

The performance of the proposed strategy are compared with those of other supervised and semi-supervised baseline methods. As supervised classifiers we considered Support Vector Machines (SVM). As semi-supervised classifiers, we used LapSVM [8,68]. LapSVM extend the SVM framework including a smoothness penalty term defined on the Laplacian adjacency graph built from both labeled and unlabeled data. For both SVM and LapSVM we experimented with the linear, RBF and χ
                        2 kernels computed on the concatenation of the three available image features as in [52]. The parameters of SVM and LapSVM have been determined by a greedy search with a three-fold cross validation on the training set. We also compared the present embodiment of CURL against Ensemble Projection coupled with a logistic regression classifier (EP+LR) as in [52].

@&#EXPERIMENTAL RESULTS@&#

As a first experiment we compared CURL against EP+LR, and against SVMs and LapSVMs with different kernels. Specifically, we tested the two co-trained classifiers operating on early-fused and late-fused representations, both employing EP for URL and LR as classifier C, that we call CURL-EF(EP+LR) and CURL-LF(EP+LR), respectively. We also included a variant of the proposed method. It differs in the number of pseudo-labeled examples that are added at each co-training round. The variant skips the non-maximum suppression step, and at each round, adds all the examples satisfying Eq. (3). We denote the two co-trained classifiers of the variant as CURL-EF
                        n
                     (EP+LR) and CURL-LF
                        n
                     (EP+LR).


                     Fig. 2
                      shows the classification performance with different numbers of labeled training images per class, in the three learning scenarios for both the Scene-15 and Caltech-101 data sets. For the CURL-based methods we considered five co-training rounds, and the reported performance corresponds to the last round. For SVM and LapSVM only the results using χ
                     2 kernel are reported, since they consistently showed the best performance across all the experiments. Detailed results for all the tested baseline methods, and for the CURL variants across the co-training rounds are available in Tables 1
                     –3
                     
                     .

The behavior of the methods is quite stable with respect to the three learning scenarios, with slightly lower MAP obtained in the case of self-taught learning. It is evident that our strategy outperformed the other methods in the state of the art included in the comparison across all the data sets and all the scenarios considered. Among the variants considered, CURL-LF(EP+LR) demonstrated to be the best in the case of a small number of labeled images, while CURL-LF
                        n
                     (EP+LR) obtained the best results when more labeled data is available. Classifiers obtained on early-fused representations performed generally worse than the corresponding ones obtained on late-fused representations, but they are still uniformly better than the original EP+LR Ensemble Projection which can be considered as their non-cotrained version. SVMs and LapSVMs performed poorly on the Scene-15 data set, but they outperformed EP+LR and some of the CURL variants on the Caltech-101 data set.

Co-training allows to make good use of the early fusion representations that otherwise lead to worse results than late fusion representations. In our opinion this happens because the two views capture different relationships among data. This fact is visible in Fig. 3, which shows 2D projections obtained by applying the t-SNE [69] method to GIST, PHOG, LBP features, their concatenation, and their learnt early- and late-fused representations.

Unsupervised
                      representation learning allows t-SNE to identify groups of images of the same class. Moreover, representations based on early and late fusion induces different relationships among the classes. For instance, in the second row of Fig. 3f the blue and the light green classes have been placed close to each other on the bottom right; in Fig. 3e, instead, the two classes are well separated. The difference in the two representations explains the effectiveness of co-training and justifies the difference in performance between CURL-EF(EP+LR) and CURL-LF(EP+LR). As further investigation, we also combined the two classifiers produced by the co-training procedure obtaining two other variants of CURL that we denoted as CURL-EF&LF(EP+LR) and CURL-EF&LF
                        n
                     (EP+LR). However, in our experiments, these variants did not caused any significant improvement when compared to CURL-LF (EP+LR).

Here we analyze in more details the performance of our strategy across the five co-training rounds. Results are reported in Fig. 4 with lines of increasing color saturation corresponding to rounds one to five. CURL-LF(EP+LR) is reported in red lines, while CURL-LF
                           n
                        (EP+LR) in blue. Results are reported in terms of MAP improvements with respect to EP+LR, which, we recall, corresponds to CURL-EF(EP+LR) with zero co-training rounds. For CURL-LF(EP+LR), performances always increase with the number of rounds. For CURL-LF
                           n
                        (EP+LR), this is not true on the Scene-15 data set with a small number of labeled examples. In CURL-LF
                           n
                        (EP+LR) each round of co-training adds all the promising unlabeled samples, with a high chance of including some of them with the wrong pseudo-label. This may result in a ‘concept drift’, with the classifiers being pulled away from the concepts represented by the labeled examples. This risk is lower on the Caltech-101 (which tends to have more homogeneous classes than Scene-15) and when there are more labeled images. The original CURL-LF(EP+LR) is more conservative, since each of its co-training rounds adds a single image per class. As a result, increasing the rounds usually increases MAP and never decreases it by an appreciable amount.

We observed the same behavior for CURL-EF(EP+LR) and CURL-EF
                           n
                        (EP+LR). We omit the relative figures for sake of brevity.

The plots confirm that CURL-LF(EP+LR) is better suited for small sets of labeled images, while CURL-LF
                           n
                        (EP+LR) is to be preferred when more labeled examples are available. The representation learned from late fused features explains part of the effectiveness of CURL. In fact, even CURL-LF(EP+LR) without co-training (zero rounds) outperforms the baseline represented by Ensemble Projection.

In this section we compare the proposed classification strategy with standard co-training [9]. Since standard co-training employs two views, we compare with all the six possible combinations of features that we can generate by assigning one feature to each view or the concatenation of two features to one view and a single feature to the other view.

The experimental results are reported in Fig. 5, for both the Scene-15 and Caltech-101 data sets. Results are reported only for the classifier obtained by combining the scores on the two views, since its performance are always better that those of each single-view classifier. We report the results in the transductive scenario only.

It can be seen that the results obtained by CURL are better than those obtained by any combination of features with standard co-training. In particular we can observe that on the Scene-15 data set, the co-training variants using only PHOG features as a view obtain the worst performance. On the Caltech-101 data set instead, the co-training variants using only GIST features as a view obtain the worst performance. This is not surprising since GIST were specifically designed to provide a rough description (i.e. the gist) of the scene.

In this further experiment we want to test if the proposed classification strategy works when more powerful features are used. Recent results indicate that the generic descriptors extracted from pre-trained Convolutional Neural Networks (CNN) are able to obtain consistently superior results compared to the highly tuned state of the art systems in all the visual classification tasks on various datasets [67]. We extract a 4096-dimensional feature vector from each image using the Caffe [70] implementation of the deep CNN described by Krizhevsky et al. [34]. The CNN was discriminatively trained on a
                         large dataset (ILSVRC 2012) with image-level annotations to classify images into 1000 different classes. Briefly, a mean-subtracted 227 × 227 RGB image is forward     propagated through five convolutional layers and two fully connected layers. Features are obtained by extracting activation values of the last hidden layer. More details about the network architecture can be found in [34,70].

We leverage the CNN features in CURL using them as a fourth feature in addition to the three used in Section 4. The discriminative power of these CNN features alone can be seen in Fig. 6, where their 2D projections obtained applying the t-SNE [69] method are reported.

The experimental results using the four features, are reported in Fig. 7, for both the Scene-15 and Caltech-101 data sets. We report the results in the transductive scenario only. It can be seen that the results using the four features are significantly better than those using only three features mainly due to the discriminative power of the CNN features. Furthermore, the CURL variants achieve better results than the baselines. This suggests that CURL is able to effectively leverage both low/mid level features as LBP, PHOG and GIST, and more powerful features as CNN.
                     

In this section we want to evaluate the CURL performance in a different embodiment. Specifically, we substitute the EP and LR components with LapSVM-based ones. In the LapSVM, first, an unsupervised geometrical deformation of the feature kernel is performed. This deformed kernel is then used for classification by a standard SVM thus by-passing an explicit definition of a new feature representation. In this CURL embodiment we exploit the unsupervised step as surrogate of the URL component, and SVM as C component. The confidence scores are computed by converting the signed distance from the separating hyperplanes in the kernel space into probabilities [71]. The EF view is obtained concatenating the GIST, PHOG, LBP and CNN features and generating the corresponding kernel, while the LF one is obtained by a linear combination of the four kernels computed on each feature. This is similar to what is done in multiple kernel learning [59]. Due to its performance in the previous experiments, the χ
                        2 kernel is used for both views. The experimental results on the Scene-15 and Caltech-101 data sets in the transductive scenario, are reported in Fig. 8. We named the variants of this CURL embodiment by adding the suffix (LapSVM). It can be seen that the behavior of the different methods is the same of the previous plots, with the LapSVM-based CURL outperforming the standard LapSVM. The plots confirm that CURL-LF(LapSVM) is better suited for small sets of labeled images, while CURL-LF
                           n
                        (LapSVM) is to be preferred when more labeled examples are available.

In Figs. 9 and 10 qualitative results for the ‘Panda’ class of the Caltech-101 data set are reported: the results are relative to the case in which a single instance is available for training and one single example is added at each co-training round (i.e. each pair of rows correspond to CURL-EF(LapSVM) and CURL-LF(LapSVM), respectively). The left part of Fig. 9 contains the training examples that are added by the CURL-EF(LapSVM) and CURL-LF(LapSVM) at each co-training round, while the right part and Fig. 10 contain the first 40 test images ordered by decreasing classification confidence. Samples belonging to the current class are surrounded by a green bounding box, while a red one is used for samples belonging to other classes.

In the sets of training images, it is possible to see that after the first co-training round, CURL-LF(LapSVM) selects new examples to add to the training set, while CURL-EF(LapSVM) adds examples selected by CURL-LF(LapSVM) in the previous round. This is a pattern that we found to occur also in other categories when very small training sets are used.

In the sets of test images, it is possible to see that more and more positive images are recovered. Moreover, we can see how the images belonging to the correct class tends to be classified with increasing confidence and move to the left, while the confidences of images belonging to other classes decrease and are pushed to the right.

In this experiment we want to test the proposed classification strategy on a large scale data set, namely the ILSVRC 2012 which contains a total of 1000 different classes. The experiment is run on the ILSVRC 2012 validation set since the training set was used to learn the CNN features. The ILSVRC 2012 validation set, which contains a total of 50 images for each class, has been randomly divided into a training and a test set containing each 25 images per class. Again, different numbers of training images per class were tested (i.e. 1, 2, 3, 5, 10, and 20). The second embodiment of CURL is used in this experiment.

The experimental results are reported in Fig. 11 and represent the average performance over ten runs with random labeled-unlabeled feature splits.

Given the large range of MAP values, the plot of MAP improvements with respect to LapSVM baseline is also reported. It can be seen that the behavior is similar to that of the previous plots, with the LapSVM-based CURL variants outperforming the LapSVM. As for the previous data sets, the plots show that CURL-EF(LapSVM) and CURL-LF(LapSVM) are better suited for small sets of labeled images, while CURL-EF
                           n
                        (LapSVM)and CURL-LF
                           n
                        (LapSVM) are to be preferred when more labeled examples are available. It is remarkable that the proposed classification strategy is able to improve the results of the LapSVM, since the CNN features were specifically learned for the ILSVRC 2012.
                        
                     

@&#CONCLUSIONS@&#

In this work we have proposed CURL, a semi-supervised image classification strategy which exploits unlabeled data in two different ways: first two image representations are obtained by unsupervised learning; then co-training is used to enlarge the labeled training set of the corresponding classifiers. The two image representations are built using two different fusion schemes: early fusion and late fusion.

The proposed strategy has been tested on the Scene-15, Caltech-101, and ILSVRC 2012 data sets, and compared with other supervised and semi-supervised methods in three different experimental scenarios: inductive learning, transductive learning, and self-taught learning. We tested two embodiments of CURL and several variants differing in the co-trained classifier used and in the number of pseudo-labeled examples that are added at each co-training round. The experimental results showed that the CURL embodiments outperformed the other methods in the state of the art included in the comparisons. In particular, the variants that add a single pseudo-labeled example per class at each co-training round, resulted to perform best in the case of a small number of labeled images, while the variants adding more examples at each round obtained the best results when more labeled data are available.

Moreover, the results of CURL using a combination of low/mid and high level features (i.e. LBP, PHOG, GIST, and CNN features) outperform those obtained on the same features by state of the art methods. This means that CURL is able to effectively leverage less discriminative features (i.e. LBP, PHOG, GIST) to boost the performance of more discriminative ones (i.e. CNN features).

@&#REFERENCES@&#

