@&#MAIN-TITLE@&#Evaluation of two-view geometry methods with automatic ground-truth generation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Easy data collection in any 3D, feature rich scene with any calibrated camera


                        
                        
                           
                           Ground truth acquired automatically by high resolution structure from motion


                        
                        
                           
                           Rigorous 2-view geometry evaluation with many reduced resolution test samples


                        
                        
                           
                           More elaborate rigid geometry tests are trivial to devise, given 3D ground truth.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Evaluation

Database

Automated ground truth

Structure from motion

Multi-view geometry

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

The problem of recovering the geometry of a pair of partially overlapping camera views has been addressed in a multitude of ways. The most common solutions consist of establishing correspondence by means of robust local image feature extraction and matching, computing an initial estimate of geometry using a linear method in a robust framework and, lastly, iterative non-linear refinement [1–4]. Evaluating the performance of such a system is in itself an important and challenging problem. While geometry estimation methods could be analysed with relative ease by means of simulation, evaluating the effectiveness of the visual components of the process is more difficult.

A number of evaluations related to multi-view geometry have been made available, as reviewed in Section 2. The majority of these methods focus on a theoretical analysis of the performance of local image features and descriptors (see [5] for a review of local image features). These evaluations suffer from various limitations, such as unrealistic and restricted scenery and small data set size, and extending them can be a difficult and expensive task.

Many evaluations involve planar scenes, which are not sufficiently challenging for emerging wide-baseline techniques. A number of 3D scene evaluations have been prepared using a robot arm to acquire the camera pose accurately. While these are more challenging, they are limited to small, artificial scenes (unless one is willing to acquire a large, expensive robotic arm). Extending any of these data sets to other spectra, such as thermal infra-red, is not feasible because thermal features are typically spatially diffuse and relatively homogeneous on continuous planes.

This paper proposes an evaluation system that makes use of calibrated video with relatively high spatio-temporal resolution to acquire ground truth geometry and generates a large number of difficult test cases by decimating the video. The ground truth is computed by means of a structure from motion (SFM) algorithm designed to maximise accuracy. The only equipment required is a calibrated camera. Test data is generated by reducing the spatial resolution of the video and selecting pairs of frames with geometry characteristics that match the desired evaluation regime. For example, in order to evaluate a system's robustness to change in depth or scale, frame pairs are selected with a range of different depth ratios, while ensuring that the change in view angle is tightly controlled to eliminate its influence on results. The high spatio-temporal resolution of the original video and the accuracy of the ground truth reconstruction system typically ensures that the error in the ground truth is several orders of magnitude smaller than what any current multi-view geometry system can produce from the decimated, wide baseline test cases.

The proposed system is useful for constructing evaluations of any part of the multi-view geometry estimation process in any scene with at least some trackable features and is not restricted to the visual domain. It is especially suitable for mid to wide baseline evaluations. As long as there is a significant reduction in resolution between the original and test video, the evaluation will produce valid results. If original and test resolution are the same, the accuracy of the ground truth may be comparable to the test system and will therefore not provide a meaningful baseline.

Example evaluation scenarios are presented in Section 4 to demonstrate the proposed system. While the presented evaluations are not exhaustive in terms of the range of evaluated techniques, they serve to demonstrate the flexibility and application domain of the proposed evaluation system.

@&#RELATED WORK@&#

A large number of data sets and evaluations have been published that are related to evaluating multi-view geometry methods. The vast majority of these focus on evaluating the performance of local feature extractors (Section 2.1), while a few are aimed at specific problems (Section 2.2).

Early methods used visual inspection and human defined ground truth, such as the location of lines and edges [6,7]. Human defined ground truth can be subjective, incompatible with what machines can detect reliably and requires many hours of work, even for simple scenes.

The majority of established evaluations make use of planar scenes in order to simplify ground truth acquisition. More recently, evaluations involving 3D scenes have been produced.

Using a planar scene or stationary camera enables relating different images by a linear coordinate mapping and makes the process of computing ground truth relatively easy. The core metric used in planar evaluations is feature repeatability, measured as the proportion of commonly visible points that are repeated in both views with sufficiently low error. Different authors use different methods for determining whether a given feature is repeated. In general, the repeatability measure shows a positive bias in the presence of high feature density due to some coincidental feature overlap and arbitrary error thresholds.

In [8] features are considered repeated if the homography transfer error is below a threshold (which is chosen arbitrarily). The ground truth is determined by projecting a reference pattern onto the planar scene and accurately extracting projected reference points. This method limits the size of the scene for which ground truth can be computed.

The definition of repeatability was expanded in [9] to use the projection of a scale normalised ellipse bounding each feature. The error between two features is measured as the proportion of overlapping ellipse area, rather than just transfer error. This method measures not only position error, but also relative shape error. Scale error is not measured, but rather normalised, since selecting a large feature scale can artificially improve overlap. Once again an arbitrary (and rather large) threshold is chosen to distinguish repeated and non-repeated features. This evaluation was used to compare the performance of affine covariant features. The test data consisted of a small set of images of planar scenes aligned by homographies. Ground truth alignment is computed in a semi-automatic manner — user specified correspondences are used to initialise an iterative intensity-based alignment. The same data set and a closely related method for determining correspondence is used in [10] to evaluate local descriptors. A similar data set and evaluation of feature performance in the thermal infra-red domain was reported in [11].

Spherical fiducial markers were used in [12] to help automate ground truth acquisition. Using a presentation board with these spherical markers at the corners limits the data to small, printed textures. The data set is aimed at evaluating visual tracking and includes long sequences with a short baseline between successive frames. In addition to using the repeatability metric, this method includes evaluating the accuracy of recovered homographies to assess the tracking result. A similar data set was reported in [13]. Here a robotic measurement arm and planar fiducials were used to acquire the camera position accurately.

The first evaluation that was not constrained to a planar scene is reported in [14]. The method computed dense stereo correspondence between two short baseline views to generate an initial set of ground truth correspondences. The correspondences are then transferred to other views using the trifocal tensor. It is not clear how camera extrinsic parameters were acquired. The data set consists of two short image sequences that are not publicly available.

A data set captured from objects on a turntable is presented in [15]. Ground truth camera positions are computed using known rotation angles of the turntable. Calibration is performed using a standard calibration object, but is not provided with the data set and is not consistent throughout the data due to capturing issues. Correspondences are validated by three view triangulation. The use of a turntable limits the data set in several ways. Objects are relatively small and very convex and always observed from the same distance (no change in scale). This is applicable to some applications, but is not very general.

A data set of image patches for training and testing local descriptors was produced in [16]. A sparse 3D reconstruction and ground truth correspondences was produced from a collection of public images of two popular landmarks using the system in [17]. Corresponding image patches were then sampled and scale normalised to generate a large set of descriptor patches, including matched pairs and unmatched patches. Random “jitter” is introduced into the test patches to simulate feature extractor variability. This data set is only suitable for evaluating local descriptors.

A high resolution multi-view stereo (MVS) data set with 3D structure ground truth acquired using light detection and ranging (LIDAR) is reported in [18]. The purpose of the data set is to compare MVS performance to LIDAR performance and encouraging the improvement of MVS to the point where it is comparable to LIDAR. The data consists of short sequences of high resolution images and LIDAR scans of the scene. A small number of landmarks placed in the scene were used to calibrate the cameras. The scenes were acquired outdoors and consisted of buildings and other large objects.

A method for measuring feature repeatability is presented in [19] and applied to an evaluation framework for transparency (x-ray) images. A sequence of images is captured using a calibrated camera and short baselines between images. Reference points are extracted from a reference frame using a test detector and tracked through the image sequence (by means of detection and matching) to a target frame. 3D reconstruction and refinement techniques are used to refine the positions of the tracked points. Features are then detected in the target frame and compared to the tracked reference points. Points with error below a threshold are considered repeated. This method essentially compares the results of a feature detector to a refined version of itself, which raises questions of validity.

A camera mounted on a robotic arm was used in [20] to produce a data set of 3D scenes, ground truth camera positions and 3D scene structure. The robotic arm allowed exact camera orientation control. The 3D scene structure was acquired using a structured light scanner. The shortcomings of this data set are the limited physical size, cost of the robot arm and structured light scanner, and only studio lighting conditions are available.

In [21,22] pairs of cameras are used to collect data sets and their epipolar geometry is used to evaluate test systems. The ground truth epipolar geometry for each pair is computed automatically by accumulating correspondences over the entire high resolution image set acquired with that pair. Test images are generated by down-sampling the high resolution images, in order to make the evaluation more difficult. The result is that the error in the ground truth geometry estimate is several orders of magnitude below what is currently achievable using the test images. The major drawback of this data set is that it does not include relative view angles or scales, which are important geometric variables in evaluating performance.

An early optical flow evaluation used a robotic arm to move a camera along a defined trajectory [23]. It is argued in [24] that the camera centre cannot be accurately obtained from the robot arm position, nor does the arm control system give a sufficiently accurate position. Instead, they use a calibration pattern as a scene background, in order to recover the camera pose accurately. As with the wide baseline local feature evaluations that use robotic arms, these data sets involve small scale models. A computer rendered synthetic data set and a real data set containing simple piecewise planar objects was presented in [25]. Ground truth for the real data set was generated by painstakingly labelling the positions of the vertices of the planar objects in each frame and computing the optical flow from the geometry. More recently, [26] made use of several methods to establish ground truth for different optical flow tests. The first method employs texture that is only made visible to a fluorescent light tracking system. The second is computer rendering said to be realistic. The third method is intended for a frame interpolation task. Video is captured at a high frame rate and a more sparse subset of the frames is presented as testing material. Interpolated frames are then compared to the additional captured frames. The fourth method uses structured light to acquire ground truth, similar to the authors' earlier work in [27].

A stereo camera system with a pan–tilt–zoom head was used in [28]. The pan–tilt head provided 2° of rotation and zoom with exact control, yielding accurate ground truth. This data set does not include camera motion, but only rotation and zoom. It is used to automatically calibrate a camera from natural images.

A multi-view stereo data set is reported in [29]. The data is acquired using a spherical gantry with studio lighting. The subjects are small, highly convex objects, similar to [15]. Ground truth object shape was acquired by merging multiple structured light scans.

A method and data set for evaluating object tracking is presented in [30]. The data consists of three video sequences in which the position of one object is manually annotated. The tracking error of the object's corners is measured and expressed as a percentage of the object size. The number of loss of locks is counted, where these are defined as a tracking error of greater than 25%.

Visual features are used in many areas other than multi-view geometry computation. Examples include object recognition [31,32] and content-based image retrieval [33]. This paper focuses only on geometry applications.

@&#DISCUSSION@&#

Extending the available evaluation data is desirable for several reasons, including producing more challenging evaluations and evaluations suited to more specific application environments. Very little has been done to evaluate features and geometry computation outside of the visible spectrum, for example. Large data sets are required for a rigorous evaluation. The evaluation methods reviewed above have some shortcomings in terms of extensibility.

While the planar methods are quite easy to extend, they are not relevant to the problem of recovering 3D geometry and not sufficiently challenging at a feature level. Methods such as those reported in [21], for example, exploit local scene planarity and can produce a large number of almost perfect correspondences, even over very wide baselines.

Existing 3D acquisition techniques are either constrained to small, indoor scenes or require expensive equipment, such as LIDAR or robotic arms. Small spaces are not suitable for all spectra and not realistic for many applications. The method proposed in this paper aims to acquire the camera pose and sparse 3D structure ground truth in all practical environments, without the constraints of a robotic arm or expensive equipment, other than a calibrated imaging device.

While producing relevant test data is important for evaluating algorithms in realistic scenarios, obtaining ground truth for multi-view geometry data can be very difficult. A system is proposed in this section that acquires ground truth camera pose and sparse 3D structure from high spatio-temporal resolution video by means of structure from motion (SFM). Test data is generated from the same video by reducing the spatial resolution of the video and selecting groups of frames that match desired geometric properties, such as relative view angle and change of depth. By applying a high accuracy SFM algorithm to video with significantly higher resolution than the test data, the ground truth structure is ensured to be of much higher accuracy than what is achievable with the test data.

The proposed method is considerably more flexible than previous approaches. It can be applied in any rigid environment where the imaging device can be deployed. Any part of the EM spectrum suitable for projection imaging can be used. It is relatively inexpensive, since no additional equipment other than an imaging device is required.

@&#OVERVIEW@&#

The SFM evaluation system consists of four stages, as illustrated in Fig. 1
                        :
                           
                              1.
                              Ground truth reconstruction.

Test case generation.

Test system.

Evaluation.

Ground truth reconstruction accurately reconstructs the camera poses and sparse 3D structure of high resolution video by means of a specially designed SFM system. It is assumed that the camera is a calibrated pinhole camera and that the observed scene is rigid. Lens distortion is removed as part of the calibration process and small non rigid scene variations are tolerated by means of automatic exclusion. The final stage of the ground truth generation system decimates the input video to test resolution. Video frames with negligible camera translation and rotation to a nearby frame are removed from the video to reduce redundancy and unbalanced weighting of results.

Test case generation consists of selecting pairs of frames to be used for testing. The selection is performed according to user specified requirements on the relative view angle and depth between frames. A set of ground truth structure points is selected for each frame pair to be used during evaluation. Post-processing is applied to the test images at this stage to allow testing effects such as image noise or compression. Each unique pair of frames generated in this process is referred to as a test case.

The test system must perform the task of computing the geometry of every test case. The input to the test system consists of the test case images. Depending on the test system design, relevant data such as calibration or trained models may also be provided. Multiple test repetitions can be run where random processes employed by the test system may result in variable output. Evaluation is performed by computing the error of selected ground truth structure points against the model computed by the test system. The system does not apply any arbitrary error thresholds to determine success or failure. It is recommended that the raw error distributions be analysed directly. It is possible to apply error thresholds during the analysis stage if desired.

A structure from motion algorithm is used to generate ground truth structure. While any SFM algorithm could be used, it is important to make use of large numbers of points and observations, and to not sacrifice accuracy in favour of computation time.

The ground truth reconstruction system used in this paper follows a fairly standard sparse structure from motion design. Individual algorithms were chosen to prioritise accuracy, rather than focus on computation time. All video frames are used in the reconstruction — data between key frames is not discarded. The reconstruction process consists of three phases: structure initialisation, forward reconstruction and reverse tracking. Each phase employs visual tracking techniques to gather observations, linear reconstruction of 3D points and cameras and non-linear refinement.

The visual front end includes feature extraction, global orientation tracking, block search and affine inverse compositional alignment (ICA) [34]. Different stages of the reconstruction system make use of these components differently depending on what information regarding the camera pose is already available.

The objective of feature extraction is to select image points that can be tracked reliably using affine ICA. This requires sufficient local edge structures within the tracking patch to constrain 6° of freedom. While this is difficult to detect, a characteristic scale feature extraction system can consistently select regions that provide at least 3 constraints.

The implemented feature extractor uses determinant of Hessian extrema [35] as basic features and the scale space feature sketch (SSFS) [36,37] is used to select characteristic scale features. The exact feature extraction method is not critical — methods such as [38] and [39] are reasonable alternatives. Weak features are modelled as zero mean Gaussian noise. Features with response within 3 standard deviations of the noise model are rejected. The number of features is not cut down for the sake of reducing processing time. As many good features as possible are tracked to maximise overall tracking accuracy. A local image patch is sampled for each extracted feature and used in the block search and affine alignment stages.

Camera rotation accounts for the majority of motion between consecutive frames captured using a hand held camera. This type of motion is easily compensated for using a linear projective transformation. Modelling this motion in the tracking stage is preferable to methods such as built in stabilisers, since these affect the calibration of the camera from frame to frame.

A global frame tracking algorithm was implemented that produces a projective homography, which can be used to guide feature tracking. It employs the following process.
                                 
                                    1.
                                    Decimate frame to have minimum dimension 200pixels wide.

Extract characteristic scale features as described above.

Compute scale invariant feature transform (SIFT) descriptors for features [38].

Features are matched across views using nearest-neighbour ratio matching [38] and a translation constraint of 20% of the image width (this can be adjusted according to the amount of expected rotation between frames).

A projective homography is computed using RANSAC [40] and then converted back to original image coordinates.

Block search is used to deal with significant feature translations due to depth parallax. A coarse-to-fine block matching approach is used. The difference between the patch and target image block is minimised by applying translations. The translation step size is progressively reduced in order to find the best local alignment down to a resolution of less than 2pixels.

Sub-pixel accurate feature tracking is achieved through inverse compositional alignment [34] using an affine geometric transformation and linear intensity transformation. This method provides highly accurate local feature tracking in the face of large view angle and scale change over a long video sequence. Good alignment initialisation is required for ICA to converge correctly. Depending on the available structure information, this is achieved through a combination of global frame tracking, block search or 3D point projection.

The first structure computation requires the selection of a pair of key frames with as many correspondences as possible, while separated by as large a baseline as possible to ensure non-degenerate geometry. At this stage it is not yet possible to meaningfully measure baseline. Instead, the quality of a linear (degenerate) model is compared with an epipolar geometry model to check for suitable geometry. The method used here is related to the GRIC score defined in [41] and the approach in [42].

Features are detected in the first (key) frame and tracked through consecutive frames as described in Section 3.2.1. At each frame, the two view geometry is computed relative to the first key frame. Both the epipolar geometry and a homography are computed. A geometry score is computed from the errors as,
                              
                                 (1)
                                 
                                    
                                       g
                                       =
                                       
                                          
                                             n
                                             f
                                          
                                          
                                             n
                                             h
                                          
                                       
                                       
                                          
                                             e
                                             h
                                          
                                          
                                             
                                                e
                                                f
                                             
                                          
                                       
                                       ,
                                    
                                 
                              
                           where nf
                            is the number of inliers for the epipolar geometry model, nh
                            is the number of homography inliers, ef
                            is the epipolar Sampson error and eh
                            is the homography Sampson error. A geometry score of g
                           >1 indicates that the epipolar geometry model better describes the geometry of the two frames than does a homography model. Outliers from the geometry computation are not immediately discarded. Features are removed from the tracking process once they have been marked as outliers in 5 frames. A frame score is computed as,
                              
                                 (2)
                                 
                                    
                                       f
                                       =
                                       
                                          
                                             g
                                             
                                                n
                                                i
                                             
                                          
                                          
                                             n
                                             0
                                          
                                       
                                       ,
                                    
                                 
                              
                           where ni
                            is the number of features tracked in the current frame, n
                           0 is the number of features detected in the first frame and g is the geometry score from Eq. (1). A simple low pass filter is applied to f to measure the overall trend in frame score, rather than sporadic variations. A key frame is selected if the filtered f decreases for three consecutive frames, while g
                           >1.

The fundamental matrix between the key frames is estimated using the 8 point algorithm [1]. It is then converted to an essential matrix by incorporating the camera calibration. The essential matrix is decomposed into two possible translations and two possible rotations [1]. The camera for the first key frame is set to 
                              
                                 
                                    
                                       
                                          K
                                       
                                       
                                          0
                                       
                                    
                                 
                              
                            (where K is the calibration matrix) and four possible cameras are computed for the second key frame from all combinations of the possible translations and rotations. 3D points are reconstructed for each camera configuration. The configuration and structure that results in the most 3D points being in front of both cameras is selected as the solution. This solution is refined using Levenberg Marquardt (LM) optimisation [43].

The intermediate cameras between the key frames are reconstructed using the efficient perspective n-point pose estimator (EPnP) [44]. Outlier observations are removed and 3D points with observations in fewer than half of the key frame sequence are eliminated. Finally, sparse bundle adjustment (SBA) [45] is applied to the whole key frame sequence.

After initial reconstruction, 3D points are tracked to each new image using the visual tracking system. The resulting 2D–3D correspondences are used to reconstruct the new camera using EPnP, followed by LM refinement. High error points are removed from the camera computation and points that yield high error for more than 5 frames are no longer tracked.

Key frames are selected based on the change in camera position relative to the commonly visible 3D structure. One of the following two criteria must be met for a pair of frames to be key frames:
                              
                                 1.
                                 The median angle between the two cameras and the visible world points must exceed 0.175.

The median change in distance between the cameras and the visible points must exceed 10%.

In other words, the camera baseline must be significantly large, relative to the visible world points.

At each key frame, the 3D structure is refined using local SBA. Global SBA is executed after every 1000 frames. Features detected at the last key frame are triangulated and incorporated into the 3D structure. New features are detected and tracked further. An attempt is made to relocate lost 3D tracks by projecting the point into the key frame, checking that it is in the field of view and attempting to align its patch with the current image. A previously lost 3D point track is reinstated if a low error alignment is achieved. All tracking data for non-key frames is retained.

The final stage of the reconstruction process consists of tracking 3D points in reverse chronological order of frames. This allows points to be tracked back past the key frame where they were detected, to where they first appeared in the scene.

The reverse tracking of a point starts at the first observation of a point after it has not been observed for some time. An image patch is extracted from the first observation frame and the point is then tracked backwards in time. The image patch is selected as a square region with size based on the detection scale of the point and the ratio of depth between the detection frame and the observation frame where the patch is extracted from. The existing camera pose estimates are used to project points to images where they were not observed in the first reconstruction pass. ICA is then applied to make a visual observation. Global SBA is executed at the end of the reverse tracking process to include the new observations in the structure estimate.

The decimation step converts the original video (that ground truth structure was computed from) to a set of images suitable for testing. The spatial resolution of the video is reduced in order to make testing much more difficult than the ground truth reconstruction. This ensures that the ground truth error is lower than what a test system could possibly achieve. Frames are removed from the sequence where the camera is temporarily stationary. This ensures a minimum amount of variation between frames and helps prevent certain portions of a scene being overrepresented and having greater impact on the evaluations than others. The scale factors and thresholds used in the decimation process are specified by the user according to experimental design requirements.

Each test case is generated by selecting a pair of test frames and a set of ground truth points visible in both frames. Depending on the objectives of the evaluation, it is possible to select cases with relative view angle and depth change in specific ranges. Once an initial set of cases with desired geometric properties has been selected, the test data can be processed to introduce further image variations, such as noise, lighting change or lens distortion. Each test image is processed to produce one or more images with the desired effect applied. Test cases are remapped to the processed images and the settings used to produce the image are recorded. Ground truth points are selected by finding structure points that are visible in both frames. No more than 100 of the lowest error points are used. Frame pairs that have fewer than 100 commonly visible structure points are not used for test cases.

@&#EVALUATION@&#

Once a test system has been applied to the set of test cases, the geometry data it produced is evaluated against the ground truth structure data. If the test system produces the epipolar geometry or a homography, then the Sampson error of the projections of the ground truth points is computed. For camera matrices, the reprojection error is computed. Other metrics relevant to the test system can also be recorded. For example, the evaluations presented in Section 4 include the number of matches produced and the proportion of the matches selected as inliers.

This section presents a selection of example data sets and analysis methods that can be prepared with the proposed evaluation system.

A small selection of existing wide baseline matching techniques have been put through the evaluation process. Three feature extractors are applied — Harris affine (HARAF), Hessian affine (HESAF) [46] and maximally stable extremal regions (MSER) [47]. The scale invariant feature transform (SIFT) descriptor [38] is used to compute a vector representation for each feature. Nearest neighbour ratio matching [38] is used to establish putative correspondences. The resulting matches are then processed in one of three ways:
                        
                           1.
                           Used as is.

The alignment between matched features is improved using the match alignment method from [21] (indicated by +Align). Some matches are rejected in the process.

The grid-expansion method from [21] is used to expand the set of matches (indicated by +Grid).

Finally, the set of matches is used to compute the two view geometry in a RANSAC framework [40].

The match processing methods from [21] attempt to improve the set of matches without attempting to recover the global geometry. Match alignment uses ICA to improve the accuracy of matches. Grid expansion uses the shape information contained in affine features to extrapolate additional correspondences on the same flat surface. ICA is used to refine the affine alignment of each match and to validate extrapolated matches.

The example evaluation will look at the difference in performance between the three feature extractors, as well as the result of applying the different match processing methods. It is possible to compare the relative performance of any other part of the geometry computation process in a similar manner.

The primary advantage of the proposed ground truth generation system is the ability to acquire a wide variety of data with relatively limited effort. The following example data sets are presented here to showcase the flexibility of the system:
                           
                              1.
                              
                                 monument — A white statue surrounded by textured structures. The distance between the camera and dominant scene components varies very little. Captured using a Canon 500D at 3megapixel resolution and 8framespersecond. Reconstructed 275 cameras and 19,406 points with a mean error of 0.897pixels2. 163 test images were selected and their resolution was decimated by factor 0.3.


                                 city — A cityscape captured from a river ferry. Features city buildings with many repeated structures, as well as park areas with a lot of foliage. Captured using a Canon 500D at 1080p resolution and 25framespersecond. Reconstructed 6659 cameras and 66,916 points with mean error of 1.32pixels2. 734 test images were selected and their resolution was decimated by factor 0.5.


                                 amphitheatre — An empty amphitheatre. Captured in a radial camera motion pattern to facilitate evaluation over depth change. Captured using a Canon 500D at 3megapixel resolution and 8 frames per second. Reconstructed 1211 cameras and 66,405 points with mean error of 1.73pixels2. 257 test images were selected and their resolution was decimated by factor 0.3.


                                 thermal — A video sequence acquired using a Thermoteknix Miricle 307K thermal infra-red camera at 640×480 resolution and 25framespersecond. Reconstructed 772 cameras and 1172 3D points with a mean error of 0.778pixels2. 275 test images were selected and their resolution was decimated by factor 0.5.

Example images from these data sets are shown in Fig. 2
                         and their 3D reconstructions are shown in (Fig. 3
                        ). The Canon 500D camera was calibrated using methods in [48] and the Thermoteknix Miricle 307K camera was calibrated using the thermal mask technique [49]. As an example of the computational cost of the ground truth reconstruction system, the city data was processed at 0.5framespersecond, on average. This includes both forward and backward visual tracking and non-linear refinement.

@&#RESULTS AND ANALYSIS@&#

The reprojection error of the ground truth data should be checked to ensure that it is significantly lower than the test results. If the ground truth error and test results are similar, then the value of the ground truth data becomes questionable. The ground truth error can be arranged and represented in a similar fashion to the test result error. Fig. 4
                            shows a box plot of the ground truth error for the monument data set, with a separate box for each view angle bin. It can be seen that the error is no larger than a tenth of a pixel squared. Note that the ground truth data error is much lower than the value reported for the original reconstruction (Section 4.1) due to the decimation process and the selection of the best data points.

When compared with the results in the following section (Fig. 5
                           ), it can be seen that the test error results are only comparable to the ground truth in short baseline cases, as may be expected. From approximately 10°–15° relative baseline onwards the ground truth error remains an order of magnitude or more smaller than the test error.


                           Figs. 5–7
                           
                            show detailed error, match count and inlier match ratio results for the monument data set. The test cases used to generate this data were controlled so that the relative change in depth between images is in the range 1–1.2, so that the effect of view angle change can be examined. Each view angle bin was set to 2° wide. A total of 3617 test cases were generated. It is possible to vary depth or the degree of post processing effect instead of view angle — examples are given in Section 4.2.3. The results are presented in the form of box plots with data divided according to view angle (relative baseline).


                           Fig. 5 shows the model error for every feature extractor and match processor combination on a logarithmic scale. A reference line indicating an error level of 10% of the image width is included to assist the reader in interpreting the scale of the data. A log error of less than 0 indicates sub-pixel accuracy. The error results exhibit an error ceiling (in this case near 106
                           pixels2) which corresponds roughly with the largest dimension of the image. Test cases with error this high can be assumed to be completely unrelated to the true geometry.

In this example the base feature detectors perform in a manner consistent with previous theoretical evaluations. Harris-affine and Hessian-affine perform similarly, while MSER maintains a lower and more compact error profile at wider view angles. The match processing methods yield a consistent improvement in combination with all base feature types. Match alignment results in reduced error for most test cases and increased error in a small proportion of cases. Grid expansion provides a further improvement. A large proportion of wide angle cases were improved from being unusable to less than 10% error (see for example HARAFF at 30°–40°). At lower viewing angles, many cases were brought down to sup-pixel accuracy.

An analysis of the number of matches produced by each method is presented in Fig. 6. For the base detectors, the decrease in matches as view angle increases stops at around 100 matches. This is the result of an increase in incorrect matches (see Fig. 7 and discussion below). The match alignment method rejects many of the incorrect matches, resulting in a more consistent decline in match count. Most interestingly, these results reveal a flaw in the grid expansion method that was not revealed in the original paper. At wide view angles the additional matches that it produces are very effective at improving accuracy. However, the method does not generalise well to lower angles, as it produces an excessive number of matches (over 104). Many of the additional matches are redundant and the improvement in accuracy does not justify the increase in computation and memory cost. A feature expansion algorithm that accounts for the presence of other features may be much more effective.


                           Fig. 7 shows the proportion of matches that were labelled inliers by the RANSAC geometry estimation process. A higher proportion of inliers results in shorter RANSAC computation times and a higher likelihood of selecting the best solution. The threshold used to select inliers was set to 0.01 in normalised data coordinates (see [1] for data normalisation methods). It can be seen that the match rejection strategy of the alignment process is effective. Grid expansion produces additional matches using the information contained in existing matches. The strong improvement in inlier ratio shows that the grid expansion method is more effective at expanding correct matches than incorrect matches.

The above results show that the grid expansion method improves accuracy drastically for a large proportion of cases, but not for all. Overall, 95% of cases show a reduction in error. It would be interesting and useful to determine if there is any pattern to which cases improve, which do not and how much improvement is achieved. Fig. 8
                            decomposes the log error ratio and proportion of improved cases according to view angle. The relative improvement in error produced by grid expansion peaks at 30°. The proportion of cases that are improved stays above 90% up to 40° view angle, after which it drops sharply to 57% at 60°. In summary, the potential benefit of using grid expansion increases as the view angle increases, but the probability of a beneficial outcome declines significantly after 40°.

While a detailed analysis of results is useful for examining the strengths, weaknesses and subtleties of algorithms, it is often excessive for publication purposes, especially when using many data sets. Median result plots can be used to present a summary of results in a more compact fashion.


                           Figs. 9–11
                           
                           
                            present median results for the four data sets plotted against view angle, with relative depth change limited to 1–1.2 and no additional processes applied. Here the match alignment and grid expansion methods are only applied to matches produced using the MSER feature extractor, since their performance is similar for all three base extractors.

The MSER extractor performs best of the three base feature extractors. HESAFF performs relatively well in the amphitheatre data set, where there is a large number of repeated, non-planar structures. In the city and thermal data, the three extractors produce very similar error profiles. Match alignment and grid expansion consistently improve results, with grid expansion yielding as much as an order of magnitude reduction in error.

Match count results in Fig. 10 show that the three base extractors perform very similarly in the visible domain, but that MSER produces as little as a third of the putative correspondences as HARAFF and HESAFF in the thermal domain. MSER appears to produce more matches at larger angles, but taking the inlier ratio into consideration eliminates this slight advantage.

Match alignment consistently eliminates incorrect matches without discarding correct matches, as is evidenced by the decrease in match count and increase in inlier ratio (Fig. 11) at large view angles. The flaw in grid expansion is again made clear by the excessive number of matches produced at small view angles (though these matches are almost all correct). All tested systems produce fewer matches and yield higher error in the thermal imaging data. It is clear that further work is required to bring performance of wide baseline matching in the thermal domain up to the level achieved in the visual domain.

All the previous examples have presented results in terms of view angle, while controlling for other variables. It is of course possible to control for view angle and evaluate the effect of many other variables instead. Fig. 12
                            shows median results for the amphitheatre data set plot against relative depth (scale change). Here the relative view angle was fixed at 10°. A change in depth also results in a change in view angle for a significant proportion of the visible scene, so it is not possible to obtain a large depth change while requiring a view angle of 0°. This type of configuration can be used to simulate forward motion, which is of particular interest to robotics and automotive applications. The experiment can give an indication of how much motion between tracking samples the visual tracking system can tolerate. It can be seen that the tested systems are not very robust to a change in depth — most reach an error of 10% of the image width at or before a depth change factor of 2. The relative performance of the tested systems is very similar to that observed in the view angle decomposition.

The evaluation system accommodates a post-processing step to evaluate the impact of various image effects. This can include effects such as image noise, lighting change, lens distortion or any other effect that can be applied in the 2D image space. Fig. 14
                           
                            shows results for an example where JPEG compression has been applied with various quality settings (example images are given in Fig. 13). All the base extractors are shown to be robust to compression up to a very severe degradation in image quality. In the first 50% of image quality reduction the reconstruction error increases only slightly because JPEG discards colour information before it discards intensity information and the colour components are not used by the extractors. HESAFF achieves the best results among the base extractors and MSER the worst. MSER shows an increase in match count and sharp decrease in inlier ratio as the compression setting drops below 20, indicating that MSER produces false features in the presence of compression artefacts. Match alignment and grid expansion again make a significant improvement.

@&#CONCLUSION@&#

This paper presents a method for collecting large data sets and ground truth geometry for the rigorous evaluation of multi-view geometry systems. The ground truth is produced using a structure from motion procedure and does not require additional equipment beyond a good quality camera. Test images are created by decimating the high resolution video and by selecting pairs of images with desired geometric properties. This evaluation system allows collecting large data sets with ground truth, using only a camera and some computation time. It does not limit the physical scale of the test environment and can be used with imaging methods other than visible light video.

An example 2-view geometry evaluation is presented that showcases the capabilities of the proposed evaluation system. Four example data sets with hundreds of test images are used to generate thousands of test cases. It is demonstrated how the results from a set of test methods can be decomposed and analysed in terms of camera view angle and relative distance to the scene. A JPEG compression example shows how the influence of a variety of imaging effects, such as image noise, lighting variation and lens effects, can be tested. The example evaluation even highlighted a flaw in a method proposed in [21] that was not exposed in previous wide-baseline evaluations. It was shown that this method does not generalise well to short baseline cases, where it produces excessive redundant correspondences.

In future the proposed framework can easily be extended in a number of ways. Test cases can be expanded to larger numbers of images and could include non-overlapping views. This will allow evaluations of methods such as automatic calibration and clustering of related views. The system can also be used to generate training data for a variety of wide-baseline methods, for example PCA SIFT (the SIFT descriptor with principal component analysis applied to reduce dimensionality) [50].

@&#ACKNOWLEDGEMENT@&#

This project was supported by Australian Research Council grant number LP0990135.

@&#REFERENCES@&#

