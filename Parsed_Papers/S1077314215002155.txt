@&#MAIN-TITLE@&#Fisher Kernel Temporal Variation-based Relevance Feedback for video retrieval

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We proposed a novel framework for Relevance Feedback based on the Fisher Kernel.


                        
                        
                           
                           The Fisher Kernel representation makes possible to capture temporal variation by using frame-based features.


                        
                        
                           
                           We experiment on a high variety of scenarios and public datasets (genre classification - Blip10000, action recognition - UCF50 / UCF101 and daily activities recognition - ADL) and show the benefits of the proposed approach which outperforms other state of the art approaches.


                        
                        
                           
                           We prove the generalization power of our approach, i.e., the framework is not dependent on a particular type of content descriptors (experiments were made with text, visual and audio features).


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Relevance feedback

Fisher Kernel representation

Multimodal content description

Video retrieval

@&#ABSTRACT@&#


               
               
                  This paper proposes a novel framework for Relevance Feedback based on the Fisher Kernel (FK). Specifically, we train a Gaussian Mixture Model (GMM) on the top retrieval results (without supervision) and use this to create a FK representation, which is therefore specialized in modelling the most relevant examples. We use the FK representation to explicitly capture temporal variation in video via frame-based features taken at different time intervals. While the GMM is being trained, a user selects from the top examples those which he is looking for. This feedback is used to train a Support Vector Machine on the FK representation, which is then applied to re-rank the top retrieved results. We show that our approach outperforms other state-of-the-art relevance feedback methods. Experiments were carried out on the Blip10000, UCF50, UCF101 and ADL standard datasets using a broad range of multi-modal content descriptors (visual, audio, and text).
               
            

@&#INTRODUCTION@&#

Understanding video content is in general a subjective process for a user. Labelling video content with a predefined set of labels can greatly facilitate search, but is unlikely to capture all possible viewpoints of users. Hence finding specific video content in the exponentially growing amount of digital video becomes increasingly difficult. One solution to this problem is to empower the user with personalized search by iteratively having the user refine its search queries. This is called Relevance Feedback (RF) and is the topic of this paper. A general RF scenario for video retrieval can be formulated as follows: First the user does an initial query using either a keyword or a specific video for which he wants related videos. After the system has returned the best matching results, the user indicates which videos are relevant and which are not. Results are updated using the input, and this refinement continues until the user is satisfied.

In this paper we propose a novel framework for Relevance Feedback based on the Fisher Kernel (FK) and Support Vector Machines (SVMs). The proposed approach operates on top of an existing retrieval system by refining the initial results. First, we alter the feature space: we train a Gaussian Mixture Model (GMM) on the top retrieved results, after which we obtain the FK representation with respect to the GMM. Hence the new feature space is specialized in representing the top results that are representative. Afterwards, we train an SVM using the user feedback, yielding a specialized classifier in the new feature space. Therefore, we have an unsupervised step which alters the feature space and a supervised step to incorporate user feedback. The entire process is illustrated in Fig. 1
                     .

Additionally, we propose to use the FK to capture temporal information as follows: we cut a video up in smaller temporal segments, extract a fixed-size feature representation for each segment, and represent the resulting feature set using the FK. Notice that since the FK captures variation in features in general, and we vary the features in time only, we effectively capture the temporal variation using this representation (but not the temporal order). This differs from other uses of the FK: The representation of images using SIFT [1] and FK leads to a representation of the local visual variation in space only while no temporal information is captured which discards meaningful video information; Representing videos using local Histograms of Oriented Gradients (HoG)/Histograms of Optical Flow (HoF) descriptors [2] and the FK leads to a representation of the local variation in both time and space simultaneously, where space and temporal information is mixed together thus reducing their individual representative power instead of exploiting it. In our approach, by having fixed-sized representations of single frames or small temporal segments with FK we manage to exploit the variation in time only thus capturing that unique video temporal characteristics. As experimental results show, this proves highly efficient in relevance feedback based retrieval scenarios.

This paper extends our previous work [3,4] by including evaluation on a new video dataset, evaluating more feature extraction schemes, analysing the influence of multiple relevance feedback iterations, and including a computational complexity analysis. To summarize, our main contributions are as follows:

                        
                           1.
                           We propose a novel method for Relevance Feedback based on a combination of the FK and SVMs. To the best of our knowledge, this is the first work that exploits the benefits of FK representations to video relevance feedback;

We explicitly model temporal variation by combining frame-based features with the FK;

We demonstrate the generality of our approach by evaluating it on a broad range of modalities: we use visual, audio, and text descriptors. We achieve better performance than other state-of-the-art relevance feedback algorithms on two standard datasets [5,6], which makes the results both relevant and reproducible.

The remainder of the paper is organized as follows. In Section 2 we present the current state-of-the-art on relevance feedback and FK and position our contribution. Section 3 presents the Fisher Kernel theory and Section 4 presents the algorithm of the proposed FK relevance feedback. Afterwards, in Section 5 we present an expansion of the method for temporal aggregation with FK. The experimental setup is presented in Section 6, while the experimental results are reported in Section 7. Finally, in Section 8 we conclude the paper.

@&#RELATED WORK@&#

For decades now, content-based retrieval systems focused on bridging the semantic gap [7] by linking the low-level video features and their high-level semantic interpretation. Video content classification remains one of the most challenging video processing problems, mainly because it implies the classification of complex semantic categories from a huge volume of multimodal data. To effectively handle the quick growth of multimedia data with the objective of providing user access to the needed information, a large number of methods have been proposed for multimedia content analysis and retrieval [69,71–74]. In this context, a standard video classification system consists of detecting sparse spatio-temporal interest points which are then described using local spatio-temporal features, e.g., Histograms of Oriented Gradients (HoG), Histograms of optical Flow (HoF), Motion Boundary Histograms (MBH) or spatio-temporal Laplacian pyramids [73]. The features are then encoded into an aggregated representation, such as Bag of Words (BoW), Fisher Kernel [4], Vector of Locally Aggregated Descriptors (VLAD) [75] representations, or sparsely-constructed Gaussian processes [74] that are then combined with a classifier.

Existing state-of-the-art algorithms for video classification can achieve promising performance in benchmarking for many research challenges, starting from genre classification to event and human activity recognition. For instance, TRECVID [12] recently introduced a Multimedia Event Detection (MED) Track on detecting complex events in web videos when only few positive exemplars are available. Some interesting approaches have been successfully implemented, such as the one proposed by Yang et al. [69] where related exemplars which convey the precise semantic meaning of an event are used for complex event detection. The relatedness is automatically learned and soft labels are assigned to related exemplars adaptively.

Convolutional Neural Networks (CNNs) have been also proved to be an effective class of models for understanding video content, giving state-of-the-art results in many video recognition problems. For instance, Xu et. al. [70] introduces a new encoding technique that generates a video representation based on CNN descriptors. In this approach, a set of latent concept descriptors are used as frame descriptors, which further diversifies the output with aggregation on multiple spatial locations at deeper stage of the network.

In order to improve the retrieval performance, an efficient solution is to take advantage directly of the user’s feedback through Relevance Feedback (RF) techniques. RF helps users improve the quality of their query statements and has been shown to be effective in many experimental environments, e.g., Ma et al. [8], Yang et al. [9], Jones and Shao [10], Wang et al. [11]. The main idea behind RF is to take the results that are initially returned for a given query, and use the user’s feedback to refine them. Recently, a relevance feedback track was organized by TREC to evaluate and compare different relevance feedback algorithms for text retrieval [12]. However, relevance feedback was successfully applied also for image [13,14], multimodal retrieval [15], biomedical approaches [16], etc.

Many RF approaches have been proposed in the literature [17]. They can be grouped into pseudo-relevance feedback, implicit relevance feedback and explicit relevance feedback.


                        Pseudo-relevance feedback [18] automatically simulates the user feedback without any interaction. The assumption is that only a small number of the top-ranked documents in the initial retrieval results are relevant, and these are used for re-ranking the results. Implicit relevance feedback [19] uses the recorded actions of the users to simulate the feedback. The user behaviour, such as clicking on documents, clicking the browser “back” button, time spent per web-page or scrolling, are unobtrusively monitored and used to expand the futures queries. Finally, explicit relevance feedback is a framework in which the user is explicitly asked which results are desirable and which not in an interactive system. It requires more effort on the user side but is also much more reliable.

In this paper we address the explicit relevance feedback scenario. There are several ways to incorporate relevance feedback: by changing the query points, by altering the feature representation, and by using classification.


                        Changing query points was done by one of the earliest and most successful RF algorithms proposed by Rocchio et al. [20]. Using the set of R relevant and N non-relevant documents selected from the current user relevance feedback window, the Rocchio’s algorithm modifies the feature of the initial query by adding the features of positive examples and subtracting the features of negative examples to the original feature. This class of RF algorithms are also known as Query Point Movement (QPM) approaches. A more recent extension of this work in the context of video data is proposed by Nguyen et al. [21].


                        Altering the feature representation was proposed by Rui et al. [22] in their Relevance Feature Estimation (RFE) algorithm, which assumes that for a given query, according to the user’s subjective judgement, some specific features may be more important than other features. Recent extensions of this technique have been proposed by Lv and Zhai [23]. It uses a probabilistic relevance model that exploits the term position and proximity evidence, and assigns more weights to closer semantic terms. The techniques from this category proved extremely fast and simple. However, one of the main drawbacks is that the re-weighted function cannot be fully generalized and adapted because of the diversification of multimedia concepts [24].

Finally, relevance feedback can be performed using classification. After an initial query the user indicates which results are positive and negative examples. These are used to train a classifier and update the results. Some of the most successful techniques use Support Vector Machines [25], classification trees, e.g., Random Forest [14], or boosting techniques, e.g., AdaBoost [26]. Another perspective of the machine learning RF are the approaches that exploit some adaptive learning techniques. Lv and Zhai [24] propose a RF algorithm which adaptively predicts the balance coefficients between query and feedback information, using a regression approach, and then, it re-ranks the documents according to these coefficients. Su et al. [27] propose a Navigation-Pattern-based RF (NPRF) to achieve high performance for web image retrieval. The NPRF search makes use of the discovered navigation patterns and various query refinement strategies, e.g., QPM and RFE, to converge the search space toward the user’s intention effectively. However, all these techniques tend to be less efficient when there is only a limited number or an asymmetric number of positive and negative feedback samples provided by the user. There have been several attempts to overcome this using Biased Discriminant Euclidean Embedding [28] and Active Re-ranking for Web Image Search [29]. In what concerns specifically the video relevance feedback we can mention the approach proposed by Mei et al. [30] which uses a combination of multimodal descriptors with relevance feedback. It is based on a weighting strategy for each modality followed by re-ranking. Another relevant example is the approach in Shao et al. [72] which introduces a new content-based video retrieval framework for searching video databases with human actions. It specifically incorporates spatio-temporal localization. They outline an efficient localization model that first performs temporal localization based on histograms of evenly spaced time-slices, followed by the spatial localization based on histograms of a 2-D spatial grid. As a final step, the framework uses a relevance feedback algorithm that enhance even more the performance of localization and ranking.

The Fisher Kernel was introduced by Jaakkola and Haussler [31] to combine generative and discriminative methods. Specifically, a collection of features is represented by its gradient with respect to a generative distribution. The resulting vector is then used in discriminative classifiers. FK where introduced in computer vision by Perronnin et al. [32], which applied the FK framework to represent collections of local visual features such as SIFT [1] using Gaussian Mixture Models as generative distribution. FKs found their application in other fields as well, starting from topic-based text segmentation [35] to web audio genre classification [34]. Sun et al. [35] proposed a latent Dirichlet allocation (LDA)-based FK to exploit text semantic similarities, then employed dynamic programming to obtain global optimization. Aran and Akarun  [33] introduced a multi-class classification strategy for a sign language data set. They applied a multi-class classification on each Fisher score space and combined the decisions of multi-class classifiers. They showed experimentally that the Fisher scores of one class provide discriminative information for the other classes as well. More recently, FK representation was used by Myers et al. [36] for detection of user-defined events. They propose a set of multimodal features (i.e., audio, motion, visual) together with a set of late fusion techniques.

In this paper we adopt the Fisher Kernel for Relevance Feedback in video for two purposes. (1) Instead of learning the generative probability distribution over all features of the data, we learn it only over the top retrieved results. Hence during relevance feedback we create a new FK representation based on the most relevant examples. (2) In addition, we use the FK to capture temporal variation. We do this by cutting up a video in smaller segments, extract a feature vector from each segment, and represent the resulting feature set using the FK model. Since the variation in features is caused by varying time only, we effectively capture the temporal variation. This approach is in particular interesting for enriching the representative power of the video description scheme.

The main idea behind FK representation is to describe a signal as the gradient of the probability density function that is a learned generative model of that signal. Intuitively, such representation measures how to modify the parameters of the probability density function in order to best fit the signal, similar to the measurements in a gradient descent algorithm for fitting a generative model [31]. The FK representation obtained is then used in a discriminative classifier to solve the classification problem.

Given a collection of T labelled multimodal video descriptors, 
                        
                           X
                           =
                           {
                           
                              x
                              1
                           
                           ,
                           
                              x
                              2
                           
                           ,
                           …
                           ,
                           
                              x
                              T
                           
                           }
                           ,
                        
                      
                     X can be represented by its gradient vector with respect to a Gaussian Mixture Model (GMM), uλ
                     , with parameters λ:

                        
                           (1)
                           
                              
                                 G
                                 
                                    
                                       (
                                       X
                                       )
                                    
                                    λ
                                 
                                 =
                                 
                                    1
                                    T
                                 
                                 
                                    ▽
                                    λ
                                 
                                 log
                                 
                                    (
                                    
                                       u
                                       λ
                                    
                                    
                                       (
                                       X
                                       )
                                    
                                    )
                                 
                              
                           
                        
                     where ▽{.} is the gradient operator. Also, we assume that the covariance matrices are diagonal.

Intuitively, the gradient of the log-likelihood describes the direction in which the model parameters should be modified to best fit the data. In our approach, we will exploit this property to learn the user’s feedback when only a small amount of samples are available. The dimensionality of this vector depends only on the number of parameters in λ, and not on the number of descriptors T [31].

The gradient vector is, by definition, the concatenation of the partial derivatives with respect to the model parameters. Let μi
                      and σi
                      be the mean and the standard deviation of i’s Gaussian centroid, γ(i) be the soft assignment of descriptor xt
                      to Gaussian i (with 
                        
                           t
                           =
                           1
                           ,
                           …
                           ,
                           T
                        
                     ), and let D denote the dimensionality of the descriptors xt
                     . 
                        
                           G
                           
                              μ
                              ,
                              σ
                              ,
                              i
                           
                           x
                        
                      is the D-dimensional gradient with respect to the mean μi
                      and standard deviation σi
                      of Gaussian i. Mathematical derivation leads to [32]:

                        
                           (2)
                           
                              
                                 
                                    
                                       G
                                    
                                    
                                       μ
                                       ,
                                       
                                          i
                                       
                                    
                                    
                                       x
                                    
                                 
                                 =
                                 
                                    1
                                    
                                       
                                          T
                                       
                                       
                                          
                                             ω
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 
                                    ∑
                                    
                                       
                                          t
                                       
                                       =
                                       1
                                    
                                    
                                       T
                                    
                                 
                                 
                                    γ
                                    
                                       (
                                       
                                          i
                                       
                                       )
                                    
                                    
                                       
                                          
                                             
                                                x
                                             
                                             
                                                t
                                             
                                          
                                          −
                                          
                                             μ
                                             
                                                i
                                             
                                          
                                       
                                       
                                          σ
                                          
                                             i
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (3)
                           
                              
                                 
                                    
                                       G
                                    
                                    
                                       σ
                                       ,
                                       
                                          i
                                       
                                    
                                    
                                       x
                                    
                                 
                                 =
                                 
                                    1
                                    
                                       
                                          T
                                       
                                       
                                          
                                             2
                                             
                                                ω
                                                
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    ∑
                                    
                                       
                                          t
                                       
                                       =
                                       1
                                    
                                    
                                       T
                                    
                                 
                                 
                                    γ
                                    
                                       (
                                       
                                          i
                                       
                                       )
                                    
                                    
                                       [
                                       
                                          
                                             
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      t
                                                   
                                                
                                                −
                                                
                                                   μ
                                                   
                                                      i
                                                   
                                                
                                                )
                                             
                                             2
                                          
                                          
                                             
                                                
                                                   σ
                                                   
                                                      i
                                                   
                                                
                                             
                                             2
                                          
                                       
                                       −
                                       1
                                       ]
                                    
                                 
                              
                           
                        
                     where the division between vectors is a term-by-term operation.

Using this representation, the final gradient vector Gx
                     , i.e., our new descriptor, is the concatenation of the 
                        
                           G
                           
                              μ
                              ,
                              i
                           
                           x
                        
                      and 
                        
                           G
                           
                              σ
                              ,
                              i
                           
                           x
                        
                      vectors, for 
                        
                           i
                           =
                           1
                           ,
                           …
                           ,
                           T
                        
                     . This leads to a 2 · T · D dimensional vector compared to the initial feature vector of size D.

In this paper, we exploit further the formalization introduced by [32]. The novelty of this paper is represented by the adaption of the algorithm to the relevance feedback problem. Also, instead of using the keypoint descriptors, the proposed FK representation is applied on a frame-based representation that allows capturing the variation in time (see Section 5). Furthermore, we demonstrate the generality of the method by using several multimodal features, stating from audio, text, motion and global visual features. The proposed approach is introduced in the following section.

Our method is visualized in Fig. 1 and presented with Algorithm 1
                      First, an initial ranking is obtained using a Nearest Neighbour query with the objective of simulating an initial retrieval system. Then, our Relevance Feedback mechanism works in two steps: (1) Altering Features. Based on the top n results we train a Gaussian Mixture Model on the top n videos. We represent the top k videos using the FK with respect to this GMM, where n ≪ k. (2) Training. After the user has labelled the top n videos (n is in general small), we train a SVM classifier on the FK representation. We apply this classifier to the top k videos. We now describe these two steps in detail.

Initially, given a user query, we use a nearest-neighbour search to return a ranking of the most likely videos. We take the top n videos and train a GMM model using a diagonal covariance matrix on the features of these videos. Hence we change the representation of our feature space based on the highest ranked examples. In our experiments, we use 
                           
                              n
                              =
                              20
                           
                        . Since our method of altering the feature space is unsupervised, the GMM can be trained in the background during the time that the user is providing feedback. Initially, in order to speed up the GMM algorithm, we initialize the centroids using k-means, that allows a fast convergence. The GMM contains several parameters which impact the performance of the algorithm: the number of clusters c, the size of video features and the normalization techniques.

The number of clusters c is proportional with the size of FK representations, thus, for a practical system the number of clusters has to be low. Also, the feature’s size represents another parameter which is proportional with the final size of FK representation. Therefore, to make the FK computationally feasible, we first apply Principal Component Analysis (PCA) on the original feature vectors of the videos. After obtaining the mixture model, we convert the original features of the top k videos into the FK representation as presented in Eqs. 2 and 3. Note that in some experiments we only use a single cluster (
                           
                              c
                              =
                              1
                           
                        ). In this case the FK representation consists of both the absolute and quadratic Mahalanobis distance to this single Gaussian cluster, which we show to be a good alteration of the feature space.

The final step is the normalization of the obtained FK representation. It has been shown in [31] that normalization significantly increases the performance of the FK representations. Details are presented in Section 7.1.

The training step is represented by a SVM classifier. The classic binary SVM training algorithm builds a linear margin that maximizes the distance between two classes, but also it can efficiently perform a non-linear classification. More recently, SVMs found their application with relevance feedback approaches. In these approaches, the relevance feedback problem can be formulated either as a two class classification of the negative and positive samples or as an one class classification problem [37], i.e., separate positive samples by negative samples.

In our algorithm, the SVM model is trained on the top n documents, according to the user’s feedback (e.g., 
                           
                              n
                              =
                              20
                           
                        ). After the training step, the top k documents are ranked according to the SVMs confidence level, where k is typically 1000 in our experiments (n ≪ k). The reason to only re-rank the top k is two-fold. First of all, it is unlikely that relevant videos are ranked lower than the top k of the initial query. Using only the top k is faster. Second, the GMM is trained on the top n examples only. This means that the feature space is highly suited for representing examples close to the top n, but less suited for examples further away.

We use cross-validation to optimize the slack-parameters. We can do this fast because due to the Relevance Feedback we have only few training examples.

Global features are often used for reasons of computational efficiency. Also, most of the video content description approaches compute a global representation obtained by aggregating those frame-based features. However, frames in videos change over time, so an important question is: how can we meaningfully aggregate frame-based features in order to preserve that variation in time? One method is to aggregate all the features in one descriptor by computing the mean over all the frames, but the variation in time is ignored [42,43]. Those approaches mix information, disregarding appearance variation over time. Alternatively, a video document can be represented as a set of multiple vectors and the similarity between two videos may be computed as the distance between two sets of points using, for example, the Earth Mover distance [45]. However, these metrics involve a huge computational cost for large databases.

The FK representation was created to model the variation of a set of vectors into a single fixed-sized representation. Hence by using features from different time-steps, we effectively model the variation in time. In the context of Relevance Feedback, we train a GMM only on the top n videos. This creates a feature space specialized to represent differences between relevant examples.

For cutting up the video into temporal chunks, we select T frames from each video and we compute a visual feature for each video frame: 
                        
                           
                              X
                              k
                           
                           =
                           
                              {
                              
                                 x
                                 1
                              
                              ,
                              
                                 x
                                 2
                              
                              ,
                              …
                              ,
                              
                                 x
                                 T
                              
                              }
                           
                        
                     . Then, we train a GMM on the features of the top n videos Xik
                     , where i ≤ n and k ≤ T. Once the generative model is trained, every training sequence of feature vectors, 
                        
                           
                              X
                              i
                           
                           =
                           
                              {
                              
                                 x
                                 1
                              
                              ,
                              
                                 x
                                 2
                              
                              ,
                              …
                              ,
                              
                                 x
                                 T
                              
                              }
                           
                           ,
                        
                      is transformed into a vector of fixed length. The main difference between the previous approach proposed in Section 4.1 and this one concerns the data the GMM was learned on. Instead of using one global aggregated video feature, we will use more features per document. By using this approach, the GMM will learn from more data and the final FK representation contains more information. It is worth mentioning that the resulting FK representation will still have the same dimension.

Experiments in Section 7.4 show the performance of the frame aggregation on the relevance feedback (denoted as T ≥ 1), while experiments in Sections 7.2 and 7.3 discuss the performance of the algorithm when we use only one video global descriptor (denoted as 
                        
                           T
                           =
                           1
                        
                     ).

In this section we discuss the evaluation framework (dataset and metrics) and the choice of content descriptors.

The validation of the proposed relevance feedback approach was carried out on four standard video datasets, namely: Blip10000 - Video Genre Tagging dataset [5], UCF50 - Sport Action Recognition dataset [6], UCF101 - Action Recognition dataset [61] and ADL - Daily Activities dataset [62].


                        Blip10000: consists of 15,000 video sequences (with more than 3,250 h of footage) retrieved from blip.tv
                           1
                        
                        
                           1
                           
                              http://blip.tv/ .
                         web platform. Each video is labelled according to 26 web specific video genre categories, namely: art, autos and vehicles, business, citizen journalism, comedy, conferences and other events, documentary, educational, food and drink, gaming, health, literature, movies and television, music and entertainment, personal or auto-biographical, politics, religion, school and education, sports, technology, the environment, the mainstream media, travel, video blogging and web development and sites. A “default category” is provided for movies which cannot be assigned to neither one of the previous categories. Apart from the video data, the dataset provides associated social metadata, automatic speech recognition transcripts (ASR transcripts) and shot information including key frames. The dataset was successfully validated during 2010–2012 MediaEval benchmarking campaigns [38].


                        UCF50: consists of 6,600 realistic videos from YouTube
                           2
                        
                        
                           2
                           
                              http://www.youtube.com/.
                         with large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc. Videos are labelled according to 50 action categories, namely: baseball pitch, basketball shooting, bench press, biking, billiards shot, breaststroke, clean and jerk, diving, drumming, fencing, golf swing, playing guitar, high jump, horse race, horse riding, hula hoop, javelin throw, juggling balls, jump rope, jumping jack, kayaking, lunges, military parade, mixing batter, nun chucks, playing piano, pizza tossing, pole vault, pommel horse, pull ups, punch, push ups, rock climbing indoor, rope climbing, rowing, salsa spins, skate boarding, skiing, soccer juggling, swing, playing tabla, TaiChi, tennis swing, trampoline jumping, playing violin, volleyball spiking, walking with a dog, and YoYo.


                        UCF101: consists of 13,320 realistic videos from YouTube with large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc. The videos are labelled according to 101 action categories, with each containing 25 groups (each group consisting of 4–7 videos of an action). The videos from the same group may share some common features, such as similar background, similar viewpoint, etc. The action categories can be divided into five types: human-object interaction, body-motion only, human-human interaction, playing musical instruments and sports.


                        ADL: contains 10 different activities, i.e., answering a phone, dialling a phone, looking up numbers in a phone book, writing on a white board, drinking water, eating a snack, peeling a banana, eating a banana, chopping a banana and eating food with silverware. Each of these activities is performed 3 times by 5 different people. These people have different genders, ethnicity, and appearance so sufficient appearance variation is available in the dataset. Each clip is in the range of 3–50 s. In total the dataset contains 150 videos.

These datasets are particularly challenging due to the diversity of video footage, and specifically the variability of videos within the same categories, as well as due to the number of high level concepts proposed. Fig. 2
                         illustrates some image examples in this respect.

@&#EVALUATION@&#

In all the experiments we consider the scenario where user feedback is automatically simulated with the known class membership of each video document, retrieved from the ground truth. This approach allows a fast and extensive simulation which is necessary to evaluate different methods and parameter settings, otherwise impossible with realtime user studies. Such simulations represent a common practice in evaluating relevance feedback scenarios [22,25]–[27,30].

To assess retrieval performance, we use several metrics. Firstly, we compute precision and recall. Precision represents the fraction of retrieved videos relevant to the find (measure of false positives) and recall is the fraction of the videos relevant to the query that are successfully retrieved (measure of false negatives). The retrieval response of the system is assessed with the precision-recall curves, which plot the precision for all the recall rates that can be obtained according to the current video class population.

Secondly, to provide a global measure of performance, we estimate the overall Mean Average Precision (MAP), which is computed as the mean of the average precision scores for each query:

                           
                              (4)
                              
                                 
                                    MAP
                                    =
                                    
                                       ∑
                                       
                                          q
                                          =
                                          1
                                       
                                       Q
                                    
                                    
                                       
                                          
                                             AP
                                             (
                                             q
                                          
                                          )
                                       
                                       Q
                                    
                                 
                              
                           
                        where Q represents the number of queries, and AP() is given by

                           
                              (5)
                              
                                 
                                    AP
                                    =
                                    
                                       1
                                       m
                                    
                                    
                                       ∑
                                       
                                          k
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       
                                          
                                             f
                                             c
                                          
                                          
                                             (
                                             
                                                v
                                                k
                                             
                                             )
                                          
                                       
                                       k
                                    
                                 
                              
                           
                        where n is the number of videos, m is the number of videos of category c, and vk
                         is the kth video in the ranked list 
                           
                              {
                              
                                 v
                                 1
                              
                              ,
                              …
                              ,
                              
                                 v
                                 n
                              
                              }
                           
                        . Finally, fc
                        () is a function which returns the number of videos of genre c in the first k videos if vk
                         is of genre c and 0 otherwise (we used the trec_eval scoring tool
                           3
                        
                        
                           3
                           
                              http://trec.nist.gov/trec_eval/.
                        ).

In our evaluation, we systematically consider each video from the database as query and retrieve the remainder of the database accordingly. Precision, recall and MAP are averaged over all retrieval experiments. Relevance feedback was collected from various browsing top n result windows, with n ranging from 10 to 30. For brevity reasons, in the following we shall present only the results obtained for 
                           
                              n
                              =
                              20
                              ,
                           
                         which we think it represents a good trade-off between user-input and the accuracy of the system. Moreover, experimenting with other values of n proved to have little influence on the overall conclusions.

Video information is represented with content descriptors. At the moment, there is a huge amount of literature in this area, and covering all the existing techniques is impossible. For evaluation, we selected some of the representative approaches that are known to perform well in many benchmarking scenarios [12,39–41] as well as which are adapted to our experimentation tasks (genre and action -based retrieval). The scope of this paper is to demonstrate the efficiency of the general Fisher Kernel representation relevance feedback framework which is not dependent on a particular type of descriptor but can be adapted to respond to various retrieval scenarios.

It is well know that different modalities tend to account for different information providing complementary discriminative power. For video content description we experiment with all the available sources of information, from the audio, and visual, to highly semantic textual information obtained with Automatic Speech Recognition (ASR) as well as user generated data (e.g., metadata that typically accompany video content on the Internet).


                        Visual descriptors:
                        
                           
                              •
                              
                                 MPEG-7 related descriptors (1009 values) [50] - we adopted standard colour and texture-based descriptors such as: Local Binary Pattern, autocorrelogram, Colour Coherence Vector, Colour Layout Pattern, Edge Histogram, Scalable Colour Descriptor, classic colour histogram and colour moments. For each sequence, we aggregate the features by taking the mean, dispersion, skewness, kurtosis, median and root mean square statistics over all frames (exploiting all the statistical moments performs better than using only a few [59]);


                                 HoG features (81 values) [51] - exploits local object appearance and shape within an image via the distribution of edge orientations. The image is divided into small connected regions (3x3) and for each of them building a pixel-wise histogram of edge orientations is computed. In the end, the combination of these histograms represent the final descriptor;


                                 structural features (1430 values) [52] - characterize the geometric properties of contours via a local/global space transformation. On this transformed space, parameters are derived to classify contour global geometry (e.g., arc, inflexion or alternating) and describe local aspects (e.g., degree of curvature, edginess, symmetry). These descriptors were reported to be successfully employed in tasks such as the annotation of photos and object categorization [53];


                                 Bag-of-Visual-Words of SIFT features (20,480 values) 
                                 [54] - we extract a bag of words model (BoVW) over a selection of key frames (uniformly sampled). We use a visual vocabulary of 4,096 words (which represent a common value for video related tasks and gives good results on both the TRECVID and Pascal VOC datasets [41,60]) and the keypoints are extracted with a dense sampling strategy. We use rgbSIFT features [54] and final descriptors are represented at two different spatial scales of a spatial pyramidal image representation [55];


                                 colour naming histograms (11 dimensions) 
                                 [56] - describes the global colour contents and it maps colours to 11 universal colour names. We select this feature, in addition to the classic colour histogram, because the colour naming histogram is designed as a perceptually based colour naming metric that is more discriminative and compact;


                                 Convolutional Neural Network descriptors (4,096 dimensions) 
                                 [63,64,68] - we use a set of Convolutional Neural Networks (CNN) features, using the protocol laid out from [63]. The employed CNNs were trained on either ImageNet 2010 or 2012 datasets, following as closely as possible the network structure parameters of Krizhevsky et al. [64]. We use the activations of the first fully-connected layer of each network as our features, which results in 4096-dimensional feature vectors.


                        Motion descriptors:
                        
                           
                              •
                              
                                 Histograms of optical Flow (72 dimensions) 
                                 [57] - computes a rough estimate of velocity at each pixel given two consecutive frames. We use optical flow at each pixel obtained using Lucas-Kanade method [57] and apply a threshold on the magnitude of the optical flow, to decide if the pixel is moving or stationary. For all the features, we divide the frames in 2x2 and 3x3 regions and then we compute the feature for each region [55];


                                 3DHoG cuboids (72 dimensions) [65] - we compute the Histogram of Oriented Gradients cuboids motion features. First of all, we compute each feature in 3D blocks with a dense sampling strategy, i.e., the gradient magnitude responses in horizontal and vertical directions are computed. Then, for each response the magnitude is quantized in k orientations, where k = 8. Finally, these responses are aggregated over blocks of pixels in both spatial and temporal directions and concatenated;


                                 Body-part features (144 values) [67] - approximate the optical flow that is computed on the body-part components. Human pose and body-part motion obtained good results in many event detection categories [66,67]. We extract the body-part components using the state-of-the-art body-part detector in [66] and compute at every frame for all 18 body parts a Histogram of Optical Flow in 8 orientations [67].


                        Audio descriptors:
                        
                           
                              •
                              
                                 block-based audio features (11,242 values)
                                 [48] - capture the temporal properties of the audio signal. We choose a set of audio descriptors that are computed from overlapping audio blocks (groups of audio frames). On each block, we compute the Spectral Pattern which characterizes the soundtrack’s timbre, delta Spectral Pattern which captures the strength of onsets, variance delta Spectral Pattern which represents the variation of the onset strength over time, Logarithmic Fluctuation Pattern which captures the rhythmic aspects, Spectral Contrast Pattern, Correlation Pattern which compute the temporal relation of loudness changes, Local Single Gaussian Model and Mel-Frequency Cepstral Coefficients (MFCC). Sequence aggregation is achieved by taking the mean, variance and median over all audio blocks;


                                 standard audio features (196 values) 
                                 [49] - we use a set of general-purpose audio descriptors, namely: Linear Predictive Coefficients, Line Spectral Pairs, MFCCs, Zero-Crossing Rate, spectral centroid, flux, rolloff and kurtosis, augmented with the variance of each feature over a certain window (we use the common setup for capturing enough local context that is equal to 1.28 s). For a sequence, we take the mean and standard deviation over all frames.


                        Text descriptors:
                        
                           
                              •
                              
                                 TF-IDF of ASR data (3466 values) 
                                 [38] - describes textual data obtained from Automatic Speech Recognition of the audio signal. Tf-Idf is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus, and it represents the product of two statistics, the term frequency (measures how frequently a term occurs in a document) and inverse document frequency (computes how much information the term provides, that is, whether the term is common or rare across all documents). For ASR we use the transcripts provided by [58] that proved highly efficient to genre classification [38].

Depending on the dataset (and available information) different descriptor combinations were employed. For Blip10000 we use all the above mentioned descriptors except for the HoF motion information; we also use the combination of all visual descriptors and all the visual, audio and text features. All the visual and audio descriptors are normalized by using the L
                        ∞ norm, and text descriptors with the cosine normalization. Descriptor aggregation is accomplished with an early fusion approach. For this dataset, we decided not to use motion features because of their high computational complexity which makes them inefficient. For UCF50 and UCF101 datasets we only use several of the visual descriptors, HoG (to account for feature points information), and colour naming histogram (to account for colour information); and motion features that are more representative for this dataset (3DHoG and HoF). Also, we added the CNN features which obtained good results in many multimedia classification tasks. We did not use audio and text information because the videos from UCF50 / UCF101 / ADL datasets do not contain sound and metadata are not available. For the ADL dataset we use only the body-part and 3DHoG features which already provided state-of-the-art results in many approaches [66,67].

@&#EXPERIMENTAL RESULTS@&#

To validate our approach we conducted several experiments which are presented in the following. The first experiment (Section 7.1) motivates the choice of the best feature - metric combination for the retrieval and we study the influence of Fisher Kernel parameters on system’s accuracy. The second experiment (Section 7.2) deals with comparing our relevance feedback with other relevant work from the literature. A third experiment (Section 7.3) studies the relevance feedback performance with a global Fisher Kernel representation by learning the GMM model on the entire sequence. The fourth experiment (Section 7.4) investigates the benefits of using the Fisher Kernels representation locally to capture the temporal variation at frame level in the sequence. The final experiment (Section 7.5) consists in assessing the computational complexity of the proposed framework.

To introduce our approach and focus solely on its performance, we simulated an environment for the retrieval system on top of which the relevance feedback will operate. We select a classic Nearest-Neighbour strategy for retrieving the initial results. Using a video as query, we achieve a rank list of videos from the dataset. The user will label only a reduced number of documents, which constitutes the initial RF window.


                        Distance metrics are often used to compare the similarity of two multimedia objects, each represented by a set of features in high-dimensional spaces. Motivated by the assumption that a better initial performance of the retrieval system will trigger a better relevance feedback performance, we have tested the performance of several metrics [44]: Euclidean, Manhattan, probabilistic divergence measures such as Canberra [47], intersection family: Cosine Distance, Chi-Square distance used in machine learning and data clustering, Bray Curtis [46], Mahalanobis [46], Kullback–Leibler divergence [46] and Earth’s Mover distance [45].

Based on this experiment, each descriptor will be associated to a specific metric which provided the best retrieval accuracy. The results point out that most of the features have their own suitable metric. For instance, on the Blip10000 dataset, the best results are obtained with five different metrics: Euclidean, Chi Square, Mahalanobis, Canberra and Bray Curtis [46]. On the other hand, most of the best results on the UCF50 dataset are obtained with the Chi Square distance, with the exception of CNN features and BoW-3DHoG, where the Euclidean distance gets the best results. Also, similar results are obtained on UCF101 dataset. Finally, on ADL dataset we obtained the best results with the Euclidean distance.

Although the descriptors provide in general for some of the metrics and same dataset more or less comparable performance, the distance measure still plays a critical role and may lead to performance variations which can be of more than 25%. For brevity reasons, we show only the best results. Table 1
                         summarizes the best performance descriptor - metric combination and their associated MAP values. These combinations are adopted in the following experiments.


                        FK parameters. Further, we study the influence of FK parameters on the system’s performance. All experiments are done in the scenario of global FK representation (when T=1).

In the first test we study the influence of the number of Gaussian centroids. For both datasets the best results are obtained using only one GMM centroid. In this case the size of FK descriptors will be only 2 times bigger than of the video descriptor.

A second test consists in analysing the influence of the FK normalization strategies on system’s performance. By increasing the number of GMM centroids, the FK representation become sparser. In order to counteract this effect, we use some normalization strategies [32]. We tested four normalizations and some combination of them, namely: L
                        1 normalization, L
                        2 normalization, power normalization (
                           
                              f
                              
                                 (
                                 x
                                 )
                              
                              =
                              s
                              i
                              g
                              n
                              
                                 (
                                 x
                                 )
                              
                              
                                 
                                    α
                                    ·
                                    |
                                    x
                                    |
                                 
                              
                              ,
                           
                         where sign(x) is the signum operator that returns 1 if x > 0, 0 if 
                           
                              x
                              =
                              0
                           
                         and 
                           −
                        1 otherwise; and α represents a parameter of the normalization 0 ≤ α ≤ 1), and logarithmic normalization (
                           
                              f
                              (
                              x
                              )
                              =
                              s
                              i
                              g
                              n
                              (
                              x
                              )
                              l
                              o
                              g
                              (
                              α
                              ·
                              |
                              x
                              |
                           
                        )). We obtained the best results when we use L
                        1 normalization, except for the text descriptors which lead to better results using the logarithmic normalization.

To compare our approach against other relevance feedback approaches from the literature we have selected the settings that provide the largest improvement in performance, i.e., one GMM centroid, L
                        1 normalization with power normalization for all UCF50/UCF101/ADL descriptors and Blip10000 audio and visual descriptors, while the logarithmic normalization is used for text descriptors.

The last parameter that has to be taken into consideration is the SVM kernel. Initial experiments showed that we obtained good results with linear and RBF kernels. In the next experiments, we will use both SVM kernels: the linear SVM classifier and the nonlinear RBF kernel to study the impact of both linear and non-linear approaches.

In this section we compare the performance of the proposed FK Relevance Feedback against other validated techniques from the literature, namely: the Rocchio’s algorithm [20] (ROCCHIO), Relevance Feature Estimation (RFE) [22], and some classification-based approaches: Support Vector Machines (SVM RF) [25], AdaBoost (BOOST RF) [26] and Random Forests (RF - RF) [14].


                        Fig. 3
                         presents the precision-recall curves after one iteration of relevance feedback for different descriptors. Generally, all relevance feedback strategies provide significant improvement in retrieval performance compared to the retrieval without relevance feedback (see the dashed green line in Fig. 3). However, the proposed FK Relevance Feedback algorithm (with linear - FK linear, and RBF - FK RBF, kernels) tends to provide better retrieval performance in all cases (see the solid black and solid blue lines in Fig. 3). For brevity reasons, we present only the charts for the Blip10000 and UCF50 datasets.

The results from Fig. 3 are synthesized in terms of MAP in Table 2
                        . On Blip10000 dataset the highest performance is obtained using the proposed FK RBF run on standard audio descriptors, with an increase of MAP from 29.3% (without RF) to 46.3%; as well as run on all the combined descriptors which yields an increase from 30.2% (without RF) to 46.8%. On the other hand, the smallest increase in performance is obtained with BoVW descriptors, which also achieve low results during MediaEval 2012 Tagging Task benchmarking [59].

On UCF50 dataset, the proposed approach obtains the best results on all the cases. The highest increase in performance is obtained using the FK RBF run on 3DHoG descriptors, an increase of MAP from 28.79% to 49.4%. Also, competitive results were obtained with the CNN features, namely 48.9% for the linear kernel and 47.5% for the RBF kernel. The smallest increase in performance is obtained with the FK RBF run on colour naming histogram, namely increase of MAP from 24.22% to 31.7%. A reason for this could be the high diversity of classes for which the colour naming histograms provide little representative power compared to the more discriminant 3DHoG descriptors. In consequence, in many cases there are not sufficient positive feedback examples for the relevance feedback algorithms to work with. Similar results are also obtained on the UCF101 dataset. The best results were obtained with the 3DHoG features (MAP 45.2%), while the lowest results were obtained with the colour naming histograms (MAP 30.1%). On the ADL dataset, the body-part features are more effective, we obtain MAP 82.7%. This represents an improvement of more than 25%. The lowest performance is obtained when we use the 3DHoG features. However, improvement is still good, from 51.22% to 75.5%.

From the information source point of view, on Blip10000 dataset, audio information proves to be highly discriminative compared to visual or text information, and leads to very good retrieval ratios (see Table 2). At genre level, audio features are more accurate at retrieving music, sports, news, and commercials, as these genres have specific audio patterns. Compared to audio descriptors, visual and text descriptors used in combination are more discriminative for categories such as educational, art or web design tutorials. Finally, the best performance is achieved by using all audio-visual-textual features combined. On the UCF50 and UCF101 datasets, the highest performance is obtained using 3DHoG motion descriptors (MAP 49.4% and 45.2%, respectively) while the smallest increase in performance is obtained again with the colour naming histograms. This is mainly due to the fact that colour may not be very important for action recognition. The colour features were employed with the idea to capture complementary information about the scene context, because the sports usually tend to have predominant hues. On the ADL dataset, we obtained the best results with the body-part features which are very efficient on modelling daily activities classification problems (MAP 82.7%).

Overall, for this one relevance feedback iteration, we conclude that in most of the cases, RFE and Random Forests RF provide good results, but the proposed approach is better. At the other end, the smallest increase in performance is obtained using BOOST. Also, it can be observed that the proposed FK RBF obtains slightly better performance than the approach using the linear kernel, FK linear. Therefore, FK RBF will be used in the further experiments.

Another experiment is to assess the performance of the relevance feedback when running several feedback sessions. Fig. 4
                         plots MAP against the number of relevance feedback iterations. For brevity reasons, we present only the best performing approaches from the previous experiment. One may observe that the retrieval performance increases with each new feedback session. The best performance is still obtained with the proposed approach, followed by the RFE algorithm. For instance, at 5 feedback sessions, the largest increase in performance on Blip10000 database is from MAP 26.18% (without relevance feedback) to 48.12% (using visual features); on UCF50 is from 27.38% (without relevance feedback) to 75.30% (using the 3DHoG descriptor); on UCF101 is from 24.79% to 70.12% (also when we use the 3DHoG descriptor); while on ADL dataset from 57.31% to 92.34% (with body-part features). Compared to the RFE, the proposed FK RBF provides an increase of MAP up to 5% on Blip10000 database, of 7% on UCF50 and more than 8% on UCF101 and ADL (see the cyan lines in Fig. 4).

In this experiment, we demonstrate that the FK representation is particularly suited for use in a relevance feedback scenario, when we use the global FK approach (when T = 1). Another alternative to the proposed method is to generate a FK representation by learning a GMM on all the data. By testing this scenario, we want to answer to the following hypothesis: do we obtain good results because the FK representation is in general more powerful than our initial features, or are our performance improvements caused by altering the features with respect to the top n results? In the first scenario, we can just alter the features once offline. In this case the computational speed will be increased and the proposed method will be similar to the SVM relevance feedback. Furthermore, we would just prove that the FK representation is more powerful than our initial features, independent of our relevance feedback settings.

To test this, we train a GMM on all the feature vectors of the whole dataset, and represent all videos as FK representations with respect to this global mixture model. We use these features in the SVM relevance feedback and compare this with our proposed FK framework. Notice that the only differences between these two systems are on what data the GMM is learned.

The results are presented in Table 3
                        . The first column presents the performance of FK representation by learning a GMM on all the data (FK RF on all data) and the second column provides the results for the proposed approach. It can be observed that by using a GMM trained on the top n results instead of using a global GMM, performance increases for the Blip10000 dataset starting from 4% for visual features and with more than 8% for audio features. Furthermore, the increase in performance is higher on the UCF50 dataset. The performance increases with 3% for the colour naming histogram and with 9% for HoF features. This experiment demonstrates that altering the data based on the top n videos is crucial for obtaining good performance.

In the following, we will show the improvements using FK on relevance feedback when we use more than one feature per video. This allows us to use multiple clusters for the GMM.

For computational reasons, we selected for the Blip10000 dataset only three types of descriptors: visual features, namely HoG and MPEG-7 related descriptors, that are more representative for the visual information, and standard audio features. On the other hand, on UCF50 / UCF101 datasets, because there are smaller sized, we used HoG, HoF, colour naming histogram descriptors and 3DHoG cuboids. We also use both body-part features and 3DHoG cuboids for the ADL dataset.

We first test what is the optimal number of centroids for the GMM used by the FK. The experiments are carried out on Blip1000 and UCF50 dataset and the results are presented in Fig. 5
                        . This shows that the best results are obtained using 6 to 10 centroids. Notice the big increase when using multiple centroids: the performance increases with 2% in all the cases. Also, it can be viewed that the performance decreases with more than 1% when the number of centroids increases.

In Table 4
                         we present a comparison between the MAP values of the previous global FK approach (with RBF kernel) and the frame aggregation FK approach. SVM is presented as a baseline for comparison. The frame aggregation FK representation for relevance feedback tends to provide better retrieval performance in all cases with more than 3% improvement.

For instance, on Blip10000 dataset, the MAP increases from 29.59% to 32.87% for HoG features and from 40.80% to 45.43% from MPEG-7 related descriptors. Also, the performance of audio features is improved from 46.30% to 49.23%. Similar improvements are obtained on the UCF50 dataset: colour naming histogram yields an improvement from 31.70% to 34.35%, HoG from 41.10% to 45.49%, HoF from 44.80% to 49.82%, CN from 47.50% to 53.82% and 3DHoG from 49.40% to 54.37%. We also obtained good results on UCF101 dataset: colour naming histograms are improved from 31.70% to 34.81, HoG from 41.10% to 45.49%, HoF from 44.80% to 49.82%, CNN from 47.50% to 53.82% and 3DHoG from 49.40% to 54.37%. On the ADL dataset the improvement is also higher with 7%: the body-part features are improved from 82.70% to 89.40% and the 3DHoG cuboids from 75.50% to 82.80%.

We conclude that modelling the variation in time using GMMs of 6–10 clusters yields significant improvements of performance.

The final experiment consists in assessing the computational efficiency of the proposed FK relevance feedback framework. We run a computation experiment on a regular PC and using a single core at 2.9 GHz (CPU Intel Xeon).

The computational speed is first of all dependent on the descriptor type and therefore size. First, we experimented by simulating various length descriptors, with size ranging from 10 to 1000 dimensions. Estimated running times are presented in Table 5
                        
                        . Methods were implemented using C++ and Matlab environment without any hardware acceleration. We use as baseline for comparison the SVM relevance feedback and all the experiments are provided on Blip10000 dataset.

Using FK in combination with RBF SVM and global video features, we generate a RF iteration in less than half of second. By aggregating all the frames with FK, the execution time of a relevance feedback iteration is near to 4 to 6 seconds (depending on feature size). However, even if the speed is lower, the performance of FK RF is superior. The increase of performance between SVM RF and FK RF is presented in Table 4. This performance gain presents an additional computational cost. Using the global FK approach, we obtain a better performance than classical relevance feedback methods and with a good computational trade-off.

A detailed overview of the computational time for each of the processing steps per retrieval is provided in Fig. 6. We present them for both the global FK RBF and the Frame Aggregation FK RBF. For the first approach, the input/output (I/O) operations take 10 ms per retrieval. The computation of the GMM takes more than 20% of the global computation time. The Fisher Kernel calculation is very fast, namely 220 ms. Finally, the classification step takes 20 ms whereas the final re-ranking takes 9 ms. On the other hand, for the Frame aggregation FK RBF the computational time is much higher. It takes more than 120 ms for I/O operations and less than a second for creating the GMM dictionary. The main time consuming step is the Fisher Kernel representation. The final classification and re-ranking steps are similar for both experiments.

We conclude that frame aggregation with relevance feedback represents a reasonable cost being very close to a real system scenario.

@&#CONCLUSIONS@&#

In this paper we formulated and analysed a new approach for relevance feedback using Fisher Kernels in the context of video retrieval. Our relevance feedback consists of two steps: (1) altering the feature space by training a Gaussian Mixture Model on the top retrieved results and re-representing those features using Fisher Kernels; (2) using the user feedback to train a personalized Support Vector Machine. Additionally, the Fisher Kernel representation made it possible to capture temporal variation (but not temporal order) by using frame-based features.

Our Relevance Feedback experiments showed that our method always performs equal or better to other methods even without using temporal information: Compared to the next best method, RFE [22], we get improvements on Blip10000 between 0% and 11% MAP, averaging 5.2% MAP. For UCF50 the next best method is Random Forest RF [14] for which we get improvements of 0.9%, 1.6%, and 8.5% MAP, respectively for colour naming histograms, HoG, and HOF.

If we capture temporal information we get even better improvements at an acceptable computational cost. By using a GMM with only 5–10 clusters a Relevance Feedback iteration becomes 4–6 s, in which the user can give its feedback. Improvements are significant: On Blip10000, we get absolute MAP improvements of 3.3%, 4.6%, and 4.9%, respectively for HoG, MPEG-7, and standard audio features. On UCF50 we get absolute MAP improvements of 3.1%, 4.4%, and 5.0% for respectively colour naming histograms, HoG, and HoF.

@&#ACKNOWLEDGMENTS@&#

Part of this work was supported under InnoRESEARCH POSDRU/159/1.5/S/132395 (2014–2015).

@&#REFERENCES@&#

