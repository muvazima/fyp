@&#MAIN-TITLE@&#Discreet verification of user identity in pervasive computing environments using a non-intrusive technique

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A user-centric, unobtrusive approach for verifying the user’s identity is proposed.


                        
                        
                           
                           The approach uses knowledge about the user’s behaviour to infer their identity.


                        
                        
                           
                           The approach uses a simple numerical algorithm to assert the user’s identity.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Pervasive computing

Identity management systems

Identity verification

Behaviour inference

@&#ABSTRACT@&#


               
               
                  This paper presents a new approach for verifying user identity in pervasive environments using a non-intrusive behaviour tracking technique that offers minimum interruption to the user’s activities. The technique, termed Non-intrusive Identity Assertion System (NIAS), uses knowledge of how the user uses the environment’s services to infer their identity. The technique monitors user behaviour through identifying certain types of activity without the need for detailed tracking of user behaviour, thus minimising intrusion on the user’s normal activities. The technique was evaluated using a simulated environment to assess its reliability. Simulation results show that the technique can be applied in various situations such as strict and relaxed security settings by applying different security policies. They also show that the technique is particularly effective where the environment has a mixture of high and low security resources in which case reliability could exceed 99%.
               
            

@&#BACKGROUND@&#

Implementing user access control in pervasive computing environments requires innovative techniques to establish the identity of users who wish to access various resources within the environment. Users in such environments use various mobile devices and technologies such as Radio Frequency Identification (RFID) tags, smart tags or smart phones when interacting with the environment. Their identity is stored within the device’s memory and is retrieved and used to identify them every time a user attempts to access a resource or service. In order to interact with the environment, users need to access resources and services such as requesting a print service, interacting with one of the embedded objects (e.g. a sensor node), accessing a protected part of a secure website or simply accessing a protected area in the building. Resources and services are spread across the environment and may be connected using a distributed network infrastructure, which adds to the difficulty of identifying users and authenticating them. This makes the design of a secure pervasive computing environment a challenging task. On one hand, designers aim to design secure systems to prevent unauthorised access to protected resources. On the other hand, they wish to allow users to gain access to such resources seamlessly and with minimum interruption. Using traditional techniques (such as password login) to authenticate users every time they access a resource (or cross a network boundary) may be effective in achieving the first goal of securing the resources, but it adds significant interruption to the users when using the environment. Alternative solutions are used with less interruption where the user’s identity is stored in a personal device such as a mobile phone or proximity card. By merely passing by or swiping the personal device into a reader allows the environment to identify the user and apply access control rules with minimal interruption to their activities. However, the level of security in this solution is questionable; the system is in fact identifying the device, not the user carrying the device. There is a chance that the person carrying the device may not be the legal owner of the device giving rise to the possibility of identity theft and unauthorised access; not to mention the failure of the environment to provide personalised services to its users. Hence, it is important for pervasive environments to be able to verify the identity of users and at the same time to maintain minimum interruption (caused by the verification process) to their activities.

Existing identity verification approaches have focused mainly on four methods; these are (1) biometric technologies, (2) using a certification authority, (3) password exchange protocols based on encryption and (4) context aware identification.

The first method uses biometric technologies such as retinal scan, face recognition, fingerprint, or pattern recognition techniques to measure physiological or behavioural characteristics [1]. Such systems verify the user’s identity using the biometric characteristics to accept (or decline) the user’s claim to their identity based on their various biometric traits [2]. Although such systems offer features such as accurate identification, universality, permanence and uniqueness, they pose considerable challenges to pervasive systems. Such challenges are privacy intrusion; the need for additional hardware and software to acquire the biometric information; and user acceptance and perception.

The second method uses a certification authority. Some employ the use of a proxy server in order to give users access to secure proxy credentials when and where needed, without requiring them to directly manage their long-lived credentials [3]. Others use a dedicated identity service such as a cloud service that acts as a verification agent [4], whilst others use solutions to manage identity and profile groups [5]. The use of services, proxies or agents provide the convenience of centralised access control of resources, but place an extra infrastructural requirement on the whole system design. It also requires effective solutions for intrusion detection and Man-in-the-Middle (MiM) problems.

The third method is based on the use of password exchange protocols which themselves are based on the use of the Single Sign On (SSO) concept [6]. Some use standard technologies such as Kerberos, OpenID or Security Assertion Markup Language SAML [7], while others define proprietary protocols such as the Identity Service Provider Protocol (ISPP) [8]. These approaches have the ability to use identity information from one domain to access resources in another domain based on trust relationships that have to be established in advance. This approach however has scalability issues due to the fact that complex trust relationships may soon evolve into a complex map of inter-domain relationships [9].

For the fourth method, which uses context information, identity verification takes a step further by utilising the system’s knowledge of its users [10]. Context-aware identification systems use knowledge of the current status of the user to verify the user’s identity [11]. Such systems utilise context information that is relevant to the user and hence has the potential to be scalable and less dependent on complex trust relationships. However, there are a number of issues with this approach. Firstly according to Dey [10], identity is one of the main pillars of context information and to use context to verify it seems cyclical. Secondly, context information is dynamic in nature and hence using it as a basis for access control may cause serious issues and require complex policies for allocating and revoking group memberships [11]. Thirdly, users are usually less inclined to surrender personal information related to their current status in the process of verifying their identity.

All four existing methods have limitations and complications. Therefore this paper proposes a new approach for verifying a user’s identity in pervasive computing environments through the use of a non-intrusive technique that offers minimum interruption to the user’s activities. The technique, termed Non-intrusive Identity Assertion System (NIAS), uses the history of a user’s previous access to the environment’s resources as a means for asserting the user’s identity before access control rules are applied. In this technique, as users access various services and resources in the environment, the knowledge of how the user uses the environment’s services is used to infer their identity in order to assert that the user is in fact who they claim to be.

The remainder of this paper is organised as follows. In the Section 2, existing approaches for inferring user behaviour are reviewed. The purpose of the review is to study candidate approaches in order to select the most suitable one for this research. Section 3, discusses the requirements for a non-intrusive verification technique, whilst Section 4 describes the principles, components and operation of the new approach. Section 5 explains how a user’s activity (events) are identified and classified. This is then followed by Section 6, where system testing and tuning is discussed. The evaluation and validation of the system is discussed in Section 7 using a discrete event simulation which was developed specifically to evaluate the performance of the system. Finally, in Section 8 a number of conclusions are drawn highlighting the achievement and limitation of this work.

In pervasive computing environments, users continuously interact with a variety of embedded objects. This interaction constitutes primitive activities that users perform while they use the environment, and may be used to infer their behaviour. Knowledge about the users’ behaviour may be used by the system to adapt its services based on the current context of the user. The system adaptation feature means it automatically derives a user’s needs based on their identities and circumstances. Bardram and Christensen [12] propose a new model called Activity-Based Computing (ABC) for inferring behaviour. According to ABC, when a user performs an activity such as writing a document, the activity will be defined in terms of primitive actions such as opening a file or using an editor, and certain environmental triggers such as location, time, and proximity to a set of entities. They define activity as: “an abstract, but comprehensive, description of the run-time state of a set of computational services”.

Like other pattern recognition applications, existing approaches to user behaviour inference used machine-learning methods to translate primitive activities into meaningful behaviour. The various methods can be categorised into three main groups as explained in the next sub-sections.

Probabilistic methods are widely used in detecting Activities of Daily Living (ADL) in home environments for health applications such as assisted living applications. As an example, Philipose et al. [13] proposed a system called Proactive Activity Toolkit (PROACT) that monitored specialised sensors attached to various objects to detect the user’s interaction with those objects. This was fed to a probabilistic engine that attempted to infer the user’s current activity. These inferred activities were then used to create a probabilistic model of the user’s behaviour using Bayes filtering. Another example was proposed by Barger et al. [14] where they used motion detection sensors and on/off switches to monitor the behaviour patterns of the elderly living in a smart house. They used series of sensor readings to infer activities of people by monitoring, for example, the sensor location, start time, the duration of time spent in a room and the activity level while in a room. They used a mixed model framework to develop a probabilistic model to distinguish between event types. Using this model, they grouped different activity types into clusters based on the statistical distributions of the collected properties of the activity. Tapia et al. [15] used naive Bayesian classifiers to monitor user activities by employing two types of activity recognition classifier; the first is a multi-class naive classifier in which the class node represents all the activities to recognise. The second activity recognition classifier used multiple binary naive Bayes classifiers for activities that are not represented as mutual exclusive activities. Philipose et al. [16] used an inference engine to infer current activities as a list of the most probable activities from a library of all possible activity. The process involved compiling activity models into Dynamic Bayes Networks (DBNs) based on a technique called particle filtering. Rashidi et al. [17] used an unsupervised clustering method for tracking and discovering frequent activities of individuals who experience difficulties living independently which could include for example, Alzheimer’s patients. A data mining method was used to determine activity patterns and the clustering method was used to group the determined patterns into activity definitions. The process (of discovering repeatable activities) was based on a sequential pattern mining technique, which they combined with the clustering algorithm creating what is called the Activity Discovery Method (ADM). ADM is used to recognise repeated activities that are part of an activity sequence which could happen frequently and with some degree of regularity. They suggested that in order to recognise a group of activities as an event, a ratio (probability) of activities recurring in a set 55–62% was found to lead to activity inference with good accuracy.

Probabilistic methods are more applicable when there is a degree of uncertainty about the predicted user behaviour, or when there is no prior detailed knowledge of the behavioural patterns. When applied to a prediction problem, these methods result in a probability distribution of possible occurrences with a degree of acceptance for each situation (likelihood). These can then be qualified using information (statistical parameters) deduced from the domain to accept or reject any of them. Their limitation is that they need large amounts of data in order to produce trusted results. In other words, they may prove unsafe when only limited information is available to perform the behaviour inferring process.

Artificial intelligence (AI) techniques (such as rule based and neural networks) use prior knowledge of the environment in order to build a behavioural model which can be used to predict user activities. Self-organising maps (SOM), ontologies and decision-making techniques are used to build a topology map which in turn is used to learn new activities. The decision-making techniques are generally used to decide which cluster of primitive activities can be converted into a recognised behaviour. Also it is not uncommon that AI techniques would be used in conjunction with probabilistic methods (such as Markov Chains) in order to achieve better results. As an example of such an approach, Schmidt and Laerhoven [18] used Kohonon’s self-organizing map (SOM) [19] to learn user contexts by producing a network of objects (sensors) which provided information about users in the environment. The use of SOM allows the system to learn new groups of activities (clusters) at any time and produce a topology-preserving map. The network topology, which is simply a mapping functionality that provides properties about neighbour relations, allows the system to predict the user’s context using these properties. After the map has been generated, a Markov chain model is used as a buffer algorithm to check if transitions between nodes of the network are probable. Another example of this approach is the Activity Recognition System by Osmani et al. [20] which is used to monitor user activities in home environments. The system uses a multi-level hierarchical activity inference process, which supports the notion of Self-Organizing Object Networks. The object networks structure allows the processing of low-level information, which comes from primitive sensors, by recursively composing the low-level information into high-level abstraction. Then the information flows to a decision making module that matches this data in a user activity map to finally infer user activity. Neural networks have also been used in behaviour inference systems such as with intrusion and fraud detection [21], where the system is initially trained to identify pre-selected examples of the problem. The training process continues by reviewing the results of the system and making refinements to the system’s configuration until the analysis satisfies a pre-determined satisfaction level normally based on a probability index.

Rule based techniques use causal representation and relationships between activities in order to predict behaviour. Hence, they are effective in translating activity primitives into meaningful behaviour. The causal events are essential to the rule based approach, which make this approach specifically useful when alternative paths may produce variations in user’s behaviour. An example sequence of particular user behaviour may be to put the kettle on, take cup, take coffee, take milk, and take sugar which corresponds to Making Coffee activity [20]. Certain variations within this sequence may be allowed and interpreted as the same activity, or certain primitives may be used as part of alternate sequences for predicting other activities. Neural networks are capable of solving similar situations, albeit in a different way, as they can handle multiple inputs in their training process, which could produce a number of alternative paths (outputs), with good degree of accuracy. However, such techniques do have a limitation due to the fact that they need prior detailed knowledge of the environment (known as constraints) in order to perform the interpretation process. In the case of rule based techniques, the prior knowledge is the causal events and relationships between activities, whilst in neural networks it is the pre-selected examples needed for the training process. In the case of absence of such knowledge these approaches would be incapable of predicting meaningful behaviour.

Logic programming and fuzzy techniques have been used by some researchers to convert a set of observations about a user’s behaviour into recognised activities using the rules abstracted in a set of heuristics (expert system). As an example of this approach, Christensen [22] presented a system for discovering everyday healthcare activities in pervasive computing environments. The system uses a component called Activity Discovery Component (ADC), which is an expert system, that monitors and senses people’s activities. The ADC has a knowledge base that consists of people and objects/equipment in addition to a list of possible activities. This component infers possible activities based on heuristics about repeating activities. Agarwal et al. [23] use a set of fuzzy rules to detect medically significant events in an operating theatre environment. The process of event detection occurs over two stages; in the first stage low level events (such as high blood pressure) are inferred based on the patient’s medical history and a knowledge base of medicines. In the second stage, the system uses a set of rules to infer medically significant events from low level events (primitives). As each medical event has a range of values, there is no set threshold that will deterministically categorise the value as normal or abnormal. Therefore, the system uses an expert system with a set of fuzzy rules to deal with the uncertainty of events.

The fuzzy and logic programming method uses a declarative way for describing user behaviour from a number of events, providing a more convenient way to build and maintain a base of existing knowledge. In essence, they infer activities based on “global” knowledge which has to be abstracted in the knowledge base. The decision making process (in such an approach) is based on choosing between a number of qualified guesses (models). They are particularly useful in predicting situations where a flow of activities is involved in a similar way to workflow type systems. However the approach has two main limitations; firstly, the need for abstracting the global knowledge of the environment is time consuming and makes the system unscalable. New rules need to be abstracted and added manually to the system every time the environment changes, for such systems are unable to learn new rules. Secondly, when the system is presented with multiple equally qualified alternatives (models), the system fails to choose one model; in such a situation manual intervention is usually required to make the final decision.

The process of inferring user behaviour involves the use of a large amount of sensor data and the detailed tracking of user activities which can then be interpreted in order to predict the true intention of the user. Although this has some benefits, it also poses a major privacy issue in that users are not usually susceptible to the idea of being monitored continuously, or having their activities recorded. In this research a technique is needed to detect user activities in order to assert their identity as they use the environment, with minimum impact on their privacy. The new technique should meet to the following requirements:
                        
                           •
                           The activity monitoring process shall not use any additional data apart from that used in monitoring access to the protected resources, which is already being collected. For example, no data should be collected to monitor the path that a user has taken to reach the protected resource.

The intention behind the user activity shall not take part in the inferring process. That is, no rules should be used to infer why the user is performing that activity.

The process shall be used only to verify the user’s identity, rather than identify the user. This means that the user has used other means to identify him/her to the system, prior to the initiation of the verification process.

Based on the above requirements, our research has ruled out the use of AI and Logic Programming approaches as they involve tracking primitive activities and using rules for interpreting them as user behaviour. They also need previous knowledge of the environment and its users in order to build the rule base or to perform the training process. In this research no prior knowledge of how the users are going to use the environment is available for us. Besides, each user’s behaviour is unrelated to those of other users as no global rules are available that govern the way users should use the resources; this is an important privacy requirement. For these reasons, our research has adopted probabilistic methods as an approach for inferring user behaviour. We have used an unsupervised clustering algorithm that uses historical data (of how users access resources in sequence) to identify patterns of activities. Because of the nature of the problem a nearest neighbour clustering algorithm has been used [24].

The basic principles of the Non-intrusive Identity Assertion System (NIAS) are illustrated in Fig. 1
                     . The proposed system uses the following three principles to verify the user identity:
                        
                           (1)
                           
                              Identification: the user will first be identified using a personal identification device such as a mobile device or a smart tag (RFID tag) which is scanned by a suitable reader attached to the protected resource.


                              User activities: The system uses patterns of previous access to resources (history) in order to assert their identity. For example, is the user known to have been accessing a certain resource at a particular time?


                              Asserting user identity: The verification of user identity is stepwise and leads to levels of assertion (confidence) that is represented as a numeric value. The numeric value signifies whether the system is confident to allow the user access to that resource.

In NIAS, identity verification is a process of monitoring users as they attempt to access the environment’s resources and convert this knowledge into a numeric value (assertion value) that represents the system’s confidence of the user’s identity. This value is computed using an algorithm called the assertion algorithm that is used to update the system’s confidence as the user performs a known activity. The amount by which the system’s confidence is altered depends on the type of activity that the user performs. NIAS classifies activities into a number of types, and uses these types in calculating the assertion value instead of the actual activities. This reduces the need to record details of the user activities usually used in the interpreting process. The system will then use the assertion value to decide whether the user can be trusted to access the resource or not based on a policy known as the assertion policy. Next, we will discuss the three main components of NIAS; the assertion algorithm, the activity classification and the assertion policy.

NIAS operates on the basis of assertion levels. If the system confidence about a user reaches the assertion level needed to gain access to a specific resource, then the user identity is said to be verified. In order to achieve this, it uses an algorithm that applies the following principles:
                           
                              (1)
                              For each user, it maintains a numeric value known as the global assertion value (g) that reflects how confident the system is about the user’s identity at any particular time.

It divides the protected resources into a number of assertion levels (n). In each level, the system keeps a value called the assertion level threshold (gt1, gt2
                                 ,…, gm
                                 ) that reflects how confident the system must be about a user in order to assert their identity before they are granted access to the resource.

The global assertion value (g) is updated continuously using Eq. (1). As shown in the equation, g is a function of the previous global assertion value (g′) and the current activity (A).
                           
                              (1)
                              
                                 g
                                 =
                                 f
                                 (
                                 
                                    
                                       g
                                    
                                    
                                       ′
                                    
                                 
                                 ,
                                 
                                 A
                                 )
                              
                           
                        
                     

The current activity is represented using a numeric value called the local assertion value (ℓ) which represents how much a particular activity can influence the system’s confidence about the identity of the person performing the activity. It determines the direction and extent by which (g) changes as a result of that activity.

Initially, the algorithm uses an initial global assertion value (g
                        0) that is specified in the assertion policy. Subsequently, the algorithm uses Eq. (2) to perform the calculation.

For each value of i:
                           
                              (2)
                              
                                 
                                    
                                       g
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 ℓ
                                 
                                 ∗
                                 
                                 Δ
                                 g
                                 +
                                 
                                    
                                       g
                                    
                                    
                                       i
                                       -
                                       1
                                    
                                 
                              
                           
                        where gi
                         – the new global assertion value such that: gi
                        
                        ∊[0,1], gi
                        
                        −1 – the previous global assertion value such that gi
                        
                        −1
                        ∊[0,1], ℓ – the local assertion value such that: ℓ
                        ∊[−1,1], Δg – the rate of change of the global assertion value, and i – an integer loop control variable such that: i
                        ∊[1,∞].

The local assertion value (which has a range between −1 and 1) is calculated for each activity performed by the user. The algorithm also defines a control parameter representing the rate of change of the assertion value and referred to as Δg. This parameter is used to control the speed by which the assertion value increases or decreases; this parameter will be explained in more details in the next sections.

For the purpose of calculating the local assertion value, NIAS classifies the user’s activities into five types, which are:
                           
                              (1)
                              A recognised recurrent activity.

A sequence of recognised activities.

A significant activity.

Unrecognised activity.

No activity.

Each type of activity has different effect (weight) on the system’s assertion value (either increase or decrease). For example, when a user performs a recognised activity such as accessing a particular resource at a specific time, the system’s confidence will elevate to a higher level of assertion. Each event type and its impact on the system’s behaviour are explained in Table 1
                        .

The assertion policy comprises setting a number of parameters that determine the behaviour of the NIAS verification process, and consequently affects the decision when to grant the user access to the protected resources. The following parameters are specified in the policy:
                           
                              (1)
                              The initial global assertion value (go
                                 ), this could either be 0, 1 or any other value depending on the environment’s security policy.

The local assertion value for the various activity types. As NIAS identifies one of the activities (presented in the previous section), it calculates the local assertion value as a function of the activity type, and as specified in the assertion policy. The policy defines a number of parameters as shown in Eq. (3). The parameters are typically assigned by an administrator depending on how strict the security policy is required in the environment.


                        
                           
                              (1)
                              The value of Δg, as an absolute value, which determines the speed of convergence of the whole verification process. This parameter determines how quickly the assertion value decays to zero if no activity has been detected.

The number of assertion levels (n) needed for the environment, and the assertion threshold for each level. Divide the resources in the environment between these levels such that the higher the security resource, the higher the assertion level, and vice versa. The threshold values may be assigned by the administrator to any values; however, NIAS recommends the method shown in Eq. (4) for calculating the threshold values which is a function of (n):

For each value of j:
                           
                              (4)
                              
                                 
                                    
                                       g
                                    
                                    
                                       tj
                                    
                                 
                                 =
                                 (
                                 j
                                 -
                                 1
                                 )
                                 ∗
                                 
                                    
                                       1
                                    
                                    
                                       n
                                    
                                 
                              
                           
                        where gtj
                         is the threshold value of the jth assertion level, n is the number of assertion levels, j is an integer loop control variable such that: j
                        ∊[1,
                        n].

In summary, the assertion policy involves tuning the values of a set of parameters as defined in the assertion policy vector matrix, Eq. (5):
                           
                              (5)
                              
                                 Assertion policy
                                 =
                                 [
                                 n
                                 ,
                                 
                                    
                                       g
                                    
                                    
                                       ti
                                    
                                 
                                 ,
                                 Δ
                                 g
                                 ,
                                 
                                    
                                       g
                                    
                                    
                                       o
                                    
                                 
                                 ,
                                 K
                                 ,
                                 
                                    
                                       K
                                    
                                    
                                       r
                                    
                                 
                                 ,
                                 
                                    
                                       K
                                    
                                    
                                       q
                                    
                                 
                                 ,
                                 
                                    
                                       K
                                    
                                    
                                       s
                                    
                                 
                                 ]
                              
                           
                        Such that: j
                        ∊[1,
                        n],
                        Kr
                        
                        ⩽
                        Kq
                        
                        ⩽
                        Ks
                         and gt
                        
                        1
                        ⩽
                        gt
                        
                        2…⩽
                        gtn
                        .

Although, the assertion policy allows the administrator to choose any values for these parameters to suit their specific security requirements, NIAS provides a number of pre-determined sets for aggressive, relaxed and moderate policies. Section 6 (System Testing and Tuning) discusses how these values have been determined.

A schematic diagram of the NIAS system architecture illustrating its operation is shown in Fig. 2
                        . When a user attempts to access a resource (for example by scanning their smart card), the system identifies this action as a user activity, which it uses to verify the user’s identity. The system maintains two databases for this purpose; one is called the activity database which stores a history of all user activities as they access various resources in the environment, and the second is called event database which stores recognised recurrent activities. Activities are represented as a vector of user identity, resource location and time of accessing the resource as described in Eq. (6).
                           
                              (6)
                              
                                 A
                                 =
                                 [
                                 id
                                 ,
                                 L
                                 ,
                                 t
                                 ]
                              
                           
                        where A – user activity, id – user’s id, L – resource location, and t – time.

In the activity classifier and mapping part, the system applies an unsupervised clustering algorithm to identify activities as events and maps them into one of the activity types. If the system recognises the activity as a recurrent, significant, or sequence of activities, it adds it to the events database. Here, we use the term event to denote a recognised user activity. If the system identifies the detected activity as an event that already exists in the events database, it determines its type and passes it to the controller.

The controller uses the type of event to calculate the local assertion value according to the assertion policy. It then applies the assertion algorithm (Eq. (2)) in order to compute the updated global assertion value. Based on this value, the assertion level of the resource and the parameters specified in the assertion policy (Eq. (5)), the controller determines whether the user’s identity is verified to access the resource or not.

As discussed in the previous section, central to the operation of NIAS is identifying activities as history of recognised events and mapping them into types. To convert activities into events in order to compute the local assertion value (Eq. (3)), NIAS uses a clustering algorithm that works by grouping related user activities to form clusters, which may be identified as events. Due to the dynamic and unpredictable nature of user behaviour, an unsupervised clustering technique is used. Since users do not behave in a uniform manner, the clustering technique is expected to produce an unidentified number of clusters (events) for each user.

The clustering technique used in this approach is based on the nearest neighbour algorithm [24], and uses the Euclidean distance measure (d) between two user activities as a criterion to measure the similarity between two activities. This can be determined by calculating the 2-norm of the difference between the two vectors; Eq. (7).
                        
                           (7)
                           
                              d
                              =
                              ‖
                              
                                 
                                    A
                                 
                                 
                                    2
                                 
                              
                              -
                              
                                 
                                    A
                                 
                                 
                                    1
                                 
                              
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          (
                                          
                                             
                                                id
                                             
                                             
                                                2
                                             
                                          
                                          -
                                          
                                             
                                                id
                                             
                                             
                                                1
                                             
                                          
                                          )
                                       
                                       
                                          2
                                       
                                    
                                    +
                                    
                                       
                                          (
                                          
                                             
                                                L
                                             
                                             
                                                2
                                             
                                          
                                          -
                                          
                                             
                                                L
                                             
                                             
                                                1
                                             
                                          
                                          )
                                       
                                       
                                          2
                                       
                                    
                                    +
                                    
                                       
                                          (
                                          
                                             
                                                t
                                             
                                             
                                                2
                                             
                                          
                                          -
                                          
                                             
                                                t
                                             
                                             
                                                1
                                             
                                          
                                          )
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           
                        
                     
                  

Assuming both activities are performed by the same user and at the same location in order to be similar, Eq. (7) can be simplified as shown in Eq. (8).
                        
                           (8)
                           
                              d
                              =
                              |
                              
                                 
                                    t
                                 
                                 
                                    2
                                 
                              
                              -
                              
                                 
                                    t
                                 
                                 
                                    1
                                 
                              
                              |
                           
                        
                     
                  

Two more quantities are used by the clustering algorithm in the process of grouping activities into events, which are:
                        
                           (1)
                           
                              Time tolerance (ɛ): the distance within which a group of user activities may be grouped to form a cluster. Time tolerance is used to ascertain that all points in a cluster must be sufficiently close (or similar) to one another.


                              Minimum cluster size (z): a minimum number of activities (presented as a ratio) that a cluster must contain in order for that cluster to be accepted as an event; see Eq. (9). It is assumed that a group which contains a certain number of activities that is greater than or equal to z, is recognised as an event and added to the events database.

Using ɛ, the clustering algorithm groups activities into a number of clusters of varying sizes. Depending on the size of a cluster, not all clusters can be recognised as events. To decide whether a cluster represents an event or not, the cluster size (s) has to be determined. This is calculated as an event probability (Pe
                     ), as shown in Eq. (10).
                        
                           (10)
                           
                              s
                              =
                              
                                 
                                    P
                                 
                                 
                                    e
                                 
                              
                              =
                              
                                 
                                    number of activities
                                 
                                 
                                    number of possible occurances
                                 
                              
                           
                        
                     Such that if Pe
                     
                     ⩾
                     z, then the activity is added to the events database. Otherwise, the activity is not recognised as an event, but kept in the activity database to give the system the ability to recognise the activity if it is repeated in the future.

Identifying events is the principal method for identifying recurrent and sequence event types. Once events of a specific user have been identified, a Markov chain model is used to measure the likelihood of two or more events taking place in a sequence. Identified sequences of events are added to the events database, with indication of the sequence degree (the number of events in the sequence). This degree is used when this type is mapped by the Mapping function and by the controller when calculating the local assertion value (ℓ).

Significant events are not subjected to the probabilistic methods as they are inherently identified and each significant activity is recognised as an event. Once a significant event has been detected and used in calculating the assertion value, it will be removed from the events database.

For the purpose of implementing the algorithm, the values of ɛ and z needed to be estimated. We conducted an experiment within the University of Salford environment to study and analyse patterns of user activities as they access various parts of a building, where they worked. We found that users, who repeatedly access the same part of the building at the same time of day, tend to access that resource within a time span of ±20min. We also found that users tend to repeat the same activity at least 50% of the time, for that activity to be recognised as a recurrent activity. This is based on a study of cycles of five working days within a week. Therefore our implementation used the values shown in Table 2
                     .

The system has been tested using a simulated environment in order to tune its behaviour and measure its reliability. The study showed that a number of parameters must be carefully chosen and tuned so that the system would implement various security settings for strict, moderate or relaxed security environments. The following situations were studied using various security settings:
                        
                           (1)
                           Determining the time period that the system waits before it assumes that no activity is detected.

Studying the effect of changing the parameters specified in the assertion policy; Eq. (5). Particularly, studying the effect of Δg,
                              K,
                              Kr
                              , Ks
                               parameters.

Studying the effect of the distribution of resources over various assertion levels in the environment.

The testing strategy has taken into consideration the system’s behaviour with respect to changing the above parameters. By fixing one parameter and changing the values of the others, the tests were repeated several times in order to gain an understanding of the system’s behaviour and to determine the optimum values for various security settings.

All tests used in evaluating the approach assumed the number of assertion levels n
                     =3. The threshold values of the assertion levels are thus calculated according to Eq. (4), and shown in Table 3
                     .

Several experiments were carried out in order to study the effect of updating (decreasing) the global assertion value when no activity is detected using periods between 10 and 120min for different values of Δg. It was found that using short periods (such as 10min) will cause the system to lose confidence too quickly and produce a large number of false positive alarms. Using long periods (such as 90 or 120min) will cause the system to be too slow to update its confidence, and hence fails to detect any fraudulent identity. We found that (50–60min) would produce a reasonable response for the system.

Changing the values of the assertion policy parameters (Eq. (5)) affects how the system’s confidence is swayed. The graphs shown in Fig. 3
                         show examples of such changes. Fig. 3(A) shows the effect of changing Δg and is illustrated by using a series of recurrent event types happening at equal time spaces. As shown in the figure, when Δg is set to values between 0.1 and 0.5, the system confidence increases slowly and takes a minimum of six events for the global assertion value to reach the next threshold value. This setting would be useful in highly secure environments. This means that in high security environments, the system requires more recurrent events to elevate its confidence (about the user) to the next assertion threshold. However, when Δg has high values such 0.7–1.0, then the system confidence rises quicker and fewer events are needed to reach the second level (around 2–3 events); this setting would be useful for more relaxed environments.

In Fig. 3(B) the results show the effect of Δg on the system’s behaviour in response to significant events. The figure shows big differences between the gradients for Δg
                        =0.1, 0.5 and 1. In the case of Δg
                        =0.1, the assertion value rises slowly and takes around three identifying significant events to reach the next assertion level; this is considered to be unfavourable as it means the system would not trust the identifying events. For Δg
                        =0.5 and 1, the assertion value increases more quickly and takes one identifying significant event to achieve the same effect, which is more preferable.

Choosing the values of the other parameters in the policy was done in a similar manner. As an example, Fig. 3(C) shows the effect of the local assertion values for recurrent events (Kr
                        ). As shown in the figure, high values in the range of (Kr
                        
                        =0.7–1.0) will turn this type of event into an identifying significant event type. Therefore, Kr
                         is recommended to be assigned a value between 0.1 and 0.5.

As a result of the testing and tuning procedure, three sets of policy settings for relaxed, moderate and high security policies have been proposed in this research (Table 4
                        ). As shown in the table the main parameter for deciding the policy is the rate of change of the global assertion value Δg.

A discrete event simulation (DES) has been used to validate our NIAS implementation. The simulation provides a test environment for studying the system for various situations of user behaviour, which is difficult to do in a real environment. The simulator generates a random number of recurrent, identifying and repudiating significant events, which are used as input to the NIAS system. The simulator does not produce any unrecognised activities as these do not have an effect on the system’s behaviour. No activity is assumed when the simulator does not produce events within a period of time. The system’s response is monitored and the number of false alarms is calculated in order to measure the system’s reliability (the ratio of successful identification attempts to overall attempts). Reliability was chosen as the performance measure to evaluate the research for two reasons. Firstly, it helps validate the approach rather than the implementation. Secondly, the use of false positives and false negatives has been recognised as a good way to evaluate similar security systems [25].

The simulator is written in C++ and uses a time driven DES strategy where time is represented as an advancing clock. Events are generated by simulating the time interval between event arrivals as a random variable. All random variables used in the simulation are assumed to fit a normal probability distribution, although other distributions such as Poisson and Exponential distributions have been considered. The simulator was used to tune the assertion algorithm parameters (Δg,
                     Kr
                     ,
                     Ks
                     ,
                     K) for a number of trials based on values calculated from the collected real data. The purpose of the tuning process is to adjust the assertion algorithm parameters and validate the algorithm response. In addition, the simulation was used to measure the system’s reliability for various environment settings.

From the study conducted, we found that the overriding factor that affects the system’s reliability is the distribution of the environment’s resources among the various assertion levels. This effect is illustrated by changing the ratio of resources placed in each assertion level. Assuming the number of all protected resources in the environment is N, and the number of resources placed in each of the three assertion levels are NL
                     ,
                     NM
                      and NH
                      for the low, medium and high assertion regions respectively, the study considered various values for the ratios (NL
                     /NH
                     ,
                     NL
                     /N and NH
                     /N). Using these values as input parameters for the simulation, the system’s reliability was estimated as a performance measure. Reliability is measured according to Eq. (11):
                        
                           (11)
                           
                              Reliability
                              =
                              
                                 
                                    
                                       1
                                       -
                                       False Alarm Rate
                                    
                                 
                              
                              ∗
                              100
                              %
                           
                        
                     where
                        
                           (12)
                           
                              False Alarm Rate
                              =
                              
                                 
                                    number of false alarms
                                 
                                 
                                    total number of attempts to access resources
                                 
                              
                           
                        
                     
                  

The system reliability for each input parameter has been calculated. The calculations show a linear relationship between the system reliability and these parameters. For a fixed number of overall resources, reliability is directly proportional to NL
                      and inversely proportional to NH
                     . These results are logical and they mean that: when there are more resources in the lower assertion region, there is more chance that users will perform more activities (accessing more resources) in this region before attempting to access resources in the higher assertion regions. This allows the system to build its confidence about the user before they attempt to access a higher assertion level resource. Our study revealed that when NL
                     
                     ≫
                     NH
                     , system reliability exceeds 99% as shown in Fig. 4
                     . This implies that NIAS is particularly effective in certain environments where resources are divided into a number of security levels and the number of higher security resources is smaller than the number of lower security resources.

In order to validate the approach, it has been compared with other verification methods. Table 5
                      shows the results of this comparison, using five criteria: verification principle, privacy, intrusion and disruption, trust and need for specialised hardware/software. As shown in the table biometric methods offer identity verification with a good degree of accuracy and confidence for they use features that are permanently associated with the users. However, the main drawback of such methods is their intrusive and disruptive nature, due to the fact that users may need to be interrupted from what they are doing in order to provide a piece of biometric information such as scanning a fingerprint. Although some biometric methods are less disruptive (such as face recognition), they still are intrusive because the use of a camera or other sensor type means that highly personalised information is collected and stored by the system to be used in the verification process. This also raises a question with regard to users’ perception and tolerance to such methods as people are usually concerned about how, where else, and by whom their information is being used. Although context aware systems use a completely different way of verifying identities because they have to deal with dynamic rather than static information, some of the information used by these methods may be regarded as personal and sensitive. Also, as context information could be stored in a variety of devices, these methods could become quite intrusive. NIAS addresses these issues by only collecting information related to the user’s access to resources, which has to be collected anyway. Collecting such information does not require users to perform additional disruptive activities in the process of verifying their identity. In addition, only activity types are used in the verification process, instead of the activities themselves, which makes NIAS less obtrusive (disruptive) than biometric and context-aware methods.

Certification authority methods use a centralised service to perform the verification. This approach has the advantage of not requiring sensitive information to be gathered but is more prone to network intrusion problems such as MiM attacks. Therefore additional infrastructural components and software are needed to mitigate risks and manage complex trust relationships. In NIAS, network attacks are not an issue because it uses historical information over a period of time in order to determine activity types that are used in the verification process. This means that an attacker needs to observe the user’s activities for a period of time (weeks) and acquire their ID pass before they can launch an attack.

Password exchange protocol methods provide a more decentralised solution that is suitable for domain federation; however, scalability in such systems remains a major issue. This is due to fact that users have different identities and/or different privileges over different domains, and hence complex trust relationships are likely to form. In contrast, NIAS provides a user-centric approach that supports scalability in which each individual identity is treated in isolation of others regardless of the domain to which the resource belongs. In other words, NIAS isolates the process of verification from other authentication and authorisation procedures, which makes the scalability issue easier to manage.

The main limitation of NIAS is its initial learning time to learn about its users and build its events database. During this period, which could last for a few weeks, and if the system is unable to verify the user’s identity, it offers the users the ability to re-authenticate (using conventional methods). This authentication event becomes an identifying significant event that is used to update the system’s confidence. Once this initial period passes, the system’s knowledge is continuously reinforced and updated, which also addresses the issue of dynamic information posed by context-aware methods.

@&#CONCLUSIONS@&#

NIAS is an approach for identity verification in pervasive environments based on using history of previous access to resources as a means for asserting the user’s identity before access control rules are applied. In NIAS, identity verification is a process of monitoring users as they attempt to access the environment’s resources and converting this knowledge into a numeric value (assertion value) that represents the system’s confidence about the user’s identity. This value is used to update the system’s confidence as the user performs a known activity or otherwise. An integral component within NIAS is the assertion policy that uses the assertion value to decide whether the user can be trusted to access the resource or not. The assertion policy consists of a number of parameters whose collective values determine the nature of the policy applied for the environment.

Using a simulated environment, NIAS has been validated using a number of tests and evaluations. The purpose of the tests was to tune the system’s parameters for a number of assertion policies. This has been accomplished successfully and a number of settings have been suggested and presented as guidelines for recommended assertion policies. The purpose of the evaluations was to measure the system’s reliability for various types of environment. The evaluation showed that the approach is successful in verifying user identity with high degree of reliability, exceeding 99% in certain situations.

In comparison to other identity verification methods, NIAS offers a user-centric approach that is less intrusive, yet scalable and dynamic. The main limitation of NIAS is the time it takes initially to build the system’s knowledge of its users. To overcome this limitation, the work could be extended in the future by adding a prediction element to the approach based on the user’s context information.

@&#REFERENCES@&#

