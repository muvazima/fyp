@&#MAIN-TITLE@&#Probabilistic topic modeling in multilingual settings: An overview of its methodology and applications

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A systematic overview of multilingual probabilistic topic modeling (MuPTM).


                        
                        
                           
                           A tutorial on methodology, modeling, training, output, inference and evaluation of MuPTM.


                        
                        
                           
                           Language-independent and language-pair independent data representations.


                        
                        
                           
                           A model-independent framework and applications in various cross-lingual tasks.


                        
                        
                           
                           A complete MuPTM-based framework for cross-lingual semantic similarity.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multilingual probabilistic topic models

Cross-lingual text mining

Cross-lingual knowledge transfer

Cross-lingual information retrieval

Language-independent data representation

Non-parallel data

@&#ABSTRACT@&#


               
               
                  Probabilistic topic models are unsupervised generative models which model document content as a two-step generation process, that is, documents are observed as mixtures of latent concepts or topics, while topics are probability distributions over vocabulary words. Recently, a significant research effort has been invested into transferring the probabilistic topic modeling concept from monolingual to multilingual settings. Novel topic models have been designed to work with parallel and comparable texts. We define multilingual probabilistic topic modeling (MuPTM) and present the first full overview of the current research, methodology, advantages and limitations in MuPTM. As a representative example, we choose a natural extension of the omnipresent LDA model to multilingual settings called bilingual LDA (BiLDA). We provide a thorough overview of this representative multilingual model from its high-level modeling assumptions down to its mathematical foundations. We demonstrate how to use the data representation by means of output sets of (i) per-topic word distributions and (ii) per-document topic distributions coming from a multilingual probabilistic topic model in various real-life cross-lingual tasks involving different languages, without any external language pair dependent translation resource: (1) cross-lingual event-centered news clustering, (2) cross-lingual document classification, (3) cross-lingual semantic similarity, and (4) cross-lingual information retrieval. We also briefly review several other applications present in the relevant literature, and introduce and illustrate two related modeling concepts: topic smoothing and topic pruning. In summary, this article encompasses the current research in multilingual probabilistic topic modeling. By presenting a series of potential applications, we reveal the importance of the language-independent and language pair independent data representations by means of MuPTM. We provide clear directions for future research in the field by providing a systematic overview of how to link and transfer aspect knowledge across corpora written in different languages via the shared space of latent cross-lingual topics, that is, how to effectively employ learned per-topic word distributions and per-document topic distributions of any multilingual probabilistic topic model in various cross-lingual applications.
               
            

@&#INTRODUCTION@&#

Probabilistic latent topic models such as probabilistic Latent Semantic Analysis (pLSA) (Hofmann, 1999b, 1999a) and Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003b) along with their numerous variants are well studied generative models for representing the content of documents in large document collections. They provide a robust and unsupervised framework for performing shallow latent semantic analysis of themes (or topics) discussed in text. The families of these probabilistic latent topic models are all based upon the idea that there exist latent variables, that is, topics, which determine how words in documents have been generated. Fitting such a generative model actually denotes finding the best set of those latent variables in order to explain the observed data. With respect to that generative process, documents are seen as mixtures of latent topics, while topics are simply probability distributions over vocabulary words. A topic representation of a document constitutes a high-level language-independent view of its content, unhindered by a specific word choice, and it improves on text representations that contain synonymous or polysemous words (Griffiths, Steyvers, & Tenenbaum, 2007).

Probabilistic topic modeling constitutes a very general framework for unsupervised topic mining, and over the years it has been employed in miscellaneous tasks in a wide variety of research domains, e.g., for object recognition in computer vision (e.g., Li & Perona, 2005; Russell, Freeman, Efros, Sivic, & Zisserman, 2006; Wang & Grimson, 2007), dialogue segmentation (e.g., Purver, Körding, Griffiths, & Tenenbaum, 2006), video analysis (e.g., Wang, Ma, & Grimson, 2009), automatic harmonic analysis in music (e.g., Arenas-Garca et al., 2007; Hu & Saul, 2009), genetics (e.g., Blei, Franks, Jordan, & Mian, 2006), and others.

Being originally proposed for textual data, probabilistic topic models have also organically found many applications in natural language processing (NLP). Discovered distributions of words over topics (further per-topic word distributions) and distributions of topics over documents (further per-document topic distributions) can be directly employed to detect main themes
                        1
                        To avoid confusion, we talk about themes when we address the true content of a document, while we talk about topics when we address the probability distributions constituting a topic model.
                     
                     
                        1
                      discussed in texts, and to provide gists or summaries for large text collections (see, e.g., Hofmann, 1999b; Blei et al., 2003b; Griffiths & Steyvers, 2004; Griffiths et al., 2007). Per-document topic distributions for each document might be observed as a low-dimensional latent semantic representation of text in a new topic-document space, potentially better than the original word-based representation in some applications. In an analogous manner, since the number of topics is usually much lower than the number of documents in a collection, per-topic word distributions also model a sort of dimensionality reduction, as the original word-document space is transferred to a lower-dimensional word-topic space. Apart from the straightforward utilization of probabilistic topic models as direct summaries of large document collections, these two sets of probability distributions have been utilized in a myriad of NLP tasks, e.g., for inferring captions for images (Blei & Jordan, 2003), sentiment analysis (e.g., Mei, Ling, Wondra, Su, & Zhai, 2007; Titov & McDonald, 2008), analyzing topic trends for different time intervals in scientific literature, social networks and e-mails (e.g., Wang & McCallum, 2006; McCallum, Wang, & Corrada-Emmanuel, 2007; Hall, Jurafsky, & Manning, 2008), language modeling in information retrieval (e.g., Wei & Croft, 2006; Yi & Allan, 2009), document classification (e.g., Blei et al., 2003b; Lacoste-Julien, Sha, & Jordan, 2008), word sense disambiguation (e.g., Boyd-Graber, Blei, & Zhu, 2007), modeling distributional similarity of terms (e.g., Ritter, Mausam, & Etzioni, 2010; Dinu & Lapata, 2010), etc. Lu, Mei, and Zhai, 2011 examine task performance of pLSA and LDA as representative monolingual topic models in typical tasks of document clustering, text categorization and ad hoc information retrieval. Data representation, i.e., representations of words and documents in all applications presented in this article will be based on those per-topic word distributions and per-document topic distributions.

However, all these models have been designed to work with monolingual data, and they have been applied in monolingual contexts only. Following the ongoing growth of the World Wide Web and its omnipresence in today’s increasingly connected world, users tend to abandon English as the lingua franca of the global network, since more and more content becomes available in their native languages or even dialects and different community languages (e.g., the idiomatic usage of the same language typically differs between scientists, social media consumers or the legislative domain). It is difficult to determine the exact number of languages in the world, but the estimations vary between 6000 and 7000 languages and almost 40,000 unofficial languages and dialects.
                        2
                        Source: http://www.ethnologue.com.
                     
                     
                        2
                      It is extremely time-consuming and labor-intensive to build quality translation resources and parallel corpora for each single language/dialect pair. Therefore, we observe an increasing interest in language-independent unsupervised corpus-based cross-lingual text mining from non-parallel corpora without any additional translation resources. High-quality parallel corpora where documents are sentence-aligned exact translations of each other (such as Europarl (Koehn, 2005)) are available only for a restricted number of languages and domains. There has been a recent interest to build parallel corpora from the Web (e.g., Resnik & Smith, 2003; Munteanu & Marcu, 2005, 2006), but the obtained parallel data still typically remain of limited size and scope as well as domain-restricted (e.g., parliamentary proceedings).

With the rapid development of Wikipedia and online social networks such as Facebook or Twitter, users have generated a huge volume of multilingual text resources. The user-generated data are often noisy and unstructured, and seldom well-paired across languages. However, unlike parallel corpora, such comparable corpora, where texts in one language are paired with texts in another language discussing the same themes or subjects, are abundant in various online sources (e.g., Wikipedia or news sites). Documents from comparable corpora do not necessarily share all their themes with their counterparts in the other language, but, for instance, Wikipedia articles discussing the same subject, or news stories discussing the same event contain a significant thematic overlap. We could say that such documents in different languages, although inherently non-parallel, are theme-aligned.


                     Multilingual probabilistic topic models (MuPTM-s) have recently emerged as a group of unsupervised, language-independent generative machine learning models that can be efficiently utilized on such large-volume non-parallel theme-aligned multilingual data and effectively deal with uncertainty in such data collections. Due to its generic language-independent nature and the power of inference on unseen documents, these models have found many interesting applications. The knowledge from learned MuPTM-s has been used in many different cross-lingual tasks such as cross-lingual event clustering (DeSmet & Moens, 2009), cross-lingual document classification (De Smet, Tang, & Moens, 2011; Ni, Sun, Hu, & Chen, 2011), cross-lingual semantic similarity of words (Mimno, Wallach, Naradowsky, Smith, & McCallum, 2009; Vulić, DeSmet, & Moens, 2011a; Vulić & Moens, 2012), cross-lingual information retrieval (Vulić, Smet, & Moens, 2011b, 2013; Vulić & Moens, 2013; Ganguly, Leveling, & Jones, 2012) and others.

The main goal of this work is to provide an overview of the recently developed multilingual probabilistic topic modeling concept. It aims to model topic discovery from multilingual data in a conceptually sound way, taking into account thematic alignment between documents in document collections given in different languages. We have decided to provide a thorough analysis of the framework of multilingual probabilistic topic modeling because we feel that the current relevant literature lacks a systematic and complete overview of the subject. Moreover, during our tutorials at ECIR 2013 and WSDM 2014 on the subject we realized even more that, after being provided with feedback on our tutorials, it would be extremely beneficial for the IR community to have an extended written overview of the whole subject, along with its formalisms, definitions and modeling perspectives (both conceptual and mathematical), relevant state-of-the-art, a broad relevant references list, and also with standard evaluation procedures, an overview of applications, and suggestions for future work.

As a representative example, we choose bilingual LDA, which has been designed as a basic and natural extension of the standard omnipresent LDA model in the multilingual settings where document-aligned articles in different languages are available (e.g., Wikipedia articles about the same subject in multiple languages). We provide a complete and comprehensive overview of that model all the way up from the conceptual and modeling level down to its core mathematical foundations as it could serve as a valuable starting point for other researchers in the field of multilingual probabilistic topic modeling and cross-lingual text mining. Alternative multilingual probabilistic topic models that build upon the idea of the standard pLSA and LDA models are presented in a nutshell. These models differ in the specific assumptions they make in their generative processes as well as in knowledge that is presupposed before training (e.g., document alignment, prior word matchings or bilingual dictionaries), but all these models have the ability to discover latent cross-lingual topics from comparable data such as Wikipedia or news. Additionally, all these models output the same basic sets of probability distributions, that is, per-topic word distributions and per-document topic distributions. Finally, we also demonstrate how to utilize the high-level structured text representations by means of per-topic word distributions and per-document topic distributions from any multilingual probabilistic topic model, and establish knowledge transfer across different languages via the shared space of latent language-independent concepts, that is, cross-lingual topics in several real-life NLP/IR tasks: (1) cross-lingual event-centered news clustering, (2) cross-lingual document classification, (3) cross-lingual semantic similarity, and (4) cross-lingual information retrieval. In this article, we show the results obtained by BiLDA, but the presented solutions are completely topic model-independent.

The results reported across all these tasks show the validity of multilingual comparable data as training data, as well as the superiority of MuPTM over monolingual probabilistic topic modeling (MoPTM) and other data-driven modeling paradigms which do not rely on any expensive translation resources. We also expose and discuss an issue present in all current multilingual topic models – a need to set the number of topics in advance before training. Different applications reach their optimal results with different number of topics set a priori and it is often difficult to accurately predict that application-dependent number of topics in advance. We also report on a mismatch between the standard intrinsic evaluation measure of perplexity and the extrinsic evaluation in terms of final scores in the cross-lingual tasks.

This section presents and defines the basic concepts and modeling assumptions related to multilingual probabilistic topic modeling, with a special focus on learning from comparable theme-aligned corpora. We also draw an analogy to the broader paradigm of latent cross-lingual concepts, and their relation to latent cross-lingual topics. Following that, the representative bilingual LDA (BiLDA) is presented in its entirety, which includes its generative story, the explanation of the Gibbs sampling training procedure for the model, the output of the model in terms of per-topic word distributions and per-document topic distributions, the procedure to infer the trained model on unseen data, and a qualitative analysis of its output in terms of the top N most important words for some selected topics. We also define several evaluation metrics utilized to compare different topic models. At the very end of the section, alternative multilingual topic models are also presented.


                        
                           Definition 1
                           Multilingual theme-aligned corpus


                           In the most general definition, a multilingual theme-aligned corpus 
                              
                                 
                                    C
                                 
                               of 
                                 
                                    l
                                    =
                                    ∣
                                    L
                                    ∣
                                 
                               languages, where 
                                 
                                    L
                                    =
                                    {
                                    
                                       
                                          L
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          L
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          L
                                       
                                       
                                          l
                                       
                                    
                                    }
                                 
                               is the set of languages, is a set of corresponding text collections 
                                 
                                    {
                                    
                                       
                                          C
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          C
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          C
                                       
                                       
                                          l
                                       
                                    
                                    }
                                 
                              . Each 
                                 
                                    
                                       
                                          C
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    {
                                    
                                       
                                          d
                                       
                                       
                                          1
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          2
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          d
                                       
                                       
                                          
                                             
                                                dn
                                             
                                             
                                                i
                                             
                                          
                                       
                                       
                                          i
                                       
                                    
                                    }
                                 
                               is a collection of documents in language 
                                 
                                    
                                       
                                          L
                                       
                                       
                                          i
                                       
                                    
                                 
                               with vocabulary 
                                 
                                    
                                       
                                          V
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    {
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          
                                             
                                                wn
                                             
                                             
                                                i
                                             
                                          
                                       
                                       
                                          i
                                       
                                    
                                    }
                                 
                              . Collections 
                                 
                                    {
                                    
                                       
                                          C
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          C
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          C
                                       
                                       
                                          l
                                       
                                    
                                    }
                                 
                               are said to be theme-aligned if they discuss at least a portion of similar themes. Here, 
                                 
                                    
                                       
                                          dn
                                       
                                       
                                          i
                                       
                                    
                                 
                               denotes the total number of documents in document collection 
                                 
                                    
                                       
                                          C
                                       
                                       
                                          i
                                       
                                    
                                 
                              , while 
                                 
                                    
                                       
                                          wn
                                       
                                       
                                          i
                                       
                                    
                                 
                               is the total number of words in 
                                 
                                    
                                       
                                          V
                                       
                                       
                                          i
                                       
                                    
                                 
                              . Moreover, 
                                 
                                    
                                       
                                          d
                                       
                                       
                                          j
                                       
                                       
                                          i
                                       
                                    
                                 
                               denotes the j-th document in document collection 
                                 
                                    
                                       
                                          C
                                       
                                       
                                          i
                                       
                                    
                                 
                              , and 
                                 
                                    
                                       
                                          w
                                       
                                       
                                          j
                                       
                                       
                                          i
                                       
                                    
                                 
                               denotes the j-th word in vocabulary 
                                 
                                    
                                       
                                          V
                                       
                                       
                                          i
                                       
                                    
                                 
                               associated with document collection 
                                 
                                    
                                       
                                          C
                                       
                                       
                                          i
                                       
                                    
                                 
                              .

A multilingual probabilistic topic model of a theme-aligned multilingual corpus 
                                 
                                    C
                                 
                               is a set of semantically coherent multinomial distributions of words with values 
                                 
                                    
                                       
                                          P
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   j
                                                
                                                
                                                   i
                                                
                                             
                                             ∣
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                          
                                       
                                    
                                    ,
                                    i
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    l
                                 
                              , for each vocabulary 
                                 
                                    
                                       
                                          V
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          V
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          V
                                       
                                       
                                          l
                                       
                                    
                                 
                               associated with text collections 
                                 
                                    
                                       
                                          C
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          C
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          C
                                       
                                       
                                          l
                                       
                                    
                                    ∈
                                    C
                                 
                               given in languages 
                                 
                                    
                                       
                                          L
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          L
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          L
                                       
                                       
                                          l
                                       
                                    
                                 
                              . 
                                 
                                    
                                       
                                          P
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   j
                                                
                                                
                                                   i
                                                
                                             
                                             ∣
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                          
                                       
                                    
                                 
                               is calculated for each 
                                 
                                    
                                       
                                          w
                                       
                                       
                                          j
                                       
                                       
                                          i
                                       
                                    
                                    ∈
                                    
                                       
                                          V
                                       
                                       
                                          i
                                       
                                    
                                 
                              . The probabilities 
                                 
                                    
                                       
                                          P
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   j
                                                
                                                
                                                   i
                                                
                                             
                                             ∣
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                          
                                       
                                    
                                 
                               build per-topic word distributions (denoted by 
                                 
                                    
                                       
                                          ϕ
                                       
                                       
                                          i
                                       
                                    
                                 
                              ), and they constitute a language-specific representation (e.g., a probability value is assigned only for words from 
                                 
                                    
                                       
                                          V
                                       
                                       
                                          i
                                       
                                    
                                 
                              ) of a language-independent latent cross-lingual concept – topic 
                                 
                                    
                                       
                                          z
                                       
                                       
                                          k
                                       
                                    
                                    ∈
                                    Z
                                 
                              . 
                                 
                                    Z
                                    =
                                    {
                                    
                                       
                                          z
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          z
                                       
                                       
                                          K
                                       
                                    
                                    }
                                 
                               represents the set of all K latent cross-lingual topics present in the multilingual corpus. Each document in the multilingual corpus is thus considered a mixture of K latent cross-lingual topics from the set 
                                 
                                    Z
                                 
                              . This mixture for some document 
                                 
                                    
                                       
                                          d
                                       
                                       
                                          j
                                       
                                       
                                          i
                                       
                                    
                                    ∈
                                    
                                       
                                          C
                                       
                                       
                                          i
                                       
                                    
                                 
                               is modeled by the probabilities 
                                 
                                    
                                       
                                          P
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             ∣
                                             
                                                
                                                   d
                                                
                                                
                                                   j
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                               that altogether build per-document topic distributions (denoted by θ). In summary, each language-independent latent cross-lingual topic 
                                 
                                    
                                       
                                          z
                                       
                                       
                                          k
                                       
                                    
                                 
                               has some probability to be found in a particular document (modeled by per-document topic distributions), and each such topic has a language-specific representation in each language (modeled by language-specific per-topic word distributions).

We can interpret Definition 2 in the following way: each cross-lingual topic from the set 
                           
                              Z
                           
                         can be observed as a latent language-independent concept present in the multilingual corpus, but each language in the corpus uses only words from its own vocabulary to describe the content of that concept (see Fig. 1
                         for an illustrative example). In other words, we could observe each latent cross-lingual topic as a set of discrete distributions over words, one for each language. For instance, having a multilingual collection in English, Italian and Dutch and discovering a topic on Soccer, that cross-lingual topic would be represented by words (actually probabilities over words) {player, goal, scorer, …} in English, {squadra (team), calcio (soccer), allenatore (coach), …} in Italian, and {doelpunt (goal), voetballer (soccer player), elftal (soccer team), …} in Dutch. We have 
                           
                              
                                 
                                    ∑
                                 
                                 
                                    
                                       
                                          w
                                       
                                       
                                          j
                                       
                                       
                                          i
                                       
                                    
                                    ∈
                                    
                                       
                                          V
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                              
                                 
                                    P
                                 
                                 
                                    i
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             w
                                          
                                          
                                             j
                                          
                                          
                                             i
                                          
                                       
                                       ∣
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                              
                              =
                              1
                           
                        , for each vocabulary 
                           
                              
                                 
                                    V
                                 
                                 
                                    i
                                 
                              
                           
                         representing language 
                           
                              
                                 
                                    L
                                 
                                 
                                    i
                                 
                              
                           
                        , and for each topic 
                           
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                              ∈
                              Z
                           
                        . We say that a latent cross-lingual topic is semantically coherent if it assigns high probabilities to words that are semantically related. Definition 2 is predominant in the MuPTM literature (e.g., see DeSmet & Moens, 2009; Mimno et al., 2009; Platt, Toutanova, & Yih, 2010).


                        Zhang, Mei, and Zhai (2010) provide an alternative, more general definition of a multilingual topic model, but we will show that their definition may be brought down to Definition 2 after a partition over the languages is performed. Namely, the whole multilingual corpus is observed as a mixture of latent cross-lingual topics from 
                           
                              Z
                           
                        . They then define a latent cross-lingual topic 
                           
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                              ∈
                              Z
                           
                         as a semantically coherent multinomial distribution over all the words in all the vocabularies of languages 
                           
                              
                                 
                                    L
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    L
                                 
                                 
                                    i
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    L
                                 
                                 
                                    l
                                 
                              
                           
                        , and 
                           
                              P
                              
                                 
                                    
                                       
                                          
                                             w
                                          
                                          
                                             j
                                          
                                       
                                       ∣
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                              
                           
                         gives the probability of any word 
                           
                              
                                 
                                    w
                                 
                                 
                                    j
                                 
                              
                              ∈
                              {
                              
                                 
                                    V
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    V
                                 
                                 
                                    i
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    V
                                 
                                 
                                    l
                                 
                              
                              }
                           
                         to be generated by topic 
                           
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                           
                        . In this case, we have 
                           
                              
                                 
                                    ∑
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    l
                                 
                              
                              
                                 
                                    ∑
                                 
                                 
                                    
                                       
                                          w
                                       
                                       
                                          j
                                       
                                    
                                    ∈
                                    
                                       
                                          V
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                              P
                              
                                 
                                    
                                       
                                          
                                             w
                                          
                                          
                                             j
                                          
                                       
                                       ∣
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                              
                              =
                              1
                           
                        . The language-specific representation for language 
                           
                              
                                 
                                    L
                                 
                                 
                                    i
                                 
                              
                           
                         of topic 
                           
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                           
                         is then obtained by retaining only probabilities for words which are present in its own vocabulary 
                           
                              
                                 
                                    V
                                 
                                 
                                    i
                                 
                              
                           
                        , and normalizing those distributions. For a word 
                           
                              
                                 
                                    w
                                 
                                 
                                    j
                                 
                                 
                                    i
                                 
                              
                              ∈
                              
                                 
                                    V
                                 
                                 
                                    i
                                 
                              
                           
                        , we have 
                           
                              
                                 
                                    P
                                 
                                 
                                    i
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             w
                                          
                                          
                                             j
                                          
                                          
                                             i
                                          
                                       
                                       ∣
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                              
                              =
                              
                                 
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   j
                                                
                                                
                                                   i
                                                
                                             
                                             ∣
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                w
                                             
                                             
                                                j
                                             
                                          
                                          ∈
                                          
                                             
                                                V
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   j
                                                
                                             
                                             ∣
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        . After the partition over languages and normalizations are performed, this definition is effectively equivalent to Definition 2. However, note that their original definition is more general than Definition 2, but it is also unbalanced over the languages from 
                           
                              L
                           
                         present in 
                           
                              C
                           
                        , that is, words from the languages that are more present in the original corpus 
                           
                              C
                           
                         might dominate the multinomial per-topic word distributions. By performing the partition and normalization over the languages, that imbalance is effectively removed.
                           Definition 3
                           Multilingual probabilistic topic modeling


                           Given a theme-aligned multilingual corpus 
                                 
                                    C
                                 
                              , the goal of multilingual probabilistic topic modeling or latent cross-lingual topic extraction is to learn and extract a set 
                                 
                                    Z
                                 
                               of K latent language-independent concepts, that is, latent cross-lingual topics 
                              
                                 
                                    Z
                                    =
                                    {
                                    
                                       
                                          z
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          z
                                       
                                       
                                          K
                                       
                                    
                                    }
                                 
                               that optimally describe the observed data, that is, the multilingual corpus 
                                 
                                    C
                                 
                              . Extracting latent cross-lingual topics actually implies learning per-document topic distributions for each document in the corpus, and discovering language-specific representations of these topics given by per-topic word distributions in each language (see Definition 2).

This shared and language-independent set of latent cross-lingual topics 
                           
                              Z
                           
                         serves as the core of unsupervised cross-lingual text mining and cross-lingual knowledge linking and transfer by means of multilingual probabilistic topic models. It is the cross-lingual connection that bridges the gap across documents in different languages and transfers knowledge across languages in case when translation resources and labeled instances are scarce or missing. The trained multilingual probabilistic topic model may be further inferred on unseen documents.
                           Definition 4
                           Inference of a multilingual probabilistic topic model


                           Given an unseen document collection 
                                 
                                    
                                       
                                          C
                                       
                                       
                                          u
                                       
                                    
                                 
                              , the inference of a multilingual topic model on the collection 
                                 
                                    
                                       
                                          C
                                       
                                       
                                          u
                                       
                                    
                                 
                               denotes learning topical representations of the unseen documents 
                                 
                                    
                                       
                                          d
                                       
                                       
                                          u
                                       
                                    
                                    ∈
                                    
                                       
                                          C
                                       
                                       
                                          u
                                       
                                    
                                 
                              , that is, acquiring per-document topic distributions for the new documents based on the previous output of the model.

Knowledge transfer in general refers to transferring knowledge learned from one corpus to another corpus, which was unavailable during the learning procedure. Cross-lingual knowledge transfer is characterized by the fact that corpora are present in more than one language.

Additionally, following the assumptions and general definitions provided in this section, monolingual probabilistic topic models such as pLSA (Hofmann, 1999b, 1999a) and LDA (Blei et al., 2003b) could be interpreted as a degenerate special case of multilingual probabilistic topic models where only one language is involved, and all the definitions and assumptions remain the same (see later the discussion in Section 2.3).

The latent cross-lingual topics presented in Section 2.1 constitute only one possibility when the aim is to detect and induce a latent semantic structure from multilingual data, that is, to extract latent cross-lingual concepts that are hidden within the data. Latent cross-lingual concepts may be interpreted as language-independent semantic concepts present in a multilingual corpus (e.g., document-aligned Wikipedia articles in English and Spanish) that have their language-specific representations in different languages. To repeat, for instance, having a multilingual collection in English, Spanish and Croatian, and discovering a latent semantic concept on Basketball, that concept would be represented by words (actually probabilities over words) {player, ball, coach, …} in English, {pelota (ball), jugador (player), partido (match), …} in Spanish, and {trener (coach), razigravač (playmaker), doigravanje (playoff), …} in Croatian.

These K semantic concepts span a latent cross-lingual semantic space. Each word w may be represented in that latent semantic space as a K-dimensional vector, where each vector component is a conditional concept probability score 
                        
                           
                              P
                              
                                 
                                    
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                       ∣
                                       w
                                    
                                 
                              
                           
                        . In other words, each word is actually represented as a multinomial probability distribution over the induced latent cross-lingual semantic concepts. Moreover, each document d, regardless of its actual language, may be represented as a multinomial probability distribution, a mixture over the same induced latent cross-lingual semantic concepts 
                           
                              P
                              
                                 
                                    
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                       ∣
                                       d
                                    
                                 
                              
                           
                        .

The description in this article relies on the multilingual probabilistic topic modeling framework, but we emphasize that all the work described in this article is independent of the actual method used to induce the latent cross-lingual concepts. The reader has to be aware of the fact that the description of how to utilize this latent knowledge in applications is generic and model-independent as they allow the usage of all other models that compute probability scores 
                           
                              P
                              (
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                              ∣
                              w
                              )
                           
                         and 
                           
                              P
                              (
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                              ∣
                              d
                              )
                           
                         (obtained from per-topic word distributions and per-document topic distributions). Besides MuPTM, a number of other models may be employed to induce the latent cross-lingual concepts. For instance, one could use cross-lingual Latent Semantic Indexing (Dumais, Landauer, & Littman, 1996), probabilistic principal component analysis (Tipping & Bishop, 1999), LDA (Blei et al., 2003b), or a probabilistic interpretation of non-negative matrix factorization (Lee & Seung, 1999; Gaussier & Goutte, 2005; Ding, Li, & Peng, 2008) on concatenated documents in aligned document pairs. Other more recent models include matching canonical correlation analysis (Haghighi, Liang, Berg-Kirkpatrick, & Klein, 2008; Daumé III & Jagarlamudi, 2011) or other families of multilingual topic models (Fukumasu, Eguchi, & Xing, 2012).

Without loss of generality, from now on we deal with bilingual probabilistic topic modeling. We work with a bilingual corpus and present cross-lingual tasks in the bilingual setting. For bilingual corpora we introduce the source language 
                           
                              
                                 
                                    L
                                 
                                 
                                    S
                                 
                              
                           
                         (further with indices S) and the target language 
                           
                              
                                 
                                    L
                                 
                                 
                                    T
                                 
                              
                           
                         (further with indices T). We will show that all the definitions and assumptions may be easily generalized to a setting where more than two languages are available.

Bilingual Latent Dirichlet Allocation (BiLDA) is a bilingual extension of the standard LDA model (Blei et al., 2003b), tailored for modeling parallel or, even more importantly, comparable theme-aligned bilingual document collections. An example of such a document collection is Wikipedia in 2 languages with paired articles. BiLDA has been independently designed by several researchers (Ni, Sun, Hu, & Chen, 2009; DeSmet & Moens, 2009; Mimno et al., 2009; Platt et al., 2010). Unlike LDA, where each document is assumed to possess its own document-specific distribution over topics, the generative process for BiLDA assumes that each aligned document pair shares the same distribution of topics. Therefore, the model assumes that we already possess document alignments in a corpus, that is, links between paired documents in different languages in a bilingual (or a multilingual) corpus. This assumption is certainly valid for multilingual Wikipedia data, where document alignment is established via cross-lingual links between articles written in different languages. These links are provided by the nature of the Wikipedia structure. Cross-lingual document alignment for news crawled from the Web is also a well-studied problem. Since the establishing of cross-lingual links between similar documents is not the focus of the research reported here, these algorithms are not elaborated in the article, but we refer the curious reader to the literature (see, e.g., Utiyama & Isahara, 2003; Resnik & Smith, 2003; Tao & Zhai, 2005; Munteanu & Marcu, 2006; Vu, Aw, & Zhang, 2009).
                              Definition 6
                              Paired bilingual corpus


                              A paired bilingual document corpus is defined as 
                                    
                                       C
                                       =
                                       {
                                       
                                          
                                             d
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             d
                                          
                                          
                                             2
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             d
                                          
                                          
                                             r
                                          
                                       
                                       }
                                       =
                                       {
                                       (
                                       
                                          
                                             d
                                          
                                          
                                             1
                                          
                                          
                                             S
                                          
                                       
                                       ,
                                       
                                          
                                             d
                                          
                                          
                                             1
                                          
                                          
                                             T
                                          
                                       
                                       )
                                       ,
                                       (
                                       
                                          
                                             d
                                          
                                          
                                             2
                                          
                                          
                                             S
                                          
                                       
                                       ,
                                       
                                          
                                             d
                                          
                                          
                                             2
                                          
                                          
                                             T
                                          
                                       
                                       )
                                       ,
                                       …
                                       ,
                                       (
                                       
                                          
                                             d
                                          
                                          
                                             r
                                          
                                          
                                             S
                                          
                                       
                                       ,
                                       
                                          
                                             d
                                          
                                          
                                             r
                                          
                                          
                                             T
                                          
                                       
                                       )
                                       }
                                    
                                 , where 
                                    
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                       
                                       =
                                       (
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                          
                                             S
                                          
                                       
                                       ,
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                          
                                             T
                                          
                                       
                                       )
                                    
                                  denotes the j-th pair of linked documents in the source language 
                                    
                                       
                                          
                                             L
                                          
                                          
                                             S
                                          
                                       
                                    
                                  and the target language 
                                    
                                       
                                          
                                             L
                                          
                                          
                                             T
                                          
                                       
                                    
                                 , respectively. The goal of bilingual probabilistic topic modeling is to learn for a (paired or non-paired) bilingual corpus a set of K latent cross-lingual topics 
                                    
                                       Z
                                    
                                 , each of which defines an associated set of words in both 
                                    
                                       
                                          
                                             L
                                          
                                          
                                             S
                                          
                                       
                                    
                                  and 
                                    
                                       
                                          
                                             L
                                          
                                          
                                             T
                                          
                                       
                                    
                                 .

BiLDA can be observed as a three-level Bayesian network that models document pairs using a latent layer of shared topics. Fig. 2
                            shows the graphical representation of the BiLDA model in plate notation, while Algorithm 1 presents its generative story.
                              Algorithm 1
                              Generative story for BiLDA 
                                    
                                       
                                    
                                 
                              

BiLDA takes advantage of the assumed topical alignment at the level of linked documents by introducing a single variable θ (see Section 2.1) shared by both documents. 
                              
                                 
                                    
                                       θ
                                    
                                    
                                       j
                                    
                                 
                              
                            denotes the distribution of latent cross-lingual topics over each document pair 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                              
                           . For each document pair 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                              
                           , a per-document topic distribution
                              3
                              The correct term here should be per-pair topic distribution for BiLDA and per-tuple topic distribution in case when more than 2 languages are involved, but we have decided to retain the original name of the distribution in order to draw a direct comparison with standard monolingual LDA.
                           
                           
                              3
                            
                           
                              
                                 
                                    
                                       θ
                                    
                                    
                                       j
                                    
                                 
                              
                            is sampled from a conjugate Dirichlet prior
                              4
                              For an introduction to conjugate distributions, priors and Bayesian inference, we refer the curious reader to the excellent Heinrich’s overview (Heinrich, 2008).
                           
                           
                              4
                            with K parameters 
                              
                                 
                                    
                                       α
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       α
                                    
                                    
                                       K
                                    
                                 
                              
                           . Then, with respect to 
                              
                                 
                                    
                                       θ
                                    
                                    
                                       j
                                    
                                 
                              
                           , a cross-lingual topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            is sampled. Each word 
                              
                                 
                                    
                                       w
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            at the position i in the source document of the current document pair 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                              
                            is then generated from a multinomial distribution 
                              
                                 
                                    
                                       ϕ
                                    
                                    
                                       
                                          
                                             z
                                          
                                          
                                             ji
                                          
                                          
                                             S
                                          
                                       
                                    
                                 
                              
                           . Similarly, each word 
                              
                                 
                                    
                                       w
                                    
                                    
                                       ji
                                    
                                    
                                       T
                                    
                                 
                              
                            of the target language
                              5
                              Both words (w-s) and topics (z-s) are annotated with a corresponding superscript S or T to denote which language they are used in.
                           
                           
                              5
                            is also sampled following the same procedure. Note that words at the same positions in source and target documents in a document pair need not be sampled from the same latent cross-lingual topic. The only constraint imposed by the model is that the overall distributions of topics over documents in a document pair modeled by 
                              
                                 
                                    
                                       θ
                                    
                                    
                                       j
                                    
                                 
                              
                            have to be the same. The validity of this assumption/constraint is dependent on the actual degree of thematic alignment of two coupled documents, as well as on the chosen topic granularity (e.g., two Wikipedia articles about the same subject may have the same focus and share global topics, but at a finer scale, they might exhibit different sub-focuses and do not share a subset of other more local topics).

According to Griffiths et al. (2007), each hyper-parameter 
                              
                                 
                                    
                                       α
                                    
                                    
                                       k
                                    
                                 
                              
                            could be interpreted as a prior observation count for the number of times topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                            is sampled in a document (or document pair) before having observed any actual words. If one is in possession of a certain prior or external knowledge (e.g., document metadata, main themes of a document collections) about the topic importance and the likelihood of its presence in the data, introducing asymmetric priors gives more preference to a subset of the most important topics, which could in the end lead to a better estimated set of output distributions (Mimno & McCallum, 2008; Jagarlamudi, DauméIII, & Udupa, 2012). However, it is often the case that we do not possess any prior knowledge about themes in a text collection, and then it is reasonable to assume that all topics are a priori equally likely. Therefore, it is convenient to use a symmetric Dirichlet distribution with a single hyper-parameter α such that 
                              
                                 
                                    
                                       α
                                    
                                    
                                       1
                                    
                                 
                                 =
                                 ⋯
                                 =
                                 
                                    
                                       α
                                    
                                    
                                       K
                                    
                                 
                                 =
                                 α
                              
                           . Similarly, a symmetric Dirichlet prior is placed on ϕ and ψ with a single hyper-parameter β. β may be interpreted as a prior observation count of the number of times words in each language are sampled from a topic before any observations of actual words. Placing these Dirichlet prior distributions on multinomial distributions θ, ϕ and ψ results in smoothed per-topic word and per-document topic distributions, where the values for α and β determine the degree of smoothing. The influence of these hyper-parameters on the quality of learned latent topics is a well-studied problem in monolingual settings (Asuncion, Welling, Smyth, & Teh, 2009; Lu et al., 2011) and it can be generalized to multilingual settings.

A natural extension of BiLDA that operates with more than two languages, called polylingual topic model (PolyLDA) has been presented by Mimno et al. (2009). A similar model has also been proposed by Ni et al. (2009, 2011). Instead of document pairs, they deal with aligned document tuples (where links between documents in a tuple are given), but the assumptions made by their model remain the same. Fig. 3
                            shows the graphical representation in plate notation of the BiLDA model generalized to l languages, 
                              
                                 l
                                 ⩾
                                 2
                              
                           , with document tuples 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                                 =
                                 {
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                    
                                       l
                                    
                                 
                                 }
                              
                            and a discrete set of l language-specific per-topic word distributions 
                              
                                 {
                                 
                                    
                                       ϕ
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       ϕ
                                    
                                    
                                       l
                                    
                                 
                                 }
                              
                            (see Section 2.1).

On the other hand, when operating with only one language, BiLDA or (more generally) PolyLDA is effectively reduced to the standard monolingual LDA model (see Fig. 4
                            and compare it with Fig. 2 or Fig. 3) (Blei et al., 2003b), that is, the monolingual LDA model is only a degenerate special case of BiLDA and PolyLDA (see also Section 2.1).

The goal of training the BiLDA model is to discover the layer of latent cross-lingual topics that describe observed data, that is, a given bilingual document collection in an optimal way. It means that the most likely values for θ, ϕ and ψ have to be found by the training procedure. In simple words, we need to detect and learn which words are important for a particular topic in each language (that is reflected in per-topic word distributions ϕ and ψ), and which topics are important for a particular document pair (as reflected in per-document topic distribution θ). Similarly to the LDA model, the topic discovery for BiLDA is complex and cannot be solved by an exact learning procedure. There exist a few approximative training techniques which aim at converging to the correct distributions. Variational estimation for the monolingual LDA was used as the estimation technique in the seminal paper by Blei et al. (2003b). Other estimation techniques for the monolingual case include Gibbs sampling (Geman & Geman, 1984; Steyvers & Griffiths, 2007), and expectation propagation (Minka & Lafferty, 2002; Griffiths & Steyvers, 2004).

An extension of the variational method to multilingual settings and its complete formulation for BiLDA was proposed and described by the authors (DeSmet & Moens, 2009). Due to its prevalent use in topic modeling literature in both monolingual and multilingual contexts (Boyd-Graber & Blei, 2009; Mimno et al., 2009; Jagarlamudi & Daumé III, 2010; Vulić et al., 2011a), we opt for Gibbs sampling as the estimation technique for the BiLDA models in all applications described in this article. Therefore, we here provide an overview of Gibbs sampling for BiLDA.


                           Gibbs sampling is a Monte Carlo Markov chain (MCMC) estimation technique. MCMC is a random walk over a Markov chain where each state represents a sample from a specific joint distribution. Starting from a random initial state, the next state is repeatedly sampled randomly from the transition probabilities, and this is repeated until the equilibrium state is reached, in which case states are samples from the joint probability distribution. Gibbs sampling considers each word token in a text collection in turn and then samples a topic for that word token, where the probability of generating the current word by each topic is calculated conditioned given all other variables (including all other topics). For BiLDA in specific, the Gibbs sampling procedure follows the steps presented in Algorithm 2.
                              Algorithm 2
                              Gibbs sampling for BiLDA: An overview 
                                 
                                    
                                       
                                    
                                 
                              

After the convergence or the equilibrium state is reached, a standard practice is to provide estimates of the output distributions as averages over several samples taken in the equilibrium state.

BiLDA requires two sets of formulas to converge to correct distributions: (i) one for each topic assignment 
                              
                                 
                                    
                                       z
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            (a topic assigned to a word position i that generated word 
                              
                                 
                                    
                                       w
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            in a document pair 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                              
                           ) and (ii) one for each topic assignment 
                              
                                 
                                    
                                       z
                                    
                                    
                                       ji
                                    
                                    
                                       T
                                    
                                 
                              
                           . θ, ψ and ϕ are not calculated directly, but estimated afterwards. Therefore, they are integrated out of all the calculations, which actually leaves topic assignments for each word position, 
                              
                                 
                                    
                                       z
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                           -s and 
                              
                                 
                                    
                                       z
                                    
                                    
                                       ji
                                    
                                    
                                       T
                                    
                                 
                              
                           -s as the only hidden variables. For the source part S of each document pair 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                              
                            and each word position i, the probability is calculated that 
                              
                                 
                                    
                                       z
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            assumes, as its new values, one of the K possible topic indices (from a set of K topics), as indicated by variable 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                           : 
                              
                                 (1)
                                 
                                    sample
                                    
                                    
                                       
                                          z
                                       
                                       
                                          ji
                                       
                                       
                                          S
                                       
                                    
                                    ∼
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   z
                                                
                                                
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             =
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             ∣
                                             
                                                
                                                   z
                                                
                                                
                                                   ¬
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   z
                                                
                                                
                                                   j
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             α
                                             ,
                                             β
                                          
                                       
                                    
                                    ∼
                                    
                                       ∫
                                       
                                          
                                             
                                                θ
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    
                                       ∫
                                       
                                          ϕ
                                       
                                    
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   z
                                                
                                                
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             =
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             ∣
                                             ,
                                             
                                                
                                                   z
                                                
                                                
                                                   ¬
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   z
                                                
                                                
                                                   j
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             α
                                             ,
                                             β
                                             ,
                                             
                                                
                                                   θ
                                                
                                                
                                                   j
                                                
                                             
                                             ,
                                             ϕ
                                          
                                       
                                    
                                    d
                                    ϕ
                                    
                                    d
                                    
                                       
                                          θ
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                           In this formula, 
                              
                                 
                                    
                                       z
                                    
                                    
                                       j
                                    
                                    
                                       T
                                    
                                 
                              
                            refers to all target topic indices for document pair 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                              
                           , and 
                              
                                 
                                    
                                       z
                                    
                                    
                                       ¬
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            denotes all source topic indices in 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                              
                            excluding 
                              
                                 
                                    
                                       z
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                           . 
                              
                                 
                                    
                                       w
                                    
                                    
                                       S
                                    
                                 
                              
                            denotes all source word tokens in the corpus, 
                              
                                 
                                    
                                       w
                                    
                                    
                                       T
                                    
                                 
                              
                            all target words. Sampling for the target side (indices T) is performed in an analogous manner:
                              
                                 (2)
                                 
                                    sample
                                    
                                    
                                       
                                          z
                                       
                                       
                                          ji
                                       
                                       
                                          T
                                       
                                    
                                    ∼
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   z
                                                
                                                
                                                   ji
                                                
                                                
                                                   T
                                                
                                             
                                             =
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             ∣
                                             
                                                
                                                   z
                                                
                                                
                                                   ¬
                                                   ji
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             
                                                
                                                   z
                                                
                                                
                                                   j
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             α
                                             ,
                                             β
                                          
                                       
                                    
                                    ∼
                                    
                                       ∫
                                       
                                          
                                             
                                                θ
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    
                                       ∫
                                       
                                          ψ
                                       
                                    
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   z
                                                
                                                
                                                   ji
                                                
                                                
                                                   T
                                                
                                             
                                             =
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             ∣
                                             ,
                                             
                                                
                                                   z
                                                
                                                
                                                   ¬
                                                   ji
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             
                                                
                                                   z
                                                
                                                
                                                   j
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             α
                                             ,
                                             β
                                             ,
                                             
                                                
                                                   θ
                                                
                                                
                                                   j
                                                
                                             
                                             ,
                                             ψ
                                          
                                       
                                    
                                    d
                                    ψ
                                    
                                    d
                                    
                                       
                                          θ
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                           We further show the derivation of the Gibbs sampler for BiLDA and explain the notation only for the source side of a bilingual corpus and the source language 
                              
                                 
                                    
                                       L
                                    
                                    
                                       S
                                    
                                 
                              
                            with indices S, since the derivation for the target side (with indices T) follows in a completely analogous manner. Starting from Eq. (1), we can further write:
                              
                                 
                                    sample
                                    
                                    
                                       
                                          z
                                       
                                       
                                          ji
                                       
                                       
                                          S
                                       
                                    
                                    ∝
                                    
                                       ∫
                                       
                                          
                                             
                                                θ
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    
                                       ∫
                                       
                                          ϕ
                                       
                                    
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   z
                                                
                                                
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             =
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             ∣
                                             
                                                
                                                   z
                                                
                                                
                                                   ¬
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   z
                                                
                                                
                                                   j
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             θ
                                             ,
                                             α
                                          
                                       
                                    
                                    ·
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             ∣
                                             
                                                
                                                   z
                                                
                                                
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             =
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             ,
                                             
                                                
                                                   z
                                                
                                                
                                                   ¬
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   ¬
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             ϕ
                                             ,
                                             β
                                          
                                       
                                    
                                    d
                                    ϕ
                                    
                                    d
                                    θ
                                    ∝
                                    
                                       ∫
                                       
                                          
                                             
                                                θ
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   z
                                                
                                                
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             =
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             ∣
                                             
                                                
                                                   θ
                                                
                                                
                                                   j
                                                
                                             
                                          
                                       
                                    
                                    ·
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   θ
                                                
                                                
                                                   j
                                                
                                             
                                             ∣
                                             
                                                
                                                   z
                                                
                                                
                                                   ¬
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   z
                                                
                                                
                                                   j
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             α
                                          
                                       
                                    
                                    d
                                    
                                       
                                          θ
                                       
                                       
                                          j
                                       
                                    
                                    ·
                                    
                                       ∫
                                       
                                          
                                             
                                                ϕ
                                             
                                             
                                                k
                                             
                                          
                                       
                                    
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             ∣
                                             
                                                
                                                   z
                                                
                                                
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             =
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             ,
                                             
                                                
                                                   ϕ
                                                
                                                
                                                   k
                                                
                                             
                                          
                                       
                                    
                                    ·
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   ϕ
                                                
                                                
                                                   k
                                                
                                             
                                             |
                                             
                                                
                                                   z
                                                
                                                
                                                   ¬
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   ¬
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             β
                                          
                                       
                                    
                                    d
                                    
                                       
                                          ϕ
                                       
                                       
                                          k
                                       
                                    
                                 
                              
                           Both θ and ϕ have a prior Dirichlet distribution and their posterior distributions are updated with the counter variable n (which counts the number of assigned topics in a document) and the counter variable v (which counts the number of assigned topics in the corpus) respectively (see the explanations of the symbols after the derivation). The expected values 
                              
                                 
                                    
                                       
                                          ∫
                                          xf
                                          (
                                          x
                                          )
                                          dx
                                       
                                    
                                 
                              
                            for θ and ϕ become:
                              
                                 (3)
                                 
                                    =
                                    
                                       
                                          E
                                       
                                       
                                          Dirichlet
                                          (
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                k
                                                ,
                                                ¬
                                                i
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                k
                                             
                                             
                                                T
                                             
                                          
                                          +
                                          α
                                          )
                                       
                                    
                                    [
                                    
                                       
                                          θ
                                       
                                       
                                          j
                                          ,
                                          k
                                       
                                    
                                    ]
                                    ·
                                    
                                       
                                          E
                                       
                                       
                                          Dirichlet
                                          (
                                          
                                             
                                                v
                                             
                                             
                                                k
                                                ,
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      ji
                                                   
                                                   
                                                      S
                                                   
                                                
                                                ,
                                                ¬
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          β
                                          )
                                       
                                    
                                    [
                                    
                                       
                                          ϕ
                                       
                                       
                                          k
                                       
                                       
                                          
                                             
                                                w
                                             
                                             
                                                ji
                                             
                                          
                                       
                                    
                                    ]
                                 
                              
                           Following Eq. (3), the final updating formulas for both source and target language for the BiLDA Gibbs sampler are as follows:
                              
                                 (4)
                                 
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   z
                                                
                                                
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             =
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             |
                                             
                                                
                                                   z
                                                
                                                
                                                   ¬
                                                   ji
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   z
                                                
                                                
                                                   j
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             α
                                             ,
                                             β
                                          
                                       
                                    
                                    ∝
                                    
                                       
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                k
                                                ,
                                                ¬
                                                i
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                k
                                             
                                             
                                                T
                                             
                                          
                                          +
                                          α
                                       
                                       
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                ·
                                                ,
                                                ¬
                                                i
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                ·
                                             
                                             
                                                T
                                             
                                          
                                          +
                                          K
                                          α
                                       
                                    
                                    ·
                                    
                                       
                                          
                                             
                                                v
                                             
                                             
                                                k
                                                ,
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      ji
                                                   
                                                   
                                                      S
                                                   
                                                
                                                ,
                                                ¬
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          β
                                       
                                       
                                          
                                             
                                                v
                                             
                                             
                                                k
                                                ,
                                                ·
                                                ,
                                                ¬
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          |
                                          
                                             
                                                V
                                             
                                             
                                                S
                                             
                                          
                                          |
                                          β
                                       
                                    
                                 
                              
                           
                           
                              
                                 (5)
                                 
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   z
                                                
                                                
                                                   ji
                                                
                                                
                                                   T
                                                
                                             
                                             =
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             |
                                             
                                                
                                                   z
                                                
                                                
                                                   ¬
                                                   ji
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             
                                                
                                                   z
                                                
                                                
                                                   j
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   T
                                                
                                             
                                             ,
                                             α
                                             ,
                                             β
                                          
                                       
                                    
                                    ∝
                                    
                                       
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                k
                                                ,
                                                ¬
                                                i
                                             
                                             
                                                T
                                             
                                          
                                          +
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                k
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          α
                                       
                                       
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                ·
                                                ,
                                                ¬
                                                i
                                             
                                             
                                                T
                                             
                                          
                                          +
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                ·
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          K
                                          α
                                       
                                    
                                    ·
                                    
                                       
                                          
                                             
                                                v
                                             
                                             
                                                k
                                                ,
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      ji
                                                   
                                                   
                                                      T
                                                   
                                                
                                                ,
                                                ¬
                                             
                                             
                                                T
                                             
                                          
                                          +
                                          β
                                       
                                       
                                          
                                             
                                                v
                                             
                                             
                                                k
                                                ,
                                                ·
                                                ,
                                                ¬
                                             
                                             
                                                T
                                             
                                          
                                          +
                                          |
                                          
                                             
                                                V
                                             
                                             
                                                T
                                             
                                          
                                          |
                                          β
                                       
                                    
                                 
                              
                           
                        

The counter variable 
                              
                                 
                                    
                                       n
                                    
                                    
                                       j
                                       ,
                                       k
                                    
                                    
                                       S
                                    
                                 
                              
                            denotes the number of times source words in the source document 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                    
                                       S
                                    
                                 
                              
                            of a document pair 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                              
                            are assigned to a latent cross-lingual topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                            (with index k), while 
                              
                                 
                                    
                                       n
                                    
                                    
                                       j
                                       ,
                                       k
                                       ,
                                       ¬
                                       i
                                    
                                    
                                       S
                                    
                                 
                              
                            has the same meaning, but not counting the current 
                              
                                 
                                    
                                       w
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            at position i (i.e., it is 
                              
                                 
                                    
                                       n
                                    
                                    
                                       j
                                       ,
                                       k
                                    
                                    
                                       S
                                    
                                 
                                 -
                                 1
                              
                           ). The same is true for the target side and the T indices. When a “·” occurs in the subscript of a counter variable, this means that the counts range over all values of the variable whose index the “·” takes. So, while 
                              
                                 
                                    
                                       n
                                    
                                    
                                       j
                                       ,
                                       k
                                    
                                    
                                       S
                                    
                                 
                              
                            counts the number of assignments of words 
                              
                                 
                                    
                                       w
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            to one latent topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                            in 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                    
                                       S
                                    
                                 
                                 ,
                                 
                                    
                                       n
                                    
                                    
                                       j
                                       ,
                                       ·
                                    
                                    
                                       S
                                    
                                 
                              
                            does so over all K topics in 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                    
                                       S
                                    
                                 
                              
                           .

The second counter variable, 
                              
                                 
                                    
                                       v
                                    
                                    
                                       k
                                       ,
                                       
                                          
                                             w
                                          
                                          
                                             ji
                                          
                                       
                                       ,
                                       ¬
                                    
                                    
                                       S
                                    
                                 
                              
                            is the number of times a word type whose token appears at position i (
                              
                                 
                                    
                                       w
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                           ) gets assigned a latent cross-lingual topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                            in the source side of the entire document collection, but not counting the current 
                              
                                 
                                    
                                       w
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            (i.e., it is 
                              
                                 
                                    
                                       v
                                    
                                    
                                       k
                                       ,
                                       
                                          
                                             w
                                          
                                          
                                             ji
                                          
                                       
                                    
                                    
                                       S
                                    
                                 
                                 -
                                 1
                              
                           ). Additionally, 
                              
                                 
                                    
                                       z
                                    
                                    
                                       j
                                    
                                    
                                       S
                                    
                                 
                              
                            denotes all latent topic assignments for the source side of the document pair 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                                 ,
                                 
                                    
                                       z
                                    
                                    
                                       ¬
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            denotes all topic assignments for the source side of 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                              
                            but excluding 
                              
                                 
                                    
                                       w
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                           . 
                              
                                 
                                    
                                       v
                                    
                                    
                                       k
                                       ,
                                       ·
                                       ,
                                       ¬
                                    
                                    
                                       S
                                    
                                 
                              
                            counts the total number of occurrences of source language words from 
                              
                                 
                                    
                                       V
                                    
                                    
                                       S
                                    
                                 
                              
                            associated with the topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                            in the whole corpus, as it is the sum over all possible source language words (a “·” appears instead of the 
                              
                                 
                                    
                                       w
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                           ). Again, because of the 
                              
                                 ¬
                              
                            symbol in the superscript, the current 
                              
                                 
                                    
                                       w
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            is not counted (i.e., the count is then 
                              
                                 
                                    
                                       v
                                    
                                    
                                       k
                                       ,
                                       ·
                                    
                                    
                                       S
                                    
                                 
                                 -
                                 1
                              
                           ). Finally, 
                              
                                 ∣
                                 
                                    
                                       V
                                    
                                    
                                       S
                                    
                                 
                                 ∣
                              
                            and 
                              
                                 ∣
                                 
                                    
                                       V
                                    
                                    
                                       T
                                    
                                 
                                 ∣
                              
                            are vocabulary sizes for the source and the target language, respectively.

As can be seen from the first term of Eqs. (4) and (5), the document pairs are linked by the counter variables 
                              
                                 
                                    
                                       n
                                    
                                    
                                       j
                                    
                                    
                                       S
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       n
                                    
                                    
                                       j
                                    
                                    
                                       T
                                    
                                 
                              
                           , as both sets of assignments: 
                              
                                 
                                    
                                       z
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       z
                                    
                                    
                                       ji
                                    
                                    
                                       T
                                    
                                 
                              
                            are drawn from the same 
                              
                                 
                                    
                                       θ
                                    
                                    
                                       j
                                    
                                 
                              
                            (see the first term which is exactly the same in the updating formulas described by Eqs. (4) and (5)). The vocabulary counter variables operate only within the language of the word token currently being considered.

With formulas (4) and (5). each 
                              
                                 
                                    
                                       z
                                    
                                    
                                       ji
                                    
                                    
                                       S
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       z
                                    
                                    
                                       ji
                                    
                                    
                                       T
                                    
                                 
                              
                            of each document pair is sampled and cyclically updated. After a random initialization, usually using a uniform distribution, the sampled values will converge to samples taken from the real joint distribution of 
                              
                                 θ
                                 ,
                                 ϕ
                              
                            and ψ, after a time called the burn-in period. From a set of complete burned-in Gibbs samples of the whole document collection, the final output probability distributions, that is, per-topic word distributions and per-document topic distributions are estimated as averages over these samples.

Language-independent per-document topic distributions provide distributions of latent cross-lingual topics for each document in a collection. They reveal how important each topic is for a particular document. We need to establish the exact formula for per-document topic distributions for documents in an aligned document pair using Eqs. (4) and (5):
                              
                                 (6)
                                 
                                    P
                                    (
                                    
                                       
                                          z
                                       
                                       
                                          k
                                       
                                    
                                    |
                                    
                                       
                                          d
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          θ
                                       
                                       
                                          j
                                          ,
                                          k
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                k
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                k
                                             
                                             
                                                T
                                             
                                          
                                          +
                                          α
                                       
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                
                                                   
                                                      k
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                                =
                                                1
                                             
                                             
                                                K
                                             
                                          
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                
                                                   
                                                      k
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          
                                             
                                                ∑
                                             
                                             
                                                
                                                   
                                                      k
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                                =
                                                1
                                             
                                             
                                                K
                                             
                                          
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                
                                                   
                                                      k
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                             
                                             
                                                T
                                             
                                          
                                          +
                                          K
                                          α
                                       
                                    
                                 
                              
                           The representation 
                              
                                 Rep
                                 
                                    
                                       (
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                    
                                       Z
                                    
                                 
                              
                            of the document 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                              
                            by means of latent cross-lingual topics 
                              
                                 Z
                              
                            is then a K-dimensional vector where the k-th dimension of the vector is exactly the probability of the latent cross-lingual topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                            in the document 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                              
                           : 
                              
                                 (7)
                                 
                                    Rep
                                    
                                       
                                          (
                                          
                                             
                                                d
                                             
                                             
                                                j
                                             
                                          
                                          )
                                       
                                       
                                          Z
                                       
                                    
                                    =
                                    [
                                    P
                                    (
                                    
                                       
                                          z
                                       
                                       
                                          1
                                       
                                    
                                    |
                                    
                                       
                                          d
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    ,
                                    P
                                    (
                                    
                                       
                                          z
                                       
                                       
                                          2
                                       
                                    
                                    |
                                    
                                       
                                          d
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    ,
                                    …
                                    ,
                                    P
                                    (
                                    
                                       
                                          z
                                       
                                       
                                          K
                                       
                                    
                                    |
                                    
                                       
                                          d
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    ]
                                 
                              
                           We may detect from Eq. (6) that two documents from an aligned document pair are enforced to have exactly the same topical representations, that is, the two documents discussing the same themes will be presented as exactly the same mixtures over the induced latent cross-lingual topics. This property is achieved by making the computation of 
                              
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 ∣
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                                 )
                              
                            for both documents in the pair explicitly dependent on the topic assignments counts from the source language document 
                              
                                 (
                                 
                                    
                                       n
                                    
                                    
                                       j
                                       ,
                                       k
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                            as well as the target language document 
                              
                                 (
                                 
                                    
                                       n
                                    
                                    
                                       j
                                       ,
                                       k
                                    
                                    
                                       T
                                    
                                 
                                 )
                              
                           . Previous standard approaches to multilingual probabilistic topic modeling (see later Section 2.5) typically trained monolingual LDA on concatenated documents from an aligned document pair, where this property was not taken into account. In other words, unlike BiLDA, monolingual LDA builds a single topical representation for the artificially created concatenated document, and does not enforce this property at all. Comparisons of monolingual LDA trained on concatenated documents forming aligned document pairs (further MixLDA) and bilingual LDA reveal the superiority of the true multilingual approach modeled by BiLDA.

Language-specific per-topic word distributions measure the importance of each word in each language for a particular latent cross-lingual topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                           . Given the source language with vocabulary 
                              
                                 
                                    
                                       V
                                    
                                    
                                       S
                                    
                                 
                              
                           , and the target language with vocabulary 
                              
                                 
                                    
                                       V
                                    
                                    
                                       T
                                    
                                 
                              
                           , and following Eq. (4), a probability that some word 
                              
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                    
                                       S
                                    
                                 
                                 ∈
                                 
                                    
                                       V
                                    
                                    
                                       S
                                    
                                 
                              
                            will be generated by the cross-lingual topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                            is given by:
                              
                                 (8)
                                 
                                    P
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                       
                                          S
                                       
                                    
                                    |
                                    
                                       
                                          z
                                       
                                       
                                          k
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          ϕ
                                       
                                       
                                          k
                                          ,
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                v
                                             
                                             
                                                k
                                                ,
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      i
                                                   
                                                   
                                                      S
                                                   
                                                
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          β
                                       
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                
                                                   
                                                      i
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                                =
                                                1
                                             
                                             
                                                |
                                                
                                                   
                                                      V
                                                   
                                                   
                                                      S
                                                   
                                                
                                                |
                                             
                                          
                                          
                                             
                                                v
                                             
                                             
                                                k
                                                ,
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      
                                                         
                                                            i
                                                         
                                                         
                                                            ∗
                                                         
                                                      
                                                   
                                                   
                                                      S
                                                   
                                                
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          |
                                          
                                             
                                                V
                                             
                                             
                                                S
                                             
                                          
                                          |
                                          β
                                       
                                    
                                 
                              
                           The same formula, but now derived from Eq. (5) is used for the per-topic word distributions (ψ) for the target language:
                              
                                 (9)
                                 
                                    P
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                       
                                          T
                                       
                                    
                                    |
                                    
                                       
                                          z
                                       
                                       
                                          k
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          ψ
                                       
                                       
                                          k
                                          ,
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                v
                                             
                                             
                                                k
                                                ,
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      i
                                                   
                                                   
                                                      T
                                                   
                                                
                                             
                                             
                                                T
                                             
                                          
                                          +
                                          β
                                       
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                
                                                   
                                                      i
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                                =
                                                1
                                             
                                             
                                                |
                                                
                                                   
                                                      V
                                                   
                                                   
                                                      T
                                                   
                                                
                                                |
                                             
                                          
                                          
                                             
                                                v
                                             
                                             
                                                k
                                                ,
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      
                                                         
                                                            i
                                                         
                                                         
                                                            ∗
                                                         
                                                      
                                                   
                                                   
                                                      T
                                                   
                                                
                                             
                                             
                                                T
                                             
                                          
                                          +
                                          |
                                          
                                             
                                                V
                                             
                                             
                                                T
                                             
                                          
                                          |
                                          β
                                       
                                    
                                 
                              
                           In summary, these per-document topic distributions and per-topic word distributions are in fact mathematical realizations of the high-level intuitions and modeling premises clearly demonstrated in Fig. 1. For a better understanding of these core concepts, we refer the reader to study that figure again.

Since the model possesses a fully generative semantics, it is possible to train the model on one multilingual corpus (e.g., multilingual Wikipedia) and then infer it on some other, previously unseen corpus. Inferring a model on a new corpus means calculating per-document topic distributions for all the unseen documents in the unseen corpus based on the output of the trained model (i.e., we effectively learn the MuPTM-based representation of an unseen document, see Definition 4 and Eq. (7)). Inference on the unseen documents is performed only one language at a time, e.g., if we train on English-Dutch Wikipedia, we can use the trained BiLDA model to learn document representations, that is, per-document topic distributions for Dutch news stories, and then separately for English news.

In short, we again randomly sample and then iteratively update topic assignments for each word position in an unseen document, but now start from the fixed v counters learned in training, and then cyclically update the probability distributions from which the topic assignments are sampled. Since the inference is performed monolingually, dependencies on the topic assignments from another language are removed from the updating formulas. Hence, similar to Eq. (4), the updating formula for the source language 
                              
                                 
                                    
                                       L
                                    
                                    
                                       S
                                    
                                 
                              
                            is:
                              
                                 (10)
                                 
                                    P
                                    (
                                    
                                       
                                          z
                                       
                                       
                                          ji
                                       
                                       
                                          S
                                       
                                    
                                    =
                                    
                                       
                                          z
                                       
                                       
                                          k
                                       
                                    
                                    |
                                    
                                       
                                          z
                                       
                                       
                                          ¬
                                          ji
                                       
                                       
                                          S
                                       
                                    
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          S
                                       
                                    
                                    ,
                                    α
                                    ,
                                    β
                                    )
                                    ∝
                                    
                                       
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                k
                                                ,
                                                ¬
                                                i
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          α
                                       
                                       
                                          
                                             
                                                n
                                             
                                             
                                                j
                                                ,
                                                ·
                                                ,
                                                ¬
                                                i
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          K
                                          α
                                       
                                    
                                    ·
                                    
                                       
                                          
                                             
                                                v
                                             
                                             
                                                k
                                                ,
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      ji
                                                   
                                                   
                                                      S
                                                   
                                                
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          β
                                       
                                       
                                          
                                             
                                                v
                                             
                                             
                                                k
                                                ,
                                                ·
                                             
                                             
                                                S
                                             
                                          
                                          +
                                          |
                                          
                                             
                                                V
                                             
                                             
                                                S
                                             
                                          
                                          |
                                          β
                                       
                                    
                                 
                              
                           Learning a multilingual topic model on one multilingual corpus and then inferring that model on previously unseen data constitutes the key concept of cross-lingual knowledge transfer by means of multilingual probabilistic topic models and that property is extensively utilized in various cross-lingual applications.

A simple way of looking at the output quality of a topic model is by simply inspecting top words associated with a particular topic learned during training. We say that a latent topic is semantically coherent if it assigns high probability scores to words that are semantically related (Gliozzo, Pennacchiotti, & Pantel, 2007; Newman, Lau, Grieser, & Baldwin, 2010; Mimno, Wallach, Talley, Leenders, & McCallum, 2011; Stevens, Kegelmeyer, Andrzejewski, & Buttler, 2012; Aletras & Stevenson, 2013; Deveaud, SanJuan, & Bellot, 2013). It is much easier for humans to judge semantic coherence of cross-lingual topics and their alignment across languages when observing the actual words constituting a topic. These words provide a shallow qualitative representation of the latent topic space, and could be seen as direct and comprehensive word-based summaries of a large document collection. In other words, humans can get the first clue “what all this text is about in the first place”.

The desirable property of semantic coherence comprises both a strong intra-semantic coherence, that is, words from the same vocabulary grouped together in the same topic are closely semantically related, as well as a strong inter-semantic coherence, that is, words across languages used to represent the same cross-lingual topic are also closely semantically related. Samples of cross-lingual topics extracted by BiLDA trained on aligned Wikipedia articles are provided in Table 1
                        . We may consider this visual inspection of the top words associated with each topic as an initial qualitative evaluation, suitable for human judges.

Besides this shallow qualitative analysis relying on the top words, there are other, theoretically well-founded evaluation metrics for quantitative analysis and comparison of different models. In the literature, latent topic models are often evaluated by their perplexity, where the perplexity or “confusion” of a model is a measure of its ability to explain a collection 
                           
                              
                                 
                                    C
                                 
                                 
                                    u
                                 
                              
                           
                         of unseen documents. The perplexity of a probabilistic topic model is expressed as follows:
                           
                              (11)
                              
                                 perp
                                 (
                                 
                                    
                                       C
                                    
                                    
                                       u
                                    
                                 
                                 )
                                 =
                                 exp
                                 
                                    
                                       
                                          -
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      d
                                                      ∈
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            u
                                                         
                                                      
                                                   
                                                
                                                log
                                                
                                                   
                                                      
                                                         
                                                            
                                                               ∏
                                                            
                                                            
                                                               w
                                                               ∈
                                                               d
                                                            
                                                         
                                                         P
                                                         (
                                                         w
                                                         )
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      d
                                                      ∈
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            u
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      d
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    N
                                 
                                 
                                    d
                                 
                              
                           
                         is defined as the number of words in a document 
                           
                              d
                              ,
                              P
                              (
                              w
                              )
                           
                         is word w’s marginal probability according to a specific model, calculated as 
                           
                              
                                 
                                    ∑
                                 
                                 
                                    k
                                 
                              
                              P
                              (
                              w
                              ∣
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                              ,
                              ϒ
                              )
                           
                        , where k ranges over all K topics in the model, and ϒ is the set of the corpus independent parameters of the model. For BiLDA, the parameter set is 
                           
                              ϒ
                              =
                              {
                              α
                              ,
                              β
                              ,
                              ϕ
                              ,
                              ψ
                              ,
                              K
                              }
                           
                        . A lower perplexity score means less confusion of the model in explaining the unseen data, and, theoretically, a better model. A good model with a low perplexity score should be well adapted to new documents and yield a good representation of those previously unseen documents. Since the perplexity measure defines the quality of a topic model independently of any application, it is considered an intrinsic or in vitro evaluation metric.

Another intrinsic evaluation metric for multilingual probabilistic topic models, named cross-collection likelihood, was proposed recently in (Zhang et al., 2010), but that measure also presupposes an existing bilingual dictionary as a critical resource. Additionally, a number of intrinsic quantitative evaluation methods (but for the monolingual settings) are proposed in (Wallach, Murray, Salakhutdinov, & Mimno, 2009). Other studies for the monolingual setting focused more on automatic evaluation of semantic coherence (e.g., Chang, Boyd-Graber, Gerrish, Wang, & Blei, 2009; Newman et al., 2010; Mimno et al., 2011). However, perplexity still remains the dominant quantitative in vitro evaluation method that is predominantly found in the literature.

Finally, the best way to evaluate multilingual probabilistic topic models is to test how well they perform in practice for different real-life tasks (e.g., document classification, information retrieval), that is, to carry out an extrinsic ex vivo evaluation. We later investigate whether there exists a mismatch between the intrinsic and extrinsic evaluation in information retrieval (see Section 6).

Similarly to LDA in the monolingual setting (for which we have already shown that it is only a special case of BiLDA operating with only one language), we believe that bilingual LDA can be considered the basic building block of this general framework of multilingual probabilistic topic modeling. It serves as a firm baseline for future advances in multilingual probabilistic topic modeling. Although MuPTM is a quite novel concept, several other models have emerged over the last years. All current state-of-the-art multilingual probabilistic topic models build upon the idea of standard monolingual pLSA and LDA and closely resemble the described BiLDA model, but they differ in the assumptions they make in their generative processes, and in knowledge that is presupposed before training (e.g., document alignments, prior word matchings or bilingual dictionaries). However, they all share the same concepts defined in Section 
                        
                           2.1
                        
                        , that is, the sets of output distributions and the set of latent cross-lingual topics that has to be discovered in a multilingual text collection.

The early approaches (see, e.g., Dumais et al., 1996; Carbonell et al., 1997) tried to mine topical structure from multilingual texts using an algebraic model, that is, Latent Semantic Analysis (LSA) and then use the discovered latent topical structure in cross-lingual information retrieval. Artificial “cross-lingual” documents were formed by concatenating aligned parallel documents in two different languages, and then LSA on a word-by-document matrix of these newly built documents was used to learn the lower dimensional document representation. Documents across languages are then compared in that lower-dimensional space.

Another line of work Zhao and Xing (2006, 2007) focused on building topic models suitable for word alignment and statistical machine translation operations. Again inspired by monolingual LDA, they have designed several variants of topic models that operate on parallel corpora aligned at sentence level. The topical structure at the level of aligned sentences or word pairs is used to re-estimate word translation probabilities and force alignments of words and phrases generated by the same topic.

However, the growth of the global network and increasing amounts of comparable theme-aligned texts have formed a need for constructing more generic models that are applicable to such large-volume, but less-structured text collections. Standard monolingual probabilistic topic models coming from the families of pLSA and LDA cannot capture and accurately represent the structure of such theme-aligned multilingual text data in a form of joint latent cross-lingual topics. That inability comes from the fact that topic models rely on word co-occurrence information to group similar words into a single topic. In case of multilingual corpora (e.g., Wikipedia articles in English and Dutch) two related words in different languages will seldom co-occur in a monolingual text, and therefore these models are unable to group such pairs of words into a single coherent topic (for examples see, e.g., Boyd-Graber & Blei, 2009; Jagarlamudi & Daumé III, 2010). In order to anticipate that issue, there have been some efforts that trained monolingual probabilistic topic models on concatenated document pairs in two languages (e.g., Dumais et al., 1996; Littman, Dumais, & Landauer, 1998; Carbonell et al., 1997; Chew, Bader, Kolda, & Abdelali, 2007; Xue, Dai, Yang, & Yu, 2008; Cimiano, Schultz, Sizov, Sorg, & Staab, 2009; Roth & Klakow, 2010), but such approaches also fail to build a shared latent cross-lingual topical space where the boundary between the topic representations with words in two languages is firmly established. In other words, when training on concatenated English and Spanish Wikipedia articles, the learned topics contain both English and Spanish words. However, we would like to learn latent cross-lingual topics for which their representation in English is completely language-specific and differs from their representation in Spanish.

Recently, several novel models have been proposed that remove such deficiency. These models are trained on the individual documents in different languages and their output are joint latent cross-lingual topics in an aligned latent cross-lingual topical space. The utility of such new topic representations is clearly displayed further in this article (see Sections 3–6). These models require alignments at document level a priori before training, which is easily obtained for Wikipedia or news articles. These document alignments provide hard links between topic-aligned semantically similar documents across languages.

Recently, there has been a growing interest in multilingual topic modeling from unaligned text, again inspired by monolingual LDA. The MuTo model (Boyd-Graber & Blei, 2009) operates with matchings instead of words, where matchings consist of pairs of words that link words from the source vocabulary to words from the target vocabulary. These matchings are induced by the matching canonical correlation analysis (MCCA) (Haghighi et al., 2008; Daumé III & Jagarlamudi, 2011) which ties together words with similar meanings across languages, where similarity is based on different features. Matchings are induced based on pointwise mutual information (PMI) from parallel texts, machine-readable dictionaries and orthographic features captured by, for instance, edit distance. A stochastic expectation–maximization (EM) algorithm is used for training, and their evaluation has been performed on a parallel corpus. A similar idea of using matchings has been investigated in (Jagarlamudi & Daumé III, 2010). In their JointLDA model, they also observe each cross-lingual topic as a mixture over these matchings (or word concepts, as they name them), where the matchings are acquired directly from a machine-readable bilingual dictionary. JointLDA uses Gibbs sampling for training and it is trained on Wikipedia data. Although these two models claim that they have removed the need for document alignment and are fit to mine latent cross-lingual topics from unaligned multilingual text data, they have introduced bilingual dictionaries as a new critical resource. These machine-readable dictionaries have to be compiled from parallel data or hand-crafted, which is typically more expensive and time-consuming than obtaining alignments for Wikipedia or news data.

Another work that aims to extract latent cross-lingual topics from unaligned datasets is presented by Zhang et al. (2010). Their Probabilistic Cross-lingual Latent Semantic Analysis (PCLSA) extends the standard pLSA model (Hofmann, 1999b) by regularizing its likelihood function with soft constraints defined by an external machine-readable bilingual dictionary. They use the generalized expectation maximization (GEM) algorithm (Mei, Cai, Zhang, & Zhai, 2008) for training. Similar to MuTo and JointLDA, a bilingual dictionary is presupposed before training and it is a critical resource for PCLSA. The dictionary-based constraints are the key to bridge the gap between languages by pushing related words in different vocabularies to occur in the same cross-lingual topics. The same relationship between pLSA and LDA (Girolami & Kabán, 2003) in the monolingual setting is also reflected between their multilingual extensions, PCLSA and BiLDA.

In this article, we will present a subset of cross-lingual applications in which any multilingual probabilistic topic model may be utilized. In specific, we show the results obtained by BiLDA and provide an overview of its task performance. The goal of this article is however not to provide a direct comparison of different multilingual probabilistic topic models in various cross-lingual tasks, but to provide a comprehensive and didactic description of a general model-independent framework for building systems that rely on such multilingual probabilistic topic models and MuPTM-based representations of words and documents, and do not exploit any external expensive knowledge resource (e.g., parallel corpora, machine-readable dictionaries, extensive human annotations). Such data-driven unsupervised systems which exploit only internal evidence are essential for languages and language pairs with limited resources. We acknowledge that there exist numerous different techniques proposed for solving the presented tasks. However, our main focus is not to detect the best technique for each cross-lingual task, but to give a “cookbook” on how to exploit the latent cross-lingual topical knowledge as one source of evidence when dealing with these tasks in an unsupervised, language-independent and language pair independent manner.

In addition, the reader has to be aware that significant portions of Application I (Section 3), Application II (Section 4), and Application IV (Section 6) contain already published work (DeSmet & Moens, 2009; De Smet et al., 2011; Vulić, DeSmet, & Moens, 2013), but we have decided to retain the essence and have rewritten the previously published work in a systematic and didactic manner in order to better stress the general applicability of text representations by means of latent cross-lingual topics in a variety of cross-lingual tasks, and to provide some new insights from observing all the applications together. Moreover, a significant portion of Application III (Section 5) is novel and previously unpublished.

The first task we have chosen to present is cross-lingual event-centered news clustering. In general, event-centered news clustering may be considered an information retrieval task in which it is necessary to group news stories into coherent clusters, where each item (i.e., each news story) in one cluster should report on the same event. A special case is cross-lingual event-centered news clustering where one has to perform the clustering of news stories now written in different languages into groups of stories that describe the same event. Implicitly, that also defines a method for linking news stories across languages. Due to the dynamic and ever-changing nature of news, one needs an unsupervised tool that can coherently capture such dynamics and provide a structured representation of news stories irrespective to their actual language. Such event-centered cross-lingual clustering of related news stories is highly desirable in systems for browsing, categorizing and summarizing large news archives given in multiple different languages (Chen et al., 2000; Pouliquen, Steinberger, Ignat, Käsper, & Temnikova, 2004; Evans, Klavans, & McKeown, 2004; Kabadjov, Atkinson, Steinberger, Steinberger, & der Goot, 2010). Cross-lingual event-centered news clustering may be observed as a special case of the cross-lingual document clustering task (e.g., Montalvo, Martínez-Unanue, Casillas, & Fresno, 2006; Wu & Lu, 2007; Tang, Xia, Zhang, Li, & Zheng, 2011), with an extra constraint which specifies that documents – news stories should be clustered together if and only if they cover the same event.

An event is defined as a well-specified happening at a certain moment in time (e.g., a single day or a short period) and space which deals with a certain set of news themes (e.g., a flood and a shortage of drinking water) and involves some named entities. Those named entities are actors of the events (e.g., persons or companies) or locations where the events occurred. Each news story typically reports on a single event, and since different sources can produce several stories on the same event in different languages, cross-lingual event-related clustering of these stories is required.

An event can be observed as a mixture of different themes, where some themes are dominant, while others are only marginally present. That phenomenon can be captured by probabilistic topic models – per-document topic distributions will be higher for topics closely related to the themes prominent in a news story. Two news stories 
                           
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                           
                         are considered similar and are most likely discussing the same event if their per-document topic distributions are similar, that is, if the values 
                           
                              P
                              (
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                              ∣
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         and 
                           
                              P
                              (
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                              ∣
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         are similar for all 
                           
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                              ∈
                              Z
                           
                        , where 
                           
                              Z
                           
                         is the set of K latent cross-lingual topics (see Section 2.1). Note that by utilizing the language-independent set 
                           
                              Z
                           
                         and per-document topic distributions as the news story representation, we are able to perform the cross-lingual event-centered news clustering, i.e., the clustering of stories written in different languages, regardless of the actual language in the story. Previous systems for cross-lingual news clustering either relied on a readily available machine translation system (Montalvo, Martínez-Unanue, Casillas, & Fresno, 2007; VanGael & Zhu, 2007) for feature or document translation, or on the knowledge of shared or cognate named entities (Montalvo et al., 2007). As already proven by DeSmet and Moens (2009), here we stress that the cross-lingual topical knowledge and the representations by means of per-document topic distributions also prove beneficial for this task.

@&#METHODOLOGY@&#

Additionally, news stories reporting on same events usually involve many shared named entities. We can therefore represent each news story with two disjunct sets of information or aspects: (1) words that describe generally applicable subjects captured by our cross-lingual topics and (2) shared named entities.
                           6
                           A named entity is considered shared if it is present in the vocabularies of both languages, e.g., Angela Merkel will occur in the same spelling in English, Dutch, German of even Finnish news stories.
                        
                        
                           6
                         For instance, in case of any earthquake story it is highly likely to detect general terms such as earthquake, damage, and casualties and the only information that unambiguously separates one event from another lies in the named entities. When a topic model is trained on the documents’ full texts, the named entities are part of the output MuPTM distributions, and this has the undesirable property that entities that were not apparent in the training set are unable to influence the topic inference of a new event. Therefore, we have decided to explicitly split each news story into two different aspect representations as: (1) a mixture of latent cross-lingual topics, that is, a MuPTM-based representation by means of per-document topic distributions and (2) a vector of shared named entities occurring in the news story. To achieve this: (1) a topic model is trained on news stories with named entities removed and (2) a score for a named entity in a news story is the probability of the entity obtained by smoothed maximum likelihood estimation. Here, since we develop a completely data-driven approach to cross-lingual news clustering, we do not use any translation resource to link named entities across languages, and follow the prior work from Montalvo et al. (2007), who also represented news stories across languages as vectors over shared and cognate named entities.

News stories are now represented by two probability distributions: (1) a probability distribution over cross-lingual topics (a per-document topic distribution) and (2) a probability distribution over shared named entities. In order to cluster the news stories based on the event they discuss, we need to choose a dissimilarity function. We use the symmetric Kullback–Leibler (KL) divergence of the O-dimensional probability distributions (where O equals K when dealing with topical representations, and some other constant when dealing with representations by means of shared named entities) 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                           
                        , defined as:
                           
                              (12)
                              
                                 KL
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   o
                                                   =
                                                   1
                                                
                                                
                                                   O
                                                
                                             
                                          
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                             
                                                o
                                             
                                          
                                          log
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               i
                                                            
                                                            
                                                               o
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               j
                                                            
                                                            
                                                               o
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          +
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   o
                                                   =
                                                   1
                                                
                                                
                                                   O
                                                
                                             
                                          
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                             
                                                o
                                             
                                          
                                          log
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               j
                                                            
                                                            
                                                               o
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               i
                                                            
                                                            
                                                               o
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        In order to obtain a final dissimilarity function between two news stories, the dissimilarities for each of the two aspects (i.e., the topical aspect and the shared named entities aspect) obtained by Eq. (12) are combined by the maximum function, which proved to yield the best result in monolingual event-centered news clustering (DeSmet & Moens, 2013). The maximum function ensures that two stories are dissimilar when at least one of the aspects has dissimilar distributions, that is, if different events and locations are detected, we assume that we deal with different events. Vice versa, events that cover different themes (represented by different cross-lingual topics) that happen at the same location or performed by the same actors are also treated as different events. The final dissimilarity function is as follows: 
                           
                              dis
                              (
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                              )
                              =
                              
                                 
                                    max
                                 
                                 
                                    a
                                 
                              
                              dis
                              
                                 
                                    
                                       
                                          
                                             A
                                          
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          
                                             a
                                          
                                       
                                       ,
                                       
                                          
                                             A
                                          
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   j
                                                
                                             
                                          
                                          
                                             a
                                          
                                       
                                    
                                 
                              
                              ,
                              a
                              =
                              1
                              →
                              ∣
                              A
                              ∣
                           
                        , where 
                           
                              ∣
                              A
                              ∣
                           
                         denotes the number of aspects a news story is split into (i.e., 
                           
                              ∣
                              A
                              ∣
                              =
                              2
                           
                         in this case), and 
                           
                              
                                 
                                    A
                                 
                                 
                                    
                                       
                                          s
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    a
                                 
                              
                           
                         is the a-th aspect representation of story 
                           
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                           
                        . We have also tried splitting news stories into more fine-grained named entity representations based on their semantic class (
                           
                              A
                              >
                              2
                           
                        , person–location–organization), but it has not improved the clustering performance.

The final story (dis) similarity 
                           
                              dis
                              (
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         is used in a clustering algorithm. We opt for a hierarchical agglomerative clustering with complete linkage (Voorhees, 1986). This algorithm does not require the number of clusters to be chosen in advance. Its adapting ability is a very important property in the dynamic news environment. The algorithm iteratively merges clusters until a certain criterion is reached. To create a natural, unsupervised stopping criterion, a fitness-condition on the clustering is used. The consequence is that the data dictates the optimal number of clusters. For each story 
                           
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                           
                         in the corpus, its fitness in cluster 
                           
                              
                                 
                                    CL
                                 
                                 
                                    i
                                 
                              
                           
                         is calculated as the normalized difference between the distance of 
                           
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                           
                         to the second best cluster 
                           
                              
                                 
                                    CL
                                 
                                 
                                    j
                                 
                              
                           
                        , and the average distance of 
                           
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                           
                         to the other stories in 
                           
                              
                                 
                                    CL
                                 
                                 
                                    i
                                 
                              
                              :
                              f
                              (
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                              )
                              =
                              
                                 
                                    h
                                    (
                                    
                                       
                                          s
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    -
                                    g
                                    (
                                    
                                       
                                          s
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                                 
                                    max
                                    {
                                    g
                                    (
                                    
                                       
                                          s
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    ,
                                    h
                                    (
                                    
                                       
                                          s
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    }
                                 
                              
                           
                        , where 
                           
                              g
                              (
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                              )
                              =
                              
                                 
                                    1
                                 
                                 
                                    ∣
                                    
                                       
                                          CL
                                       
                                       
                                          i
                                       
                                    
                                    ∣
                                    -
                                    1
                                 
                              
                              
                                 
                                    ∑
                                 
                                 
                                    
                                       
                                          s
                                       
                                       
                                          j
                                       
                                    
                                    ∈
                                    
                                       
                                          CL
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                              dis
                              (
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         and 
                           
                              h
                              (
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                              )
                              =
                              arg
                              
                                 
                                    min
                                 
                                 
                                    
                                       
                                          CL
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                              
                                 
                                    1
                                 
                                 
                                    ∣
                                    
                                       
                                          CL
                                       
                                       
                                          j
                                       
                                    
                                    ∣
                                 
                              
                              
                                 
                                    ∑
                                 
                                 
                                    
                                       
                                          s
                                       
                                       
                                          j
                                       
                                    
                                    ∈
                                    
                                       
                                          CL
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                              dis
                              (
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                              )
                           
                        . If 
                           
                              
                                 
                                    CL
                                 
                                 
                                    i
                                 
                              
                           
                         is a singleton cluster (containing only 
                           
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                           
                        ), we set 
                           
                              f
                              (
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                              
                              )
                              =
                              0
                           
                        . We search for the clustering that maximizes the average of f over all stories, over all possible stops in the hierarchy.

In all our applications, our training corpora are Wikipedia datasets for four language pairs, aligned through the cross-lingual links provided by the Wikipedia metadata.
                              7
                              All Wikipedia articles for all experiments are downloaded from Wikipedia dumps: 
                                 http://dumps.wikimedia.org/.
                           
                           
                              7
                            There are 7612 aligned English–Dutch Wikipedia articles, 18,898 English–Italian, 18,911 English–French and 13,696 English–Spanish articles.
                              8
                              These datasets are available online: http://people.cs.kuleuven.be/∼ivan.vulic/software/.
                           
                           
                              8
                            Since it is not true that every Wikipedia article appears in each language, the statistics of the datasets in terms of the number of tokens and vocabulary words vary over different language pairs. We don’t use all training datasets in all applications. Additionally, since the aims and evaluation procedures for each application differ, we use different benchmarking datasets for testing in each application. Those datasets will be explicitly mentioned.

For the event-centered news clustering, we train on the English–Dutch Wikipedia articles. The Dutch articles are usually shorter and of a lesser quality (84 words tokens on average) than the English articles (986 word tokens on average). We have trained BiLDA with 
                              
                                 K
                                 =
                                 100
                              
                            topics, and hyper-parameters were set to 
                              
                                 α
                                 =
                                 50
                                 /
                                 K
                              
                            and 
                              
                                 β
                                 =
                                 0.01
                              
                            (Griffiths & Steyvers, 2004; Steyvers & Griffiths, 2007). Unless noted otherwise, this is our default setting for the hyper-parameters in all further experiments. We have also trained standard LDA with 
                              
                                 K
                                 =
                                 100
                              
                            in the monolingual context for both languages to measure the difference between the monolingual and multilingual environment. In another experiment, we have trained BiLDA with 
                              
                                 K
                                 =
                                 20
                              
                           , 50, 100, 200, 300 topics and compare with LDA trained on concatenated documents (MixLDA) trained on the same data with the same number of topics K.

Following VanGael and Zhu (2007), we have created our test set by compiling 18 events from Google news, forming 18 clusters of English and Dutch documents. In these 18 clusters, there are in total 50 English and 60 Dutch documents. A multilingual probabilistic topic model trained on Wikipedia is inferred on this test collection. Therefore, we perform knowledge transfer via the latent space of cross-lingual topics.

Evaluation is done using the B-Cubed metric (Bagga & Baldwin, 1998). Let 
                              
                                 
                                    
                                       CL
                                    
                                    
                                       i
                                    
                                 
                              
                            be the cluster that story 
                              
                                 
                                    
                                       s
                                    
                                    
                                       i
                                    
                                 
                              
                            gets clustered in, and 
                              
                                 
                                    
                                       G
                                    
                                    
                                       i
                                    
                                 
                              
                            its correct cluster from the ground truth. The B-Cubed metric then calculates 
                              
                                 Precision
                                 =
                                 
                                    
                                       ∣
                                       
                                          
                                             CL
                                          
                                          
                                             i
                                          
                                       
                                       ∩
                                       
                                          
                                             G
                                          
                                          
                                             i
                                          
                                       
                                       ∣
                                    
                                    
                                       ∣
                                       
                                          
                                             CL
                                          
                                          
                                             i
                                          
                                       
                                       ∣
                                    
                                 
                              
                            and 
                              
                                 Recall
                                 =
                                 
                                    
                                       ∣
                                       
                                          
                                             CL
                                          
                                          
                                             i
                                          
                                       
                                       ∩
                                       
                                          
                                             G
                                          
                                          
                                             i
                                          
                                       
                                       ∣
                                    
                                    
                                       ∣
                                       
                                          
                                             G
                                          
                                          
                                             i
                                          
                                       
                                       ∣
                                    
                                 
                              
                           . The total precision and recall of the clustering are taken as the average of the precision and recall scores over all stories. The B-Cubed metric rewards a singleton clustering with a precision score of 100%, as no story is clustered together with an unrelated one, but recall is very low in case of singleton clustering. Therefore, we present our results in terms of the 
                              
                                 
                                    
                                       F
                                    
                                    
                                       1
                                    
                                 
                              
                            measure that balances between the two: 
                              
                                 
                                    
                                       F
                                    
                                    
                                       1
                                    
                                 
                                 =
                                 
                                    
                                       2
                                       ·
                                       Precision
                                       ·
                                       Recall
                                    
                                    
                                       Precision
                                       +
                                       Recall
                                    
                                 
                              
                           .

@&#RESULTS AND DISCUSSION@&#


                        Table 2
                         shows the results of clustering the news stories according to the event discussed in the story. In the monolingual setting, the event-centered news clustering is quite accurate, and we can observe two interesting phenomena: (i) results for English are significantly higher than results for Dutch and (ii) there is a huge difference in favor of English between the results that rely on per-document topic distributions only. Wikipedia articles in Dutch are in general shorter and contain much less content, which disturbs the correct topic estimation, and effectively leads to learning per-document topic distributions of a lesser quality for Dutch news stories. Moreover, we can see that representations relying on named entities score better for Dutch than for English. That difference is attributed to the fact that Dutch news stories are on average more than 10 times shorter and contain less variation in wording.

As expected, when transferring the problem to the multilingual setting, results decrease, but it has to be noted that the results are higher than the results obtained by a baseline system (56.5%(36)) similar to the one reported in (VanGael & Zhu, 2007), which uses a translation resource (Google Translate) to perform automatic machine translation of Dutch stories to English, and then the cosine metric to measure distances between the news stories. The 
                           
                              
                                 
                                    F
                                 
                                 
                                    1
                                 
                              
                           
                         score of 63.2% obtained by per-document topic distributions only is especially promising because it does not rely on any external knowledge, and it still relies on a training corpus of limited size. It is also very interesting to note the cross-lingual news clustering relying only on the topical representation outscores the representation which relies on shared named entities which was previously used in (Montalvo et al., 2007).

In another experiment, we have varied the number of topics K which also influences the dimension of the representation of the test news articles. We have made a comparison of the BiLDA-based topical representation against the representation obtained by training the standard monolingual LDA model on concatenated documents from aligned document pairs and then inferring this model on test news articles (MixLDA). The results and the comparison are provided in Table 3
                        . We may observe that after a sufficient granularity of representation is used (given by the number of topics/dimensions K), the clustering results are comparable over different K-s (from 50 to 300). On the other hand, the 20-dimensional representation is too coarse-grained to produce comparable clustering results. The results also reveal that training BiLDA on separate documents in aligned document pairs yields better topical representations than concatenating these documents into one artificial document as with MixLDA. By training on separate documents and imposing the constraint that two documents discussing the same themes need to have the same topical representations by means of per-document topic distributions, we are able to effectively able to remove an imbalance which occurs when two documents are mixed together into a single large document. This imbalance might occur because a document in one language may be much longer and more informative than its counterpart, and hence “dominate” the sampling of topic assignments and the final representation of its counterpart. The main strength of BiLDA lies in enforcing the topics of the two coupled documents to be sampled from the same distribution θ (see Section 2.3 again).

In summary, the application of multilingual probabilistic topic models to cross-lingual event-centered news clustering provides first evidence that utilizing true multilingual probabilistic topic modeling (as with BiLDA) is more beneficial than pseudo-multilingual probabilistic modeling (as with MixLDA). These small experiments have shown (i) the validity of topical knowledge in cross-lingual event-centered news clustering and (ii) the better performance of an BiLDA-based clustering model over the clustering model which relies on topical representations obtained by standard LDA trained on artificially created concatenated documents. The work on cross-lingual event-centered news clustering follows the previous work in the monolingual event-centered news clustering (DeSmet & Moens, 2013), where it was already demonstrated that the topical representation of news stories by means of per-document topic distributions leads to improved clustering scores. Here, we have briefly described a similar cross-lingual framework where similarities between texts/news stories written in different languages can also be obtained by measuring similarities of cross-lingual topics’ distributions over those texts.

The goal of this section was to give the reader the first insight on how to exploit language-independent representations of documents by means of latent cross-lingual topics. This cross-lingual news clustering framework provides ample room for future work and building more advanced cross-lingual news clustering models. For instance, one might try to build event-centered clusters separately for each language, and then design a method to align these monolingual clusters and merge them into larger cross-lingual clusters discussing the same events. As another line of future work it is also worth investigating whether the topical knowledge might prove beneficial when combined with models that rely on readily available bilingual dictionaries or machine translation tools.

Another task where representations of documents written in different languages as mixtures of cross-lingual topics (as represented by per-document topic distributions) prove to be useful is cross-lingual document classification. Cross-lingual document classification starts from a set of labeled documents in the source language 
                           
                              
                                 
                                    L
                                 
                                 
                                    S
                                 
                              
                           
                         and unlabeled documents in the target language 
                           
                              
                                 
                                    L
                                 
                                 
                                    T
                                 
                              
                           
                        . The objective is to learn a classification model from the labeled documents in the source language and then apply it to the classification of documents in the target language. The task obviously cannot be achieved by a method that only uses words from the labeled documents as features, since there is a minimal or no word overlap between the two languages. Hence, we have to find another solution. Here, we again deal with cross-lingual knowledge transfer, where the knowledge that is transferred across languages are text categories, that is, high-level labels that describe the content of a text.

Unlike in the previous application, where the similarity between two news stories or (more generally) documents has been established directly according to their respective per-document topic distributions, here we can observe each document as a data instance and use the probabilities 
                           
                              P
                              (
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                              ∣
                              
                                 
                                    d
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         from their per-document topic distributions as classification features. Again, by having the language independent set 
                           
                              Z
                           
                         of K cross-lingual topics, we can operate in the same shared cross-lingual feature space regardless of the actual languages in which documents were written.

@&#METHODOLOGY@&#

A multilingual probabilistic topic model is first learned on a general multilingual corpus (e.g., Wikipedia). Then, given a cross-lingual document classification task, that is, a labeled document collection 
                           
                              
                                 
                                    L
                                 
                                 
                                    S
                                 
                              
                           
                         in 
                           
                              
                                 
                                    L
                                 
                                 
                                    S
                                 
                              
                           
                        , and an unlabeled document collection 
                           
                              
                                 
                                    U
                                 
                                 
                                    T
                                 
                              
                           
                         in 
                           
                              
                                 
                                    L
                                 
                                 
                                    T
                                 
                              
                           
                        , the learned cross-lingual topics are used to infer their per-document topic distributions on each document in 
                           
                              
                                 
                                    L
                                 
                                 
                                    S
                                 
                              
                           
                         and 
                           
                              
                                 
                                    U
                                 
                                 
                                    T
                                 
                              
                           
                        . The methodology was proposed and described by De Smet et al. (2011).

Each document from 
                           
                              
                                 
                                    L
                                 
                                 
                                    S
                                 
                              
                           
                         and 
                           
                              
                                 
                                    U
                                 
                                 
                                    T
                                 
                              
                           
                         is then taken as a data instance in the classification model, where its features are the inferred per-document topic distributions. The exact value of each classification feature of an instance, e.g., of document 
                           
                              
                                 
                                    d
                                 
                                 
                                    i
                                 
                                 
                                    S
                                 
                              
                           
                         is exactly the probability 
                           
                              P
                              (
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                              ∣
                              
                                 
                                    d
                                 
                                 
                                    i
                                 
                                 
                                    S
                                 
                              
                              )
                           
                        , for all 
                           
                              k
                              =
                              1
                              ,
                              …
                              ,
                              K
                           
                        . The same is valid for some target document 
                           
                              
                                 
                                    d
                                 
                                 
                                    j
                                 
                                 
                                    T
                                 
                              
                           
                        . Since the documents in both languages are represented by the language-independent features (the distributions of cross-lingual topics over the documents), supervision in only one language is needed and labels from the documents in the source language are then propagated to the unlabeled documents in the target language according to the learned classification, that is, we again perform cross-lingual knowledge transfer. For the classification model, one can choose any existing classifier such as Naive Bayes, Perceptron, Maximum Entropy or Support Vector Machines (SVM). This choice is beyond the research reported in this article, and we present the classification results obtained by SVM on the aforementioned feature vectors comprising inferred per-document topic distributions as dimensions.
                           9
                           For SVM, we employ the SVM-Light package (Joachims, 1999, chap. 11): http://svmlight.joachims.org/ with default parameter settings.
                        
                        
                           9
                        
                     


                           Training has been conducted on English–Italian, English–French and English–Spanish Wikipedia (see Section 3.3). English is considered the source language 
                              
                                 
                                    
                                       L
                                    
                                    
                                       S
                                    
                                 
                              
                            in all classification experiments.

A test dataset, also obtained from Wikipedia, but different from the training corpus, is used for classification. It is collected by exploiting the category labels of Wikipedia. Specifically, five high level categories were first collected: books (“book”), films (“film”), programming languages (“prog”), sports (“sport”) and video games (“video”), and then for each category up to 1000 articles annotated with the category label were extracted.
                              10
                              Note that a Wikipedia article may have more than one label, and these labels can be very specific. In order to obtain a workable set with enough documents for classification, we have opted for the broad categories. In cases when we have collected more than 1000 articles for a category, we have randomly sampled 1000 articles to be included in the final test set.
                           
                           
                              10
                            Since Wikipedia variants differ in the number of articles, sometimes fewer than 1000 articles were collected for Italian, French and Spanish. We have extracted 1000 articles in the classification dataset for each category, except for the following: (i) there are 263 Spanish, 592 French and 290 Italian articles respectively, labeled with “prog” and (ii) there are 764 Italian articles labeled with “video”. The BiLDA model has been trained on the three bilingual corpora, ranging the number of topics from 10 to 200 in steps of 20. All results, except when noted, are obtained as an average over classification results with each model.

Again, we calculate the precision and recall scores and then combine them into the balanced 
                              
                                 
                                    
                                       F
                                    
                                    
                                       1
                                    
                                 
                              
                            score. Precision is now the number of correctly labeled documents divided by the total number of documents labeled that way, and recall is the number of correctly labeled documents divided by the actual number of documents with that label as found in the ground truth.

@&#RESULTS AND DISCUSSION@&#


                        Table 4
                         displays the performance of the models in terms of their average 
                           
                              
                                 
                                    F
                                 
                                 
                                    1
                                 
                              
                           
                         scores for each of our chosen classes and for each language pair. Average perplexity scores after inferring the models on classification datasets are also presented in Table 4. We observe that the results vary over language pairs and over categories for each language pair. With respect to the fact that the approach is completely unsupervised and without any additional resource (e.g., a machine translation system or a bilingual dictionary), the obtained results are reasonably high for all language pairs. Perplexity scores reveal that the English side of our training corpora is of a higher quality for all language pairs. Additional inspection of perplexity also exposes the mismatch between those scores and the actual classification results across categories for different languages. A lower perplexity of a model does not necessarily imply a better classification score (e.g., see the results over the last three categories for English–Italian and English–Spanish).

As already mentioned in Section 3, instead of utilizing a multilingual topic model, a common approach when dealing with cross-lingual tasks is to combine each bilingual pair of documents from training into a single document, mixing the words from both languages, and then to employ a monolingual topic model (e.g., LDA) to extract the shared latent topic space (MixLDA). The monolingual topic model may then again inferred on the test dataset. The two languages again share only one common topic space, and the learned per-document topic distributions are again used as the features for SVM to learn the classification model. In order to test the utility of MuPTM, we have compared the classification results of our BiLDA-based classification models with the classification results which rely on LDA trained on concatenated document pairs. Based on the results from Fig. 6
                           , one may conclude that the cross-lingual knowledge transfer by means of BiLDA leads to much better classification results than the one relying on LDA. The difference in results in favor of the BiLDA-based classification models is reported for all three language pairs and the majority of classification categories. The main causes of such a difference in results between BiLDA and MixLDA have already been discussed in Section 3.4.

An additional experimental study tests how the classification results change related to the number of topics K set before training. Fig. 5
                            shows the 
                              
                                 
                                    
                                       F
                                    
                                    
                                       1
                                    
                                 
                              
                            scores of the classification results depending on K. Again, similar to the finding reported for the task of cross-lingual event-centered news clustering (see Section 3.4 and Table 3), we can observe that in general the classification results for all languages and almost all categories are not very sensitive to the number of topics, except when K is very small. Once K is set large enough to produce a finer-grained representation of documents based on cross-lingual topics, per-document topic distributions as features will be distinctive enough to group this document with similar documents in a hyperplane and produce the correct label for the document.

The described classification framework also allows us to combine per-document topic distributions from different models as features for classification. Therefore, instead of averaging over scores obtained by different topic models, we could use all their per-document topic distributions and enrich the feature set. Per-document topic distributions from all 10 trained BiLDA models from the previous experiment are used as features. As the comparison of classification scores displayed in Fig. 7
                            reveals, we can observe a significant improvement in the classification results (+8.5% on average) by employing this procedure called topic smoothing.

In summary, we have shown how to utilize per-document topic distributions of a multilingual probabilistic topic model in the cross-lingual document classification across languages where no labeled instances are given a priori. In this case we deal with a low number of categories, and topic models with smaller values for K (e.g., 50 or higher) already provide a representation of documents that is fine-grained enough to help us decide which category label to assign to documents.

So far, we have only used the per-document topic distributions in the previous applications. But, what about per-topic word distributions that provide language-specific representations of each latent cross-lingual topic in each language? Since we have already detected that there ideally should exist a strong intra semantic and inter semantic coherence within cross-lingual topics, we could use the per-topic word distributions for mining semantically similar words across languages, that is, providing a list of target language words that are semantically similar to a given source language word 
                           
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                                 
                                    S
                                 
                              
                           
                        . For each source word 
                           
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                                 
                                    S
                                 
                              
                           
                        , we can build a ranked list 
                        
                           
                              RL
                              (
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                                 
                                    S
                                 
                              
                              )
                           
                         which consists of all words 
                           
                              
                                 
                                    w
                                 
                                 
                                    j
                                 
                                 
                                    T
                                 
                              
                              ∈
                              
                                 
                                    V
                                 
                                 
                                    T
                                 
                              
                           
                         ranked according to their respective similarity scores 
                           
                              sim
                              (
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                                 
                                    S
                                 
                              
                              ,
                              
                                 
                                    w
                                 
                                 
                                    j
                                 
                                 
                                    T
                                 
                              
                              )
                           
                        . In the similar fashion, we can build a ranked list 
                           
                              RL
                              (
                              
                                 
                                    w
                                 
                                 
                                    2
                                 
                                 
                                    T
                                 
                              
                              )
                           
                        , for each target word 
                           
                              
                                 
                                    w
                                 
                                 
                                    2
                                 
                                 
                                    T
                                 
                              
                           
                        . We call the top M best scoring target words 
                           
                              
                                 
                                    w
                                 
                                 
                                    j
                                 
                                 
                                    T
                                 
                              
                           
                         for some source word 
                           
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                                 
                                    S
                                 
                              
                           
                         its M nearest neighbors. The ranked list for 
                           
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                                 
                                    S
                                 
                              
                           
                         comprising only its M nearest neighbors is called pruned ranked list (i.e., the ranked list is effectively pruned at position M), and we denote it as 
                           
                              
                                 
                                    RL
                                 
                                 
                                    M
                                 
                              
                              (
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                                 
                                    S
                                 
                              
                              )
                           
                        . In the monolingual setting, the nearest neighbor in the list is usually a direct synonym of the word. The single nearest cross-lingual neighbor for 
                           
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                                 
                                    S
                                 
                              
                           
                         is called its translation candidate. By retaining only the first target candidate from the list, we can build a one-to-one word bilingual lexicon. Additionally, by retaining the list of a few first word candidates in the target language along with their scores, we actually build a probabilistic lexicon that could be used, for instance, in query expansion techniques for cross-lingual information retrieval (Vulić et al., 2013). The question is, however, how to obtain those lists of semantically similar words across languages by exploiting the knowledge present in per-topic word distributions of a multilingual topic model. The following section provides the answer. It introduces the complete framework for modeling cross-lingual semantic similarity by means of MuPTM.

@&#METHODOLOGY@&#

A straightforward approach to building one-to-one bilingual lexicons based on the knowledge from per-topic word distributions was presented by Mimno et al. (2009). For each topic 
                           
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                           
                        , they select a small number 
                           
                              
                                 
                                    M
                                 
                                 
                                    ′
                                 
                              
                           
                         of the most probable words in the source and the target language representations of that topic, according to the respective per-topic word distributions. Following that, they add the Cartesian product of these two sets to a final set of candidate translation pairs 
                           
                              T
                           
                        . In a similar fashion, Boyd-Graber and Blei (2009) use learned matchings as one-to-one bilingual word lexicon entries (see Section 2.5). These approaches suffer from several issues: (1) by observing only the top candidates for each topic, the size of the set of candidate translation pairs is limited to only the best scoring words, and the size heavily depends on the chosen number of topics, (2) expanding 
                           
                              
                                 
                                    M
                                 
                                 
                                    ′
                                 
                              
                           
                         increases the size of 
                           
                              T
                           
                        , but it introduces many incorrect one-to-one translation pairs, and (3) they do not exploit the fact that each word can be important for more than only one latent cross-lingual topic, i.e., they do not exploit the actual probability distributions of words over topics to mine semantically similar words across languages.

In order to fully exploit those per-topic word distributions, we make a connection between latent cross-lingual topics and an idea known as the distributional hypothesis (Harris, 1954). It states that words with similar meanings are likely to appear in similar contexts across languages. Here, cross-lingual topics provide a sound mathematical representation of this context and may be regarded as context features. In other words, we consider that word 
                           
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                                 
                                    S
                                 
                              
                           
                         in 
                           
                              
                                 
                                    L
                                 
                                 
                                    S
                                 
                              
                           
                         and word 
                           
                              
                                 
                                    w
                                 
                                 
                                    2
                                 
                                 
                                    T
                                 
                              
                           
                         in 
                           
                              
                                 
                                    L
                                 
                                 
                                    T
                                 
                              
                           
                         are semantically similar if they are often present and almost equally important in the same latent cross-lingual topics, and not observed or marginally present in other latent cross-lingual topics, that is, the word 
                           
                              
                                 
                                    w
                                 
                                 
                                    2
                                 
                                 
                                    T
                                 
                              
                           
                         is semantically similar to 
                           
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                                 
                                    S
                                 
                              
                           
                         if the distribution of 
                           
                              
                                 
                                    w
                                 
                                 
                                    2
                                 
                                 
                                    T
                                 
                              
                           
                         over latent cross-lingual topics (extracted from per-topic word distributions for 
                           
                              
                                 
                                    L
                                 
                                 
                                    T
                                 
                              
                           
                        ) is similar to the probability distribution of 
                           
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                                 
                                    S
                                 
                              
                           
                         over the same set of latent topics (extracted from per-topic word distributions for 
                           
                              
                                 
                                    L
                                 
                                 
                                    S
                                 
                              
                           
                        ). In this article, we present and evaluate a series of models of semantic similarity which rely on this essential hypothesis and significantly extend our earlier work (Vulić et al., 2011a).

After training, a multilingual topic model outputs per-topic word distributions with probability scores 
                              
                                 P
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 ∣
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 )
                              
                            and 
                              
                                 P
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 ∣
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 )
                              
                           , for each 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 ∈
                                 
                                    
                                       V
                                    
                                    
                                       S
                                    
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 ∈
                                 
                                    
                                       V
                                    
                                    
                                       T
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 ∈
                                 Z
                              
                           . It holds that 
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       k
                                       =
                                       1
                                    
                                    
                                       K
                                    
                                 
                                 P
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 ∣
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 )
                                 =
                                 1
                              
                            and 
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       k
                                       =
                                       1
                                    
                                    
                                       K
                                    
                                 
                                 P
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 ∣
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 )
                                 =
                                 1
                              
                           , since each language has its own language-specific distribution over vocabulary words (see Sections 2.1 and 2.2).

In order to quantify the similarity between two words 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 ∈
                                 
                                    
                                       V
                                    
                                    
                                       S
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 ∈
                                 
                                    
                                       V
                                    
                                    
                                       T
                                    
                                 
                              
                           , we may employ the same trick that has been used for obtaining the degree of similarity between two documents in the monolingual setting (Steyvers & Griffiths, 2007) and the cross-lingual setting (Ni et al., 2011) (see also Eq. (7) in Section 2.3). Since each document 
                              
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                              
                            is represented as a mixture of topics by means of per-document topic distributions given by the probability scores 
                              
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 |
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                                 )
                              
                           , the similarity between two documents can be established by measuring the similarity of these probability distributions. When dealing with the similarity of words 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                              
                           , we need to measure the similarity of their respective conditional topic distributions, given by the probability scores 
                              
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 |
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                            and 
                              
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 |
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 )
                              
                           , for each 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 ∈
                                 Z
                              
                           . Each word, regardless of its actual language, is then represented by its K-dimensional vector 
                              
                                 vec
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                            (where features are latent cross-lingual topics) as a point in a K-dimensional latent semantic space. In other words, each word, irrespective to the language, is represented as a distribution over the K latent topics/concepts, where the K-dimensional vector representation of 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 ∈
                                 
                                    
                                       V
                                    
                                    
                                       S
                                    
                                 
                              
                            (similar for 
                              
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 ∈
                                 
                                    
                                       V
                                    
                                    
                                       T
                                    
                                 
                              
                           ) is:
                              
                                 (13)
                                 
                                    vec
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    =
                                    [
                                    P
                                    (
                                    
                                       
                                          z
                                       
                                       
                                          1
                                       
                                    
                                    |
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    ,
                                    …
                                    ,
                                    P
                                    (
                                    
                                       
                                          z
                                       
                                       
                                          k
                                       
                                    
                                    |
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    ,
                                    …
                                    ,
                                    P
                                    (
                                    
                                       
                                          z
                                       
                                       
                                          K
                                       
                                    
                                    |
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    ]
                                 
                              
                           Using Bayes’ rule, we can compute these probability scores:
                              
                                 (14)
                                 
                                    P
                                    (
                                    
                                       
                                          z
                                       
                                       
                                          k
                                       
                                    
                                    |
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          P
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                1
                                             
                                             
                                                S
                                             
                                          
                                          |
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          )
                                          P
                                          (
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          )
                                       
                                       
                                          P
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                1
                                             
                                             
                                                S
                                             
                                          
                                          )
                                       
                                    
                                    =
                                    
                                       
                                          P
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                1
                                             
                                             
                                                S
                                             
                                          
                                          |
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          )
                                          P
                                          (
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          )
                                       
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                
                                                   
                                                      k
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                                =
                                                1
                                             
                                             
                                                K
                                             
                                          
                                          P
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                1
                                             
                                             
                                                S
                                             
                                          
                                          |
                                          
                                             
                                                z
                                             
                                             
                                                
                                                   
                                                      k
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                             
                                          
                                          )
                                          P
                                          (
                                          
                                             
                                                z
                                             
                                             
                                                
                                                   
                                                      k
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           where 
                              
                                 P
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 |
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 )
                              
                            is known directly from the per-topic word distributions. 
                              
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 )
                              
                            is the prior topic distribution which can be used to assign higher a priori importance to some cross-lingual topics from the set 
                              
                                 Z
                              
                            (Jagarlamudi et al., 2012). However, in a typical setting where we do not possess any prior knowledge about the corpus and the likelihood of finding specific latent topics in that corpus, we assume the uniform prior over latent cross-lingual concepts/topics (Griffiths et al., 2007) (i.e., that all topics/concepts are equally likely before we observe any training data). The probability scores 
                              
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 |
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                            from Eq. (14) for conditional topic distributions in that case may be further simplified:
                              
                                 (15)
                                 
                                    P
                                    (
                                    
                                       
                                          z
                                       
                                       
                                          k
                                       
                                    
                                    |
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          P
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                1
                                             
                                             
                                                S
                                             
                                          
                                          |
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          )
                                       
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                
                                                   
                                                      k
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                                =
                                                1
                                             
                                             
                                                K
                                             
                                          
                                          P
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                1
                                             
                                             
                                                S
                                             
                                          
                                          |
                                          
                                             
                                                z
                                             
                                             
                                                
                                                   
                                                      k
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                             
                                          
                                          )
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                ϕ
                                             
                                             
                                                k
                                                ,
                                                1
                                             
                                             
                                                S
                                             
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                
                                                   
                                                      k
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                                =
                                                1
                                             
                                             
                                                K
                                             
                                          
                                          
                                             
                                                ϕ
                                             
                                             
                                                
                                                   
                                                      k
                                                   
                                                   
                                                      ∗
                                                   
                                                
                                                ,
                                                1
                                             
                                             
                                                S
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                ϕ
                                             
                                             
                                                k
                                                ,
                                                1
                                             
                                             
                                                S
                                             
                                          
                                       
                                       
                                          
                                             
                                                Norm
                                             
                                             
                                                
                                                   
                                                      ϕ
                                                   
                                                   
                                                      ·
                                                      ,
                                                      1
                                                   
                                                   
                                                      S
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where we denote the normalization factor 
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             k
                                          
                                          
                                             ∗
                                          
                                       
                                       =
                                       1
                                    
                                    
                                       K
                                    
                                 
                                 
                                    
                                       ϕ
                                    
                                    
                                       l
                                       ,
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                            as 
                              
                                 
                                    
                                       Norm
                                    
                                    
                                       
                                          
                                             ϕ
                                          
                                          
                                             ·
                                             ,
                                             1
                                          
                                          
                                             S
                                          
                                       
                                    
                                 
                              
                           . A similar derivation follows for each 
                              
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 ∈
                                 
                                    
                                       V
                                    
                                    
                                       T
                                    
                                 
                              
                            and the similarity between two words may then be computed as the similarity between their conditional topic distributions as given by Eq. (14) or Eq. (15). We will use this property extensively in our models of cross-lingual similarity.

Now, once the conditional topic distributions are computed, any similarity metric may be used as a similarity function (SF) between word vectors to quantify the degree of similarity between the representations of words by means of these conditional topic distributions. We present and evaluate a series of models which employ the most popular SF-s reported in the relevant literature. Each SF in fact gives rise to a new model of cross-lingual semantic similarity!

The first model relies on the Kullback–Leibler (KL) divergence which is a common measure of (dis) similarity between two probability distributions (Lin, 1991). The KL divergence of conditional topic distributions for two words 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                              
                            is an asymmetric measure computed as follows (our KL model):
                              
                                 (16)
                                 
                                    sim
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    =
                                    KL
                                    (
                                    vec
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    ,
                                    vec
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             K
                                          
                                       
                                    
                                    P
                                    (
                                    
                                       
                                          z
                                       
                                       
                                          k
                                       
                                    
                                    |
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    log
                                    
                                       
                                          P
                                          (
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          |
                                          
                                             
                                                w
                                             
                                             
                                                1
                                             
                                             
                                                S
                                             
                                          
                                          )
                                       
                                       
                                          P
                                          (
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          |
                                          
                                             
                                                w
                                             
                                             
                                                2
                                             
                                             
                                                T
                                             
                                          
                                          )
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             K
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                ϕ
                                             
                                             
                                                k
                                                ,
                                                1
                                             
                                             
                                                S
                                             
                                          
                                       
                                       
                                          
                                             
                                                Norm
                                             
                                             
                                                
                                                   
                                                      ϕ
                                                   
                                                   
                                                      ·
                                                      ,
                                                      1
                                                   
                                                   
                                                      S
                                                   
                                                
                                             
                                          
                                       
                                    
                                    log
                                    
                                       
                                          
                                             
                                                ϕ
                                             
                                             
                                                k
                                                ,
                                                1
                                             
                                             
                                                S
                                             
                                          
                                          ·
                                          
                                             
                                                Norm
                                             
                                             
                                                
                                                   
                                                      ψ
                                                   
                                                   
                                                      ·
                                                      ,
                                                      2
                                                   
                                                   
                                                      T
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                ψ
                                             
                                             
                                                k
                                                ,
                                                2
                                             
                                             
                                                T
                                             
                                          
                                          ·
                                          
                                             
                                                Norm
                                             
                                             
                                                
                                                   
                                                      ϕ
                                                   
                                                   
                                                      ·
                                                      ,
                                                      1
                                                   
                                                   
                                                      S
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           The Jensen–Shannon (JS) divergence (Lin, 1991; Dagan, Pereira, & Lee, 1994; Dagan, Lee, & Pereira, 1997) is a symmetric (dis) similarity measure closely related to the KL divergence, defined as the average of the KL divergence of each of two distributions to their average distribution. The dissimilarity of conditional topic distributions is computed as follows (our JS model):
                              
                                 (17)
                                 
                                    sim
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    =
                                    JS
                                    (
                                    vec
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    ,
                                    vec
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    )
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   
                                                      K
                                                   
                                                
                                             
                                             P
                                             (
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             |
                                             
                                                
                                                   w
                                                
                                                
                                                   1
                                                
                                                
                                                   S
                                                
                                             
                                             )
                                             log
                                             
                                                
                                                   P
                                                   (
                                                   
                                                      
                                                         z
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         1
                                                      
                                                      
                                                         S
                                                      
                                                   
                                                   )
                                                
                                                
                                                   
                                                      
                                                         P
                                                      
                                                      
                                                         avg
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         z
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         1
                                                      
                                                      
                                                         S
                                                      
                                                   
                                                   )
                                                
                                             
                                             +
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   
                                                      K
                                                   
                                                
                                             
                                             P
                                             (
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             |
                                             
                                                
                                                   w
                                                
                                                
                                                   2
                                                
                                                
                                                   T
                                                
                                             
                                             )
                                             log
                                             
                                                
                                                   P
                                                   (
                                                   
                                                      
                                                         z
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         2
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                   )
                                                
                                                
                                                   
                                                      
                                                         P
                                                      
                                                      
                                                         avg
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         z
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         2
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       P
                                    
                                    
                                       avg
                                    
                                 
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 ∣
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       P
                                    
                                    
                                       avg
                                    
                                 
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 ∣
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       P
                                       (
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                       ∣
                                       
                                          
                                             w
                                          
                                          
                                             1
                                          
                                          
                                             S
                                          
                                       
                                       )
                                       +
                                       P
                                       (
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                       ∣
                                       
                                          
                                             w
                                          
                                          
                                             2
                                          
                                          
                                             T
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                              
                           , for each 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 ∈
                                 Z
                              
                           . Both KL and JS output non-negative scores, where a lower output score implies a lower divergence between two words, and therefore, a closer semantic similarity. Both KL and JS are defined only if they deal with real probability distributions, that is, if probability scores sum up to 1, Additionally, 
                              
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 ∣
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 >
                                 0
                              
                            and 
                              
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 ∣
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                 
                                 )
                                 >
                                 0
                              
                            have to hold for each 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                           . Conditional topic distributions satisfy all these conditions. A ranked list 
                              
                                 RL
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                            may be obtained by sorting words 
                              
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 ∈
                                 
                                    
                                       V
                                    
                                    
                                       T
                                    
                                 
                              
                            in ascending order based on their respective similarity/divergence scores computed by Eq. (16) (KL) or Eq. (17) (JS).

The next distance measure is the cosine similarity which is one of the most popular choices for SF in distributional semantics (Fung & Yee, 1998; Bullinaria & Levy, 2007; Turney & Pantel, 2010). Cosine similarity is a measure of similarity between two vectors of an inner product space that measures the cosine of the angle between them. The cosine similarity of conditional topic distributions (our TCos model) is computed as follows:
                              
                                 (18)
                                 
                                    sim
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    =
                                    TCos
                                    (
                                    vec
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    ,
                                    vec
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                k
                                                =
                                                1
                                             
                                             
                                                K
                                             
                                          
                                          P
                                          (
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          |
                                          
                                             
                                                w
                                             
                                             
                                                1
                                             
                                             
                                                S
                                             
                                          
                                          )
                                          P
                                          (
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          |
                                          
                                             
                                                w
                                             
                                             
                                                2
                                             
                                             
                                                T
                                             
                                          
                                          )
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   
                                                      K
                                                   
                                                
                                                P
                                                
                                                   
                                                      (
                                                      
                                                         
                                                            z
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      |
                                                      
                                                         
                                                            w
                                                         
                                                         
                                                            1
                                                         
                                                         
                                                            S
                                                         
                                                      
                                                      )
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                          ·
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   
                                                      K
                                                   
                                                
                                                P
                                                
                                                   
                                                      (
                                                      
                                                         
                                                            z
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      |
                                                      
                                                         
                                                            w
                                                         
                                                         
                                                            2
                                                         
                                                         
                                                            T
                                                         
                                                      
                                                      )
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           The higher the score in the range [0,1] (all dimensions of our vectors are positive numbers and therefore the lower similarity bound is 0 instead of −1), the higher the similarity between two words. A ranked list 
                              
                                 RL
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                            may be obtained by sorting words 
                              
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 ∈
                                 
                                    
                                       V
                                    
                                    
                                       T
                                    
                                 
                              
                            in descending order based on their respective scores computed by Eq. (18).

Another similarity measure is the Bhattacharyya coefficient (BC) (Bhattacharyya, 1943; Kazama, Saeger, Kuroda, Murata, & Torisawa, 2010). The similarity of two words based on this similarity measure is defined as follows (our BC model):
                              
                                 (19)
                                 
                                    sim
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    =
                                    BC
                                    (
                                    vec
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    ,
                                    vec
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             K
                                          
                                       
                                    
                                    
                                       
                                          P
                                          (
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          |
                                          
                                             
                                                w
                                             
                                             
                                                1
                                             
                                             
                                                S
                                             
                                          
                                          )
                                          P
                                          (
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          |
                                          
                                             
                                                w
                                             
                                             
                                                2
                                             
                                             
                                                T
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           In general, it measures the amount of overlap between two statistical samples which, unlike for KL and JS, do not have to be described by proper probability distributions. A higher score again implies a stronger semantic similarity between two words. The utility of the BC measure has not been investigated well in the literature on distributional semantics. Our experiments will reveal its potential in identifying semantically similar words across languages.

Another way of utilizing per-topic word distributions is to directly model the probability 
                              
                                 P
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 |
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                           , where semantically most similar target words should have the highest probability to be generated as a response to a cue source word. The probability 
                              
                                 P
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 ∣
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                            emphasizes the (cross-lingual) associative relation between words (Griffiths et al., 2007). Again, under the assumption of uniform topic prior, we can decompose the probability 
                              
                                 P
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 ∣
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                            as follows (our Cue model):
                              
                                 (20)
                                 
                                    sim
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   1
                                                
                                                
                                                   S
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   2
                                                
                                                
                                                   T
                                                
                                             
                                          
                                       
                                    
                                    =
                                    Cue
                                    
                                       
                                          
                                             vec
                                             (
                                             
                                                
                                                   w
                                                
                                                
                                                   1
                                                
                                                
                                                   S
                                                
                                             
                                             )
                                             ,
                                             vec
                                             (
                                             
                                                
                                                   w
                                                
                                                
                                                   2
                                                
                                                
                                                   T
                                                
                                             
                                             )
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             K
                                          
                                       
                                    
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   2
                                                
                                                
                                                   T
                                                
                                             
                                             |
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                          
                                       
                                    
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   z
                                                
                                                
                                                   k
                                                
                                             
                                             |
                                             
                                                
                                                   w
                                                
                                                
                                                   1
                                                
                                                
                                                   S
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           The probability value directly provides the degree of semantic similarity and a ranked list 
                              
                                 RL
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                            may be obtained by sorting words 
                              
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 ∈
                                 
                                    
                                       V
                                    
                                    
                                       T
                                    
                                 
                              
                            in descending order based on their respective probability scores computed by Eq. (20).

The next model moves away from utilizing conditional topic distributions explicitly and aims to exploit latent cross-lingual topics in a different way. It builds a context vector 
                              
                                 vec
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                            as follows:
                              
                                 (21)
                                 
                                    vec
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    =
                                    [
                                    TTF
                                    -
                                    ITF
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    ,
                                    
                                       
                                          z
                                       
                                       
                                          1
                                       
                                    
                                    )
                                    ,
                                    …
                                    ,
                                    TTF
                                    -
                                    ITF
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    ,
                                    
                                       
                                          z
                                       
                                       
                                          K
                                       
                                    
                                    )
                                    ]
                                 
                              
                           TTF-ITF (term-topic frequency – inverse topic frequency) is a novel weighting scheme which is analogous to and directly inspired by the TF-IDF (term frequency – inverse document frequency) weighting scheme in IR (SparckJones, 1973; Salton, Wong, & Yang, 1975; Manning & Schütze, 1999; Manning, Raghavan, & Schütze, 2008). Instead of conditional topic probability scores 
                              
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 ∣
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                            the context features 
                              
                                 
                                    
                                       sc
                                    
                                    
                                       1
                                    
                                 
                                 (
                                 
                                    
                                       c
                                    
                                    
                                       k
                                    
                                 
                                 )
                              
                            are now TTF-ITF scores. In our TTF-ITF weighting scheme, the TTF
                              
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 ,
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 )
                              
                            part of the complete score TTF-ITF
                              
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 ,
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 )
                              
                            measures importance of 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                            for the particular topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                           . It denotes the number of assignments of the latent cross-lingual topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                            to the occurrences of 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                            in the whole training corpus (i.e., that number is exactly the Gibbs count variable 
                              
                                 
                                    
                                       v
                                    
                                    
                                       k
                                       ,
                                       
                                          
                                             w
                                          
                                          
                                             1
                                          
                                          
                                             S
                                          
                                       
                                    
                                    
                                       S
                                    
                                 
                              
                            which is one of the variables utilized to obtain the output per-topic word distributions in Section 2.3). The ITF
                              
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                            score measures global importance of 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                            across all latent cross-lingual topics. Words that are prominent for only a small subset of topics from 
                              
                                 Z
                              
                            are given higher importance for these topics as such words are in general more descriptive for these specific topics than high-frequency words that occur frequently over all topics. The inverse topic frequency for the word 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                            across the set of cross-lingual topics is computed as 
                              
                                 ITF
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                                 =
                                 log
                                 
                                    
                                       K
                                    
                                    
                                       1
                                       +
                                       ∣
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                       :
                                       
                                          
                                             v
                                          
                                          
                                             k
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   1
                                                
                                                
                                                   S
                                                
                                             
                                          
                                          
                                             S
                                          
                                       
                                       >
                                       0
                                       ∣
                                    
                                 
                              
                           . The final TTF-ITF
                              
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 ,
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 )
                              
                            score for the source language word 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                            and the topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                              
                            is then calculated as TTF-ITF
                              
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 ,
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 )
                                 =
                                 TTF
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 ,
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 )
                                 ·
                                 ITF
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                           . Once the same vector representation for 
                              
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                                 ∈
                                 
                                    
                                       V
                                    
                                    
                                       T
                                    
                                 
                              
                            has been obtained, the similarity between words 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                              
                            may again be computed by means of theirK-dimensional vector representations from Eq. (21) using the cosine similarity (or any other SF) as follows (our TI model):
                              
                                 (22)
                                 
                                    sim
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    =
                                    TI
                                    (
                                    vec
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    )
                                    ,
                                    vec
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                k
                                                =
                                                1
                                             
                                             
                                                K
                                             
                                          
                                          TTF
                                          -
                                          ITF
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                1
                                             
                                             
                                                S
                                             
                                          
                                          ,
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          )
                                          ·
                                          TTF
                                          -
                                          ITF
                                          (
                                          
                                             
                                                w
                                             
                                             
                                                2
                                             
                                             
                                                T
                                             
                                          
                                          ,
                                          
                                             
                                                z
                                             
                                             
                                                k
                                             
                                          
                                          )
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   
                                                      K
                                                   
                                                
                                                TTF
                                                -
                                                ITF
                                                (
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      1
                                                   
                                                   
                                                      S
                                                   
                                                
                                                ,
                                                
                                                   
                                                      z
                                                   
                                                   
                                                      k
                                                   
                                                
                                                )
                                                ·
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            k
                                                            =
                                                            1
                                                         
                                                         
                                                            K
                                                         
                                                      
                                                      TTF
                                                      -
                                                      ITF
                                                      (
                                                      
                                                         
                                                            w
                                                         
                                                         
                                                            1
                                                         
                                                         
                                                            S
                                                         
                                                      
                                                      ,
                                                      
                                                         
                                                            z
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

In the original paper (Vulić et al., 2011a) we have discussed that the Cue model and the TI model interpret and exploit the shared set of latent cross-lingual topics in different ways. Therefore, by combining the two models and capturing different evidences of similarity, we should be able to boost the quality of obtained ranked lists. As in (Vulić et al., 2011a), we present a linear combination of the two models (with γ as the interpolation parameter), where the overall score is computed as follows (our TI
                           +
                           Cue model):
                              
                                 (23)
                                 
                                    sim
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    =
                                    γ
                                    
                                       
                                          sim
                                       
                                       
                                          TI
                                       
                                    
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                    +
                                    (
                                    1
                                    -
                                    γ
                                    )
                                    
                                       
                                          sim
                                       
                                       
                                          Cue
                                       
                                    
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          1
                                       
                                       
                                          S
                                       
                                    
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          2
                                       
                                       
                                          T
                                       
                                    
                                    )
                                 
                              
                           Following Vulić et al. (2011a), the parameter γ is set to 0.1.

All these models of similarity have a straightforward theoretical explanation – they assign high similarity scores for pairs of words that assign similar importance to the same latent cross-lingual topics/concepts, that is, the same axes in the shared semantic space. In the core of all models of similarity are point-wise additive formulas, i.e., the models perform calculations over each cross-lingual topic 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 ∈
                                 Z
                              
                            and each calculation contributes to the overall sum. However, each word is usually important for only a limited number of topics/concepts. For models that make use of conditional topic distributions it means that words exhibit high conditional topic probability scores for only a small subset of cross-lingual topics. In a typical setting for mining semantically similar words using latent topic models in both monolingual (Griffiths et al., 2007; Dinu & Lapata, 2010) and cross-lingual settings (Vulić et al., 2011a), the best results are obtained with the number of topics set to a few thousands 
                              
                                 (
                                 ≈
                                 2000
                                 )
                              
                           . Therefore, the procedure of topic pruning might lead to improvements in terms of overall quality and the speed of calculation.

For the sake of simplicity, the description is valid for models that utilize conditional topic distributions with scores 
                              
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 |
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                              
                           . Since 
                              
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 |
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                                 >
                                 0
                              
                            for each 
                              
                                 
                                    
                                       z
                                    
                                    
                                       k
                                    
                                 
                                 ∈
                                 Z
                              
                           , a lot of probability mass is assigned to latent topics that are not relevant to the given word. Reducing the dimensionality of the semantic representation a posteriori to only a smaller number of the most informative semantic axes in the latent space should decrease the effects of that statistical noise, and even more firmly emphasize the latent correlation among words. Therefore, given two words 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                              
                           , we prune the representation of these words in the shared latent semantic space spanned by cross-lingual topics as summarized in Algorithm 3.
                              Algorithm 3
                              Topic pruning 
                                 
                                    
                                       
                                          
                                          
                                             
                                                1: Obtain a subset 
                                                      
                                                         
                                                            
                                                               Z
                                                            
                                                            
                                                               
                                                                  
                                                                     K
                                                                  
                                                                  
                                                                     ′
                                                                  
                                                               
                                                            
                                                         
                                                         ⊆
                                                         Z
                                                      
                                                    of 
                                                      
                                                         
                                                            
                                                               K
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                         ⩽
                                                         K
                                                      
                                                    latent cross-lingual topics with the highest values 
                                                      
                                                         P
                                                         (
                                                         
                                                            
                                                               z
                                                            
                                                            
                                                               k
                                                            
                                                         
                                                         ∣
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               1
                                                            
                                                            
                                                               S
                                                            
                                                         
                                                         )
                                                      
                                                   .
                                             
                                             
                                                Calculating the similarity score 
                                                      
                                                         sim
                                                         (
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               1
                                                            
                                                            
                                                               S
                                                            
                                                         
                                                         ,
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               2
                                                            
                                                            
                                                               T
                                                            
                                                         
                                                         )
                                                      
                                                    may be interpreted as: “Given a word 
                                                      
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               1
                                                            
                                                            
                                                               S
                                                            
                                                         
                                                      
                                                    detect how similar another word 
                                                      
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               2
                                                            
                                                            
                                                               T
                                                            
                                                         
                                                      
                                                    is to the word 
                                                      
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               1
                                                            
                                                            
                                                               S
                                                            
                                                         
                                                      
                                                   .” Therefore, when calculating 
                                                      
                                                         sim
                                                         (
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               1
                                                            
                                                            
                                                               S
                                                            
                                                         
                                                         ,
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               2
                                                            
                                                            
                                                               T
                                                            
                                                         
                                                         )
                                                      
                                                   , even when dealing with symmetric similarity functions such as JS or BC, we always consider only the ranking of scores 
                                                      
                                                         P
                                                         (
                                                         
                                                            
                                                               z
                                                            
                                                            
                                                               k
                                                            
                                                         
                                                         ∣
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               1
                                                            
                                                            
                                                               S
                                                            
                                                         
                                                         )
                                                      
                                                    for pruning. The subset 
                                                      
                                                         
                                                            
                                                               Z
                                                            
                                                            
                                                               
                                                                  
                                                                     K
                                                                  
                                                                  
                                                                     ′
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                    is then 
                                                      
                                                         {
                                                         
                                                            
                                                               z
                                                            
                                                            
                                                               1
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                         ,
                                                         …
                                                         ,
                                                         
                                                            
                                                               z
                                                            
                                                            
                                                               
                                                                  
                                                                     K
                                                                  
                                                                  
                                                                     ′
                                                                  
                                                               
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                         }
                                                      
                                                   .
                                             
                                             
                                                2: Retain conditional topic probability scores 
                                                      
                                                         P
                                                         (
                                                         
                                                            
                                                               z
                                                            
                                                            
                                                               k
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                         ∣
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               1
                                                            
                                                            
                                                               S
                                                            
                                                         
                                                         )
                                                      
                                                    for the word 
                                                      
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               1
                                                            
                                                            
                                                               S
                                                            
                                                         
                                                      
                                                    only over topics 
                                                      
                                                         
                                                            
                                                               z
                                                            
                                                            
                                                               k
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                         ∈
                                                         
                                                            
                                                               Z
                                                            
                                                            
                                                               
                                                                  
                                                                     K
                                                                  
                                                                  
                                                                     ′
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   .
                                             
                                             
                                                3: Retain conditional topic probability scores 
                                                      
                                                         P
                                                         (
                                                         
                                                            
                                                               z
                                                            
                                                            
                                                               k
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                         ∣
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               2
                                                            
                                                            
                                                               T
                                                            
                                                         
                                                         )
                                                      
                                                    for 
                                                      
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               2
                                                            
                                                            
                                                               T
                                                            
                                                         
                                                      
                                                    over the same cross-lingual topics 
                                                      
                                                         
                                                            
                                                               z
                                                            
                                                            
                                                               k
                                                            
                                                            
                                                               ′
                                                            
                                                         
                                                         ∈
                                                         
                                                            
                                                               Z
                                                            
                                                            
                                                               
                                                                  
                                                                     K
                                                                  
                                                                  
                                                                     ′
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   .
                                             
                                          
                                       
                                    
                                 
                              

Both 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       w
                                    
                                    
                                       2
                                    
                                    
                                       T
                                    
                                 
                              
                            are now represented by their 
                              
                                 
                                    
                                       K
                                    
                                    
                                       ′
                                    
                                 
                              
                           -dimensional context vectors: for 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                            the pruned vector is 
                              
                                 vec
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                                 =
                                 [
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       1
                                    
                                    
                                       ′
                                    
                                 
                                 ∣
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                                 ,
                                 …
                                 ,
                                 P
                                 (
                                 
                                    
                                       z
                                    
                                    
                                       
                                          
                                             K
                                          
                                          
                                             ′
                                          
                                       
                                    
                                    
                                       ′
                                    
                                 
                                 ∣
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 )
                                 ]
                              
                           , where context features are now the semantically most relevant cross-lingual topics for the word 
                              
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                              
                           . We may again employ any SF (e.g., JS, BC, TCos) on these reduced representations, that is, pruned feature vectors with the adjusted conditional topic probability scores to calculate similarity.

We train on English-Italian Wikipedia. Following a common practice in relevant related work (Koehn & Knight, 2002; Haghighi et al., 2008; Boyd-Graber & Blei, 2009; Prochasson & Fung, 2011), we focus only on translation of nouns. Again, following related work, we use TreeTagger (Schmid, 1994) for POS-tagging and lemmatization of the corpora, and then retain only nouns that occur at least 5 times in the corpus. We record the lemmatized form when available, and the original form otherwise. Therefore, our final vocabularies consist of 7160 Italian nouns and 9166 English nouns.

We designed a set of ground truth one-to-one translation pairs to measure the ability of these models of similarity to extract one-to-one word translations from the data (the bilingual lexicon extraction task). We randomly sampled a set of 1000 Italian nouns from our Wikipedia corpora (i.e., we conduct our experiments in the Italian-to-English direction) which are to be regarded as our test words. Following that, we used the Google Translate tool plus an additional annotator to translate those words to English. The annotator manually revised the lists and retained only words that have their corresponding translation in the English vocabulary. Additionally, only one possible translation was annotated as correct. When more than one translation is possible (e.g., when dealing with polysemous words), the annotator marked as correct the translation that occurs more frequently in the English part of our Wikipedia data.
                              11
                              The test set is available online: http://people.cs.kuleuven.be/∼ivan.vulic/software/.
                           
                           
                              11
                           
                        

For the multilingual topic model training, we have varied the number of topics K for BiLDA from 200 to 3500 with steps of 300 or 400 to measure the influence of the parameter K on the overall scores, that is, to test how the granularity of the shared topical space influences the quality of our models of similarity.

The first, stricter evaluation metric calculates 
                              
                                 
                                    
                                       Acc
                                    
                                    
                                       1
                                    
                                 
                              
                            scores (Gaussier, Renders, Matveeva, Goutte, & Déjean, 2004; Tamura, Watanabe, & Sumita, 2012), that is, the percentage of words where the first word from the ranked list is actually the correct translation according to the ground truth). This metric directly measures the quality of one-to-one non-probabilistic bilingual lexicons. In a more lenient evaluation setting, we also measure 
                              
                                 
                                    
                                       Acc
                                    
                                    
                                       10
                                    
                                 
                              
                            scores (i.e., the correct translation should appear among top 10 best scoring words in the ranked list for each word), and we also employ the mean reciprocal rank (MRR) (Voorhees, 1999). Here, we retain the entire ranked list, and MRR rewards if the correct translation is found higher in the list.

We evaluate all our MuPTM-based models of similarity in the BLE task (their codes are ∗
                           -MuPTM, e.g., KL-MuPTM or JS-MuPTM). We compare them against baseline models which also exploit document alignments when mining semantically similar words and translation candidates from comparable corpora:
                              
                                 (1)
                                 The first set of models utilizes standard monolingual LDA (Blei et al., 2003b) (see Section 2.3) on concatenated aligned Wikipedia articles (MixLDA, as in Sections 3 and 4). The LDA models are trained with exactly the same parameter setup as our BiLDA models. As mentioned before, by the concatenation of aligned Wikipedia articles given in the source and the target language, we effectively remove the gap between the languages and train on the obtained set of “merged” documents and acquire a shared set of latent topics represented by words in both languages. However, we may still distinguish between source language words and target language words, and use only a subset of all words comprising all target language words in final ranked lists. The goal of this comparison is again to test whether it is useful to provide separate topic representations in two languages by jointly training on separate documents (see Section 2.5), and how it affects the final word representations (as given by Eq. (13)). We again test all models from the previous sections as with MuPTM. Since the obtained set of models effectively relies on a monolingual topic model, their codes are ∗
                                    -MoPTM, e.g., KL-MoPTM, JS-MoPTM.

Another baseline model is conceptually similar to our TI model. The model constructs feature vectors, but now in the original word-document space (instead of the lower-dimensional word-topic space) using the TF-IDF weighting scheme (which is completely analogous to the TTF-ITF weighting scheme) and the cosine similarity on the obtained word vectors. This comparison serves to test whether we gain some extra contextual information by translating our problem from the word-document to the word-topic space, besides the obvious fact that we produce a sort of dimensionality reduction which in turn speeds up computations. In other words, we test whether topic models have the ability to build clusters of words which might not always co-occur together in the same textual units and therefore add extra information of similarity besides a direct co-occurrence captured by this baseline model (BaseCos).

@&#RESULTS AND DISCUSSION@&#

We conduct two different batches of experiments: (1) We compare all our proposed models against the baseline models, and measure the influence of the number of topics K on the overall results in the BLE task and (2) we test and report the effect of topic pruning for a selection of models.


                           
                              
                                 
                                    
                                       Acc
                                    
                                    
                                       1
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       Acc
                                    
                                    
                                       10
                                    
                                 
                              
                            scores in the BLE task for all models relying directly on the similarity function operating on context vectors with conditional topic probability scores (KL-∗, JS-∗, BC-∗, TCos-∗ models) are displayed in Fig. 8
                           a and b respectively. 
                              
                                 
                                    
                                       Acc
                                    
                                    
                                       1
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       Acc
                                    
                                    
                                       10
                                    
                                 
                              
                            for all other models are displayed in Fig. 9
                           a and b respectively. Additionally, Table 5
                            lists the best results for all models along with the optimal number of topics K with which these results have been obtained. Based on all these results, we may derive a series of important conclusions:
                              
                                 (i)
                                 A comparison of all ∗-MuPTM models (blue lines) and all ∗-MoPTM models (red lines) clearly reveals the utility of training BiLDA on separate documents in place of training standard LDA on concatenated documents. All MuPTM-based models significantly outscore their MoPTM-based variants with LDA trained with exactly the same parameters as BiLDA. By training LDA on concatenated documents, we inherently introduce imbalance in the model, since one of the languages might clearly dominate the latent topic estimation (e.g., in cases when, for instance, English data is of higher quality than Italian data).

The choice of a similarity function matters. If we compare strictly SF-s operating with exactly the same representations (context vectors comprising conditional topic distributions) as given in Fig. 8a and b, we may observe that the KL model is significantly outperformed by the related JS model (which effectively performs a sort of symmetrization) and two other novel similarity models (the TCos model and the BC model) operating with exactly the same word representations.

Based on these initial results, the TI model which relies on the representation with the new TTF-ITF weighting scheme is the best scoring basic model of similarity. However, we will later show that by pruning the topic space, other basic models such as JS and BC may outperform the TI model. Moreover, it is very interesting to note that the TI model, which is effectively the same model as BaseCos, but with a shift from the original word-document space to the newly induced word-topic space, outscores the BaseCos model. All other MuPTM-based models (except for KL-MuPTM) are at least on a par with BaseCos. Additionally, for large document collections, the methods such as BaseCos which operate in the original word-document space might become computationally infeasible. This insight shows that reducing the dimensionality of the feature space in word representations might lead both to more effective and computationally tractable models of similarity.

We may observe that by combining the TI-MuPTM model and the Cue-MuPTM model, we are able to boost the overall performance. The results of the combined TI+Cue-MuPTM model outperform the results obtained by using any of the component models alone. The combined TI+Cue model displays the best overall performance across all compared models of similarity.

Our models of similarity reach their optimal performances with larger values of K (e.g., around the 2000 topics mark). While the tasks that required only coarse categorizations, such as event-centered news clustering (Section 3) or document classification (Section 4) typically used a lower number of topics (in the [50–300] interval), cross-lingual semantic word similarity and bilingual lexicon extraction require a set of fine-grained latent cross-lingual topics which consequently leads to finer-grained topical representations of words. Based on these results, in all further experiments, we will set 
                                       
                                          K
                                          =
                                          2000
                                       
                                    , unless noted otherwise.

Additionally, it has been noted for both monolingual (Turney & Pantel, 2010) and cross-lingual settings (Peirsman & Padó, 2011) that for distributional models synonymy is not the only semantic relation detected within the (pruned) ranked lists of words. The same is true for our distributional models relying on topical knowledge. For instance, besides direct cross-lingual synonymy, that is, the actual translational equivalence, we observe other semantic relations with words ranked highly in the lists (in top ten candidate words): near-synonymy (e.g., incidente (accident) – crash), antonymy (e.g., guerra (war) – peace), hyponymy (e.g., particella (particle) – boson), hypernymy (e.g., ragazzo (boy) – child), meronymy (e.g., soldato (soldier) – troop), holonymy (e.g., mazzo (deck) – card) and other, uncategorized semantic relations (e.g., vescovo (bishop) – episcopate). The quantitative analysis (as performed in (Peirsman & Padó, 2011)) of the semantic relations detected by the models is beyond the scope of this work and will not be further investigated. Ranked lists of semantically similar words provide comprehensible and useful contextual information in the target language given a source word, even when the correct translation candidate is missing, as might be seen in Table 6
                                    . This finding may be exploited when building information retrieval models with query expansion (Vulić et al., 2013).

In the next set of experiments, we analyze the influence of topic pruning on the behavior of our MuPTM-based models of cross-lingual similarity. All models use output per-topic word distributions from the BiLDA model trained with 
                              
                                 K
                                 =
                                 2000
                              
                            topics. Table 7
                            displays the results over different values for the pruning parameter 
                              
                                 
                                    
                                       K
                                    
                                    
                                       ′
                                    
                                 
                              
                           . For the sake of clear presentation, we omit the results for: (1) the KL model whose behavior resembles the behavior of the JS model, only with lower overall scores, (2) the Cue model where we have not detected any major influence on the overall scores (i.e., the pruning is useful since it reduces execution time, but it does not lead to any improvements in scores), and (3) TI and TI+Cue models which rely on the cosine similarity and whose behavior resembles the behavior of the TCos model. Additionally, Fig. 10
                           a and b display the change in overall scores for JS and BC over different values of 
                              
                                 
                                    
                                       K
                                    
                                    
                                       ′
                                    
                                 
                              
                            along with execution times for all pruned JS and BC models. These time-related experiments were conducted on an Intel(R) Xeon(R) CPU E5-2667 2.9GHz processor. We may notice several interesting phenomena:
                              
                                 (i)
                                 Topic pruning helps to obtain higher results in the BLE task for the JS model (the increase is 7.5% even when probability scores do not sum up to 1) and the BC model (the increase is 21.7%). Using only a small subset of possible features (e.g., 
                                       
                                          
                                             
                                                K
                                             
                                             
                                                ′
                                             
                                          
                                          =
                                          10
                                       
                                    ), we are able to acquire bilingual lexicons of higher quality with these models as reflected in 
                                       
                                          
                                             
                                                Acc
                                             
                                             
                                                1
                                             
                                          
                                       
                                     scores. The improvement in 
                                       
                                          
                                             
                                                Acc
                                             
                                             
                                                10
                                             
                                          
                                       
                                     and MRR scores is also prominent. Moreover, as Fig. 10a and b reveal, the utility of topic pruning is especially visible when we compare execution times needed to retrieve ranked lists for test words. For instance, while the BC model needs 1454.3s to obtain ranked lists when we operate with full K-dimensional representations, the execution time is only 4.2s with 
                                       
                                          
                                             
                                                K
                                             
                                             
                                                ′
                                             
                                          
                                          =
                                          10
                                       
                                    , and we even obtain better results.

The reader might wonder why it is useful to induce a fine-grained latent topic space with a large number of topics, and then to perform pruning of the space afterwards, that is, to select only a small subset of the most informative features to represent words. The answer is as follows: While we desire to have a semantic space in which a large number of linguistic phenomena and topics are covered (a large set 
                                       
                                          Z
                                       
                                    ), only a small subset of these topics is relevant to a particular word (topic space pruning). For instance, although we require that our semantic space is expressive enough to present topics and words related to marine biology or radioactive isotopes, these topics are completely irrelevant when we build a representation for a word playmaker.

We have detected that topic space pruning for similarity models relying on the cosine similarity (TCos, TI, TI+Cue) negatively affects the performance. In the cosine similarity, since the normalization of an inner product is performed (unlike in BC), the absolute weights/probability scores are neglected and the direction (instead of the absolute score) in the semantic space dominates the similarity function (Jones & Furnas, 1987). With a limited number of dimensions, the semantic space is not expressive enough and simply assigns high scores for many irrelevant target language words whose vectors are proportionally similar to the given source language word. For instance, take an extreme case where only one feature is left after pruning. The cosine similarity will assign a perfect similarity score of 1.0 for each target language word regardless of the actual absolute weights, while BC will not produce equal similarity scores for all words.

The best overall results are obtained with the BC model with 
                                       
                                          
                                             
                                                K
                                             
                                             
                                                ′
                                             
                                          
                                          =
                                          100
                                       
                                    , and we may observe a major improvement over the baseline BaseCos model. A similar behavior is observed with the JS model. Moreover, the BC model is also the fastest model of all proposed models (e.g., 26.9s compared to 51.5s of TCos and 155.3s of JS with 
                                       
                                          
                                             
                                                K
                                             
                                             
                                                ′
                                             
                                          
                                          =
                                          100
                                       
                                    ). In summary, by performing topic pruning, we are able to improve our models of cross-lingual similarity both in terms of accuracy and speed.

Finally, we show how to employ both the per-topic word distributions and per-document topic distributions of a multilingual probabilistic topic model together, and how to exploit its inference power on an unseen collection in a language modeling (LM) approach to the task of cross-lingual information retrieval (CLIR). CLIR deals with retrieval of documents that are written in a language different from the language of the user’s query. In a typical CLIR setting, queries (or rarely documents) are translated using a machine-readable dictionary or a machine translation system, and then a myriad of techniques for the monolingual retrieval may be applied (e.g., Ponte & Croft, 1998; Berger & Lafferty, 1999; Lavrenko & Croft, 2001). In case when readily available translation resources are unavailable, multilingual probabilistic topic models may serve as a valid tool to build a CLIR system that does not rely on any external translation resource and can be trained on general-domain non-parallel data (e.g., Wikipedia) and later inferred and used on in-domain data (e.g., news corpora). Therefore, we again deal with the cross-lingual knowledge transfer.

Each target document 
                           
                              
                                 
                                    d
                                 
                                 
                                    j
                                 
                                 
                                    T
                                 
                              
                           
                         can again be presented as a mixture over cross-lingual topics from the set 
                           
                              Z
                           
                         (see Section 2.1) as given by per-document topic distributions (with the values 
                           
                              P
                              
                                 
                                    
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                       |
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                          
                                             T
                                          
                                       
                                    
                                 
                              
                           
                        ). Additionally, the values 
                           
                              P
                              
                                 
                                    
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                          
                                             S
                                          
                                       
                                       |
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                              
                           
                         from per-topic word distributions may be used to calculate the probability that cross-lingual topic 
                           
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                           
                         will generate some source word 
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                                 
                                    S
                                 
                              
                           
                        . If that word 
                           
                              
                                 
                                    w
                                 
                                 
                                    i
                                 
                                 
                                    S
                                 
                              
                           
                         is actually a word from the user’s query written in the source language, the cross-lingual topics again serve as a bridge that links semantics of the query in the source language with semantics of the document written in the target language.

@&#METHODOLOGY@&#

Given a monolingual setting with only one language 
                           
                              
                                 
                                    L
                                 
                                 
                                    S
                                 
                              
                           
                        , the basic approach for using language models in information retrieval is the query likelihood model, where each document is scored by the likelihood of its model 
                           
                              
                                 
                                    d
                                 
                                 
                                    j
                                 
                                 
                                    S
                                 
                              
                           
                         to generate a query 
                           
                              
                                 
                                    Q
                                 
                                 
                                    S
                                 
                              
                           
                         of length 
                           
                              m
                              :
                              P
                              (
                              
                                 
                                    Q
                                 
                                 
                                    S
                                 
                              
                              |
                              
                                 
                                    d
                                 
                                 
                                    j
                                 
                                 
                                    S
                                 
                              
                              )
                              =
                              
                                 
                                    ∏
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    m
                                 
                              
                              P
                              (
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                                 
                                    S
                                 
                              
                              |
                              
                                 
                                    d
                                 
                                 
                                    j
                                 
                                 
                                    S
                                 
                              
                              )
                           
                        , where 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                                 
                                    S
                                 
                              
                           
                         denotes a query term from 
                           
                              
                                 
                                    Q
                                 
                                 
                                    S
                                 
                              
                           
                        . We assume the independence between the query terms, i.e., the unigram language model. Since we here deal with cross-lingual information retrieval, where documents are in the target language 
                           
                              
                                 
                                    L
                                 
                                 
                                    T
                                 
                              
                           
                         and query terms are in the source language 
                           
                              
                                 
                                    L
                                 
                                 
                                    S
                                 
                              
                           
                        , we need to establish the cross-lingual connection between them by means of a multilingual probabilistic topic model. The basic MuPTM-based language model for CLIR (Vulić et al., 2011b) that uses only knowledge from the trained multilingual probabilistic topic model follows these steps:
                           
                              1.
                              Train the model on a (usually general-domain) training corpus and learn per-topic word distributions ϕ and ψ, and per-document topic distributions.

Infer the trained model on the target collection given in the target language 
                                    
                                       
                                          
                                             L
                                          
                                          
                                             T
                                          
                                       
                                    
                                  and obtain per-document topic distributions 
                                    
                                       
                                          
                                             θ
                                          
                                          
                                             T
                                          
                                       
                                    
                                  for all documents in the collection.

For each term 
                                    
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                          
                                             S
                                          
                                       
                                       ∈
                                       
                                          
                                             Q
                                          
                                          
                                             S
                                          
                                       
                                    
                                  do: (a) Obtain probabilities 
                                    
                                       
                                          
                                             ϕ
                                          
                                          
                                             k
                                             ,
                                             i
                                          
                                       
                                       =
                                       P
                                       (
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                          
                                             S
                                          
                                       
                                       |
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                       )
                                    
                                  from per-topic word distributions for 
                                    
                                       
                                          
                                             L
                                          
                                          
                                             S
                                          
                                       
                                    
                                 , for all 
                                    
                                       k
                                       =
                                       1
                                       ,
                                       …
                                       ,
                                       K
                                    
                                 ; (b) Obtain probabilities 
                                    
                                       
                                          
                                             θ
                                          
                                          
                                             j
                                             ,
                                             k
                                          
                                          
                                             T
                                          
                                       
                                       =
                                       P
                                       (
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                       |
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                          
                                             T
                                          
                                       
                                       )
                                    
                                  for a document 
                                    
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                          
                                             T
                                          
                                       
                                    
                                  from the target collection for all 
                                    
                                       k
                                       =
                                       1
                                       ,
                                       …
                                       ,
                                       K
                                    
                                 ; and (c) Combine the probabilities to obtain the final probability that a source term 
                                    
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                          
                                             S
                                          
                                       
                                    
                                  is generated by a document model 
                                    
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                          
                                             T
                                          
                                       
                                    
                                  via the latent layer of cross-lingual topics: 
                                    
                                       
                                          
                                             P
                                          
                                          
                                             muptm
                                          
                                       
                                       (
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                          
                                             S
                                          
                                       
                                       |
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                          
                                             T
                                          
                                       
                                       )
                                       =
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             K
                                          
                                       
                                       P
                                       (
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                          
                                             S
                                          
                                       
                                       |
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                       )
                                       P
                                       (
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                       |
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                          
                                             T
                                          
                                       
                                       )
                                    
                                 .

Compute the final query likelihood for the entire query:
                                    
                                       (24)
                                       
                                          
                                             
                                                P
                                             
                                             
                                                muptm
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         Q
                                                      
                                                      
                                                         S
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         d
                                                      
                                                      
                                                         j
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                
                                             
                                          
                                          =
                                          
                                             
                                                
                                                   ∏
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   m
                                                
                                             
                                          
                                          
                                             
                                                P
                                             
                                             
                                                muptm
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         S
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         d
                                                      
                                                      
                                                         j
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                
                                             
                                          
                                          =
                                          
                                             
                                                
                                                   ∏
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   m
                                                
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                
                                                   K
                                                
                                             
                                          
                                          P
                                          
                                             
                                                
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         S
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         z
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                
                                             
                                          
                                          P
                                          
                                             
                                                
                                                   
                                                      
                                                         z
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         d
                                                      
                                                      
                                                         j
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                
                                             
                                          
                                          =
                                          
                                             
                                                
                                                   ∏
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   m
                                                
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                
                                                   K
                                                
                                             
                                          
                                          
                                             
                                                ϕ
                                             
                                             
                                                k
                                                ,
                                                i
                                             
                                          
                                          
                                             
                                                θ
                                             
                                             
                                                j
                                                ,
                                                k
                                             
                                             
                                                T
                                             
                                          
                                       
                                    
                                 
                              

Eq. (24) provides a query likelihood score for one document, so it has to be repeated for all documents in the collection. Finally, documents are ranked according to their respective query likelihood scores. The intuitive graphical representation of this CLIR technique that connects documents given in target language 
                           
                              
                                 
                                    L
                                 
                                 
                                    T
                                 
                              
                           
                         with query words given in source language 
                           
                              
                                 
                                    L
                                 
                                 
                                    S
                                 
                              
                           
                         is displayed in Fig. 11
                        . There, each target document is represented as a mixture of latent language-independent cross-lingual topics (colored bars that denote per-document topic distributions) and assigns a probability value 
                           
                              P
                              
                                 
                                    
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                       |
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                          
                                             T
                                          
                                       
                                    
                                 
                              
                           
                         for each 
                           
                              
                                 
                                    z
                                 
                                 
                                    k
                                 
                              
                              ∈
                              Z
                           
                         (edges between documents and topics). Moreover, each cross-lingual topic may generate each query word by the probability 
                           
                              P
                              
                                 
                                    
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                          
                                             S
                                          
                                       
                                       |
                                       
                                          
                                             z
                                          
                                          
                                             k
                                          
                                       
                                    
                                 
                              
                           
                         given by per-topic word distributions (edges between topics and query words). We will call this model the MuPTM-Basic model.

This basic model can be efficiently combined with other models that capture additional evidence for estimating 
                           
                              P
                              
                                 
                                    
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                          
                                             S
                                          
                                       
                                       |
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                          
                                             T
                                          
                                       
                                    
                                 
                              
                           
                        . When dealing with monolingual retrieval, Wei and Croft (2006) have detected that their model that relies on knowledge from a probabilistic topic model (e.g., LDA) is too coarse to be used as the only representation for retrieval and to produce quality retrieval results. Therefore, they have linearly combined it with the original document model in the monolingual context and observed a major improvement in their results. We can follow the same principle in the cross-lingual setting, but with limited efficacy, since there is sometimes a minimum word overlap between languages. However, that model proves to be useful for languages from the same family, since, for instance, many named entities do not change across languages.
                           12
                           If the user is searching for the volcano Mauna Loa in Croatian or Dutch, there is a fair chance that relevant documents in English, German or even Hungarian and Finnish (which do not come from the same phylum) may be retrieved since the term Mauna Loa remains unchanged across all these languages.
                        
                        
                           12
                         The MuPTM-Unigram CLIR model combines the representation by means of a multilingual probabilistic topic model with the knowledge of the shared words across languages within the unified language modeling framework with the Jelinek–Mercer and Dirichlet smoothing (Zhai & Lafferty, 2004). The model is as follows:
                           
                              (25)
                              
                                 P
                                 
                                    
                                       
                                          
                                             
                                                q
                                             
                                             
                                                i
                                             
                                             
                                                S
                                             
                                          
                                          |
                                          
                                             
                                                d
                                             
                                             
                                                j
                                             
                                             
                                                T
                                             
                                          
                                       
                                    
                                 
                                 =
                                 λ
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            j
                                                         
                                                         
                                                            T
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            j
                                                         
                                                         
                                                            T
                                                         
                                                      
                                                   
                                                
                                                +
                                                μ
                                             
                                          
                                          
                                             
                                                P
                                             
                                             
                                                mle
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         S
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         d
                                                      
                                                      
                                                         j
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                
                                             
                                          
                                          +
                                          
                                             
                                                
                                                   1
                                                   -
                                                   
                                                      
                                                         
                                                            
                                                               N
                                                            
                                                            
                                                               
                                                                  
                                                                     d
                                                                  
                                                                  
                                                                     j
                                                                  
                                                                  
                                                                     T
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               N
                                                            
                                                            
                                                               
                                                                  
                                                                     d
                                                                  
                                                                  
                                                                     j
                                                                  
                                                                  
                                                                     T
                                                                  
                                                               
                                                            
                                                         
                                                         +
                                                         μ
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                P
                                             
                                             
                                                mle
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         S
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         Coll
                                                      
                                                      
                                                         T
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 +
                                 (
                                 1
                                 -
                                 λ
                                 )
                                 
                                    
                                       P
                                    
                                    
                                       muptm
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                q
                                             
                                             
                                                i
                                             
                                             
                                                S
                                             
                                          
                                          |
                                          
                                             
                                                d
                                             
                                             
                                                j
                                             
                                             
                                                T
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    N
                                 
                                 
                                    
                                       
                                          d
                                       
                                       
                                          j
                                       
                                       
                                          T
                                       
                                    
                                 
                              
                           
                         is the length in words of the document 
                           
                              
                                 
                                    d
                                 
                                 
                                    j
                                 
                                 
                                    T
                                 
                              
                              ,
                              
                                 
                                    P
                                 
                                 
                                    mle
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                          
                                             S
                                          
                                       
                                       ∣
                                       
                                          
                                             d
                                          
                                          
                                             j
                                          
                                          
                                             T
                                          
                                       
                                    
                                 
                              
                           
                         is the maximum likelihood estimate of the source query term 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                                 
                                    S
                                 
                              
                           
                         in the target document 
                           
                              
                                 
                                    d
                                 
                                 
                                    j
                                 
                                 
                                    T
                                 
                              
                              ,
                              
                                 
                                    P
                                 
                                 
                                    mle
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                          
                                             S
                                          
                                       
                                       ∣
                                       
                                          
                                             Coll
                                          
                                          
                                             T
                                          
                                       
                                    
                                 
                              
                           
                         is the maximum likelihood estimate of 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                                 
                                    S
                                 
                              
                           
                         in the entire target collection, μ is the Dirichlet coefficient (Zhai & Lafferty, 2004), λ is the interpolation parameter, and 
                           
                              
                                 
                                    P
                                 
                                 
                                    muptm
                                 
                              
                              (
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                                 
                                    S
                                 
                              
                              ∣
                              
                                 
                                    d
                                 
                                 
                                    j
                                 
                                 
                                    T
                                 
                              
                              )
                           
                         is given by Eq. (24).

In this overview, we only report results obtained by these two basic MuPTM-based CLIR models. The language modeling framework for IR allows combining more various evidences in the query likelihood model. The framework is also topic model-independent and it allows experimentations and comparisons of different multilingual probabilistic topic models. More complex LM CLIR models that rely on the representation by means of multilingual topic models are described and evaluated by Vulić et al. (2013), and we refer the interested reader to that article. For instance, there we use bilingual MuPTM-based probabilistic dictionaries obtained by the methods from Section 5 instead of the shared words across languages and perform query expansion. The LM framework also allows blending knowledge from the external resources (e.g., machine-readable dictionaries) with the topical representation, but that research is beyond the scope of this work.

For training, we use English–Dutch Wikipedia articles, but, to reduce data sparsity, we augment that dataset with 6206 Europarl (Koehn, 2005) English–Dutch document pairs.
                              13
                              We do not take advantage of the structure of the parallel documents and do not make us of the sentence-level alignments to improve the scores. We use the exact same approach that we apply for the Wikipedia documents and presuppose only document alignment before training.
                           
                           
                              13
                            We train the BiLDA model with 
                              
                                 K
                                 =
                                 400
                              
                           , 1000, and 2200 topics.

Experiments were conducted on three test datasets taken from the CLEF 2001–2003 CLIR campaigns: the LA Times 1994 (LAT), the LA Times 1994 plus the Glasgow Herald 1995 (LAT
                           +
                           GH) in English, and the NRC Handelsblad 1994–1995 plus the Algemeen Dagblad 1994–1995 (NC
                           +
                           AD) in Dutch. Queries were extracted from the title and description fields of all CLEF themes for each year and queries without relevant documents were removed from the query sets. The overall statistics are provided in Table 8
                           a and b (Vulić et al., 2013). We set 
                              
                                 λ
                                 =
                                 0.3
                              
                            to assign more weight to topic models, and 
                              
                                 μ
                                 =
                                 2000
                              
                           .

The performance of the CLIR models is reported in the mean average precision (MAP) scores and/or 11-pt precision-recall diagrams for all experiments.

@&#RESULTS AND DISCUSSION@&#


                        Table 9
                        a shows the perplexity scores after the inference of the BiLDA model on the CLEF test collections, while Table 9b displays the MAP scores for all campaigns in both retrieval directions (i.e., English queries – Dutch documents, and vice versa) for the two models presented in Section 6.2. We can observe that, similar to the monolingual setting, using only topical representation seems to be too coarse to produce quality retrieval models. The topical knowledge is, however, useful as an extra portion of information that can be easily embedded within the language modeling framework. The synergy between different evidences when performing retrieval leads to better unsupervised models. Due to a high percentage of shared words between English and Dutch, there is a possibility that the MuPTM-Unigram model draws its performance mainly from the part specified by the “non-MuPTM” part of the model (see Eq. (25)). This model called Unigram-Basic which can be obtained by setting 
                           
                              λ
                              =
                              1
                           
                         in Eq. (25) does not use any topical knowledge and relies only on the shared words. However, Fig. 12
                        a and b clearly show that the final combined MuPTM-Unigram model clearly works as a positive synergy between the two simpler basic models, outperforming both of them. However, MuPTM-Basic provides higher scores than Unigram-Basic and is therefore more important for the overall performance of the combined model. As in Sections 4 and 5, we have also experimented with monolingual LDA trained on merged document pairs (MixLDA), but the reported retrieval results with the MoPTM-Basic retrieval model are in the range [0.01–0.03] in terms of MAP scores which is extremely lower than the MAP scores reported for our MuPTM-Basic model in Table 9b.

The reported results also show the potential of training the multilingual probabilistic topic model on huge-volume out-of-domain data and its inference and usage on another collection, which is not necessarily completely theme-aligned to the training corpus. It is also again difficult to predict the “sweet spot” for K for which the best retrieval results are expected.

Since the goal in this section is to give the reader a basic insight on how to utilize per-topic word distributions and per-document topic distributions of a multilingual probabilistic topic model in CLIR, we do not provide a thorough discussion here. That, along with many more experiments and comparisons (e.g., a comparison where it was shown that these results with MuPTM-s are comparable to results of the models which rely on translation tools such as Google Translate) might again be found in (Vulić et al., 2013), and we encourage the interested reader to study that article in more detail. One important aspect lacking in that article is the reported mismatch between intrinsic perplexity scores and extrinsic evaluation results, that is, MAP scores for CLIR. The comparison of the results in Table 9a and b clearly shows that the theoretical in vitro measure of perplexity, often used to compare the quality of probabilistic topic models, does not guarantee a better in vivo performance in actual applications such as CLIR. The same general conclusion for language models in information retrieval is also drawn by Azzopardi, Girolami, and van Rijsbergen (2003). Additionally, these findings strengthen the claims from Section 4.4 where we have also stated that better perplexity scores of a probabilistic topic model do not necessarily reflect in a better “real-life” classification task performance.

In this article, we have conducted the first systematic and thorough overview of the current advances in multilingual probabilistic topic modeling, with a special focus on unsupervised learning from non-parallel data easily obtainable from the Web, and cross-lingual knowledge transfer across corpora written in different languages by means of the multilingual probabilistic topic models. We have provided precise formal definitions of this modeling concept and drew analogies with its monolingual variants and a broader concept of inducing latent cross-lingual concepts from multilingual data. Additionally, we have described the importance of obtaining a shared latent topical space in the form of latent cross-lingual topics. Multilingual probabilistic topic models which induce such a language-independent cross-lingual topical space in general comprise two sets of probability distributions: (1) per-document topic distributions that define topic importance in a document and (2) per-topic word distributions that define importance of vocabulary words in each language for each cross-lingual topic. As a case study, we have shown how to utilize these sets of distributions by providing a short survey of various cross-lingual tasks and the utility of the distributions in these tasks. An insight into these applications reveals several interesting phenomena.

In this article, we have shown that monolingual probabilistic topic models are only a special, degenerate case of multilingual topic models (e.g., LDA is only a special case of BiLDA which operates with only one language). As a consequence, all frameworks presented in this paper which tackle cross-lingual tasks are easily adapted to and functional in the monolingual settings.

A standard approach to induce topical knowledge when handling cross-lingual tasks in cases when an external translation resource is absent is to train a monolingual topic model such as LDA on merged/concatenated documents from an aligned document pair with documents in two different languages). However, our comparisons across different cross-lingual tasks (see Sections 3–6) clearly indicate that training a multilingual topic model on separate documents from a document pair instead of a monolingual topic model on the “merged” artificially created document leads to significantly higher (sometimes even to extremely higher) scores in all these applications.

We also observe that the optimal setting of a priori parameters (e.g., the number of topics) is heavily application-dependent, and it is not apparent how to detect the “sweet spot”, that is, the optimal number of topics K. The tasks that require only coarse categorizations, such as event-centered news clustering or document classification typically also require coarse-grained text representations, and use a lower number of topics. It seems that such coarse semantic representations are sufficient to successfully learn the correct category matchings. In Section 4, we have also shown that the technique of topic smoothing which combines representations obtained by different K-s may lead to a more robust final model. On the other hand, tasks like cross-lingual semantic similarity of words and information retrieval needed finer representations, and, even with the high number of topics topical representations lack sufficient detail.

Another advantage of multilingual probabilistic modeling lies in the fact that their output sets of distributions implicitly provide dimensionality reduction similar to other latent semantic models (e.g., LSA). By learning per-document topic distributions, the document representation is translated from the original high-dimensional word-document space to a lower-dimensional topic-document space. In a similar fashion, by learning per-topic word distributions, the word representation is effectively translated from the original high-dimensional word-document space to a lower-dimensional word-topic space. In Section 5, where a complete framework for modeling cross-lingual semantic similarity by means of MuPTM has been introduced and described, we have also introduced the paradigm of topic pruning, which selects only a subset of highly relevant topics to represent a word or a document. By selecting only a subset of reduced features/topics, we are able to further decrease the dimensionality of the representation and obtain such pruned representations, which lead to final improvements in both the quality of results and the speed of computations.

The results across these applications also reveal that the lower-dimensional representations (i.e., word-topic space and topic-document space) of the original word-document space alone might not be discriminative enough for the tasks that require fine granularity matchings (e.g., cross-lingual information retrieval in Section 6). However, it seems that combining the latent semantic lower-dimensional representations with the original higher-dimensional representations leads to more effective and robust models. For instance, the MuPTM-Unigram model that combines unigrams shared across languages with topical knowledge and topical representation of documents leads to much better retrieval scores than MuPTM-Basic that uses only topical knowledge as the only representation for retrieval.

Finally, since all topic models in this article have been trained on comparable Wikipedia data, we have also implicitly shown the validity of training on such high-volume easily obtainable comparable datasets in a wide spectrum of NLP/IR tasks. The presented MuPTM framework is unsupervised and language pair independent in its design (since it does not rely on any external translation resource and induces knowledge directly from the given multilingual data). Consequently, that makes it potentially applicable to many language pairs.

We have demonstrated how to make use of the output per-document topic distributions and per-topic word distributions in four fundamental cross-lingual tasks. Besides cross-lingual event-centered news clustering (which is only a special case of cross-lingual document clustering), cross-lingual document classification, cross-lingual semantic similarity and cross-lingual information retrieval that have been tackled and perused here, MuPTM has been applied to a series of other NLP/IR cross-lingual tasks which we briefly list here:
                           
                              •
                              
                                 Cross-lingual keyword recommendation (Takasu, 2010). Both the text and the keywords are mapped into the same latent cross-lingual topical space (induced from parallel data), and a ranked list of keywords is provided based on the learned per-topic word distributions and per-document topic distributions.


                                 Cross-lingual document matching (Platt et al., 2010; Zhu, Li, Chen, & Yang, 2013). The goal of the task is to retrieve the most similar document in the target language given a document in the source language. The knowledge of tightly coupled documents may be used to build high quality aligned multilingual datasets. Document representations by means of per-document topic distributions are again used to provide language independent representations of documents and allow their comparison in the shared latent cross-lingual space.


                                 Cross-lingual sentiment analysis (Boyd-Graber & Resnik, 2010). A multilingual topic model is used to discover a consistent, unified picture of sentiment across multiple languages.


                                 Transliteration mining (Richardson, Nakazawa, & Kurohashi, 2013). A framework for cross-lingual semantic similarity described in Section 5 (i.e., the knowledge from per-topic word distributions) has been used to provide information about semantic similarity between potential transliteration pairs.


                                 Cross-lingual entity linking (Zhang, Liu, & Zhao, 2013). Given an entity mention, the goal of the task is to link the mention of an entity to some given knowledge base (e.g., Wikipedia). The output of the BiLDA model is again used to compute the similarity between the context of the entity mention and the appropriate Wikipedia page to which the mention should be linked.


                                 Cross-lingual word sense disambiguation (Tan & Bond, 2013). Given a sentence along with a polysemous word, the goal is to provide a correct sense for the polysemous word. A multilingual topic model may be used in this task to match the query sentence to a list of sentences in the other language based on the most probable topics of these sentences. The correct sense is then extracted from the topically most similar sentence in the other language.


                                 Bilingual lexicon extraction (Vulić & Moens, 2012; Liu, Duh, & Matsumoto, 2013; Chu, Nakazawa, & Kurohashi, 2013). Models of MuPTM-based semantic similarity from Section 5 and the knowledge coming from language-specific per-topic word distributions may be used in various ways to extract word translation pairs from multilingual data.


                                 More advanced CLIR models (Ganguly et al., 2012; Vulić & Moens, 2013). The topical knowledge obtained by a multilingual topic model may be embedded into the relevance modeling framework for cross-lingual information retrieval. This combination results in more effective and more robust models of cross-lingual IR.

All these tasks can be accomplished by means of the MuPTM-based representations of words and documents, that is, by the output per-document topic distributions and the per-topic word distributions.

@&#FUTURE WORK@&#

A straightforward line of future work leads to investigating more applications of the MuPTM framework (e.g., cross-lingual document summarization, keyword and keyphrase extraction).

In order to improve the quality of the lower-dimensional topical representations of documents in the multilingual domain, there is a huge number of paths that could be followed. In the same manner as for the natural “LDA to BiLDA” extension, other more sophisticated and application-oriented probabilistic topic models developed for the monolingual setting could be ported into the multilingual setting (e.g., Wallach, 2006; Blei & McAuliffe, 2007; Gormley, Dredze, Durme, & Eisner, 2012). These extensions include, among others, the use of multi-word expressions and collocations along with single words (Wallach, 2006), the use of sentence information or word ordering (using Hidden Markov Models) to yield more coherent topic distributions over documents (e.g., Griffiths, Steyvers, Blei, & Tenenbaum, 2004; Boyd-Graber & Blei, 2008). The use of hierarchical topics (general super-topics connected with more focused sub-topics, see, e.g., Blei, Griffiths, Jordan, & Tenenbaum, 2003a; Mimno, Li, & McCallum, 2007) is another interesting field of research in the multilingual setting. Moreover, there is a need to develop multilingual probabilistic topic models that fit data which is less comparable and more divergent and unstructured than Wikipedia or news stories, where only a subset of latent cross-lingual topics overlaps across documents written in different languages. Additionally, the more data-driven topic models should be able to learn the optimal number of topics dynamically according to the properties of training data itself (the so-called non-parametrized models, Li, Blei, & McCallum, 2007; Zavitsanos, Paliouras, & Vouros, 2011), and clearly distinguish between shared and non-shared topics in a multilingual corpus.

Additionally, besides being a multilingual environment, the Web and the world of information are also locales for multiple idioms of the same language. For instance, the “language” of the social media consumers or typical end-users differs from the language of Wikipedia entries, online shops or legal terminology. Different domains also display different usage of language. Therefore, one line of future research also lies in studying and applying the models initially tailored for the multilingual setting within this multi-idiomatic setting, or building new multi-idiomatic topic models. The first step in that direction has already been made as it was proven that the topical knowledge combined with the (CL) IR framework from Section 6 is extremely useful in a new task of linking Pinterest users to relevant online shops based on the content the users post on their personal pages (Zoghbi, Vulić, & Moens, 2013b, 2013a; Vulić, Zoghbi, & Moens, 2014).

Finally, as the probabilistic topic models have proven to work in the multilingual settings with comparable corpora, we feel that it is time to expand the current research and carry it into the multimodal domain. Since there exists a significant semantic gap between the “visual words” and textual words, the comparability in the multimodal setting is inherent, and the multilingual probabilistic topic models that operate with comparable data should be transferred and adapted to the multimodal setting, with some preliminary steps already made in that direction (Feng & Lapata, 2010; Bruni, Uijlings, Baroni, & Sebe, 2012; Roller & Schulteim Walde, 2013). These multimodal models should be capable of dealing with the inherent comparable nature of any dataset consisting of text and images or video. However, some initial studies have revealed that the multimodal context serves as a more complex setting, and additional expansions of the multimodal topic models are needed to effectively handle the existing gap between different modalities.

@&#ACKNOWLEDGMENTS@&#

The research presented in this article has been carried out in the framework of several research projects. It has been partially supported by the following projects: CLASS (EU FP6-027978) financed by the EU Sixth Framework Programme ICT, AMASS++ (SBO-060051) financed by Instituut voor de Aanmoediging van Innovatie door Wetenschap en Technologie in Vlaanderen (IWT), WebInsight (BIL/08/08), and TermWise (IOF-KP/09/001) financed by the Flemish government.

@&#REFERENCES@&#

