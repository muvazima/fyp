@&#MAIN-TITLE@&#RELOAD extension for data discovery and transfer in data-centric publish–subscribe environments

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a novel extension to the RELOAD protocol specification.


                        
                        
                           
                           This extension adds Data-Centric Publish–Subscribe support to RELOAD.


                        
                        
                           
                           We cover multiple Internet of Things use-cases.


                        
                        
                           
                           We demonstrate our proposal's scalability by using overlays with 500 to 10,000 nodes.


                        
                        
                           
                           We demonstrate our proposal's robustness in scenarios with very high levels of churn.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Data-centric

Publish–subscribe

Discovery

Rendezvous

Reload

DDS

@&#ABSTRACT@&#


               
               
                  Data-Centric Publish–Subscribe (DCPS) is an architectural and communication paradigm where applications exchange data-content through a common data-space using publish-subscribe interactions. Due to its focus on data-content, DCPS is especially suitable for deploying IoT systems. However, some problems must be solved to support large deployments. In this paper we define a novel extension to the IETF REsource LOcation And Discovery (RELOAD) protocol specification for providing content discovery and transfer in big scale IoT deployments. We have conducted a set of experiments over multiple simulated networks of 500 to 10,000 nodes that demonstrate the viability, scalability, and robustness of our proposal.
               
            

@&#INTRODUCTION@&#

The Internet of Things (IoT) [1] has been gaining momentum in the last years. The reduction of technology costs and the great market penetration of handsets have allowed people to stay permanently connected to the Internet. In this context–where people already make an extensive use of the Internet–the next step is to connect every device that generates relevant information to the Cloud [2], such that any object becomes smart and accessible.

In this new approach, users and applications will be able to interact among themselves and their intelligent environment. For instance, users will receive alerts about expired food from their fridge, or will check whether their garage door is closed; it will even be possible for coffee machines to automatically prepare the coffee in the morning when the alarm goes off.

The core of IoT is the data-content shared by non-traditional-Internet devices, such as appliances, doors, alarm-clocks, cars, etc. These devices can generate a lot of data-content–e.g., temperature updates, device status reports, car metrics, and many others–hence, IoT deployments demand efficient and robust communications with special scalability support.

IoT scenarios are especially suited for taking advantage of Data-Centric Publish–Subscribe (DCPS) approaches. DCPS is a programming model that is not focused on data location, but on the data-content itself instead.

The DCPS model is based on the following concepts:
                        
                           •
                           
                              Data-Space: An aggregation of data of different types and sources. More precisely, a data-space is a distributed cache, whose content is accessed and updated by subscribers and publishers.


                              Topic: A portion of a data-space that groups data of the same type. Information contained in a topic is accessed and updated by subscribers and publishers.


                              Publisher: An entity that pushes data-content updates to a topic.


                              Subscriber: An entity that takes data-content updates from a topic.


                              Data-Participant: An application that interacts with multiple topics in a data-space. A data-participant accesses to topics' information by means of publishers and subscribers.

Publishers and subscribers access to data-content in a DCPS basis. That is, instead of requesting for a resource location, they create a publication (or a subscription) to a segment of a data-space (i.e., a topic). Topics are decoupled spatially, and therefore subscribers can access to topic's content without a prior knowledge of publishers' location.

Another advantage of DCPS systems is that they are not data-agnostic, on the contrary DCPS allows for performing advanced operations over data-content, such as content-based filtering, data transformation, and data composition.

However, prior to sharing data, publishers and subscribers must discover each other (i.e., a rendezvous function is needed). This rendezvous function has to support multiple publishers and subscribers sharing a particular topic from several different locations.

A simple solution for small LAN deployments is to use multicast for discovering publishers and subscribers. However, this solution does not scale well and is not suitable for environments with NATs and firewalls.

Another possible solution is to set up a server for keeping all the rendezvous information. The use of a properly configured server avoids the NAT traversal problem for discovery, but does not scale in large-scale deployments, does not solve NAT for information exchange, and introduces a single point of failure.

A third solution is to distribute the discovery information among the members of a P2P overlay. P2P deployments are highly scalable because they distribute computing and traffic load among all the members of the overlay. In addition, some P2P solutions support advanced functionalities like NAT traversal. One of these solutions is REsource LOcation And Discovery (RELOAD) [3], an IETF specification for building and maintaining P2P systems on the Internet. RELOAD provides an abstract overlay service that can be extended to support specific application usages.

In this paper we propose a scalable RELOAD-based solution to the discovery problem in DCPS environments. Our proposal demonstrates the extensibility of RELOAD by proposing a RELOAD usage that adapts it for performing rendezvous in DCPS-based deployments. We define the Resource Naming Format, the data types for storing the rendezvous information, and the interactions among the entities performing the rendezvous.

Once two particular nodes complete the rendezvous, they can establish a direct connection by means of existing RELOAD mechanisms. Using this connection, nodes can exchange IoT data-content using a publish–subscribe protocol. For example, they can use a protocol for constrained nodes, such as Constrained Application Protocol (CoAP) [4]; or a DCPS middleware, such as the OMG Data Distribution Service (DDS) [5].

In order to demonstrate the scalability of our solution, we have implemented a proof of concept prototype and conducted a set of experiments over multiple simulated networks of 500 to 10,000 nodes. Our results show that the average and 95 percentile latency for storing and retrieving the discovery information of a DCPS follow a logarithmic growth as a function of network size. In addition, we also report on the lookup fail rate when the overlay is suffering a particular churn rate. We found that even for very unstable systems topologies (up to 2 joins and 2 leaves per second), our system provides a percentage of failures below 19%, which demonstrates the robustness of our proposal.

The remainder of the paper is organized as follows. In Section 2 we provide a basic RELOAD background. In Section 3 we present the use cases and we identify the limitations of RELOAD we need to overcome. In Sections 4 and 5 we study the system design and operation. In Sections 6 and 7 we address the experimental setup and the conducted experiments, and we analyze the obtained results. Finally, in Section 8 we discuss related work, and in Section 9 we summarize the main conclusions of our work.

RELOAD [3] is an IETF protocol for building and maintaining P2P systems on the Internet. It provides a generic, self-organizing overlay network service, allowing nodes to efficiently route messages to other nodes and to efficiently store and retrieve data in the overlay.

RELOAD provides several features that are critical for a successful P2P protocol for the Internet [3]:
                        
                           –
                           
                              Security framework: P2P networks are often established among untrusted peers. RELOAD leverages a central enrollment server to provide credentials for each peer which can then be used to authenticate each operation. These credentials consist of a certificate assigned to each node that joins the overlay.


                              NAT Traversal: RELOAD is designed to function in environments where many (if not most) of the nodes are behind NATs or firewalls. RELOAD uses Interactive Connectivity Establishment (ICE) [6] for providing NAT traversal functionality.


                              High Performance Routing: RELOAD has been defined with a simple, light-weight forwarding header, thus minimizing the amount of effort required by intermediate peers.


                              Pluggable Overlay Algorithms: For the sake of extensibility, RELOAD defines an abstract interface to the overlay layer to simplify implementing a variety of structured and unstructured overlay algorithms. Additionally, RELOAD uses a Chord-based Distributed Hash Table (DHT) [7,8] as its default overlay algorithm.


                              Usage Model: RELOAD is designed to support a variety of (existent and future) applications. RELOAD allows the definition of new application usages, each of which can define its own data types, along with the rules for their use. In RELOAD, Kind is the definition of a data type and its accessing rules. Therefore, new usages must define the Kinds they store in the RELOAD overlay.

These properties allow RELOAD to adapt to different scenarios, while providing efficient and secure resource discovery.

A RELOAD overlay instance consists of a set of nodes arranged in a partly connected graph. Each node in the overlay is assigned a numeric Node-ID for the lifetime of the node which determines–together with the specific overlay algorithm in use–its position in the graph and the set of nodes it connects to. The Node-ID is also tightly coupled to a certificate.

RELOAD defines two different types of nodes:
                           
                              –
                              
                                 Peer: A host that is participating in the overlay. Peers are responsible for holding some portion of the data that has been stored in the overlay and also route messages on behalf of other hosts as required by the overlay algorithm.


                                 Client: A host that is able to store data in and retrieve data from the overlay but does not perform routing or data storage for the overlay.

As a P2P protocol, RELOAD defines a set of operations for joining, leaving, and maintaining the overlay. There are also operations for storing and retrieving data.

In addition, RELOAD defines two new operations for interacting with other overlay nodes: Attach and AppAttach.

Attach allows nodes to establish a direct TCP or UDP connection to other overlay nodes. This connection is a direct channel between two Nodes exchanging RELOAD messages.

AppAttach is similar to Attach, but in this case nodes use the generated connection for exchanging application data (and not RELOAD traffic). In this sense, one of the parameters of AppAttach is the Application-ID, which identifies connection's target application.

RELOAD nodes send both Attach and AppAttach requests using the destination Node-ID (and not the IP address). In this sense, RELOAD allows nodes for establishing direct connections to any member of the overlay. This is feasible even if the target node is behind a NAT.

RELOAD's original usage was focused in providing a rendezvous service for SIP (Session Initiation Protocol). In this way, communicating peers can use RELOAD for establishing a connection, and then they can perform the usual SIP negotiation [9].

There are other RELOAD usages like [10], which allows federating different SIP domains; or [11], which enables for managing the RELOAD overlay using SNMP. However, there is still no usage for supporting DCPS-based systems.

RELOAD, as it is, lacks several features for supporting DCPS requirements. In this section we present a set of motivating use cases for DCPS-based IoT scenarios (each use case associated to a different set of DCPS entities) and we identify a set of features currently unsupported by RELOAD.

In this use case, a user is able to obtain information of his “user data-space”, which is the set of data related to him.

A typical user data-space contains information about user's vitals, car position, gas remaining at his car, status of his fridge, and status of user's house doors. All of these information are relevant to a particular user, and should be accessible only by him.

This use case will allow a user to discover and subscribe to any piece of information within his data-space. Consequently, it requires the user's application to discover both the publishers and the subscribers associated to a certain data-space, without restrictions on the types of exchanged data-content.

In this scenario, a constant number of stock-share publishers publish information of share prices to a data-space. A variable number of subscribers (users who want to obtain the latest share values) subscribe to this information.

This scenario requires subscribers to be able to discover stock-share publishers associated to a certain market. Once discovered, the subscriber is able to subscribe to the publishers of interest.

In this use case, a set of temperature sensors publishes temperature values to a data-space. These values must reach a temperature monitor that is subscribed to the temperature updates. A temperature monitor is a node that gathers and stores temperature values and may generate alerts. These alerts could be used for triggering automatic responses or they can just generate a notification to a human operator. A temperature monitor node is always available, but new temperature-publishers may be added later.

This scenario requires the temperature monitor to be able to subscribe to temperature sensors, even if they are not yet active. In this sense, once a new temperature sensor joins the system, it will discover the temperature monitor that is subscribed to its temperature updates.

In this use case, an Air Traffic Controller (ATC) receives the information of planes flying in his Flight Information Region (FIR). As planes enter and exit the FIR, their associate publishers (in this example, with information about the sensors in the plane) join and leave respectively the FIR data-space. Additionally, an authorized ATC could join later to the data-space, so the system should deliver the existing plane information to this new ATC.

This use case requires both publishers (planes) and subscribers (ATCs) to be able to discover already existing data-space participants.

As an additional constraint, the information about existing publishers (planes) should be visible only to the authorized ATCs. Similarly, information about subscribers should be only visible to allowed publishers.

In all of the existing RELOAD usages (see Section 2.3), the system was user- or service-centric. That is, it stores the location of a particular user agent or of a peer providing a service.

However, DCPS systems–and specifically the use cases described in the previous section–require the following functionalities not currently supported by RELOAD:
                           
                              –
                              
                                 Discovery of multiple publishers updating a particular topic. This feature will allow subscribers to perform rendezvous with compatible publishers.


                                 Discovery of multiple subscribers interested in a particular topic. This feature will allow publishers to locate subscribers interested in their data updates.


                                 Discovery of multiple data-participants associated to a certain data-space. This feature will allow applications to locate other applications associated to a particular data-space (i.e., to a collection of topics).

In the next sections we propose a new RELOAD usage that provides this functionality.

In this section we study the characteristics of the proposed architecture and the adopted design decisions.

We have selected RELOAD for performing the rendezvous among DCPS entities because it provides high scalability, robustness, and NAT traversal. High scalability is a requirement, as our system must be suitable for large-scale deployments. NAT traversal is also necessary, as it allows administrators to deploy our architecture in WAN environments that include NATs and firewalls.

Robustness is also a main concern, as we assume that nodes in the overlay can fail, and a single point of failure is not admissible.


                        Fig. 1
                         depicts the proposed layer architecture, which is divided in three layers: RELOAD, Discovery-and-Data, and DCPS application. This architecture adopts a layered design based on the following motivations. First, it increases system modularity, by allowing the specific implementation of each layer to be changed without impacting the rest of the layers. Second, it allows existing applications to remain independent of new proposed entities while benefiting from the system. And third, it simplifies the overall understanding of the system, because it classifies entities of a different functionality in different layers.

The bottom layer is named RELOAD, and contains RELOAD nodes (peers and clients). In order to identify these nodes, each node has a unique Node-ID, obtained during the process of joining the overlay (see Section 5.1). Node-ID allows RELOAD to locate and deliver messages to any member of the overlay infrastructure.

On top of RELOAD we define the Discovery-and-Data layer. This layer enables publishers, subscribers, and data-participants at the DCPS Application layer to perform rendezvous and to exchange data-content. Discovery-and-Data layer is populated by D-NODES and S-NODES, both defined in Section 4.2. As we will see in Section 4.3, these nodes form groups according to the DCPS Application layer entities that they host.

Finally, the DCPS Application layer contains the DCPS entities (publishers, subscribers, and data-participants) that exchange data-content using a DCPS approach.

As part of our design, we propose a new RELOAD usage that is adapted to the unique requirements of DCPS systems. This new DCPS usage for RELOAD consists of three different aspects:
                           
                              1.
                              The creation of new Kinds that respond to DCPS system requirements, proposed in Section 4.4.

The specification of a format for addressing data-content, described in Section 4.5.

The description of the usage operation, detailed in Section 5.

Since our proposal is data-centric, nodes in the overlay are relevant for the data that they share with the system. In this way, the relation between nodes and overlay is based on the data that they publish or subscribe.

As a result, nodes should be discoverable only if they have some piece of data to share. However, all the nodes that are sharing information do not necessarily have to be discoverable (see use cases in Sections 3.1.2 and 3.1.3).

Note that with “discoverable” we mean that a given node is available for rendezvous at the application level. Therefore, this has no impact on maintenance or routing operations of the underlying layers.

Our proposal defines two different node roles for the Discovery-and-Data-layer. These roles cover two different relations between nodes and the overlay.
                           
                              –
                              
                                 Discoverable Node (D-NODE): A member of the overlay that is visible to other nodes. It can act as subscriber, publisher, or data-participant. Since its rendezvous information is available to the nodes of certain DCPS-Entity-Group (see Section 4.3), those nodes can dialog with it.


                                 Spectator Node (S-NODE): A member of the overlay that does not share its presence and it is not discoverable. Consequently, S-NODEs can access D-NODEs' discovery information, but their own information is not visible to other nodes. S-NODEs may eventually become D-NODEs (and vice versa).

These roles are independent of RELOAD node types: clients and peers can both act as D-NODEs or as S-NODEs. These roles are also independent of the hosted DCPS entities: D-NODEs and S-NODEs can both host publishers, subscribers, and data-participants.

In order to univocally identify content using a DCPS approach we propose the following two concepts:
                           
                              –
                              
                                 FQDN (Full Qualified Data-Space Name): An identifier that uniquely identifies a particular data-space.


                                 FQTN (Full Qualified Topic Name): An identifier that uniquely identifies a data topic. Since a topic is a segment of a data-space, we decided that FQTN should explicitly contain a FQDN, as this will provide a greater flexibility when building identifiers.

We can group DCPS entities using two characteristics: their kind (i.e., participant, publisher, or subscriber), and the data-segment they access (i.e., a data-space or a particular topic). This classification is relevant for our system, since entities of the same kind and data-segment will be usually interested in discovering the same set of DCPS entities.

In order to identify groups of nodes with the same interests we have defined the DCPS-Entity-Group. It is an identifier shared among a group of nodes that host a particular group of DCPS entities. Specifically, each group contains either a set of publishers of the same topic, a set of subscribers of the same topic, or a set of participants of the same data-space.

We also propose the concept of DCPS-Entity-Name for identifying DCPS entities. As we will study in Section 4.5, DCPS-Entity-Name is a URI that univocally identifies a participant, publisher, or subscriber within a scope. Consequently, it can include a meaningful name that helps human administrators to identify the associated entity (e.g., ../temperature-sensor01).
                     

S-NODEs can turn into D-NODEs (and vice versa). Once an S-NODE turns into a D-NODE, it becomes discoverable by a particular set of nodes.

Specifically, for dealing with the use cases in Section 3.1, we identify three different discovery types:
                           
                              –
                              
                                 Topic publisher discovery: This is the procedure of discovering all the nodes associated to a certain topic as publishers (i.e., those nodes that publish data of a certain type).


                                 Topic subscriber discovery: This is the procedure of discovering all the nodes interested in a certain topic (i.e., those nodes subscribed to data of a certain type).


                                 Data-Space Discovery: This is the procedure of discovering nodes running applications that interact with a certain data-space. The discovered nodes can contain multiple publishers and/or multiple subscribers of different data types.

In our system, the conversion between S-NODE and D-NODE is performed through a DataRegistration.

We use DataRegistration for enabling discovery and for starting a publication or a subscription. A DataRegistration is a RELOAD store request of a specific RELOAD Kind (see Section 2).

In the following sections we propose three new RELOAD Kinds that constitute a new RELOAD Usage for DCPS systems. These new Kinds cover the three different discovery types we have described above.

Topic publisher discovery and topic subscriber discovery are respectively the procedures of locating publishers and subscribers of a particular topic. In our system, these discovery types are accomplished by means of TopicPublisher and TopicSubscriber Registrations.

TopicPublisher Registration is a new RELOAD Kind for storing lists of publishers. Specifically, each list contains all the nodes declared as publishers of a particular topic. Any node that is able to access to a particular TopicPublisher Registration will also be able to perform rendezvous with the publishing nodes contained in that Registration. In this sense, a node that creates a TopicPublisher Registration entry is implicitly allowing a group of subscribers to subscribe to the publishers associated to that node.

TopicSubscriber Registration is a new RELOAD Kind for storing lists of subscribers. A node that registers into a TopicSubscriber Registration is implicitly creating a subscription to a topic, and therefore is allowing topic publishers to deliver topic updates to the subscribers associated to that node.

One of the benefits of this approach is that nodes can create an entry to a particular TopicSubscriber Registration even if there is no active publisher for the associated topic. In this way, as soon as a publisher joins the overlay, it can perform rendezvous (and start sending topic updates) using the TopicSubscriber Registration information.

We have adopted RELOAD dictionary [3] (see Section 2) as the data model for storing TopicPublisher and TopicSubscriber Registrations. The dictionary model allows for univocally identifying each list entry with a key. In our design, the dictionary key for an entry is the Node-ID of the storing node. This characteristic avoids the duplication of entries, as each node can have at most one entry stored for a particular TopicPublisher (or TopicSubscriber) Registration.

Regarding content addressing, each dictionary record is univocally identified by a Resource-ID. This Resource-ID identifies the topic and type (publisher or subscriber) of the DCPS entities stored in the dictionary. We will explain how the Resource-ID is calculated in Section 4.5.

As we have explained above, each node can store at most one entry per dictionary record. However, there are cases where a given node contains multiple DCPS entities of the same type and data-segment (e.g., a node that contains multiple temperature publishers). In order to univocally identify the entities associated to a particular node, each dictionary entry contains a list of DCPS-Entity-Names, each one associated to an entity.

In order to preserve the integrity and authenticity of the information, the overlay processes a given store request only if the storing node's certificate has an adequate Node-ID (i.e., the same as the one used as a storing key), and if the certificate contains a DCPS-Entity-Group that matches with the Resource-ID of the store request.

If the application requires privacy, nodes can encrypt stored data by using a key associated to the DCPS-Entity-Group.

In this way, only nodes belonging to the right DCPS-Entity-Group will be able to successfully read the content of a given dictionary record.

In addition to publisher and subscriber discovery, our system supports rendezvous among applications bound to a particular data-space. These applications can interact with multiple topics of a given data-space, using multiple publishers and/or subscribers.

DataSpaceParticipant Registration enables nodes to perform rendezvous with all the discoverable nodes that host applications associated to a particular data-space. In this way, applications do not have to create multiple TopicPublisher and TopicSubscriber Registrations, instead they access to a unique DataSpaceParticipant Registration.

DataSpaceParticipant is especially useful for deployments where one or more monitors access to data associated to several different topics (an example of this scenario is the use case described in Section 3.1.1).

DataSpaceParticipant Registration is similar to TopicPublisher/TopicSubscriber Registrations. However, it includes a DCPS-Entity-Descriptor associated to each DCPS-Entity-Name to indicate if the entity is a publisher, a subscriber, or a data-participant.

DataSpaceParticipant uses the same mechanisms as the TopicPublisher and TopicSubscriber Registrations for providing integrity, authenticity, and privacy.

In RELOAD, stored information is accessed by its Resource-ID. In this section we explain how our system calculates the Resource-ID for each Data-Registration type.

In our proposal, a Resource-ID is generated from an URI that identifies certain data-space segment. Our URI design has been developed around the concepts proposed in Section 4.3.

We have chosen a URI format for describing resources. This decision is due to the following reasons:
                           
                              –
                              The format is flexible enough to support scenarios with different hierarchy levels (from scenarios where only one data-space is associated to the overlay, to scenarios where there are several sub-levels of data-space).

This format is easily readable and intuitive.

The proposed URI format is easy to map into CoAP URI format [4], which is the format used by one of the proposed protocols for data transferring.

In particular, we build the URI for a given FQDN as follows:dcps://<overlay>/<data-space-name>/
                     

where dcps stands for Data-Centric Publish-Subscribe, overlay is a DNS domain name identifying a P2P overlay, and data-space-name could be a hierarchy of data-space names (separated by forward slashes), for example:dcps://ugr.es/labs/microwaves/
                     

Since a topic is a segment of a data-space, the URI for a FQTN is built by attaching the topic-name to the FQDN of the topic's data-space:dcps://<overlay>/<data-space-name>/<topic-name>/.
                     

The URI associated to TopicPublisher, TopicSubscriber, and DataSpaceParticipant Registration is a DCPS-Entity-Group URI. In this regard, nodes build TopicPublisher, TopicSubscriber, and DataSpaceParticipant Registration by adding/pub/,/sub/,or/.par/suffixes to a FQDN or FQTN URI.

Our design also uses this URI format for the DCPS-Entity-Names. In order to save storage space and traffic load, our system stores DCPS-Entity-Names as relative URIs (using the dictionary record's URI as base).

In this section we describe the operation of our system through an example. In particular, we will use user data-space use case from Section 3.1.1. We have chosen this use case because it covers all the different DataRegistration types.

In this example we assume that RELOAD uses Chord [7,8] as DHT algorithm. Chord uses SHA-1 [12] as the base hash function for assigning each node and resource a unique m-bit identifier (respectively called Node-ID and Resource-ID). These identifiers are used for creating a ring of 2
                        m
                      nodes, which is called the Chord ring.

Each member of the ring maintains a routing table consisting of a finger table and a neighbor table, which allows nodes to locate resources on the overlay. The neighbor table contains a list of a node's immediate successors and predecessors. The finger table stores information about O(log
                     N) nodes distributed along the ring. Specifically, in Chord the ith entry in the finger table at node n contains the identity of the first node, s, that succeeds n by at least 2(i
                        −1) on the ring. In RELOAD, finger table entries are indexed in opposite order (i.e., finger[0] is the node 180° around the ring from the original node).

As we have no made modifications to the underlying Chord DHT, in this section we will describe the operation from a RELOAD-level point of view. For more information about how RELOAD functions are mapped to Chord DHT, please refer to RELOAD specification [3].

Prior to joining an overlay (in our example scenario, iotugr.es), any joining node must know at least one node which is currently a member of the overlay. This is a common issue in P2P networks and is referred to as the bootstrapping process.

In RELOAD, the bootstrapping nodes (i.e., the nodes used to form the first connection to the overlay) must have public IP addresses so that new nodes can reach them. Once a peer has connected to one or more bootstrap nodes, it can form connections using existing RELOAD operations.

In our system, the bootstrap node also acts as the certificate authority (located at iotugr.es/bootstrap), providing the joining node a Node-ID (an integer, e.g., 1,234,567) and a certificate associated to that Node-ID.

Once a peer has connected to the overlay for the first time, it can cache a list of public IP addresses the peer has successfully reached, and use these addresses as future bootstrap nodes.

After joining is completed (see Fig. 2
                        ), the joining peer is a full member of the overlay and can process Store/Fetch requests.

In this section we explain the procedures for registering the three different types of DataRegistration: TopicPublisher, TopicSubscriber, and Data-SpaceParticipant.

Continuing with our example, when the user jmlvega installs a IoT-enabled alarm clock, the clock registers itself into the tevent topic as a publisher (in our example, this topic contains time-driven events, such as calendar alerts or alarm clock events). In order to register, the alarm clock performs a RELOAD Store using the following URI:dcps://iotugr.es/jmlvega/home/tevent/.pub/.
                        

Since Chord uses SHA-1 hashes for addressing resources, the node calculates the SHA-1 of the URI and then performs a store of the Kind “TOPIC_ PUBLISHER_REG” with the following data:
                              
                                 
                                    Resource-ID: 53b573958039726dec72d3ce0e06367f569be4db
                                 


                                    DICTIONARY-KEY: 1234567
                                 


                                    VALUE: {../alarmclock}.
                                 

After performing a successful store, this information is available in the overlay and can be fetched later by any node. In order to provide data privacy, stored data-content can be encrypted using a public key. In this way only authorized nodes (the ones with the proper private key) will be able to read the stored data later.

Locked doors that are accessible to the user jmlvega (his own house's doors, and certain doors at his workplace) can use TopicSubscriber for registering their discovery information. In this way, the user can connect to a particular lock and then publish the proper open/close signal. Since door locks are usually constrained devices, they are typically accessed through a proxy. In our example, garage and front doors of the user's house register through the domopc node, which acts as a proxy. In order to register the locks, domopc performs a RELOAD store using the following URI:dcps://iotugr.es/jmlvega/lock/.sub/.
                        

Once the SHA-1 has been calculated, the node performs a store of the Kind “TOPIC_SUBSCRIBER_REG” with the following data:
                              
                                 
                                    Resource-ID: d15138ddbf356e8e6726c6985cea423c9cae7a28
                                 


                                    DICTIONARY-KEY: 0000000
                                 


                                    VALUE: {../../home/domopc/garaged;
                                 


                                    ../../home/domopc/frontd}.
                                 

As in the previous case, this information will be available for any member of the overlay, and therefore it should be encrypted in order to provide data confidentiality.

The fact of registering both publishers and subscribers (instead of registering only publishers) enable nodes to discover matching nodes as soon as they join the network, without relying in periodic fetches from subscribers. Additionally, it eases the balancing of load and requests among the overlay.

There are situations where nodes need access to a significant number of topics or even to a complete data-space. For example, a domotic house central computer should be able to discover all the sensors of the house, even if those sensors publish information through different topics. For these cases, we have designed DataSpaceParticipant Registration.

Continuing with our example, the computer at the domotic house could register itself to the data-space jmlvega/home using the following URI:dcps://iotugr.es/jmlvega/home/.par/.
                        

Once the SHA-1 has been generated, the node performs a store of the Kind “DATA_SPACE_REG” with the following data:
                              
                                 
                                    Resource-ID: 4bffc1a6f31a40b054584f27464f84ba2ac32316
                                 


                                    DICTIONARY-KEY: 0000000
                                 


                                    VALUE: {../domopc [DP]}.
                                 

In this store, the node has registered only one URI as a data-participant (DP). Registering the type of the DCPS entity (in this case, data-participant) eases the processing of rendezvous information. Thus, the fetching nodes can filter dictionary entries according to their interests.

Our system also supports nodes that host several DCPS entities. Consider a mobile device that simultaneously maintains a calendar, a domotic house control panel, and an application for registering alerts generated by a domotic house. This device will perform the following store for registering three publishers (in this case, it uses the same Resource-ID, but a different Node-ID):
                              
                                 
                                    Resource-ID: 4bffc1a6f31a40b054584f27464f84ba2ac32316
                                 


                                    DICTIONARY-KEY: 5996172
                                 


                                    VALUE: {../mobile/calendar [PB];
                                 


                                    ../mobile/domocontrol [PB];
                                 


                                    ../mobile/domoalerts [PS]}.
                                 


                           Fig. 3
                            depicts the overlay after performing the DataRegistrations described in our example. In the following sections we study how nodes can retrieve this information.

In this section we explain the procedures for discovering nodes in the overlay using the three available DataRegistration types: TopicPublisher, TopicSubscriber, and DataSpaceParticipant.

Continuing with our example, a calendar application needs to retrieve planned events from a user data-space. After joining the overlay, the node that hosts the application calculates the SHA-1 for the following URI:dcps://iotugr.es/jmlvega/home/tevent/.pub/.
                        

Once the SHA-1 is obtained, the node sends a fetch using the SHA-1 as the Resource-ID, and “TOPIC_PUBLISHER_REG” as the Kind-ID. This operation returns the following entries:
                              
                                 
                                    Resource-ID: 53b573958039726dec72d3ce0e06367f569be4db
                                    
                                       
                                          
                                             1. DICTIONARY-KEY: 1234567
                                          


                                             VALUE: {../alarmclock}
                                          


                                             2. DICTIONARY-KEY: 5996172
                                          


                                             VALUE: {../mobile/calendar}
                                          


                                             3. DICTIONARY-KEY: 6352145
                                          


                                             VALUE: {../calendarservice}.
                                          

The fetch has returned three results. The first one is the one in the example at Section 5.2.1. The other two entries correspond to calendar services that had registered its presence information later. With this information, the application is able to access those publishers through the overlay and then request for the data-content.

After joining the overlay, a user application for managing multiple locked doors performs a fetch to the Resource-ID associated to the URI dcps://iotugr.es/jmlvega/lock/.sub/. Since the application is looking for subscribers, it uses “TOPIC_SUBSCRIBER_ REG” as Kind-ID. As a result, it obtains the discovery information of subscribers associated to that data-space and topic:
                              
                                 
                                    Resource-ID: d15138ddbf356e8e6726c6985cea423c9cae7a28
                                    
                                       
                                          
                                             1. DICTIONARY-KEY: 0000000
                                          


                                             VALUE: {../../home/domopc/garaged;
                                          


                                             ../../home/domopc/frontd}
                                          


                                             2. DICTIONARY-KEY: 9456721
                                          


                                             VALUE: {../../ugr/lab01/lock}.
                                          

From this point, the application is able to connect to those nodes using RELOAD AppAttach method, and then the user can select what door lock he wants to activate.

Continuing with the example in Section 5.2.3, if the computer at the domotic house performs a fetch to the Resource-ID associated to the URI dcps://iotugr.es/jmlvega/home/.par/ using “DATA_SPACE_REG” as Kind-ID, the obtained result will be:
                              
                                 
                                    Resource-ID: 4bffc1a6f31a40b054584f27464f84ba2ac32316
                                    
                                       
                                          
                                             1. DICTIONARY-KEY: 0000000
                                          


                                             VALUE: {../domopc [DP]}
                                          


                                             2. DICTIONARY-KEY: 5996172
                                          


                                             VALUE: {../mobile/calendar [PB];
                                          


                                             ../mobile/domocontrol [PB];
                                          


                                             ../mobile/domoalerts [PS]}.
                                          

where the first entry corresponds to its own discovery information. Using these results, the computer is now able to connect to those nodes and, therefore, to exchange information with them.

Once a node has obtained a list with Node-IDs and URIs associated to matching nodes, it can establish a connection to those nodes. This connection is established using RELOAD AppAttach.

Our system has been designed to be generic. In this sense, nodes can perform data transfer using any publish–subscribe based protocol. For example, applications can use CoAP [4] for exchanging data-content over an existing RELOAD AppAttach connection. CoAP is an emerging protocol suited for working with low-power devices (the ones typically used in IoT deployments). Consequently, a CoAP-based solution allows for integrating these constrained devices to the overlay.

Another possible alternative is to use DDS-RTPS [13]. DDS-RTPS is part of the DDS standard family [5] and it is defined for transferring data in DCPS systems. DDS-RTPS solution is adequate for medium to high complexity distributed systems that require advanced features as data reliability, QoS negotiation, or data persistence.

In the following sections we describe the process of data transferring using these two protocols.

Once a node that hosts a subscriber (referred to as I-Node) receives the rendezvous information of a node that hosts a publisher (referred to as T-Node), I-Node sends an AppAttach request to T-Node. Upon AppAttach reception, T-Node generates an AppAttach response that establishes a connection between I-Node and T-Node.

Using this connection, the I-Node sends a subscription request to the T-Node.

This request follows the procedure described in [14]. In particular, I-Node sends a GET request (using the open connection) to the URI associated to a particular DCPS-Entity-Name (e.g., coap://iotugr.es/jmlvega/home/tevent/mobile/calendar). This URI is the one obtained during the rendezvous process, after replacing “dcps://” with “coap://”. From this point, the publisher mobile/calendar delivers its updates to the monitoring node using CoAP notifications over the active connection.

If the initiator of the dialog is a publisher, the procedure is almost the same as the described for a subscriber, but an additional step is necessary: the node that contains the publisher must notify the subscriber's node about the existence of an active publication.

Once a node that hosts a DDS participant (referred to as I-Node) receives the rendezvous information of a node that hosts another DDS participant (referred to as T-Node), I-Node sends an AppAttach request to T-Node. Upon AppAttach reception, T-Node generates an AppAttach response that establishes a connection between I-Node and T-Node.

Once rendezvous is finished, DDS implementations can perform their own procedures for exchanging data-content.

To validate our proposal, we have implemented a prototype. It is an extension to the one used in [15,16], and [17], which is a Java Standard Edition (J2SE) implementation of RELOAD owned by ERICSSON-RESEARCH.

Our prototype implements the three different Registrations proposed in Section 4.4. In this regard, it allows overlay peers to discover three different kinds of DCPS entities (data-participants, publishers, and subscribers). Regarding content addressing, we have used the URI format proposed in Section 4.5.

In order to perform stores and fetches, the URIs are encoded using SHA-1. The prototype implements a CoAP-based procedure for transferring data between peers.

We have conducted a series of experiments to validate our prototype. In the next subsection, we detail the experimental setup.

Our prototype includes the same code base as the RELOAD/P2P-SIP prototype used by ERICSSON-RESEARCH in [18,19], and [20] to run experiments in PlanetLab [21]. For the experiments reported in this chapter we ran multiple instances of our prototype over a simulated overlay. We decided to use a simulated overlay to be able to experiment with large-scale overlays (up to 10,000 nodes). The validity of the measures of the used simulator has been already demonstrated in [17], which includes a comparison between simulation and real PlanetLab deployment results.

Our simulator uses a topology generator that assigns peers randomly to 206 different locations around the world. These locations correspond to PlanetLab sites. The pairwise delays between peers were set based on real pairwise delays that we measured between nodes at these PlanetLab sites.

We have chosen Chord as the DHT algorithm for creating and organizing the overlay. The reason to use Chord is because RELOAD specifies it as the mandatory-to-implement DHT. The size of the Chord's successor list and finger table were set to log2
                        N, while the size of the predecessors' list was set to 0.5∗log
                        2
                        N, where N is the maximum network size for each experiment (as we will see later, it ranges from 500 to a maximum of 10,000 nodes).

We have assumed an unreliable transport protocol for peer communications.

We have also assumed that 90% of peers were located behind a NAT that uses endpoint-independent mapping and filtering behavior. In this sense, our simulations consider the time of ICE negotiation for performing NAT traversal when necessary.

In our system, data reliability and system robustness are a main concern, as discovery information loses prevent nodes for being discovered at Discovery-and-Data layer.

In order to provide robustness against node failures and ungraceful disconnections, we configured our prototype for replicating among multiple peers the information stored in the overlay. We have chosen a replication factor equal to 3 for the stored DCPS registers (i.e., information is replicated in three additional nodes). We have assumed that 10% of peer leaves are crashes.

In Section 4.2 we defined two node roles: S-NODES and D-NODES. In order to test the worst-case-scenario, we have assumed that all the nodes in the overlay are D-NODES (i.e., every node stores its discovery information in the overlay, and therefore generates the associated additional traffic).

In the experiments, each D-NODE registers one DCPS entity to the overlay. The kind for this entity is generated randomly. Specifically, 20% of the nodes register a DCPS data-participant, 40% of the nodes register a DCPS publisher, and 40% of the nodes register a DCPS subscriber.

The simulator generates the URI associated to each register randomly from a pool of URIs. Since the URI format is different for each new RELOAD Kind, the simulator has three different URI pools: one for data-participants, one for publishers, and one for subscribers.

We have tested both the rendezvous and AppAttach procedures. In this sense, each node performs a lookup for a randomly generated URI and starts an AppAttach with all the obtained entries (i.e., all the entities of the same DCPS-Entity-Group). This is also a worst-case scenario, as in the typical use-case the node will not necessarily open a connection with all the discovered nodes, but a subset of them. The simulator obtains lookup URIs from the same three pools used during the store; however, in this case Publishers use the Subscriber URI pool and vice versa.

In order to obtain comparable experimental data from different network sizes, the average number of entries per Resource-ID should be the same for all the experiments (if not, the different sizes of lookup responses will impact on the results). Our simulator achieves this behavior by adjusting the URI pool sizes proportionally to the maximum network size.

In our experiments, we have assumed an average of 20 entries per Resource-ID. This means that a node will connect an average of 20 nodes after a successful lookup result (i.e., the nodes that share a particular segment of the DCPS data-space).

We have conducted two different types of experiments for testing the system scalability and robustness. In the first set of experiments, we study how the number of nodes in the overlay (from 500 to 10,000) impacts the system performance (i.e., the average latency for stores, lookups, and inter-peer connection establishment).

In the second set of experiments, we evaluate how the churn rate (modeled as the rate at which nodes join and leave the overlay) impacts the reliability of the system.

@&#EXPERIMENTAL RESULTS@&#

The first set of experiments evaluates the scalability of the system. In particular, we measure the incurred latencies for storing and obtaining the proposed RELOAD Kinds in different network sizes (specifically 500, 1000, 5000, and 10,000 nodes).

We have also measured the time necessary for discovering a particular DCPS-Entity-Group and establishing a connection with a node associated to that group.

In order to measure these latencies, the simulations have two phases: overlay creation and measurement.

During the overlay creation phase, the simulator creates an overlay with approximate N nodes, where N is the tested network size. In this phase, the simulator creates new nodes, whose locations are randomly chosen from our PlanetLab data-base, and joins them to the overlay.

Once a particular node has joined to the overlay, it performs a store to one of the three new proposed RELOAD Kinds (chosen randomly as we described in the previous section).

Once the ring is created, the measurement phase starts: during this phase new nodes join the network, perform a store, obtain the discovery information for a randomly chosen DCPS-Entity-Group, and open a connection with all the obtained nodes.

The join rate during this phase has been modeled as a Poisson process with an average of 0.5 joins per second. The simulated time for this phase was 3600s, and each experiment has been repeated three times (for a total simulation time of 10,800s per experiment).


                     Fig. 4
                      depicts the average and 95% percentile latency associated to the store and lookup processes of the new proposed RELOAD Kinds, for multiple network sizes. The results show that the system scales well in terms of latency as the number of nodes in the network increases. The average and 95% percentile latency for storing the discovery information of a DCPS entity follows a logarithmic growth as a function of network size.

In the same way, the average and 95% percentile latency for retrieving the discovery information of all the nodes associated to a particular DCPS-Entity-Group also follows the same trend.

In addition to the store and lookup latency, we have measured the total time from the request of a particular DCPS-Entity-Group (i.e., after starting the rendezvous) until the creation of an AppAttach connection (using ICE if necessary) and the arrival of the first CoAP update. Fig. 5
                      shows the average delays for different network sizes. Although the average presents a certain deviation (due to a reduced number of measures with a very high latency), the 95% percentile grows logarithmically as a function of the network size. From the obtained results we can conclude that the average time for discovering and connecting to a set of nodes of interest scales well as the total size of the network increases.

In the second set of experiments, we have studied the robustness of our system, evaluating the impact of the churn rate on reliability. We have measured the percentage of lookup fails when the overlay is suffering a particular churn rate, modeled as a combination of join rate and leave rate. Specifically, we have simulated rates of 0.1, 0.2, 1, and 2 join/leaves per second.

As in the case of the scalability experiments, each simulation has two phases: overlay creation and measurement.

During the first phase, the simulator creates an overlay of approximately 5000 nodes. After the overlay creation, the simulator simulates a particular churn rate (i.e., a particular join and leave rate). After each new join, the joined node performs a lookup. The simulator stores the result of this lookup and calculates the percentage of lookup fails once the simulation is over. During the simulation, as we stated in the previous section, 10% of leaves are node crashes.

In Fig. 6
                      we include the different lookup failure percentages for the simulated churn rates. As expected the system is robust, and the percentage of failures for even high levels of churn (we have stressed the system with rates up to 2 joins and 2 leaves per second, which makes the topology of the system very unstable) remains below 18.17% of failures.

@&#RELATED WORK@&#

A number of papers have aimed to demonstrate the feasibility of RELOAD as a stable and reliable peer-to-peer architecture. For instance, its performance in wireless environments has been studied in [15]. In [20], a study of the performance of RELOAD on ICE-based environment for NAT traversal is done. Another contribution is [14], which addresses the problem of resource discovery and notification in CoAP environments.

Scalability in publish/subscribe systems is an important issue. Discovery and dissemination of publications in large deployments are challenging problems, specially due to the great amount of resources required to maintain a database of all the publications and subscriptions. The paper [22] addresses the problem of scalable discovery in DDS environments by applying Bloom filters. However, this work only focuses on the discovery problem itself, not addressing other issues such as maintaining an overlay to provide fault-tolerance, or NAT-traversal. In contrast, we address the NAT traversal and resource naming problems, which are two open issues in the DDS-based system.

To provide fault-tolerance, the authors of REVENGE [23] propose a self-organizing P2P substrate built atop of DDS to support DCPS communications. In REVENGE, instead of maintaining direct communication between publishers and subscribers, an overlay is utilized for routing the publications, thus alleviating the data sources from the responsibility of disseminating publication updates to each interested subscriber. REVENGE relies in the deployment of relay nodes that manage different data-spaces. Our proposal supports this behavior, and it also allows for distributing the same data-space across multiple peers in the overlay, regardless of their location.

Other works also address the publication/subscription paradigm using P2P-based architectures. The authors of [24] propose SIENA, a publish–subscribe event notification service that supports using a P2P architecture. The authors of [25] propose to partition the event space among the peers in the system, and to broadcast events and subscriptions to all nodes in the system. However, this approach may require all peers in the system to be contacted to install a subscription. To reduce the number of peers to be contacted, [26] proposes Meghdoot, a content-based publish–subscribe system built on top of Content Addressable Network (CAN) infrastructure. However, none of these works are based on a standard P2P framework, as our proposal is.

In the literature we can find several proposals that take advantage of P2P-based architectures for performing resource discovery. For example, [27] proposes Spider, a framework for Web Service discovery. It supports the lookup of Web Services based on keywords, ontologies, or behavior. Also related to Web Service discovery, the authors of [28] propose a scalable indexing system for P2P-based architectures that incorporates load-balancing mechanisms. However, these two works are focused in Web Service discovery, and therefore they are not suited to support the specific requirements of a publish–subscribe middleware (for example, DDS requires to treat publishers, subscribers, and participants differently).

Finally, and very close to our work, the authors of [29] propose a publish/subscribe protocol built atop of RELOAD. Although it provides the basic features needed for a publish/subscribe system, it relies completely on tunneling publish/subscribe samples. Our work is complementary to [29], as it takes advantage of RELOAD AppAttach mechanism, which provides a great flexibility and extensibility to our architecture.

Once the publications are discovered and matched to interested subscriptions, the communication phase is done either by CoAP or DDS thus ensuring that all the features of these technologies (such as minimal resource usage of QoS requirements).

@&#CONCLUSIONS@&#

In this paper, we have proposed a RELOAD extension for performing content discovery and data transfer on Data-Centric Publish–Subscribe (DCPS) environments.

In particular, we have defined three new registration types that extend the functionality of RELOAD. These new types cover three different DCPS entities: data-participants, publishers, and subscribers.

Our system provides rendezvous function for locating sensors, actuators, and applications that share a common data-space or topic of interest.

Once the rendezvous is complete, nodes use existing RELOAD features for establishing a connection (our solution provides NAT traversal using ICE). User applications can use this connection for transferring data using a publish–subscribe approach.

Our proposal is extensible, and any publish–subscribe application can benefit from it, for instance, those based on CoAP or on DDS.

In order to validate our proposal–and evaluate its scalability and robustness–we have conducted a series of experiments over multiple simulated networks of 500 to 10,000 peers. The topology of this network is based on real-world data, specifically from the PlanetLab network.

The obtained results show that our proposal is scalable, as the average delay for the main operations of our system follows a logarithmic growth as a function of the network size.

Regarding robustness, we have stressed a 5000-sized network with different churn rates. The obtained experimental results show that the percentage of lookup failures remains low even for very high churn levels. In this sense, for a level of churn equivalent to 2 joins and 2 leaves per second (i.e., the 25% of the nodes change every 10min), the percentage of lookup failures is lower than 18.17%.

As future work, we want to focus our research on introducing load balancing mechanisms for addressing scenarios with a large number of entities per DCPS-Entity-Group (i.e., scenarios where the number of entries associated to the same Resource-ID is high). Regarding data-sharing, we would like to add DDS-based data-transfer to our prototype, and to compare our system with other existing DDS solutions using PlanetLab. We have also plans to standardize the proposed RELOAD extension as an IETF RELOAD usage.

@&#ACKNOWLEDGMENTS@&#

We would like to thank Jouni Maenpaa and Jaime J. Bolonio for their support with the development of the prototype. The research of Jose M. Lopez-Vega has been funded by the Education Spanish Ministry Ph.D. Student Mobility Program (Programa de Ayuda para Estancias Breves FPU 2011).

@&#REFERENCES@&#

