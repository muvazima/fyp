@&#MAIN-TITLE@&#A neural algorithm for the non-uniform and adaptive sampling of biomedical data

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Algorithm to under-sample data with a frequency lower than Nyquist limit.


                        
                        
                           
                           An adaptive neural predictor selects the subsequent samples to be measured.


                        
                        
                           
                           Example applications: EMG, ECG, EEG and acceleration data.


                        
                        
                           
                           The method outperforms uniform sampling and compressive sensing.


                        
                        
                           
                           Many potential applications to save energy and to reduce the memory storage.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Electromyography (EMG)

Electrocardiography (ECG)

Electroencephalography (EEG)

Accelerometer

Non-uniform sampling

Compressive sensing

Nyquist limit

@&#ABSTRACT@&#


               Graphical abstract
               
                  The paper proposes an innovative real time method to under-sample non-stationary data, providing applications to four different biomedical signals: electromyogram (EMG), electrocardiogram (ECG), electroencephalogram (EEG) and body acceleration. An example of application is shown in Figure 1, where an EMG is sampled under the Nyquist limit, adapting the sampling schedule to the data. The sampling rate is automatically increased during bursts of activity of the muscle and lowered when when the EMG has small amplitude (reflecting cross-talk from nearby muscles or noise). The Power Spectral Density (PSD) is fairly well represented by the adaptive under-sampling till the highest frequency components, even under a drastic reduction of the number of samples.
                  The method outperforms both a uniform under-sampling and compressive sensing (CS) applied to the same data with the same compression ratio. The algorithm opens interesting perspectives for potential applications in body sensor networks (which are finding increasing applications, e.g. in self-monitoring and for the surveillance of sensitive people). Specifically, it could allow to lower the number of wireless communications (saving sensor power) and to reduce the occupation of memory.
                  
                     
                        
                           fx1
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

There are applications in which sampling at the Nyquist frequency is not efficient. For example, sparse signals are decomposed using a few significant components in compressive sensing (CS) [1]. Reducing the sampling rate is useful when many body sensors are used to monitor continuously the lifestyle of a healthy person or the condition of sensitive people [2]. Sensors are lightweight, non-invasive, wearable or embedded in cloth and they include a wireless communication with a storage and decision making system. Many physiological data can be sensed, e.g., acceleration, bioelectric activity, blood pressure, galvanic skin response, and breathing [3]. Monitoring these data supports the individual self-assessment which allows to develop a personalized health care that helps healthy people to maintain their well-being [4]. Moreover, many diseases can benefit from a continuous monitoring, like cardiovascular problems, diabetes, Alzheimer׳s and Parkinson׳s diseases, renal failure, chronic obstructive pulmonary disease, post-operative conditions, stress or sudden infant death syndrome [5]. The remote patient monitoring could allow a rapid intervention when needed, developing an individualized care [6], with positive effects on the management of clinical services and on the quality of life of patients [7]. Many additional applications of body sensors are found, e.g., in military monitoring, interactive gaming [8], recognition of dietary activity [9], rehabilitation [10], personal information sharing and secure authentication [11].

Some recorded signals can show burst activity, reflecting the alternation of periods in which the investigated physiological system is either silent or active: for example, surface electromyogram (EMG) during cyclic tasks [12] or in pathological conditions (e.g., motor tremor induced by epileptic seizures [13]); electrocardiogram (ECG), with the QRS complex including most of the high frequency content [14] (unless pathological behaviors arise [15]); electroencephalogram (EEG), when the brain is performing different tasks or in pathological conditions (e.g., seizure in epileptic patients [16]); body acceleration during different activities [18] or in pathological conditions [19].

When data contain bursts, a high sampling rate is required if a uniform sampling is adopted, even if there are portions of the signal which could be down-sampled without loss of information. A non-uniform sampling, using a high sample rate only during the bursts, would allow both memory and energy saving (very important for wireless sensors, which are supplied with scarce resources [20]).

A predetermined non-uniform sampling schedule cannot be established for biomedical applications, but the sampling should be adaptively defined on the basis of the data. Adaptive techniques, like AZTEC, CORTES, SLOPE, or Fan [21], have been developed specifically for ECG data compression. They can reduce considerably the number of acquired samples by a non-uniform sampling schedule, but they do not allow a real time modulation of the sampling frequency, as they require to measure each sample in order to decide if to keep it. A real time adaptive solution was proposed in [14], increasing the sampling rate when the signal curvature was high. The method showed higher performances than uniform sampling, but a-priori knowledge on the signal was required, limiting versatility [22]. In this respect, CS is considered more generally applicable [22], showing low compression ratio (CR, ratio between number of acquired versus original samples) and good accuracy [23,24]. However, it recovers the signal by an offline procedure applied on time epochs, introducing a delay. On the other hand, a real time adaptive sampling schedule could allow to save energy (to sample and transmit data) and to implement simple decision making control even on the sensor (e.g., a fall detector based on a threshold on the body acceleration). Thus, a versatile adaptive sampling algorithm could provide an important contribution.

The method proposed in [20] increased automatically the sampling rate when the investigated signal became unpredictable or provided high frequency contributions. A conceptual framework was discussed in [20], showing different general applications, but without optimizing the algorithm on specific data and the under-sampling was not measured relative to Nyquist frequency (problem affecting also the literature on CS applied to biomedical data [1], where approximation errors at specific CR are often provided without caring about the possible over-sampling of the original signal; see Discussion for details). The present work investigates these open issues. The algorithm proposed in [20] is improved, by an automatic tuning on the data, based on an offline analysis of a training set. The adaptive algorithm is compared with uniform sampling and CS with the same CR, when applied to different biomedical signals sampled at the Nyquist frequency: EMG, ECG, EEG and body acceleration.

The adaptive sampling is based on a prediction algorithm and on its application to estimate the uncertainty of a predicted sample. It can be split into three parts:
                           
                              1.
                              selection of optimal predictors (based on the theory of time series embedding, Section 2.1.1);

training of an adaptive algorithm to predict the next sample (a multi-layer perceptron, MLP, was used, Section 2.1.2);

real time schedule of the sampling rate (based on the uncertainty of the prediction, Section 2.1.3).

Embedding theory was applied to the data [25–27]. Specifically, the time series were supposed to be extracted from a deterministic physiological system described by a set of unknown deterministic rules
                              
                                 (1)
                                 
                                    
                                       
                                          
                                             d
                                             
                                             
                                                x
                                                →
                                             
                                          
                                          
                                             d
                                             
                                             t
                                          
                                       
                                       =
                                       
                                          F
                                          →
                                       
                                       (
                                       
                                          x
                                          →
                                       
                                       )
                                    
                                 
                              
                           where 
                              
                                 x
                                 →
                              
                            is the vector of state variables of the system and 
                              
                                 F
                                 →
                              
                            is a set of functions called the vector field (defining the evolution of the state variables), which was assumed not to be an explicit function of time (i.e., the system was assumed to be autonomous). The recorded time series 
                              
                                 y
                                 (
                                 t
                                 )
                              
                            (where 
                              t
                            from now on is a discrete time variable) was assumed to be extracted from the system through a measurement process described by an unknown function 
                              
                                 g
                                 (
                                 ⋅
                                 )
                              
                            of the state variables:
                              
                                 (2)
                                 
                                    
                                       y
                                       (
                                       t
                                       )
                                       =
                                       g
                                       (
                                       
                                          x
                                          →
                                       
                                       (
                                       t
                                       )
                                       )
                                    
                                 
                              
                           
                        

Given a single measurement, a vector of time delayed versions (delayed coordinates) was built [25]
                           
                              
                                 (3)
                                 
                                    
                                       
                                          Y
                                          →
                                       
                                       (
                                       t
                                       )
                                       =
                                       
                                          [
                                          
                                             
                                                
                                                   
                                                      
                                                         y
                                                         (
                                                         t
                                                         )
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         y
                                                         (
                                                         t
                                                         −
                                                         τ
                                                         )
                                                      
                                                   
                                                
                                                
                                                   
                                                      ⋮
                                                   
                                                
                                                
                                                   
                                                      
                                                         y
                                                         (
                                                         t
                                                         −
                                                         (
                                                         m
                                                         −
                                                         1
                                                         )
                                                         τ
                                                         )
                                                      
                                                   
                                                
                                             
                                          
                                          ]
                                       
                                    
                                 
                              
                           where the time delay 
                              τ
                            was chosen so that two delayed coordinates provided different information and the number m of elements of the vector is called the embedding dimension (as it is the dimension of the so called phase space in which the trajectory 
                              
                                 
                                    Y
                                    →
                                 
                                 (
                                 t
                                 )
                              
                            is embedded) [25–27].

The time delay 
                              τ
                            and the embedding dimension m were computed as follows.
                              
                                 •
                                 Time delay. The mutual information of the original and delayed data was computed:
                                       
                                          (4)
                                          
                                             
                                                
                                                   
                                                      M
                                                   
                                                   
                                                      I
                                                   
                                                
                                                (
                                                τ
                                                )
                                                =
                                                
                                                   
                                                      ∫
                                                   
                                                   
                                                      A
                                                   
                                                
                                                
                                                   
                                                      
                                                         ∫
                                                      
                                                      
                                                         B
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            P
                                                         
                                                         
                                                            A
                                                            B
                                                         
                                                      
                                                      (
                                                      a
                                                      ,
                                                      b
                                                      )
                                                      
                                                      
                                                      
                                                      
                                                      
                                                      ln
                                                      
                                                         (
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        P
                                                                     
                                                                     
                                                                        A
                                                                        B
                                                                     
                                                                  
                                                                  (
                                                                  a
                                                                  ,
                                                                  b
                                                                  )
                                                               
                                                               
                                                                  
                                                                     
                                                                        P
                                                                     
                                                                     
                                                                        A
                                                                     
                                                                  
                                                                  (
                                                                  a
                                                                  )
                                                                  
                                                                     
                                                                        P
                                                                     
                                                                     
                                                                        B
                                                                     
                                                                  
                                                                  (
                                                                  b
                                                                  )
                                                               
                                                            
                                                         
                                                         )
                                                      
                                                      d
                                                      a
                                                      d
                                                      b
                                                   
                                                
                                             
                                          
                                       
                                    where the time series 
                                       
                                          y
                                          (
                                          t
                                          )
                                       
                                     and 
                                       
                                          y
                                          (
                                          t
                                          −
                                          τ
                                          )
                                       
                                     are considered as random variables A and B, respectively, with joint probability density 
                                       
                                          
                                             
                                                P
                                             
                                             
                                                A
                                                B
                                             
                                          
                                          (
                                          a
                                          ,
                                          b
                                          )
                                       
                                     and marginal probabilities 
                                       
                                          
                                             
                                                P
                                             
                                             
                                                A
                                             
                                          
                                          (
                                          a
                                          )
                                       
                                     and 
                                       
                                          
                                             
                                                P
                                             
                                             
                                                B
                                             
                                          
                                          (
                                          b
                                          )
                                       
                                    , respectively. The minimum between the delays corresponding to the first local minimum or to a 90% decrease of 
                                       
                                          
                                             M
                                          
                                          
                                             I
                                          
                                          (
                                          τ
                                          )
                                       
                                     was selected as the time delay 
                                       τ
                                     of the delayed coordinates.

Embedding dimension. Cao׳s method was used [27,28]. It is based on the number of points of the trajectory, described by the vector in (3), which are neighbors of other points of the trajectory itself. When increasing the embedding dimension by adding one element to the vector (3), neighboring points which were close only due to the projection of the trajectory in a low dimensional space (false near neighbors) may turn away. Thus, the number of neighboring points decreases by increasing the embedding dimension, till false neighbors are present. The minimum phase space dimension allowing to remove the false near neighbors was selected as the embedding dimension m: it allows to identify uniquely the dynamics of the trajectory and possibly to predict it. Specifically, Cao׳s method investigates the following function of the embedding dimension [28]
                                    
                                       
                                          (5)
                                          
                                             
                                                E
                                                1
                                                (
                                                m
                                                )
                                                =
                                                
                                                   
                                                      E
                                                      (
                                                      m
                                                      +
                                                      1
                                                      )
                                                   
                                                   
                                                      E
                                                      (
                                                      m
                                                      )
                                                   
                                                
                                                
                                                ,
                                                
                                                
                                                
                                                   
                                                      where
                                                   
                                                   
                                                
                                                E
                                                (
                                                m
                                                )
                                                =
                                                
                                                   
                                                      1
                                                   
                                                   
                                                      N
                                                      −
                                                      m
                                                      τ
                                                   
                                                
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   
                                                      N
                                                      −
                                                      m
                                                      τ
                                                   
                                                
                                                
                                                   
                                                      
                                                         ‖
                                                         
                                                            
                                                               
                                                                  Y
                                                               
                                                               
                                                                  m
                                                                  +
                                                                  1
                                                               
                                                            
                                                            (
                                                            i
                                                            )
                                                            −
                                                            
                                                               
                                                                  Y
                                                               
                                                               
                                                                  m
                                                                  +
                                                                  1
                                                               
                                                            
                                                            (
                                                            n
                                                            (
                                                            i
                                                            ,
                                                            m
                                                            )
                                                            )
                                                         
                                                         ‖
                                                      
                                                      
                                                         ‖
                                                         
                                                            
                                                               
                                                                  Y
                                                               
                                                               
                                                                  m
                                                               
                                                            
                                                            (
                                                            i
                                                            )
                                                            −
                                                            
                                                               
                                                                  Y
                                                               
                                                               
                                                                  m
                                                               
                                                            
                                                            (
                                                            n
                                                            (
                                                            i
                                                            ,
                                                            m
                                                            )
                                                            )
                                                         
                                                         ‖
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    where N is the number of considered samples of the time series, 
                                       
                                          ‖
                                          
                                             ⋅
                                          
                                          ‖
                                       
                                     is the absolute distance norm, 
                                       
                                          
                                             
                                                Y
                                             
                                             
                                                m
                                             
                                          
                                          (
                                          i
                                          )
                                       
                                     is the ith sample of the reconstructed vector with embedding dimension m and 
                                       
                                          n
                                          (
                                          i
                                          ,
                                          m
                                          )
                                       
                                     indicates its nearest neighbor in the m-dimensional reconstructed phase space. The function 
                                       
                                          E
                                          1
                                          (
                                          m
                                          )
                                       
                                     saturates when all false near neighbors are removed. Thus, such a function has a point of maximum curvature (corresponding to the correct embedding dimension m), separating a region of increase from a plateau. Such a point was estimated automatically, considering the best approximation of 
                                       
                                          E
                                          1
                                          (
                                          m
                                          )
                                       
                                     by 2 lines [27].

The delay 
                              τ
                            and the embedding dimension m were selected considering only a portion of data (the training set defined in Section 2.1.2).

An MLP forecasted the subsequent sample of the time series with a constant interval using the delayed coordinates as inputs. Thus, given the time delay 
                              τ
                            and the embedding dimension m (estimated as described in Section 2.1.1), in order to predict the value of the sample number (i+1) of the time series, the MLP used as inputs the m values 
                              
                                 y
                                 (
                                 i
                                 )
                                 ,
                                 
                                 
                                 
                                 
                                 
                                 
                                 y
                                 (
                                 i
                                 −
                                 τ
                                 )
                                 ,
                                 …
                                 
                                 ,
                                 
                                 y
                                 (
                                 i
                                 −
                                 (
                                 m
                                 −
                                 1
                                 )
                                 τ
                                 )
                              
                           .

Different MLPs were investigated, from which the optimal one was chosen as that with best generalization performances. The following procedure was adopted to select such an optimal MLP. The data were split into training, validation and test sets (50%, 25% and 25% of data, respectively). MLPs with a single hidden layer were used (notice that a single hidden layer is sufficient to approximate any nonlinear function [29]). Sigmoidal activation functions were used for the hidden neurons. Their number was chosen in the range 10−50. The output neuron had a linear activation function. The MLPs were trained on the training data, applying the quasi-Newton algorithm [30] for a number of iterations in the range of 25−350. The optimal MLP was selected choosing the topology (i.e., the number of hidden neurons) and the parameters (i.e., the synaptic weights and bias, after a specific number of iterations of the optimization algorithm) with best performances on the validation set (measured in terms of the mean square error).

The sampling schedule was defined by selecting which sample to measure next, on the basis of the uncertainty of its prediction obtained by the optimal MLP described in Section 2.1.2. The algorithm was applied on the test set. The prediction and an estimate of its uncertainty were performed for each time sample, using the available (sampled or predicted) data [20]. All delayed coordinates (Section 2.1.1) used as inputs of the neural predictor (i.e., the optimal MLP, Section 2.1.2) were characterized by their (predicted or measured) values and uncertainties.

The prediction of a new value was obtained as follows: 1000 random input data were simulated, using for each data a uniform distribution centered around its value and with a range given by its uncertainty; the neural predictor was run for each of these 1000 inputs; the prediction was the median of the distribution of the obtained estimates.

The uncertainty of a data was chosen distinguishing between two cases, i.e., when the data was measured or predicted. The uncertainty of a measurement was defined by the user, considering the accuracy of the sensor and the expected noise level. The uncertainty of a predicted sample was defined as the mean of two terms (see [20] for details):
                              
                                 1.
                                 the range of predicted values obtained from the 1000 random tests indicated above (excluding 10 possible outliers);

the estimated prediction error obtained integrating in time the rate of prediction error, defined as the ratio between the estimation error (available when a new sample was measured and defined as the absolute difference between the measured and the predicted values) divided by the time delay from the last acquired sample; a memory term was also included, by computing the weighted average of the last two rates of prediction error (with weights 65% and 35%, respectively); finally, a saturation was considered, by imposing a minimum CR.

An additional measurement was required from a sensor when the associated uncertainty overcame a threshold, chosen by the user. This free parameter and the minimum CR mentioned above allow the user to tune the level of under-sampling to the application at hand.

Different experimental signals were used to test the algorithm. The first 3 signals were provided by different institutions, acknowledged at the end of the paper, whereas the last one was acquired by the author (in accordance with the Declaration of Helsinki).
                           
                              1.
                              Surface EMG was recorded from the tibialis anterior muscle of a healthy subject walking on a treadmill at a self-selected speed (bipolar electrodes placed on the muscle belly; ground electrode on the right patella; sampling rate 2048Hz). The experiment lasted about 20s. The signal had a bandwidth of about 400Hz. It was low-pass filtered at 400Hz offline (4th order anti-causal, anti-aliasing Butterworth filter) and resampled at 800Hz, in order to be at the Nyquist limit.

ECG was recorded from the wrists of a healthy subject [31]. The experiment lasted about 75s. The signal was sampled at 2kHz, but its bandwidth was about 30Hz. Thus, it was resampled at 60Hz (as the EMG described above).

A scalp EEG was recorded from an epileptic patient for about 50min. The channel O2 was selected (with reference on the earlobe). The EEG was sampled at 1024Hz (Brain Explorer, EB-Neuro© amplifier), but the signal (after removing the frequency components under 0.5Hz) had more than 95% of energy under 14Hz: it was down-sampled to 32Hz (notice that with this preliminary down-sampling, only theta, delta and alpha rhythms are available; however, for this representative application, the main objective is identifying the epileptic seizures).

Acceleration data were recorded for 5min from a subject keeping fixed in a bag a Tablet embedding the 3 axial accelerometer LSM330DLC. Quite standing, walking and jumping were the investigated activities. The signal was sampled at 50Hz. Its bandwidth was about 15Hz. It was resampled at 32Hz.

The algorithm was applied to the test data (i.e., a portion of the signals described in Section 2.2, as indicated in Section 2.1.2). Different thresholds were used, obtaining different CRs. Then, data were uniformly down-sampled with the same CRs, by cubic interpolation. From the two time series, the samples of the original signal were estimated by cubic interpolation. The fit was assessed in terms of the averaged rectified error (ARE) and the percentage root mean squared difference (PRD)
                           
                              (6)
                              
                                 
                                    
                                       
                                          A
                                       
                                       
                                          R
                                       
                                       
                                          E
                                       
                                    
                                    
                                    =
                                    
                                    100
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                i
                                             
                                          
                                          
                                             |
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                −
                                                
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            ^
                                                         
                                                      
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                             |
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                i
                                             
                                          
                                          
                                             |
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                             |
                                          
                                       
                                    
                                    
                                    
                                    
                                    
                                    (
                                    %
                                    )
                                    
                                    
                                    
                                       
                                          P
                                       
                                       
                                          R
                                       
                                       
                                          D
                                       
                                    
                                    
                                    =
                                    
                                    100
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                   
                                                
                                                
                                                   
                                                      
                                                         |
                                                         
                                                            
                                                               
                                                                  x
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                            −
                                                            
                                                               
                                                                  
                                                                     
                                                                        x
                                                                     
                                                                     
                                                                        ^
                                                                     
                                                                  
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                         
                                                         |
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                   
                                                
                                                
                                                   
                                                      
                                                         |
                                                         
                                                            
                                                               
                                                                  x
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                         
                                                         |
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                    (
                                    %
                                    )
                                 
                              
                           
                        where 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 
                                    x
                                    ^
                                 
                              
                              
                                 i
                              
                           
                         are the original and approximated data, respectively. The power spectral densities (PSD) of the three time series (original and estimated from either of the two under-sampled signals) were also compared.

Some tests were also performed using CS, with the principal components of the data as basis functions [32]. Specifically, principal components were computed from all adjacent epochs extracted from the training and validation sets. The duration of the epochs was chosen as that providing maximal accuracy on the test set (1s for all considered signals except for EMG, for which epochs of 50 ms were used).

The down-sampling was written as the multiplication of the data with a sensing matrix. Such a matrix was chosen using one of the following criteria: 1) a random selection of a percentage of the rows of the identity matrix (tests were performed also choosing a sensing matrix with entries equal to either 1 or 0, drawn from a binomial distribution with probability of generating 1 equal to 0.6, as suggested in [23]; equivalent results were obtained, so that this possibility is not further considered in the following); 2) selecting the rows of the identity matrix corresponding to the samples chosen by the adaptive algorithm. The coefficients were estimated minimizing their L
                        1 norm, imposing their sparseness [33]. The original signal was finally recovered as a sum of the basis functions weighted by the estimated coefficients.

@&#RESULTS@&#

The signals were embedded obtaining the dimensions and delays listed in 
                     Table 1. The optimal MLP for the prediction of the subsequent samples was investigated as explained in Section 2.1.2, resulting in the selection of a number of hidden neurons and of iterations of the optimization algorithm indicated in Table 1. The algorithm was implemented in Matlab on a PC with the following characteristics: Intel(R) Core i7−2630QM, Quad-Core, clock frequency of 2GHz, 6GB of RAM and 64bits operating system. A sequential, interpreted implementation was considered. The average processing time for epochs of duration 1s is indicated in Table 1: it is always about an order of magnitude shorter than the processed epoch, indicating that the sampling schedule can be adjusted in real time.


                     
                     Fig. 1 shows an application of the method to surface EMG (similar figures are given in the Supplementary Material for the other test data). A high sampling rate is selected for the portions of largest activity (panels A and C) and low rate when the EMG has small amplitude (reflecting cross-talk from nearby muscles or noise). This is evident in the two magnifications in Fig. 1A, which show that the uniform sampling has a better reconstruction of the noisy portions of the signal (where the adaptive method reduces the sampling to a minimum), whereas the adaptive algorithm provides a better estimate of the bursts of activity. The comparison of the PSDs (Fig. 1D) indicates that the adaptive under-sampling allows to estimate properly the high frequency components, even under a drastic reduction of the number of samples. On the contrary, a uniform under-sampling cannot estimate the high frequency content, as the sampling frequency is under the Nyquist limit.


                     
                     Fig. 2 shows the comparison between the adaptive algorithm and uniform sampling, when applied to the four test data. Different uncertainty thresholds (ranging from about 1 and 2 times the measurement uncertainty) reflect into different CRs and reconstruction errors. Keeping the same CR, the ARE and the PRD are lower when using the adaptive algorithm instead of a uniform schedule.


                     
                     Fig. 3 shows the accuracy of CS in estimating the original signal when using either random samples or those selected by the adaptive algorithm, considering the same CRs. The performances of CS are superior in the latter case.

@&#DISCUSSION@&#

Body sensor networks found many applications in self-monitoring and surveillance of patients [2,4,5,8,9,11,18,34]. They support the individual care of healthy people and the long-term outpatient monitoring, which is important in Holter acquisitions or when rare events are monitored (like epileptic seizures [16] or possible falls [17]). Remote monitoring requires wireless sensors to sample and transmit data to a base station. Decreasing the amount of measured samples is important to reduce power consumption and memory occupation [1,22,24]. This paper discusses a method to select automatically the information to be acquired, scheduling a non-uniform sampling that adapts to the investigated signal.

Some applications are provided.
                           
                              1.
                              The application on EMG (Fig. 1) shows that the algorithm is able to automatically identify the muscle activity intervals to be sampled at a high rate.

When applied to ECG (Fig. I of the Supplementary Material), the sampling rate was reduced between two heartbeats and increased at the beginning of the P wave, with QRS complex and T wave being sampled at about the maximal frequency.

The sampling rate was increased to improve the approximation of EEG sharp waves preceding or following a crisis and during the seizures (Fig. II of the Supplementary Material). These events are quite rare; as a consequence, the average error was only a bit lower than that obtained by uniform under-sampling and the power spectrum was estimated only slightly better. However, the resolution was improved exactly during crucial events for the study of epileptic seizures [38].

Applied to accelerometer data, the algorithm used high sampling rate only during the main activities of the subject (Fig. III of the Supplementary Material). Only a short delay (related to the selected minimum CR) was needed to identify the beginning of the activity and to boost the sampling frequency.

The PSDs of the signals were fairly well estimated, even when the average sampling rate was far below Nyquist limit. Moreover, the algorithm provided good accuracy in the estimate of test data, outperforming both uniform down-sampling schedule and CS with the same CRs (Figs. 2 and 3).

In literature, high performances are indicated for CS [1]. For example, low CRs were obtained with ECG, electro-oculogram (EOG) and needle EMG [22,23], which are quite sparse in time. Lower performances were obtained when processing signals with more interference, like surface EMG [23],
                           2
                        
                        
                           2
                           Notice that the sampling frequency of the original EMG studied in [23] was only 250Hz.
                         and scalp EEG [24]. The proposed algorithm, as it is adaptive, provides satisfactory results in all considered tests. On the other hand, CS did not provide impressive performances on the data here investigated (Fig. 3). Possibly, this is due to the dictionary, selected by principal component analysis, instead of further optimizing the basis functions for each data. However, the shown performances are not so far from those reported in literature, if we consider that our data were sampled at the Nyquist limit, whereas this is sometime questionable for the literature: for example, EEG was sampled at frequencies not lower than 200Hz in [23,24] and the PRD was about 20% when the CR was 0.4, i.e., sampling at an average rate of 80Hz (which is higher than the Nyquist limit for the EEG considered here); ECG was sampled at 360Hz in [23,39], which is above the Nyquist limit (indeed, the 99% of the energy of most of the records of the MIT-BIH Arrhythmia Database considered in [23,39] is below 50Hz); the needle EMG considered in [22] was sampled at 4kHz, but 95% of its energy was below 700Hz. Considering again our data (resampled at the Nyquist limit), the performances of CS were improved when it was applied to the samples selected by the proposed adaptive algorithm. This is somehow in line with [39], where improved performances of CS were obtained after pre-processing the ECG.
                           3
                        
                        
                           3
                           The QRS complex was identified in [39], in order to fix the period of each heartbeat and to select a specific number of beats each time. However, this procedure is fit to the specific data (i.e., ECG from a healthy subject) and it is not clear how the proposed method would cope with a highly varying heart rate or occurrence of an arrhythmia [1].
                         Dynamic thresholding, selecting only the largest signal amplitudes to increase the sparsity, is another pre-processing technique that improved CS performances [22].

The transmission module of a wireless sensor can be switched off for most of the time (saving energy) and activated only when needed [20].
                           4
                        
                        
                           4
                           Using a radiofrequency transmitter for wireless communication (e.g., nRF24L01 by Nordic Semiconductor, http://www.nordicsemi.com/eng/Products/2.4GHz-RF/nRF24L01), the times needed to switch on (from stand-by) or off the transmission module are about 150μs. Thus, the time to switch on and off the module is lower than the minimum interval between measurements (larger than a ms for EMG and larger than tenth of ms for the other data).
                         Thus, energy could be saved computing in advance which sample to acquire next and switching on the transmission module only when new data are acquired. Different possibilities could also be explored to further reduce data transfers: the transmission could be enabled only when a certain amount of data is available, or in specific conditions (e.g., when there is a burst of activity); data could be compressed [35]; transmission could be suppressed when the acquired data match a prediction model [36].

Notice that the proposed algorithm reduces the power needed to sample and transmit data, but it requires energy to perform computations. The base station could perform them [20,37], but then the communication with the sensors should be properly scheduled or the transmission module should be in stand-by and not switched off, with lower energy saving. Different alternatives could be tested in future.

@&#LIMITATIONS@&#

The proposed algorithm requires to train the neural predictor on a dataset. This preliminary processing can be considered as a limitation of the method, which cannot be applied directly online. However, this pre-processing step results also in the high flexibility of the method, which can adapt to the data (learning how to predict them) and to the application (selecting the CR still guaranteeing the needed resolution).

The online application could be applied without degradation of the performances as long as the training set fairly represents the dynamics of the signal to be acquired. On the other hand, if the data undergo a drift (e.g., for acquisition problems) or if the physiological system generating them undergoes some variation (e.g., the person goes to sleep), a decrease of the performance of the algorithm is expected. This means that the prediction errors will increase and the algorithm will adopt a high sampling rate to correct it. In such a case, the recorded samples could be used to train again the predictor algorithm, in order to adapt to the variations of the investigated system.

Data were first down-sampled at the Nyquist limit and the performances were tested in terms of the recovery of high amplitude or energetic components (related to ARE and PRD, respectively). However, some signals contain important high frequency contributions with low energy. For example, EEG includes components in the beta and gamma bands (13−30Hz and more than 30Hz, respectively) which were excluded in this study. Different applications could require to maintain specific low energy components. In such cases, the performances of the algorithm should not be based on average reconstruction errors, but on the ability of extracting the information of interest. Applications could be proposed for specific biomedical signals (e.g., estimating the average heart rate or its variability from down-sampled ECG: a high reconstruction error could be tolerated, but no R peak should be lost).

The algorithm was implemented on a PC with average performances, using an interpreted routine written in Matlab working on a single processor. Using a compiled routine, a sensible reduction of the processing time could be obtained. Moreover, the hidden neurons of the MLP can process data in parallel, reducing further the processing time depending on the number of available cores (e.g., using a GPU or an FPGA implementation, the prediction of the subsequent samples could be obtained largely reducing the number of clock cycles). Consider also that, as indicated in Section 2.1.3, 1000 predictions were performed for each sample (to predict its value and to assess the prediction risk), but preliminary tests show that equivalent results could be obtained reducing their number of an order of magnitude, as in [20] (which would reduce significantly the computational time).

A specific prediction algorithm (i.e., MLP with a single hidden layer) was considered. Preliminary tests were also performed with both a simpler and a more sophisticated regression algorithm: using a linear predictor, poor performances were obtained; on the other hand, considering an MLP with three hidden layers (with the same number of neurons as the optimal MLP with a single hidden layer, for each of the three layers), the performances were not statistically different from those of the simpler MLP, but with a larger computational cost. Thus, the MLP with a single hidden layer is preferable than the considered alternatives; however, further different possibilities could be tested in future.

Further work should also be devoted to the implementation of the proposed algorithm in a wireless body sensor system. Specific applications could then be tested, indicating the performances in terms of reduction of power and occupation of memory, still preserving the information of interest: for example, surface EMG in non-stationary conditions, extracting activity intervals or fatigue indexes [40]; sleep bruxism monitored from ECG and masseter EMG [41]; continuous EEG (e.g., during sleep [42], coma [43] or long detections waiting for an epileptic seizure [16]); body acceleration to detect dangerous conditions (e.g., possible falls of elderly people or of workers [17]).

The algorithm could also be used in a network of body sensors, either applied to each of them in turn, or integrating their joint information to improve the prediction [20].

@&#CONCLUSIONS@&#

This paper discusses a real time algorithm that schedules a non-uniform sampling adapted to the measured data, that reduces the number of acquired samples, but still preserving high frequency information, going beyond the Nyquist limit. The tests on experiments are promising (Figs. 2 and 3), showing that the adaptive algorithm outperforms both uniform under-sampling and CS using the same CRs.

If applied in wireless sensor networks, the method lowers the required data transmissions, saving energy and allowing an efficient management of the resources. Moreover, it could be integrated with other techniques, for a further compression or a more accurate recovery of the original data. Many potential future investigations are expected, e.g., for the individual self-assessment of healthy people or the remote monitoring of patients.

The author states no conflict of interest.

There was no source of funding for this research. Ethical approval was given by the local committee for the acquisition of acceleration data.

@&#ACKNOWLEDGMENTS@&#

The author thanks Prof. Dario Farina (Department of Neurorehabilitation Engineering, University Medical Center, Goettingen), Prof. Eros Pasero (Dipartimento di Elettronica e Telecomunicazioni, Politecnico di Torino) and Dr. Elisa Montalenti (Epilepsy Service, Department of Neuroscience, University of Torino) who provided the EMG, ECG and EEG, respectively.

Supplementary data associated with this article can be found in the online version at http://dx.doi.org/10.1016/j.compbiomed.2016.02.004.


                     
                        
                           
                              Supplementary material
                           
                           
                        
                     
                  

@&#REFERENCES@&#

