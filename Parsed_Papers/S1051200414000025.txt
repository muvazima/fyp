@&#MAIN-TITLE@&#An empirical study on improving dissimilarity-based classifications using one-shot similarity measure

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We study enhancing the classification accuracy of dissimilarity-based classifications (DBC).


                        
                        
                           
                           The study is done empirically when measuring the dissimilarity with one-shot similarity (OSS).


                        
                        
                           
                           Two DBC approaches using the Euclidean distance and the OSS distance measures are compared.


                        
                        
                           
                           The latter, albeit not always, enhances the classification accuracy for certain kinds of data.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Statistical pattern recognition

Dissimilarity-based classification (DBC)

Semi-supervised learning (SSL)

One-shot similarity (OSS) measure

@&#ABSTRACT@&#


               
               
                  This paper reports an experimental result obtained by additionally using unlabeled data together with labeled ones to improve the classification accuracy of dissimilarity-based methods, namely, dissimilarity-based classifications (DBC) [25]. In DBC, classifiers among classes are not based on the feature measurements of individual objects, but on a suitable dissimilarity measure among the objects instead. In order to measure the dissimilarity distance between pairwise objects, an approach using the one-shot similarity (OSS) [30] measuring technique instead of the Euclidean distance is investigated in this paper. In DBC using OSS, the unlabeled set can be used to extend the set of prototypes as well as to compute the OSS distance. The experimental results, obtained with artificial and real-life benchmark datasets, demonstrate that designing the classifiers in the OSS dissimilarity matrices instead of expanding the set of prototypes can further improve the classification accuracy in comparison with the traditional Euclidean approach. Moreover, the results demonstrate that the proposed setting does not work with non-Euclidean data.
               
            

@&#INTRODUCTION@&#

The aim of this paper is to report an empirical result obtained by additionally using unlabeled data together with labeled ones to improve the classification accuracy of dissimilarity-based methods, namely, dissimilarity-based classifications (DBC) [25]. In DBC, defining classifiers among the classes is not based on the feature measurements of individual objects, but rather on a suitable dissimilarity measure among the individual objects. The advantage of this strategy is that it offers a different way to include expert knowledge on the objects in classifying them [10]. A few of the issues we encounter when designing DBCs are as follows: selecting (creating) the prototype subset from a given data set [18,21,26]; reducing the dimensionality of the dissimilarity space [16,28]; solving non-Euclidean problems in the dissimilarity space (pseudo-Euclidean embedding) [10]; increasing the robustness of the dissimilarity space (or combining dissimilarity representations) [17]; optimizing classification (or clustering) based on dissimilarity increments (i.e., differentiation of dissimilarity distances) [2,13].

In order to explore the other issues, various strategies have been proposed in the literature. Among them, investigations have focused specifically on generalizing the dissimilarity representation by using various methods, such as feature lines and feature planes [23,24] and hidden Markov models [3]. In [23], the authors enrich (generalize) the dissimilarity representation by using the nearest feature rules. The generalization provided by the feature lines and/or planes covers all the possible intra-class pairs and triplets of prototypes to find the intrinsic geometric information available at the pairwise dissimilarities. The enrichment of the dissimilarity representation is beneficial for a specific structure of data, such as correlated (cigar-like or elongated) datasets having, possibly, a moderately nonlinear structure. In this strategy, however, objects are represented by a vector of dissimilarities with prototype feature lines (or planes) that are computed between objects of the same class. Consequently, the strategy has two drawbacks: the high amount of generated feature lines that increase computational cost [24] and the use of the labels of objects that leads to a supervised learning system.

On the other hand, when designing a DBC with a measuring system, we sometimes suffer from the difficulty of collecting sufficient (labeled) training data for each class. Labeled instances, for example, are often difficult, expensive, or time-consuming to obtain, as they require the services of an experienced expert. Meanwhile, unlabeled data, defined as the samples that do not belong to the classes being learned, may be relatively easy to collect, but the use of this type of data is limited. To address this problem, in a learning framework of semi-supervised learning (SSL) [1,4,6,29,32,33], a large amount of unlabeled data, together with labeled data, can be utilized to build better classifiers. Because SSL requires less human effort and results in higher accuracy, it is of great interest both in theory and in practice [14,19,20].

In DBC, the SSL strategy can also be considered to improve the classification performance. One of the easiest ways with which unlabeled data contribute to learn DBC classifiers is to simply append them to the representation set. Assume that the cardinalities of a training set, T, and the prototype subset (representation set), P, are denoted by 
                        |
                        T
                        |
                      and 
                        |
                        P
                        |
                     , respectively. When employing an additional unlabeled data U (where the sample size of the set U is 
                        |
                        U
                        |
                     ), the cardinality and the dimensionality of the dissimilarity row vectors that result are 
                        |
                        T
                        |
                      and 
                        |
                        P
                        |
                        +
                        |
                        U
                        |
                     , respectively. Here, a prototype selection method can be utilized to reduce the dimensionality of the dissimilarity space. Consequently, in the traditional feature-based classification (FBC), employing SSL strategy leads to increasing the cardinality of the training data, while, in DBC, utilizing the above strategy results in increasing the dimensionality of the training data. However, as in FBC, it is not also guaranteed that increasing the dimensionality leads to a situation in which the classification accuracy is improved.

In order to improve the classification performance of DBC in an SSL fashion, in this paper we use the well-known one-shot similarity (OSS) [30,31] measuring scheme based on the background information of available extra (unlabeled) data. To achieve this improvement, we first compute the confidence levels of the training data with the OSS distance. We then construct the dissimilarity matrices, where the dissimilarity is measured with the averaged OSS confidence levels. In OSS, when given two vectors, 
                        
                           
                              x
                           
                           
                              i
                           
                        
                      and 
                        
                           
                              x
                           
                           
                              j
                           
                        
                     , and an additionally available (unlabeled) data set, A, a measure of the (dis)similarity between 
                        
                           
                              x
                           
                           
                              i
                           
                        
                      and 
                        
                           
                              x
                           
                           
                              j
                           
                        
                      is computed as follows. First, a discriminative model is learned with 
                        
                           
                              x
                           
                           
                              i
                           
                        
                      as a single positive example and A as a set of negative examples. This model is then used to classify the other vector 
                        
                           
                              x
                           
                           
                              j
                           
                        
                     , and to obtain a confidence score. Next, a second such score is obtained by repeating the same process with the roles of 
                        
                           
                              x
                           
                           
                              i
                           
                        
                      and 
                        
                           
                              x
                           
                           
                              j
                           
                        
                      switched. Finally, the (dis)similarity of the two vectors can be obtained by averaging the above two scores.

The major task of this study is to deal with how the dissimilarity distance can be effectively measured. However, when a limited number of objects are available or the representational capability is insufficient to cover the possible variations of data, it is difficult to achieve the desired classification performance in the dissimilarity representation. To overcome this limitation and thereby improve the classification performance of DBC, in this paper we study a way of exploiting additionally available unlabeled data when measuring the dissimilarity distance with the OSS distance. As in SSL for FBC, we use the easily collected unlabeled data as the background data set, A, with which we can enrich the representational capability of the dissimilarity measures. That is, our goal is to effectively measure the dissimilarity distance with the additional unlabeled data as well as the labeled ones. In DBC, the SSL process is realized in representation stage, while, in FBC, it is implemented in generalization stage.

The main contribution of this paper is to demonstrate that the classification accuracy of DBC can be improved by using the OSS measuring technique based on unlabeled data. More specifically, experiments have been performed to demonstrate that the OSS distance measure performs better than the Euclidean distance measure. Here, the additional unlabeled set is used as well, but now differently than for building the set of prototypes: it is used in the distance measure and not in building the dissimilarity space. The remainder of the paper is organized as follows. In Section 2, after providing a brief introduction to DBC and OSS, we present an explanation for the use of OSS in DBC and an SSL-type DBC algorithm. Following this, in Section 3, we present an experimental setup for the traditional DBC and proposed DBC algorithms. In Section 4, we present the experimental results of artificial and real-life datasets. Finally, in Section 5, we present our concluding remarks as well as some feature works that deserve further study.

@&#RELATED WORK@&#

In this section, we briefly review the dissimilarity-based classification (DBC) approach and the one-shot similarity (OSS) measure, which are closely related to the present empirical study. The details of these algorithms can be found in the related literature [25,30,31].

A dissimilarity representation of a set of objects, 
                           T
                           =
                           
                              
                                 {
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 }
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 n
                              
                           
                           ⊂
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                         (d-dimensional examples), is based on pair-wise comparisons, and is expressed, for example, as an 
                           n
                           ×
                           m
                         dissimilarity matrix, 
                           
                              
                                 D
                              
                              
                                 T
                                 ,
                                 P
                              
                           
                           [
                           ⋅
                           ,
                           ⋅
                           ]
                        , where 
                           P
                           =
                           
                              
                                 {
                                 
                                    
                                       p
                                    
                                    
                                       j
                                    
                                 
                                 }
                              
                              
                                 j
                                 =
                                 1
                              
                              
                                 m
                              
                           
                           ⊂
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                        , a prototype subset, is extracted from T. The subscripts of D represent the set of elements, on which the dissimilarities are evaluated. Thus, each row, 
                           
                              
                                 D
                              
                              
                                 T
                                 ,
                                 P
                              
                           
                           [
                           i
                           ,
                           j
                           ]
                        , corresponds to the dissimilarity between the pairs of objects, 
                           〈
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 p
                              
                              
                                 j
                              
                           
                           〉
                        , where 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ∈
                           T
                         and 
                           
                              
                                 p
                              
                              
                                 j
                              
                           
                           ∈
                           P
                        . Consequently, when given a distance measure between two objects, 
                           ρ
                           (
                           ⋅
                           ,
                           ⋅
                           )
                        , an object, 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         (
                           1
                           ⩽
                           i
                           ⩽
                           n
                        ), is represented as a new feature vector, 
                           δ
                           (
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ,
                           P
                           )
                        , as follows:
                           
                              (1)
                              
                                 δ
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 P
                                 )
                                 =
                                 
                                    [
                                    ρ
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          1
                                       
                                    
                                    )
                                    ,
                                    ρ
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          2
                                       
                                    
                                    )
                                    ,
                                    …
                                    ,
                                    ρ
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          m
                                       
                                    
                                    )
                                    ]
                                 
                                 .
                              
                           
                        
                     

Here, the generated dissimilarity matrix, 
                           
                              
                                 D
                              
                              
                                 T
                                 ,
                                 P
                              
                           
                           [
                           ⋅
                           ,
                           ⋅
                           ]
                        , defines vectors in a dissimilarity space, on which the d-dimensional object, 
                           x
                        , given in the original feature space, is represented as an m-dimensional vector, 
                           δ
                           (
                           x
                           ,
                           P
                           )
                         or shortly 
                           δ
                           (
                           x
                           )
                        . Thus, for a test sample, 
                           z
                        , we can achieve the classification by invoking a classifier built in the dissimilarity space and operating it on the m-dimensional vector 
                           δ
                           (
                           z
                           )
                        .

As mentioned previously, the dissimilarity approach is originally developed for objects, not for feature vectors. However, the dissimilarities are now used as features and can be replaced without any problem by similarities. Thus, it should be noted that an approach defined for arbitrary distances between full objects is used for distances measured in a feature space.
                           1
                        
                        
                           1
                           We are grateful to the anonymous referee for providing us with the insight into this.
                        
                     

On the basis of what we have just explained briefly, a conventional algorithm for DBC is summarized as follows:
                           
                              1.
                              Select the prototype subset, P, from the training set, T, by using one of the prototype selection methods described in the literature [26].

Using Eq. (1), compute the dissimilarity matrix, 
                                    
                                       
                                          D
                                       
                                       
                                          T
                                          ,
                                          P
                                       
                                    
                                    [
                                    ⋅
                                    ,
                                    ⋅
                                    ]
                                 , in which the dissimilarity distance is computed on the basis of the given measure 
                                    ρ
                                    (
                                    ⋅
                                    ,
                                    ⋅
                                    )
                                 , such as the Euclidean distance (
                                    
                                       
                                          l
                                       
                                       
                                          2
                                       
                                    
                                  norm).

For a testing sample, 
                                    z
                                 , compute the corresponding row vector, 
                                    δ
                                    (
                                    z
                                    )
                                 , by using the same prototype subset and the distance measure used in Step 2.

Achieve the classification by invoking a classifier built in the dissimilarity space and operating it on the vector 
                                    δ
                                    (
                                    z
                                    )
                                 .

Assume that we have two feature vectors, 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                        , and an additionally available data set, A, which contains samples of objects not belonging to the same class as either 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         or 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                        , but are otherwise unlabeled. To measure OSS, a measure of the (dis)similarity of the two vectors, we first generate a hyperplane that separates 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         and A (and also 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                         and A). Then, we count the distance from 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                         (and also 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                        ) to the hyperplane decision surface. For a 2-class classification problem, for example, we begin with a simple case of designing a linear classifier described by 
                           g
                           (
                           x
                           )
                           =
                           
                              
                                 w
                              
                              
                                 T
                              
                           
                           x
                           +
                           
                              
                                 w
                              
                              
                                 0
                              
                           
                        . To make it clear, we focus on the binary LDA (Fisher's linear discriminant analysis). Then, we can derive a projection matrix, 
                           w
                        , by maximizing the Rayleigh quotient [8]. Once 
                           w
                         is estimated with the minimum mean-squared-error approximation and the pseudo-inverse technique [8], an unknown vector, 
                           z
                        , can be classified to class-1 (or class-2) if 
                           g
                           (
                           z
                           )
                           >
                           0
                         (or 
                           g
                           (
                           z
                           )
                           <
                           0
                        ); which generates a hyperplane decision surface.

Using LDA-based OSS mentioned above, the dissimilarity distance between the pairs of 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                         can be computed as follows. First, by assuming that the class-1 contains a single vector 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         and the class-2 corresponds to the set of A, we compute the absolute distance of 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         to the hyperplane that separates 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                         and A, 
                           
                              
                                 |
                                 g
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 |
                              
                              
                                 ‖
                                 w
                                 ‖
                              
                           
                        , where 
                           |
                           ⋅
                           |
                         (and 
                           ‖
                           ⋅
                           ‖
                        ) denotes the absolute value of a scaler (and the Euclidean norm of a vector), as follows [30]:
                           
                              (2)
                              
                                 γ
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 ,
                                 A
                                 )
                                 =
                                 
                                    
                                       |
                                       
                                          
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   j
                                                
                                             
                                             −
                                             
                                                
                                                   μ
                                                
                                                
                                                   A
                                                
                                             
                                             )
                                          
                                          
                                             T
                                          
                                       
                                       
                                          
                                             S
                                          
                                          
                                             W
                                          
                                          
                                             −
                                             1
                                          
                                       
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          
                                             
                                                
                                                   x
                                                
                                                
                                                   j
                                                
                                             
                                             +
                                             
                                                
                                                   μ
                                                
                                                
                                                   A
                                                
                                             
                                          
                                          2
                                       
                                       )
                                       |
                                    
                                    
                                       ‖
                                       
                                          
                                             S
                                          
                                          
                                             W
                                          
                                          
                                             −
                                             1
                                          
                                       
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                       −
                                       
                                          
                                             μ
                                          
                                          
                                             A
                                          
                                       
                                       )
                                       ‖
                                    
                                 
                                 ,
                              
                           
                         where 
                           
                              
                                 μ
                              
                              
                                 A
                              
                           
                         (and 
                           
                              
                                 S
                              
                              
                                 W
                              
                              
                                 −
                                 1
                              
                           
                        ) denotes the mean of all vectors (and the pseudo-inverse of the within-class covariance matrix) of A. Also, the bias term is computed using 
                           w
                         and the mean of 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                         and 
                           
                              
                                 μ
                              
                              
                                 A
                              
                           
                        , as follows: 
                           
                              
                                 w
                              
                              
                                 0
                              
                           
                           =
                           −
                           
                              
                                 w
                              
                              
                                 T
                              
                           
                           
                              
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 +
                                 
                                    
                                       μ
                                    
                                    
                                       A
                                    
                                 
                              
                              2
                           
                        .

Similarly, by repeating the same process with the roles of 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                         switched, we compute the distance of 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                         to the hyperplane that separates 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         and A as follows:
                           
                              (3)
                              
                                 γ
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 A
                                 )
                                 =
                                 
                                    
                                       |
                                       
                                          
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                             −
                                             
                                                
                                                   μ
                                                
                                                
                                                   A
                                                
                                             
                                             )
                                          
                                          
                                             T
                                          
                                       
                                       
                                          
                                             S
                                          
                                          
                                             W
                                          
                                          
                                             −
                                             1
                                          
                                       
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                       −
                                       
                                          
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                             +
                                             
                                                
                                                   μ
                                                
                                                
                                                   A
                                                
                                             
                                          
                                          2
                                       
                                       )
                                       |
                                    
                                    
                                       ‖
                                       
                                          
                                             S
                                          
                                          
                                             W
                                          
                                          
                                             −
                                             1
                                          
                                       
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          
                                             μ
                                          
                                          
                                             A
                                          
                                       
                                       )
                                       ‖
                                    
                                 
                                 .
                              
                           
                        
                     

Finally, by averaging these two distances, we can compute the dissimilarity of (1) as follows:
                           
                              (4)
                              
                                 
                                    
                                       ρ
                                    
                                    
                                       OSS
                                    
                                 
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    1
                                    2
                                 
                                 
                                    (
                                    γ
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                    ,
                                    A
                                    )
                                    +
                                    γ
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    A
                                    )
                                    )
                                 
                                 ,
                              
                           
                         where 
                           
                              
                                 x
                              
                              
                                 j
                              
                           
                         plays as a 
                           
                              
                                 p
                              
                              
                                 j
                              
                           
                         (
                           1
                           ⩽
                           j
                           ⩽
                           m
                        ).

The complexity for OSS, per pair, is thus 
                           O
                           (
                           
                              
                                 d
                              
                              
                                 2
                              
                           
                           )
                         once the (pseudo-)inverse 
                           
                              
                                 S
                              
                              
                                 W
                              
                           
                         has been computed. The details of this measuring scheme are omitted here, but can be found in the related literature [30,31].

When given an unlabeled data set, 
                           A
                           ⊂
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                        , 
                           l
                           =
                           |
                           A
                           |
                         (where 
                           |
                           ⋅
                           |
                         denotes the cardinality of a set), in addition to the existing prototype set, P, 
                           m
                           =
                           |
                           P
                           |
                        , the cardinality of the representation set, 
                           
                              
                                 P
                              
                              
                                 ′
                              
                           
                         (
                           =
                           P
                           ∪
                           A
                        ) 
                           ⊂
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                        , is 
                           m
                           +
                           l
                         when the entire set of the training data is selected as the prototypes. Thus, each row of the dissimilarity matrix (
                           
                              
                                 D
                              
                              
                                 T
                                 ,
                                 
                                    
                                       P
                                    
                                    
                                       ′
                                    
                                 
                              
                           
                           [
                           i
                           ,
                           j
                           ]
                        , 
                           1
                           ⩽
                           i
                           ⩽
                           n
                           ;
                           1
                           ⩽
                           j
                           ⩽
                           m
                           +
                           l
                        ), 
                           δ
                           (
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 P
                              
                              
                                 ′
                              
                           
                           )
                        , is represented as follows:
                           
                              (5)
                              
                                 δ
                                 
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          P
                                       
                                       
                                          ′
                                       
                                    
                                    )
                                 
                                 =
                                 
                                    [
                                    ρ
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          1
                                       
                                    
                                    )
                                    ,
                                    …
                                    ,
                                    ρ
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          m
                                       
                                    
                                    )
                                    ,
                                    ρ
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          m
                                          +
                                          1
                                       
                                    
                                    )
                                    ,
                                    …
                                    ,
                                    ρ
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          m
                                          +
                                          l
                                       
                                    
                                    )
                                    ]
                                 
                                 ,
                              
                           
                         where 
                           
                              
                                 p
                              
                              
                                 j
                              
                           
                           ∈
                           
                              
                                 P
                              
                              
                                 ′
                              
                           
                         and 
                           m
                           +
                           l
                           =
                           |
                           
                              
                                 P
                              
                              
                                 ′
                              
                           
                           |
                        .

In DBC, another way of utilizing the additional unlabeled data A is to measure the dissimilarity between the pairwise objects in the OSS distance by employing A as the background data. When measuring the OSS together with A, each row of the dissimilarity matrix (
                           
                              
                                 D
                              
                              
                                 T
                                 ,
                                 P
                              
                           
                           [
                           i
                           ,
                           j
                           ]
                        , 
                           1
                           ⩽
                           i
                           ⩽
                           n
                        ; 
                           1
                           ⩽
                           j
                           ⩽
                           m
                        ), 
                           
                              
                                 δ
                              
                              
                                 OSS
                              
                           
                           (
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ,
                           P
                           )
                        , is computed as follows:
                           
                              (6)
                              
                                 
                                    
                                       δ
                                    
                                    
                                       OSS
                                    
                                 
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 P
                                 )
                                 =
                                 
                                    [
                                    
                                       
                                          ρ
                                       
                                       
                                          OSS
                                       
                                    
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          1
                                       
                                    
                                    )
                                    ,
                                    
                                       
                                          ρ
                                       
                                       
                                          OSS
                                       
                                    
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          q
                                       
                                       
                                          2
                                       
                                    
                                    )
                                    ,
                                    …
                                    ,
                                    
                                       
                                          ρ
                                       
                                       
                                          OSS
                                       
                                    
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          m
                                       
                                    
                                    )
                                    ]
                                 
                                 ,
                              
                           
                         where 
                           
                              
                                 p
                              
                              
                                 j
                              
                           
                           ∈
                           P
                         and 
                           m
                           =
                           |
                           P
                           |
                        .

On the basis of what we explained previously, an algorithm for SSL-type DBC is summarized as follows:
                           
                              1.
                              Obtain the labeled training set T, the prototype subset P, and the unlabeled set A as input datasets.

Using Eq. (5) or (6), rather than Eq. (1), compute the dissimilarity matrix, 
                                    
                                       
                                          D
                                       
                                       
                                          T
                                          ,
                                          P
                                       
                                    
                                    [
                                    ⋅
                                    ,
                                    ⋅
                                    ]
                                 , in which the dissimilarity distance is computed on the basis of the Euclidean/OSS distance described in Section 2.2.

This step is the same as Step 3 in the DBC described in Section 2.1.

This step is the same as Step 4 in the DBC described in Section 2.1.

The rationale for the above algorithm is presented in subsequent sections together with the experimental results.

The time complexities of both (conventional) DBC and SSL-type DBC algorithms can be analyzed as follows. As in the case of DBC, almost all the processing CPU-time of SSL-type DBC is also consumed in computing the dissimilarity matrix. So, the difference in magnitude between the computational complexities of the two algorithms depends on the computational costs associated with the two matrices. More specifically, Step 1 requires 
                           O
                           (
                           1
                           )
                         time in both algorithms. Then, in DBC, Step 2 of computing the 
                           n
                           ×
                           m
                         dissimilarity matrix requires 
                           O
                           (
                           d
                           n
                           m
                           )
                         time, where d, n, and m are, respectively, the dimensionality, the number of training samples, and the number of prototypes. On the other hand, the computation of that of SSL-type DBC needs 
                           O
                           (
                           
                              
                                 d
                              
                              
                                 2
                              
                           
                           +
                           d
                           n
                           (
                           m
                           +
                           l
                           )
                           )
                         for Eq. (5) (or 
                           O
                           (
                           
                              
                                 d
                              
                              
                                 2
                              
                           
                           +
                           d
                           n
                           m
                           )
                         for Eq. (6)) time
                           2
                        
                        
                           2
                           It should be noted that the time needed for Step 2 is a sum of the times to compute the LDA-based OSS measure 
                                 γ
                                 (
                                 ⋅
                                 ,
                                 ⋅
                                 ,
                                 A
                                 )
                               and the dissimilarity matrix 
                                 
                                    
                                       D
                                    
                                    
                                       T
                                       ,
                                       P
                                    
                                 
                                 [
                                 ⋅
                                 ,
                                 ⋅
                                 ]
                              , where the two terms can be computed in 
                                 O
                                 (
                                 
                                    
                                       d
                                    
                                    
                                       2
                                    
                                 
                                 )
                               and 
                                 O
                                 (
                                 d
                                 n
                                 m
                                 )
                               operations, respectively [30].
                         in executing Step 2, where l is the cardinality of the unlabeled data set, and the term 
                           
                              
                                 d
                              
                              
                                 2
                              
                           
                         is for computing the OSS confidence between the pairwise objects. Next, in DBC and SSL-type DBC, the times of computing 
                           δ
                           (
                           z
                           )
                         at Step 3 require 
                           O
                           (
                           d
                           m
                           )
                         and 
                           O
                           (
                           d
                           (
                           m
                           +
                           l
                           )
                           )
                         (or 
                           O
                           (
                           d
                           m
                           )
                         for Eq. (6)) times, respectively. Finally, in both algorithms, classification requires 
                           O
                           (
                           
                              
                                 t
                              
                              
                                 c
                              
                           
                           )
                         time, where 
                           
                              
                                 t
                              
                              
                                 c
                              
                           
                         is the time for classifying the test vector with a classifier designed in the dissimilarity space. From this analysis, it can be seen that the required time for SSL-type DBC is more sensitive to the dimensionality, the number of examples, and the number of prototypes (and the cardinality of the unlabeled data set) than that for DBC.

The proposed approach has been tested and compared with the traditional ones. This was done by performing experiments on two artificial data, namely, the Difficult (a normally distributed 5-dimensional 2-class) data [11]
                        
                           3
                        
                        
                           3
                           
                              http://prtools.org/.
                         and the Balls3D (an uniformly distributed 3-dimensional 2-class) data [9,11]
                        
                           4
                        
                        
                           4
                           
                              http://prlab.tudelft.nl/software/DisTools.html.
                         and other multivariate datasets cited from the UCI machine learning repository [12]
                        
                           5
                        
                        
                           5
                           
                              http://www.ics.uci.edu/~mlearn/MLRepository.html.
                         and SSL-type benchmarks [6].
                           6
                        
                        
                           6
                           
                              http://www.kyb.tuebingen.mpg.de/ssl-book/.
                         Characteristics of the UCI and SSL-type benchmark datasets are summarized in Table 1
                        . Here, we chose four different numbers of classes of data: 2, 6, 9, and 20 classes. Moreover, the reader should observe that some datasets are class-imbalance problems, while other sets are small sample size problems.

The experiment focuses on a few simple binary and multi-class classification problems, where all datasets are initially split into three subsets,
                           7
                        
                        
                           7
                           Specific distinct letters A and U are used just for ease notation. The available extra data subsets are first specified in terms of the letter A, but then the unlabeled data subsets are represented using the letter U.
                         labeled training data, L, labeled test (evaluation) data, E, and unlabeled data, U, at a ratio of 20% : 10% : 70%. After dividing each data set into these subsets 30 times, the training and test procedures are repeated 30 times per division and the results obtained are averaged. As a consequence, the total number of repetitions is 900 (
                           =
                           30
                           ×
                           30
                        ) times.

Specifically, in this experiment, classifications are carried out in four ways in the dissimilarity space, which are named 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , and 
                           
                              
                                 D
                              
                              
                                 L
                                 E
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        . The details of the four DBC approaches are itemized as follows:
                           
                              1.
                              
                                 
                                    
                                       
                                          D
                                       
                                       
                                          L
                                          −
                                          
                                             
                                                l
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 : the representation set (prototypes), P, is randomly selected from labeled training set L (i.e., 
                                    P
                                    ⊆
                                    L
                                 ) and the dissimilarity distance between the pairwise objects, 
                                    δ
                                    (
                                    ⋅
                                    ,
                                    P
                                    )
                                 , is measured with Eq. (1), in which 
                                    ρ
                                    (
                                    ⋅
                                    ,
                                    ⋅
                                    )
                                  is the Euclidean distance (
                                    
                                       
                                          l
                                       
                                       
                                          2
                                       
                                    
                                  fmetric). Here, U is precluded.


                                 
                                    
                                       
                                          D
                                       
                                       
                                          OSS
                                       
                                    
                                 : as in 
                                    
                                       
                                          D
                                       
                                       
                                          L
                                          −
                                          
                                             
                                                l
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 , the representation set P is also randomly selected from labeled training set L (i.e. 
                                    P
                                    ⊆
                                    L
                                 ), but the dissimilarity distance, 
                                    
                                       
                                          δ
                                       
                                       
                                          OSS
                                       
                                    
                                    (
                                    ⋅
                                    ,
                                    P
                                    )
                                 , is measured with Eq. (6), in which 
                                    
                                       
                                          ρ
                                       
                                       
                                          OSS
                                       
                                    
                                    (
                                    ⋅
                                    ,
                                    ⋅
                                    )
                                  is the LDA-based OSS distance, 
                                    
                                       
                                          γ
                                       
                                       
                                          ¯
                                       
                                    
                                    (
                                    ⋅
                                    ,
                                    ⋅
                                    ,
                                    A
                                    )
                                 , utilizing U as an A.


                                 
                                    
                                       
                                          D
                                       
                                       
                                          L
                                          U
                                          −
                                          
                                             
                                                l
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 : the representation set P is randomly selected from the union of labeled set L and unlabeled set U (i.e. 
                                    P
                                    ⊂
                                    {
                                    L
                                    ∪
                                    U
                                    }
                                  having 
                                    |
                                    P
                                    |
                                    ⩽
                                    |
                                    L
                                    |
                                 ) and the dissimilarity distance, 
                                    δ
                                    (
                                    ⋅
                                    ,
                                    P
                                    )
                                 , is measured with Eq. (5), in which 
                                    ρ
                                    (
                                    ⋅
                                    ,
                                    ⋅
                                    )
                                  is also the Euclidean distance (
                                    
                                       
                                          l
                                       
                                       
                                          2
                                       
                                    
                                  metric).


                                 
                                    
                                       
                                          D
                                       
                                       
                                          L
                                          E
                                          −
                                          
                                             
                                                l
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 : the representation set P is randomly selected from the union of labeled set L and evaluation set E (i.e. 
                                    P
                                    ⊂
                                    {
                                    L
                                    ∪
                                    E
                                    }
                                  having 
                                    |
                                    P
                                    |
                                    ⩽
                                    |
                                    L
                                    |
                                 ) and the dissimilarity distance, 
                                    δ
                                    (
                                    ⋅
                                    ,
                                    P
                                    )
                                 , is measured with Eq. (5), in which 
                                    ρ
                                    (
                                    ⋅
                                    ,
                                    ⋅
                                    )
                                  is also the Euclidean distance (
                                    
                                       
                                          l
                                       
                                       
                                          2
                                       
                                    
                                  metric) [7].

Here, 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         is designed as a typical DBC approach in which the classifier is trained using L only. Then, 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         (and 
                           
                              
                                 D
                              
                              
                                 L
                                 E
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        ) is implemented as a reference to the DBC approaches, in which U (and E) is used for expending the set of prototypes: it is used in building the dissimilarity space and not in measuring the dissimilarity distance. In both the approaches, in order to select prototypes from L, 
                           {
                           L
                           ∪
                           U
                           }
                        , or 
                           {
                           L
                           ∪
                           E
                           }
                        , the Random selection is utilized in the experiment. However, other various methods described in the literature [25,26], such as RandomC, KCentres, ModeSeek, LinProg, FeatSel, KCentres-LP, EdiCon, etc., can be considered. Also, it should be pointed out that 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 L
                                 E
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         may have different prototypes, selected from 
                           {
                           L
                           ∪
                           U
                           }
                         and 
                           {
                           L
                           ∪
                           E
                           }
                        , respectively, while 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         have the same ones.

Finally, to evaluate the classification accuracies of all of the approaches, two classifiers based on the k-nearest neighbor rule and the support vector machine algorithm are utilized. The two classifiers were implemented with PRTools [11] and denoted in subsequent sections as NN (where 
                           k
                           =
                           1
                        ) and SVM, respectively. In particular, SVM was implemented using the software provided in LIBSVM [5] (i.e. svmtrain and svmpredict) and using the linear kernel function.

@&#EXPERIMENTAL RESULTS@&#

First, the experimental results obtained with the two classifiers trained in the four approaches (i.e., 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , and 
                           
                              
                                 D
                              
                              
                                 L
                                 E
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        ) for two artificial data, Difficult Data [11] and Balls3D [9,11] data, were probed into. We first generated a 5-dimensional 2-class Difficult Data of the positive and negative examples of 
                           [
                           300
                           ,
                           300
                           ]
                        , and divided them into L, E, and U subsets at a ratio of 
                           20
                           %
                           :
                           10
                           %
                           :
                           70
                           %
                        . Then, we performed the experiment as mentioned previously: classification error rates were evaluated against the cardinality of the representation set (prototypes) P, the (labeled) training set L, and the unlabeled set U. First, Fig. 1
                         presents the error rates of NN and SVM trained in the four approaches for Difficult Data with varying the cardinality of the representation set (prototypes) P, while 
                           |
                           L
                           |
                         and 
                           |
                           U
                           |
                         are kept constant. Here, the x and y axes represent the seven different cardinalities of P and the averaged error rates, respectively. Also, in the x-axis, the value of 120 is the maximum cardinality of P.

Observations obtained from the two pictures shown in Fig. 1 are the following. First, in Fig. 1(a) for Difficult Data, it should be pointed out that the error rates of NN trained in the 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , and 
                           
                              
                                 D
                              
                              
                                 L
                                 E
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         approaches, marked with the ◯, □, ◁, and ▷ symbols, decrease uniformly as the cardinality of P increases. In particular, four curves marked with different symbols have the same shape in general, maintaining a consistent difference from each other. This comparison demonstrates that the classification accuracy of NN trained in 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , marked with the □ symbol, is always the lowest among the four error rates when having an appropriate number of prototypes. Next, in Fig. 1(b) for Difficult Data, it should also be pointed out that the error rates of SVM trained in 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , marked with the ◯ and □ symbols, decrease uniformly too, while the other two error rates obtained in 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 L
                                 E
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         are almost the same for different numbers of prototypes. From this observation, we can see that simply including an available unlabeled data set (and evaluation data set), U (and E), to the existing prototype set or its randomly selected subset, i.e., 
                           P
                           ⊂
                           {
                           L
                           ∪
                           U
                           }
                         (and 
                           P
                           ⊂
                           {
                           L
                           ∪
                           E
                           }
                        ), having the cardinality of 
                           |
                           L
                           |
                        , does not succeed in enhancing the classification performance. On the other hand, however, using U in DBC through the OSS distance, i.e., 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , leads to increasing performance. Also, we can see that the error rates become stable when the cardinality of the prototype set (P) is larger than some percent of that of the labeled training set (L), which means that, as reported in the related literature [7] and [26], it is not required to select the whole training data as the representation set.

Another characteristic of the semi-supervised setting through the OSS distance was further investigated by varying the cardinalities of the training set. In this experiment, NN and SVM trained in 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         approaches were evaluated with respect to different cardinalities of the training set; the entire training set selected served as the representation set (
                           P
                           =
                           L
                        ). Fig. 2
                         presents the error rates of NN and SVM trained in 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         for Difficult Data with varying the cardinality of the labeled training set L, having 
                           P
                           =
                           L
                        . Here, the x and y axes represent the seven different cardinalities of L and the averaged error rates, respectively. Also, in the x-axis, the value of 420 is the maximum cardinality of L.

Observations obtained from the two pictures shown in Fig. 2 are similar to those of Fig. 1 as a whole: the error rates of NN and SVM trained in 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         approaches decrease uniformly as the cardinality of the (labeled) training set (L) increases. This observation demonstrates that the classification accuracy of NN and SVM trained in 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , marked with the □ symbol, is always lower than that of the classifiers trained in 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , marked with the ◯ symbol, when having an appropriate number of training samples. Here, for fair comparison, 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 L
                                 E
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         approaches, which are of expending the set of prototypes, were excluded.

Finally, it is also interesting to investigate impacts of the cardinalities of the unlabeled data set. To address this issue, we evaluated the error rates of NN and SVM trained in 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         with varying the cardinality of the unlabeled set U, under the condition 
                           P
                           =
                           L
                        .

Observations obtained from the experimental results on comparing the error rates of the two classifiers are the following. For Difficult Data, the error rates obtained with NN slightly decrease and become stable as the cardinality of U increases, while those of SVM are almost the same as the U's size increases (i.e., the curve is flat). From this observation, we can see that, in 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , the cardinality of the unlabeled set and the improvement in the error rates may not have a deep relationship.

In order to investigate the characteristics of the approaches for non-Euclidean data, we repeated the above experiment with Balls3D data.
                           8
                        
                        
                           8
                           We are grateful to the anonymous Referees who pointed out this issue to us.
                         As mentioned previously, this dataset has been generated by the DisTools command genballd([100 100], 3, [0.02 0.04]) which generates the given numbers of 3-D balls with sizes [0.02 0.04] in a 3-D hypercube. After generating a 3-dimensional 2-class Balls3D data set of the size of 
                           [
                           100
                           ,
                           100
                           ]
                        , we first divided them into the L, E, and U subsets at a ratio of 20% : 10% : 70%. Then, we performed the experiment as have done for Figs. 1 and 2.

Here, the dissimilarity distances of the DBC approaches were measured as follows. In 
                           
                              
                                 D
                              
                              
                                 L
                                 -nonEuclidean
                              
                           
                        , 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 -nonEuclidean
                              
                           
                        , and 
                           
                              
                                 D
                              
                              
                                 L
                                 E
                                 - nonEuclidean
                              
                           
                         approaches, the dissimilarity was computed as the shortest distance between two points on the surface of two balls [9]. On the other hand, in 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , the dissimilarity was measured in the OSS distance between two 3-dimensional feature vectors.

The aim of this experiment is to explore the characteristics of the DBC approaches and to further investigate its impact on non-Euclidean data. First, we evaluated the error rates of NN and SVM trained in the four approaches for Balls3D data with varying the cardinality of the representation set (prototypes) P. Here, 
                           |
                           L
                           |
                         and 
                           |
                           U
                           |
                         are kept constant.

Observations obtained from the results obtained are the following. Unlike those of Difficult Data shown in Fig. 1, all of the classification error rates obtained with the four approaches are almost the same (i.e., the curves are flat) for different numbers of prototypes, which means that, for the non-Euclidean data, enhancing DBC using the OSS distance as a semi-supervised learning strategy does not work. Therefore, as a preprocessing stage for the proposed method, utilizing certain kind of embedding scheme, such as pseudo Euclidean space (PES) neglecting the negative definite subspace [9,10], could be considered and remains unchallenged.

Next, for Balls3D data, it is also interesting to investigate impacts of the cardinalities of the (labeled) training set and the unlabeled set. To address these issues, we evaluated the error rates of NN and SVM trained in 
                           
                              
                                 D
                              
                              
                                 L
                                 -nonEuclidean
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         approaches with varying 
                           |
                           L
                           |
                        . From the results obtained, we observed that classification error rates obtained are almost the same too; all of the curves of the error rates are flat, which is very similar to the above observation.

On the other hand, in the above experiments, especially for Difficult Data, it was observed that using U through the OSS distance can lead to increasing the classification accuracy of DBC. Although the class-label of U was not used in the training phase, however, U as well as both L and E have been selected from a given training data set T, i.e., 
                           {
                           L
                           ∪
                           E
                           ∪
                           U
                           }
                           ⊆
                           T
                        . As a consequence, the three subsets of L, E, and U share the same distribution characteristics. From this point of view, the following question is an interesting issue to investigate: is the classification accuracy of 
                        
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         
                        superior or inferior to that of the traditional scheme when U is collected from a completely different data set? Addressing this issue remains open.

In summary, from these observations, it can be mentioned that the classification accuracy of DBC classifiers, such as NN and SVM in the case of Euclidean data, can be further improved by using an available U through the OSS distance. However, simply including the whole set of U or partially selecting them as a P does not work. Also, it should be noted that, for non-Euclidean data, the error rates obtained with the supervised and semi-supervised settings are almost the same as the cardinality of P (and L) increases. The characteristic of the DBC using OSS can be observed again in subsequent experiments.

In order to further investigate the characteristics of the 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         approach, and, especially, to find out which kinds of significant datasets are more suitable for the approach, we repeated the experiment with a few of the UCI and SSL-type benchmark datasets, as summarized in Table 1. After dividing each data set into the L, E, and U subsets 30 times at a ratio of 20% : 10% : 70%, for each division, we performed the training and evaluation procedures 30 times again and computed the error rates by averaging the results obtained. Also, classification was carried out with two classifiers, namely, NN (
                           k
                           =
                           1
                        ) and SVM (linear kernel function), which were, respectively, designed in the four approaches of 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , and 
                           
                              
                                 D
                              
                              
                                 L
                                 E
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        .


                        Table 2
                         and Table 3
                         present a numerical comparison of the mean error rates (± standard deviations) (%) obtained with the two classifiers, respectively. Here, the results shown in the fifth and the sixth columns, i.e. those of 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 L
                                 E
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , are obtained with two specific cases of 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        . In the former case, the prototype set is randomly selected from the union set of the labeled and unlabeled sets, i.e. 
                           P
                           ⊂
                           {
                           L
                           ∪
                           U
                           }
                        , having the cardinality of 
                           |
                           L
                           |
                        . Meanwhile, in the latter case, it is randomly selected from the union set of the labeled and evaluation sets, i.e. 
                           P
                           ⊂
                           {
                           L
                           ∪
                           E
                           }
                        , having the cardinality of 
                           |
                           L
                           |
                        . Besides, in both 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , the entire set of L is served as a P, while, in 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , U is utilized as an A. In addition, in order to facilitate the comparison, the lowest error rate in each data set is highlighted with a 
                           
                              
                              
                                 ⁎
                              
                           
                         marker.


                        Table 2, presenting the error rates obtained with NN designed in the four DBC approaches for the thirteen datasets, shows the similar characteristics as the ones we obtained in Fig. 1(a). In the table, we observed that all of the lowest error rates (
                           
                              
                              
                                 ⁎
                              
                           
                         marked) were achieved with 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        . In contrast, Table 3, presenting the error rates obtained with SVM, shows different features: in the table, especially for SSL-type datasets, we observed that almost all of the highlighted ones were acquired with 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , while, for UCI datasets, most of the error rates highlighted with the 
                           
                              
                              
                                 ⁎
                              
                           
                         marker were achieved with 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        . This means that, in SVM, which is a “kernelized” classifier, there is no specific approach that yields the best results for all the families of applications in terms of its classification accuracy. The best classifier and/or DBC approach for one data set is not the best for another data set.

Although it is hard to quantitatively compare the four approaches, to render this comparative evaluation more complete, we have attempted to do exactly this. In order to achieve this goal, we have first graded every DBC approach tested in terms of their quality indices, where the latter is of counting the number of the best accuracies obtained in the experiment. More specifically, for each DBC approach, we merely counted the numbers of the error rates highlighted with the ⁎ marker, obtained with the nine UCI and four SSL-type datasets. For example, consider the thirteen datasets in Table 2. Here, the numbers of the counted error rates highlighted with the * marker for the four columns of 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , and 
                           
                              
                                 D
                              
                              
                                 L
                                 E
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         are, respectively, 0, 13, 0, and 0. Meanwhile, for the datasets in Table 3, the numbers of the highlighted rates for the four columns are, respectively again, 1, 7, 3, and 2. From this counting, it can be observed that 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         performs slightly better (higher score) than the other DBCs in terms of the classification accuracy. Meanwhile, it should also be pointed out that it is sometimes inferior to other DBC approach, i.e. 
                           
                              
                                 D
                              
                              
                                 L
                                 U
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         or 
                           
                              
                                 D
                              
                              
                                 L
                                 E
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , when dealing with SVM for the UCI/SSL-type datasets (refer to Table 3).

In addition to this simplistic comparison, in order to demonstrate the significant differences in the error rates between the four approaches used in the experiments, for the means (μ) and standard deviations (σ) shown in Table 2 and Table 3, the Student's statistical two-sample test [15] can be conducted. More specifically, using a publicly available t-test package (e.g. ttest
                        
                           9
                        
                        
                           9
                           
                              http://www.mathworks.com/.
                        ), the p-value can be obtained in order to determine the significance of the difference between these DBC approaches. Fig. 3
                         presents a comparison of the p-values obtained using the t-test for the UCI/SSL-type datasets, where the p-value represents the probability that the error rates of the proposed 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         approach are generally smaller than those of the traditional 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         approach.

From Fig. 3(a), it can be observed that some datasets have significant differences between the two approaches. For example, for the Heart dataset with 
                           
                              
                                 μ
                              
                              
                                 1
                              
                           
                           (
                           
                              
                                 σ
                              
                              
                                 1
                              
                           
                           )
                           =
                           0.3484
                           (
                           0.0862
                           )
                         for the 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         approach and 
                           
                              
                                 μ
                              
                              
                                 2
                              
                           
                           (
                           
                              
                                 σ
                              
                              
                                 2
                              
                           
                           )
                           =
                           0.3865
                           (
                           0.0789
                           )
                         for the 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         approach (refer to Table 2), a p-value of 0.9603 was obtained for the two approaches. As a consequence, because 
                           p
                           >
                           0.95
                         at the 
                           5
                           %
                         significance level, the null hypothesis 
                           
                              
                                 H
                              
                              
                                 0
                              
                           
                           :
                           
                              
                                 μ
                              
                              
                                 1
                              
                           
                           (
                           
                              
                                 σ
                              
                              
                                 1
                              
                           
                           )
                           =
                           
                              
                                 μ
                              
                              
                                 2
                              
                           
                           (
                           
                              
                                 σ
                              
                              
                                 2
                              
                           
                           )
                         was rejected and the alternative hypothesis 
                           
                              
                                 H
                              
                              
                                 1
                              
                           
                           :
                           
                              
                                 μ
                              
                              
                                 1
                              
                           
                           (
                           
                              
                                 σ
                              
                              
                                 1
                              
                           
                           )
                           <
                           
                              
                                 μ
                              
                              
                                 2
                              
                           
                           (
                           
                              
                                 σ
                              
                              
                                 2
                              
                           
                           )
                         was accepted. In a similar manner, it can be observed that the seven datasets, including the Dermatology, Heart, Laryngeal1, Yeast, COIL, COIL2, and USPS datasets, performed better at significant levels of both 5% and 10%. In contrast, from Fig. 3(b), it can be observed that the four datasets, including the Dermatology, Diabetes, Heart, and Malaysia datasets, performed better at significant levels of both 5% and 10%. From this observation, it should be noted that, for certain types of datasets, the classification error rate of the NN designed in 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         approach, albeit not always, is smaller than that of the 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         approach.

In the above experiment, the OSS dissimilarity matrix (
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        ) was considered as a set of row vectors (i.e. feature vectors). Nonetheless, the 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrix can also be considered as a kernel matrix on the original feature space. In other words, the methods of using 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         can be divided into two categories: one is to use it metric-like; the other is to use it as a kernel matrix. From this point of view, it is of interest to investigate how much the two methods are different when designing a classifier. In order to address this issue, we designed two SVMs, named SVM-
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         and SVM-
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        -as-kernel, using the UCI/SSL-type datasets. In SVM-
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , as in the above experiments, the 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrix was considered as a set of row vectors, one for every object. However, in SVM-
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        -as-kernel, the 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrix was used as a kernel matrix. Fig. 4
                         presents a comparison of the classification error rates obtained with the SVMs, implemented differently using the UCI/SSL-type data. Here, 
                           
                              
                                 S
                              
                              
                                 3
                              
                           
                           V
                           
                              
                                 M
                              
                              
                                 Euclidean
                              
                           
                        , which is a semi-supervised SVM classifier trained in the original feature space, is presented as a reference to provide a fair comparison. In this experiment, the 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrix was constructed using the LDA-based OSS measure, while (linear) SVM and 
                           
                              
                                 S
                              
                              
                                 3
                              
                           
                           V
                           
                              
                                 M
                              
                              
                                 Euclidean
                              
                           
                         were implemented using publicly available software (see Sections 2.2 and 3.2).

Observations obtained from the figure are as follows. When comparing SVM-
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         to SVM-
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        -as-kernel (as well as 
                           
                              
                                 S
                              
                              
                                 3
                              
                           
                           V
                           
                              
                                 M
                              
                              
                                 Euclidean
                              
                           
                        ), there is no absolute winner. Specifically, for some datasets, SVM-
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         is superior to SVM-
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        -as-kernel (as well as 
                           
                              
                                 S
                              
                              
                                 3
                              
                           
                           V
                           
                              
                                 M
                              
                              
                                 Euclidean
                              
                           
                        ), in terms of classification accuracy. On the contrary, for the other datasets, the result is the opposite; the former is inferior to the latter. For example, for the USPS data, SVM-
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         is superior to SVM-
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        -as-kernel (and 
                           
                              
                                 S
                              
                              
                                 3
                              
                           
                           V
                           
                              
                                 M
                              
                              
                                 Euclidean
                              
                           
                        ), whereas for the Dermatology data, the result is the opposite.

From these observations, it is also of interest to investigate why and when the 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrix is beneficial. In order to address this issue, non-Euclidean behavior [27] of the 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrix was investigated. In particular, when using pseudo-Euclidean embedding algorithms, such as a PRTools package pe_em (pseudo-Euclidean linear embedding),
                           10
                        
                        
                           10
                           
                              http://prtools.org/.
                         the symmetric 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrix was embedded in a pseudo-Euclidean space (PES) [9,22,25]. From the derived PES, in order to inspect the amount of non-Euclidean influence, two indices, number of positive and negative eigenvalues (
                           
                              
                                 λ
                              
                              
                                 i
                              
                           
                           >
                           0
                         and 
                           
                              
                                 λ
                              
                              
                                 j
                              
                           
                           <
                           0
                        ) and negative eigenfraction (NEF) [25], were measured. Here, NEF was computed as 
                           
                              
                                 ∑
                              
                              
                                 j
                                 =
                                 p
                                 +
                                 1
                              
                              
                                 p
                                 +
                                 q
                              
                           
                           |
                           
                              
                                 λ
                              
                              
                                 j
                              
                           
                           |
                           /
                           (
                           
                              
                                 ∑
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 p
                              
                           
                           |
                           
                              
                                 λ
                              
                              
                                 i
                              
                           
                           |
                           +
                           
                              
                                 ∑
                              
                              
                                 j
                                 =
                                 p
                                 +
                                 1
                              
                              
                                 p
                                 +
                                 q
                              
                           
                           |
                           
                              
                                 λ
                              
                              
                                 j
                              
                           
                           |
                           )
                           ∈
                           [
                           0
                           ,
                           1
                           ]
                        . Table 4
                         presents a comparison of the two indices measured in the embedded pseudo-Euclidean spaces for the two different dissimilarity matrices, i.e. the 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 l
                                 2
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrices, using the UCI/SSL-type data.

From Table 4, for the 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 l
                                 2
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrices, it can be observed that the indices are significantly different. First, values of NEF for the 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 l
                                 2
                              
                           
                         matrix are all zero across the data, while those of the 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrix are in the range of 0.4 to 0.5 (refer to the fifth and the sixth columns of the table). Second, for the number of negative eigenvalues 
                           (
                           
                              
                                 λ
                              
                              
                                 j
                              
                           
                           <
                           0
                           )
                        , the 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrix has much greater values than the 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 l
                                 2
                              
                           
                         matrix. For example, for the 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrix of the Diabetes data, # of 
                           
                              
                                 λ
                              
                              
                                 i
                              
                           
                           (
                           >
                           0
                           )
                        /# of 
                           
                              
                                 λ
                              
                              
                                 j
                              
                           
                           (
                           <
                           0
                           )
                         is 85.8/69.2, while for the 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         matrix of the ame data, it is 154.5/0.5. On the contrary, for the COIL data, both 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         and 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         have values at 299.7/0.3 and 299.4/0.6, respectively. From this observation, however, it is also difficult to find certain types of relation between classification accuracies and non-Euclidean properties. In view of the above considerations, a question arises: why does the SVM designed with 
                        
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         
                        not work for certain applications? The theoretical investigation of the underlying reason for this remains unchallenged.

In review, it is not easy to decide which types of significant approaches are more suitable for the DBC to use the OSS distance based on additional unlabeled data. However, in order to achieve this goal, the classification accuracies of the two SVMs, i.e. SVM-
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         and SVM-
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        -as-kernel, were compared first. Then, non-Euclidean behaviors of the 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                         matrix were continuously investigated. Taking into account these considerations, the reader should observe that the classification accuracy of NN designed with 
                           
                              
                                 D
                              
                              
                                 OSS
                              
                           
                        , rather than SVM, is better than that of the classifiers designed in 
                           
                              
                                 D
                              
                              
                                 L
                                 −
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        .

@&#CONCLUSIONS@&#

In our efforts to improve the classification performance of DBC, we empirically evaluated an approach using the OSS measuring technique based on the background information of available unlabeled data. After dividing each data set into the subsets L, E, and U, we constructed dissimilarity matrices through the Euclidean distance or the OSS distance using all three subsets or only one part. The proposed approach, named 
                        
                           
                              D
                           
                           
                              OSS
                           
                        
                     , was tested on two artificially generated Euclidean and non-Euclidean datasets (i.e. Difficult Data and Balls3D) and thirteen UCI/SSL-type datasets. The results obtained were compared with those of three traditional approaches, named 
                        
                           
                              D
                           
                           
                              L
                              −
                              
                                 
                                    l
                                 
                                 
                                    2
                                 
                              
                           
                        
                     , 
                        
                           
                              D
                           
                           
                              L
                              U
                              −
                              
                                 
                                    l
                                 
                                 
                                    2
                                 
                              
                           
                        
                     , and 
                        
                           
                              D
                           
                           
                              L
                              E
                              −
                              
                                 
                                    l
                                 
                                 
                                    2
                                 
                              
                           
                        
                     . Here, 
                        
                           
                              D
                           
                           
                              L
                              −
                              
                                 
                                    l
                                 
                                 
                                    2
                                 
                              
                           
                        
                      represents a traditional DBC, in which the dissimilarity distance was measured in the Euclidean distance using L only, while 
                        
                           
                              D
                           
                           
                              L
                              U
                              −
                              
                                 
                                    l
                                 
                                 
                                    2
                                 
                              
                           
                        
                      denotes a DBC extending the set of prototypes using U as well as L, and measuring the dissimilarity distance in the Euclidean distance. 
                        
                           
                              D
                           
                           
                              L
                              E
                              −
                              
                                 
                                    l
                                 
                                 
                                    2
                                 
                              
                           
                        
                      is similar to 
                        
                           
                              D
                           
                           
                              L
                              U
                              −
                              
                                 
                                    l
                                 
                                 
                                    2
                                 
                              
                           
                        
                      but uses E instead of U.

Our experimental results, obtained from the numerical and statistical (t-test) comparisons, demonstrate that the classification accuracy of the NN classifiers designed in 
                        
                           
                              D
                           
                           
                              OSS
                           
                        
                      approach is better than that of the other approaches when the cardinality of the prototype subset (P) has been appropriately chosen. This means that, in DBC for certain datasets, the OSS distance measure performs better than the Euclidean distance measure as well as the approaches that extend the set of prototypes. However, the results obtained from the experiment on Balls3D data demonstrate that the proposed approach does not work with non-Euclidean data. Meanwhile, the size of U (amount of unlabeled data) does not play a significant role. In order to render the evaluation more complete, two ways using the 
                        
                           
                              D
                           
                           
                              OSS
                           
                        
                      matrix were subsequently investigated. However, the experimental results obtained show that there is no absolute winner in terms of classification accuracies. Moreover, based on a study on the embedded pseudo-Euclidean space for the 
                        
                           
                              D
                           
                           
                              OSS
                           
                        
                      matrix, a remarkable relation between classification accuracies and non-Euclidean properties could not be found.

Although we have shown that DBC can be improved using the OSS measure, many tasks remain unexplored. One of them is to further improve the classification accuracy by selecting an optimal, or nearly optimal, cardinality of P (and U) and utilizing various distance learning techniques in the 
                        
                           
                              D
                           
                           
                              OSS
                           
                        
                      approach. Also, it is not yet clear which types of significant datasets and classifiers are more suitable for the DBC using OSS. Next, even the class-label of U is not used in the training phase, U as well as both L and E are selected from the same data. As a result, the three subsets share the same distribution characteristics. Thus, a question arises: is the classification accuracy of the 
                     
                        
                           
                              D
                           
                           
                              OSS
                           
                        
                      
                     approach superior or inferior to that of the approach when U is collected from a completely different data set? In addition, why does SVM designed in 
                     
                        
                           
                              D
                           
                           
                              OSS
                           
                        
                      
                     not work for certain applications? The investigation of the underlying reason for these remains unchallenged. Finally, the proposed 
                        
                           
                              D
                           
                           
                              OSS
                           
                        
                      approach lacks details to support its technical soundness, while comparisons are done based on fairly simplistic models. In conclusion, the problems of theoretically investigating the OSS measuring technique invoked for DBC and developing a more scientific model for comparison remain to be challenged.

@&#REFERENCES@&#

