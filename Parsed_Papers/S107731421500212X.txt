@&#MAIN-TITLE@&#Localizing scene texts by fuzzy inference systems and low rank matrix recovery model

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Focused and incidental scene text images are processed in a separate manner.


                        
                        
                           
                           Low rank matrix recovery is exploited to process the incidental scene text images.


                        
                        
                           
                           A text confidence map was designed via fuzzy inference system.


                        
                        
                           
                           The proposed algorithm handles both Latin and Farsi/Arabic scripts.


                        
                        
                           
                           Farsi/Arabic scene texts at arbitrary orientations are localized for the first time.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Low rank matrix recovery

Scene text localization

Maximally stable extremal regions

Fuzzy inference system

@&#ABSTRACT@&#


               
               
                  In this paper a framework is proposed to localize both Farsi/Arabic and Latin scene texts with different sizes, fonts and orientations. First, candidate text regions are extracted via an MSER detector enhanced by weighted median filtering to adopt the low resolution texts. At the same time based on fuzzy inference system (FIS), the input image is classified into images with a focused text content and incidental scene text images which the image does not focus on the text content. For the focused scene text images the non-text candidates are filtered via an FIS. On the other hand, for the incidental scene text images apart from the FIS, an extra filtering algorithm based on low rank matrix recovery is proposed. Finally, a new approach based on the clustering, minimum area rectangle and radon transform techniques is proposed to create the single arbitrarily oriented text lines from the remaining text regions. To evaluate the proposed algorithm, we created a collection of natural images containing both Farsi/Arabic and Latin texts. Compared with the state-of-the-art methods, the proposed method achieves the best performance on our and Epshtein datasets and competitive performances on the ICDAR dataset.
               
            

@&#INTRODUCTION@&#

Localization of scene text within an image is a significant issue in many applications such as content-based image retrieval (CBIR), visual impairment assistance system, automatic robot navigation in urban environments and tourist assistance systems. Once location of the text is determined, it could be further analyzed. Locating text in the natural images, however, due to the variations of font, scale, color, shadow and lighting conditions is very difficult. Since the primary objective of the commercial OCR systems is to recognize the text within document images scanned by flatbed scanners, they are not able to recognize the scene text well. Thus, a preprocessing stage is needed to localize text in natural images before applying the commercial OCR systems. Despite the fact that much research in this area has been done, yet the problem of localizing text in natural images, remains very challenging. In fact, since usually geometric distortions as well as unseen complex background are simultaneously available in the natural images, many proposed algorithms in the literature have problems in terms of speed and accuracy requirements [1–3]. Most research in this area can be generally categorized as: texture-based, connected component-based (CC-based) and hybrid approaches. Texture-based approaches [4,5] localize the scene text in a top-down scheme including the feature extraction, classification and multi-scale merging steps. In contrast, CC-based approaches [6,7] are actually bottom-up ones, whose main steps include candidate connected component (CC) extraction, verification and grouping. More recently, hybrid approaches [8,9] are enjoying more popularity. Intuitively, they are the combination of the two aforementioned methods.

Most research in this area have focused on the Latin or Chinese scripts. A few of them, however, addressed the Farsi and Arabic scripts [10,11]. Some of the salient characteristics of the Farsi/Arabic script include existence of different connecting characters and different shapes for many characters depending on their position in the word. Most research in Latin or Chinese scene text localization has exploited the character isolation property of the text specially in the connected component (CC) based methods at the non-text filtering and grouping stages. Thus, regarding the non-isolated characters in the Farsi/Arabic script, the state-of-the-art methods in this area do not generate robust and reliable results for Farsi/Arabic scene text localization. It is worth noting that in Indic scripts such as Bangla or Devanagari, also neighboring characters due to the matra feature might become connected. The matra feature unlike Arabic script makes certain header lines in the Bangla or Devanagari scripts and therefore was exploited as a major cue for many of the Bangla or Devanagari scene text localization algorithms [12,13]. In [12], existence of the certain header lines in the Bangla and Devanagari scripts was exploited to extract the sufficiently large scene texts via morphological operations.

There are a number of contributions in this paper. First, an enhanced MSER detector is proposed to extract the candidate scene text regions which are robust to the poor quality scene texts. Second, a classification stage based on fuzzy inference system (FIS) is proposed to classify the natural images into images focused on the text content and incidental scene text images. Third, an FIS is proposed to build a confidence map indicating the likelihood of being text for the extracted candidate regions and finally to discard the non-text candidates. Fourth, based on the result of the image classification block, the proposed method takes into account different approaches for focused scene text and incidental scene text images. Fifth, a novel false positive reduction approach based on low rank matrix recovery (LRMR) was proposed for the incidental scene text images. Sixth, a new approach based on the clustering, minimum area rectangle and radon transform techniques is proposed to create the single arbitrarily oriented text lines. Seventh, the proposed method handles both Farsi/Arabic and Latin scripts at the same time.

Experimental results in this paper show that the proposed algorithm could promisingly address the above contributions.

The organization of the rest of the paper is as follows. In Section 2 we discuss the related works. In Section 3 we describe the details of the proposed method. Section 4 provides the related experimental results. We finally conclude the paper in Section 5.

@&#RELATED WORKS@&#

Most research in the field of scene text localization can be generally categorized as: texture-based, connected component (CC)-based and hybrid methods.

Texture-based methods [5] are based on the idea that texts in images have distinct textural characteristics distinguishing them from the background. They extract distinct textural characteristics such as histograms of oriented gradients (HOG), local binary patterns (LBP), Gabor filters and wavelets from sliding windows. Afterward a classifier which has been trained using machine learning techniques (e.g. AdaBoost [5]) or heuristics is used to determine the label of each window as being text or non-text. Finally areas obtained at different scales are merged together to generate final text regions. In [14], a classification-based algorithm for text detection using a sparse representation with discriminative dictionaries was proposed. First the edge map of an input image was extracted by the wavelet transform. Then candidate text areas were obtained by performing classification using sparse representation with discriminative dictionaries. Finally, the adaptive run-length smoothing algorithm and projection profile analysis were used to merge the candidate text areas. These methods localize scene texts relatively accurately. Due to the high computational load of the sliding window classification, however, their runtime speed is relatively slow. These approaches are also sensitive to the alignment orientation of the text in the images. Because Farsi characters may stick together and make new combinations, preparing a training set including jointly different text orientations, fonts and character combinations for the sliding window classification seems to be a very hard task. Consequently it seems these methods are not very efficient for localizing the Farsi scene text.

On the other hand, CC-based methods group small components into successively larger components until all regions in the image are identified. These methods first extract potential characters through edge detection [15,16], maximally stable extremal regions (MSER) detection [6,17] or color clustering [18] approaches. Afterward, geometric and statistical constraints are used to reject non-character regions. Finally, the remaining regions are clustered into words and text lines through comparing the similarities between them. In [15] as a typical and state-of-the-art CC-based method, stroke width transform which is a local image operator was proposed. First, each pixel was assigned with the most likely stroke width value and then pixels with similar stroke widths were grouped to candidate CCs. Afterward, a series of rules was used to remove the non-text CCs. Bottom-up methods based on extremal regions (ER) were proposed in [6,9]. MSER was used in [6] to extract the character CCs. Then the non-text CCs were filtered out using stroke width information. In [19], a method to localize and recognize text in natural images was proposed. First, a set of simple and efficient features which could discriminate between character and non-character objects based on geometric and gradient properties was proposed. Thus, the text was localized in the image using these features in a CC-based approach. Second, the previously localized text was recognized using gradient features and dynamic programming. An advantage of the CC-based approaches is that their results can be sent directly to the Optical Character Recognition (OCR) software for recognition process [20].

Hybrid approaches [8,9,17,21–23] are a combination of the texture-based and CC-based approaches. In [8], first the potential text regions were extracted by the SWT. Following this, to reject the non-text regions, two random forest classifiers were used.

It was demonstrated in [24–26] that using salient object detection models in the early stages of the scene text localization task may improve the results. In [24], for example, a saliency model [27] was used to localize the scene text. In [25] usefulness of the four state-of-the-art saliency detection methods [28–31] for localizing the scene text was investigated and among them, Torralba et al.’s method [31] achieved the best performance. It is worth noting that the salient object detection models used in the aforementioned methods are task-independent and not designed for detecting salient text regions. For example they do not discriminate between text and objects such as face. Thus performance of the scene text localization task in [25] was not improved dramatically.

In [20] a salient region detection model designed specifically for text was proposed. Individual and bounded units of the text(character) rather than regions with the text-like characteristics were identified by this model. Thus first individual characters were identified and second MRF graphical models were used to consider the inter-dependency of the characters in order to make the desired text words and lines. This approach, however, due to the focusing on the characters at the early stages of the algorithm, is not applicable to the Farsi script which includes a combination of isolated characters and subwords.

In [10] the text candidate regions were extracted by a combination of the color and edge features. Afterward, to verify the candidates HOG and wavelet features were extracted for the candidate regions and finally the non-text regions were filtered by the SVM classifier. This approach had low performance on the real natural images which the text regions may not necessarily occupy a large part of the image. In [11] the localization of the artificial Farsi text in the video images by corner detection method was proposed. It is worth noting that the scene text is different from the artificial text and therefore the methodologies for localizing them in the images are very different.

@&#OVERVIEW@&#

The flow diagram of the proposed method is shown in Fig. 1
                        . First, in Section 3.2 the input image is classified into focused scene text and incidental scene text images via fuzzy inference system (FIS) (Fig. 1
                        
                           a
                        ). Meanwhile in parallel in Section 3.3 the candidate text regions are extracted. In Section 3.4 non-text candidate CCs are filtered via a simple FIS model. However, for the incidental scene text images, an extra non-text filtering process based on low rank matrix recovery (LRMR) is done before the FIS. Finally in Section 3.5 the remaining CCs are grouped to readable text lines.

Natural images with a non-focused text contents, incidental scene texts, compared with the ones with a focused text content need more complex text localization algorithms especially in the filtering process of the non-text candidates. In this paper, first the input image was classified into focused scene text images and incidental scene text ones (Fig. 1
                        a). The focused scene text images are usually taken when the user intentionally focus on the text region in order to be used in applications such as translation and etc. On the other hand the incidental scene text images are taken without any bias for focusing on the text regions and may contain many background objects.

In [32] the video frame was classified into simple, normal and complex ones based on the edge density for the overlay or artificial text localization task. However, edge density could not be an informative feature for the natural image case. In this paper a new approach was proposed to classify the image into focused and incidental scene text ones. In order to do this, first three features based on the following observations were extracted for the input image.

First, as introduced by [33], for a natural image the Sobel and Canny edge detection operators give uniform patterns for the text regions (foreground) and non-uniform patterns for the non-text regions (background). In fact Sobel operator extracts less edges for the non-text regions. Therefore the difference between Canny and Sobel edges of the input image could be a cue for background-based image discrimination and therefore focused/incidental scene text image classification. Thus its normalized version, CSDiff, was selected as the first feature (Fig. 2
                         ).

Second, usually texts in the urban scenes are located in convex regions such as sign boards, billboards or other frame-like objects. Thus, number of the frames with remarkable internal objects, nF, was selected as the second feature. In order to do this, the closed boundary edges were obtained via Laplacian-of-Gaussian (LOG) operator with a zero threshold. These closed boundaries were not robust to the illumination issues and contained a lot of non-informative small closed edges inside objects. Therefore an approach was proposed to remove these closed edges based on the phase congruency edge detection. Phase congruency as a frequency-based edge detection method unlike the gradient-based ones detects features from the image which are invariant to the brightness or contrast and insensitive to non-uniform illumination in camera captured images [34]. By intersecting the phase congruency based edges and the LOG based closed boundaries the small and non-informative closed boundaries generated by uneven illumination issues were discarded and the whole closed boundaries became sparser. In order to preserve the closed boundaries, the phase congruency edges were first morphologically dilated. Finally, after filling the remaining closed edges, the closed boundary regions with a remarkable number of inner regions were considered as the frame-like objects.

Third, usually incidental scene text images contain a remarkable number of colors (nC). Therefore first, the input image was converted to the L*a*b* space and then was segmented using the color information via Meanshift clustering algorithm. Number of clusters or segments occupying at least three percent of the whole image were set as the nC. Using these features separately could not classify the focused/incidental scene text images. Therefore in this paper, a fuzzy inference system (FIS) as an expert system was proposed to make decision based on the knowledge base.

Two types of the fuzzy inference systems are Mamdani and Assilian [35] and Takagi–Sugeno models [36]. The main difference between them is the form of the consequent. In Mamdani model, the output member function can be evaluated independently from the input variables, while in Takagi–Sugeno model, the output member function is a function of its inputs. In this paper Mamdani FIS model was used. Basic components of an FIS model are : Fuzzification; Knowledge Base; Inference Engine; Defuzzification.

In the Fuzzification step, generalized bell function as the membership function (MF) was exploited to define the fuzzy sets. The generalized bell function depends on the three parameters named a, b and c given by

                           
                              (1)
                              
                                 
                                    μ
                                    
                                       (
                                       x
                                       ;
                                       a
                                       ,
                                       b
                                       ,
                                       c
                                       )
                                    
                                    =
                                    
                                       1
                                       
                                          1
                                          +
                                          
                                             
                                                |
                                                
                                                   
                                                      
                                                         x
                                                         −
                                                         c
                                                      
                                                      a
                                                   
                                                
                                                |
                                             
                                             
                                                2
                                                b
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        Table 1
                         shows the details of each membership function used in the FIS model. Apart from the MF details to complete the Knowledge base, number of rules were designed to make a fair discrimination between focused and incidental scene text images (Table 2
                        ). In order to perform the fuzzy logic in the FIS model, the logical “AND” and “OR” operators were handled as the “min” and “max” operations on the corresponding membership functions respectively. Afterward the aggregation of the rules was implemented by applying the “max” operation on the consequent of the corresponding rules. In the Defuzzification step, centroid which is the center of the area under the curve was exploited to make the crisp value for the output variable (Complexity). Finally an image which its Complexity value was bigger than the 
                           
                              T
                              
                                 F
                                 _
                                 I
                              
                           
                         threshold was considered as the incidental scene text image and the rest was set as focused one. The FIS model for the sample focused and incidental scene text images are shown in Fig. 3
                        .

It has been proven that MSER due to its robustness against changes in scale and viewpoint and lighting conditions, is one of the best region detectors used for scene text extraction. However, MSER does not appear to be robust in respect of poor quality scene texts. In order to ensure the extraction of all text regions by the MSER feature detector from the noisy and low quality natural images, a small stability parameter value, Δ, for the MSER should be exploited. However, presence of the blur effect between the neighboring text regions in the low quality images, most likely creates false connections between these regions in the corresponding MSER image (Fig. 4
                        
                        g). These false connections could be removed by increasing the Δ parameter (Fig. 4
                        e and f). On the other hand as a consequence of increasing Δ, some of the inhomogeneous text regions are not extracted by the MSER (Fig. 4
                        e). In [20], guided image filter as a linear smoothing filter was exploited to address this problem. In this paper, MSER detector with a relatively large Δ was exploited to prevent the false connections and extract as less as possible non-text regions. Besides, a non-linear edge preserving smoothing filter called weighted median filter (WMF) [37] was exploited to not only suppress the impulsive noise in the inhomogeneous text regions but also to preserve the edges in the image (Fig. 4
                        h). The basic idea in WMF is to give higher weights to some samples, according to their affinity of the pixels in the corresponding feature map such as intensity or color maps of the image. In this paper, the efficient version of WMF was exploited [38].

Finally since width of the stroke through a Latin and Farsi/Arabic character or subword does not vary dramatically, therefore as a cue for the texts, the extracted CCs with an almost uniform stroke width were remained and the rest were discarded. Efficient computation of the stroke widths of a CC which was proposed in [20] is as follows: first, skeleton of the CC is extracted. Second, the shortest distance between each pixel of the skeleton and boundary of the CC is computed and finally the result times by two is defined as the corresponding stroke width of that part of the CC. CCs with a remarkable stroke width variance were discarded as the non-text regions.

In this stage, to filter the non-text CCs different methods were adopted for focused and incidental scene text images which were classified in the Section 3.2. According to the filtering process of the non-uniform stroke width CCs in the previous stage, for the focused scene text images a simple filtering method based on FIS was exploited. On the contrary for the incidental scene text images due to the large amount of complex non-text CCs, an extra filtering process based on the low rank matrix recovery (LRMR) was also exploited.

In this stage, human-like reasoning via an FIS was exploited to build a confidence map which indicates the likelihood of being text for the candidate regions extracted in the previous stage. First, five features for each CC were extracted (Table 3
                           ) and then they were given to the FIS model as the input variables. Afterward, the FIS model built the confidence map represented by its output variable, Textness.

Presence of remarkable color contrast for many of the scene texts was exploited as the first feature named as Center surround color contrast ( CSCC ). CSCC was computed as follows: histograms of the foreground and background pixels located inside the corresponding bounding box of each CC (FBB
                            and BBB
                            respectively) were calculated. Distance of these histograms for the red, green and blue channels were computed via KL-Divergence. CSCC was obtained by the resultant of the three aforementioned distances. Number of holes in each CC was considered as the second feature. In addition, Singularity of a CC, the third feature, was extracted to measure its separation from the neighboring CCs. Solidity of a CC, the fourth feature, indicates its solidity and as an auxiliary feature contributes the discrimination between text and non-text regions. The fifth feature, Elongation indicates the elongation of the CC based on its major and minor axis length properties. In the Fuzzification step, generalized bell function as the membership function (MF) was exploited to define the fuzzy sets. Table 4
                            shows the details of each membership function used in the FIS model. Apart from the MF details to complete the Knowledge base, five rules were designed to make a fair discrimination between text and non-text CCs (Table 5
                           ). In the Defuzzification step, centroid was exploited to make the crisp value for the output variable (Textness). Finally CCs which their Textness values are bigger than the Tfilter
                            threshold were considered as the text regions and the rest were discarded. As can be seen in Fig. 5, unlike most of the non-text CCs, text regions acquired the high Textness values in the confidence map. One of the most appealing aspect of the FIS in the proposed method is that the expert could easily interfere with the system according to his/her experience and edit the rules in the knowledge base in order to address multilingual issues.

In natural images, text often draws attention to itself, even when it is placed in a cluttered background. Motivated by this fact a saliency detection method based on low rank matrix recovery (LRMR) model was proposed to filter the non-text regions in the incidental scene text images. In fact an incidental scene text image is represented as a low-rank matrix plus sparse noises in a feature space. The low-rank matrix represents the non-salient regions (or background), and the sparse noises explain the salient regions. The reason is that the background usually lies in a low-dimensional subspace, while the salient regions that are somehow unique and are different from the rest of the image are considered as noises (i.e., deviating from this subspace). Since in this paper, aim of using saliency detection was to discard the non-text regions in the incidental scene text image, the definition of saliency/background was updated to the text/non-text regions.

In [39] the LRMR model was exploited to recover the salient objects. The image features used in the LRMR model were represented by sparse coding for uniformly sampled patches. As explained in [40] sparse coding of the uniform patches do not guarantee that the salient objects are sparse in the entire image and also background regions become low rank. Therefore in [40] instead of extracting features in uniformly sampled patches, features were extracted in regions obtained via over-segmentation methods. Image features were also modulated with a learnt transform matrix and high level priors to meet the sparsity and low rank properties.

Low rank matrix recovery(LRMR) model. First three types of visual features for each pixel in the image were extracted as follows: (1) 5 color features including three RGB color values as well as the hue and the saturation components, (2) 12 steerable pyramid filters with four directions on three different scales and (3) 36 Gabor filter responses with twelve orientations and three scales. All these 53 features were stacked together to form a feature vector for each pixel. Then the image was decomposed into segments 
                              
                                 
                                    {
                                    
                                       p
                                       i
                                    
                                    }
                                 
                                 
                                    i
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    ⋯
                                    ,
                                    N
                                 
                              
                            based on these features via Mean-shift clustering, where N is the number of segments. The bandwidth parameter of the Meanshift was chosen so that the image was over-segmented to guarantee the sparsity and low rank properties. The mean of the 53-dimensional feature vectors of the pixels located in each segment was treated as the feature vector of that segment A
                           
                              i
                            ∈ R
                           
                              d × 1. Stacking them for all segments formed the matrix representation of the image 
                              
                                 A
                                 =
                                 
                                    [
                                    
                                       A
                                       1
                                    
                                    ,
                                    
                                       A
                                       2
                                    
                                    ,
                                    …
                                    
                                       A
                                       N
                                    
                                    ]
                                 
                              
                           . The feature matrix A could be decomposed into two parts 
                              
                                 A
                                 =
                                 L
                                 +
                                 S
                                 ,
                              
                            a low-rank matrix, 
                              
                                 L
                                 =
                                 
                                    [
                                    
                                       L
                                       1
                                    
                                    ,
                                    
                                       L
                                       2
                                    
                                    ,
                                    …
                                    
                                       L
                                       N
                                    
                                    ]
                                 
                                 ∈
                                 
                                    R
                                    
                                       d
                                       ×
                                       N
                                    
                                 
                              
                            representing the background and a sparse matrix, 
                              
                                 S
                                 =
                                 
                                    [
                                    
                                       S
                                       1
                                    
                                    ,
                                    
                                       S
                                       2
                                    
                                    ,
                                    …
                                    
                                       S
                                       N
                                    
                                    ]
                                 
                                 ∈
                                 
                                    R
                                    
                                       d
                                       ×
                                       N
                                    
                                 
                              
                            corresponding the salient region.

The low rank matrix recovery problem was formulated as:

                              
                                 (2)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   (
                                                   
                                                      
                                                         L
                                                      
                                                      *
                                                   
                                                   ,
                                                   
                                                      
                                                         S
                                                      
                                                      *
                                                   
                                                   )
                                                
                                                =
                                                a
                                                r
                                                g
                                                
                                                   min
                                                   
                                                      L
                                                      ,
                                                      S
                                                   
                                                
                                                
                                                   (
                                                   r
                                                   a
                                                   n
                                                   k
                                                
                                                
                                                   (
                                                   L
                                                   )
                                                
                                                +
                                                
                                                   
                                                      λ
                                                      ∥
                                                      S
                                                      ∥
                                                   
                                                   0
                                                
                                                
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                s
                                                .
                                                t
                                                
                                                A
                                                =
                                                L
                                                +
                                                S
                                             
                                          
                                       
                                    
                                 
                              
                           Since due to the fact that matrix rank and l
                           0 operations are not convex, the above problem was NP-hard. Recently in [41] it was shown that under weak assumptions the low rank matrix L and the sparse matrix S could be exactly recovered by:

                              
                                 (3)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   (
                                                   
                                                      
                                                         L
                                                      
                                                      *
                                                   
                                                   ,
                                                   
                                                      
                                                         S
                                                      
                                                      *
                                                   
                                                   )
                                                
                                                =
                                                a
                                                r
                                                g
                                                
                                                   min
                                                   
                                                      L
                                                      ,
                                                      S
                                                   
                                                
                                                
                                                   
                                                      (
                                                      ∥
                                                      L
                                                      ∥
                                                   
                                                   *
                                                
                                                +
                                                
                                                   
                                                      λ
                                                      ∥
                                                      S
                                                      ∥
                                                   
                                                   1
                                                
                                                
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                s
                                                .
                                                t
                                                
                                                A
                                                =
                                                L
                                                +
                                                S
                                             
                                          
                                       
                                    
                                 
                              
                           where ‖L‖* is the nuclear norm of L and ‖.‖1 indicates l
                           1-norm. After that S was obtained, the l
                           1-norm of each row Si
                            in S was used to measure the saliency of corresponding segments to finally make the saliency map.

Since in this paper the aim of using a saliency detection is to discard the non-text regions in the incidental scene text image, the definition of saliency/background should be updated to the text/non-text regions. Therefore the image features matrix (A) should be modulated so that it guarantees the sparsity of the text regions and low rank property of the non-text regions as the background. Thus the modulation process gives small weights to feature vectors of those segments which are more likely to be non-text regions (background) and large weights to those corresponding to the text regions. In [40,42] high level priors were exploited to modulate the image feature matrix to improve the saliency detection. In [40] three higher-level priors including center prior, color prior and face prior were exploited. The center prior considers the fact that when people take photographs, they prefer that the object of interest to be placed in the center of the frame. Color prior represents that the warm colors such as red and yellow are more attractive to people. Finally the face prior is due to the fact that when people watching the images, they often pay attention to objects such as faces. In [42] the high-level prior was extracted via an image segmentation process.

In this paper the modulator matrix was obtained via the text regions candidates which were extracted in Section 3.3. First, this prior text regions candidates become sparser by discarding the CCs with a very large or small size and also with a very large aspect-ratio. Afterward, distance transform was applied on this sparser image to make the prior map. Finally according to this prior map, Text map matrix (
                              
                                 T
                                 =
                                 
                                    
                                       {
                                       
                                          t
                                          i
                                       
                                       }
                                    
                                    
                                       i
                                       =
                                       1
                                       ,
                                       2
                                       ,
                                       ⋯
                                       ,
                                       N
                                    
                                 
                              
                           ) as the modulator matrix determines the probability of being text(salient) for the center of each segment 
                              
                                 
                                    {
                                    
                                       p
                                       i
                                    
                                    }
                                 
                                 
                                    i
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    ⋯
                                    ,
                                    N
                                 
                              
                           .

Since the {ti
                           } as the weights determines the level of being text(salient) for a segment in a high-level manner, therefore the modulated matrix was defined as

                              
                                 (4)
                                 
                                    
                                       B
                                       =
                                       [
                                       
                                          t
                                          1
                                       
                                       
                                          A
                                          1
                                       
                                       ,
                                       
                                          t
                                          2
                                       
                                       
                                          A
                                          2
                                       
                                       ,
                                       …
                                       ,
                                       
                                          t
                                          N
                                       
                                       
                                          A
                                          N
                                       
                                       ]
                                    
                                 
                              
                           
                        

As a result, the modulated matrix (B) was used as the input feature matrix for the LRMR model

                              
                                 (5)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   (
                                                   
                                                      
                                                         L
                                                      
                                                      *
                                                   
                                                   ,
                                                   
                                                      
                                                         S
                                                      
                                                      *
                                                   
                                                   )
                                                
                                                =
                                                a
                                                r
                                                g
                                                
                                                   min
                                                   
                                                      L
                                                      ,
                                                      S
                                                   
                                                
                                                
                                                   
                                                      (
                                                      ∥
                                                      L
                                                      ∥
                                                   
                                                   *
                                                
                                                +
                                                
                                                   
                                                      λ
                                                      ∥
                                                      S
                                                      ∥
                                                   
                                                   1
                                                
                                                
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                s
                                                .
                                                t
                                                
                                                B
                                                =
                                                L
                                                +
                                                S
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

As an intuitive example, a synthesized image containing a single character ‘Q’ and a flat background is shown in Fig. 6
                           . At first 
                              
                                 N
                                 =
                                 141
                              
                            segments are obtained via performing the mean-shift clustering process. Afterward the [B]141*53 as the modulated matrix is given to the LRMR model. The LRMR model gives the L and S matrices which are the same size as the B matrix. Rank of the L matrix for this example is computed as zero. For the S matrix, the l
                           1-norm of each row Si
                            in S was computed to obtain a 141*1 vector. This vector gives the gray-scale values for the corresponding segments as shown in the right side of Fig. 6 to finally make the saliency map.

once the saliency map was obtained, in order to make more integrated and contiguous regions at the end of the saliency detection algorithm, the dilation morphological operator was applied.

Afterward, in order to generate the mask image for the salient areas, the dilated image was binarized using OTSU thresholding method which is a data-driven binarization approach.
 Finally, the posterior text regions candidates were obtained via intersecting the saliency mask with the prior text regions candidates. As can be seen in Fig. 7
                            many of the non-text CCs were removed by the saliency mask.


                           Non-text Filtering via FIS. Once many of the non-text CCs were filtered via the LRMR model in the previous stage, the output of the LRMR model like a focused scene text image went through the FIS-based filtering process which was used in the previous stage to discard the remaining false positives (Fig. 8
                           ).

In the previous stage, as much as possible, the non-text candidate CCs were omitted. In this stage, the remaining CCs were grouped to readable text lines in order to provide an opportunity to evaluate the scene text localization task. In this work, at first for each remaining CC, average stroke width and average color features were calculated. The stroke widths of a CC were computed as described in sec. 3.3. The average stroke width feature is calculated by averaging the stroke widths of a CC. Afterward these two features were used to group the CCs into clusters through Meanshift algorithm. Then, in each cluster, the pairwise normalized distance between CCs within the cluster was calculated. The normalized distance between CCs is defined by Euclidean distance between the centers of them divided by the summation of their minor axis length and major axis length. Afterward, the close CCs within a cluster were grouped together. Those CCs that had been left alone, were considered as non-text ones and discarded. Thus if any non-text CC had been passed through the filter in the previous step, it would have been removed here. Finally, for each group the minimum area rectangle was obtained to handle the arbitrarily oriented scene texts. Sometimes for the multiple element text groups, the close text lines might form a joint group. To ensure the formation of the single text lines, projection of the texts within each group along the minor axis of the minimum area rectangle was obtained via radon transform. Afterward, single text lines were extracted exploiting the peaks and valleys in the these profiles. As a result in this step we have two achievements including applying an additional non-text filtering on the remaining CCs and grouping the remaining ones to the text lines. As a result, these text lines are ready for the OCR engines.

@&#EXPERIMENTAL RESULTS@&#

Since, to the best of our knowledge there is no benchmarked dataset on the Farsi/Arabic scene text localization task available in the literature, we created our dataset to evaluate the proposed algorithm. The dataset was composed of 500 color images of indoor and outdoor scenes containing arbitrary-oriented texts which are captured with different digital cameras and resolutions. In addition a significant number of the images contain both Farsi/Arabic and Latin texts simultaneously. Also many of the images in this dataset are incidental scene text ones. Some of the typical images from this dataset are shown in Fig. 9
                        . It is worth noting the dataset will be publicly available soon to the research community. In order to evaluate the algorithm on this dataset, the evaluation approach of the PASCAL object detection application [43] was used in a similar way. Thus the true or false positives were obtained by the overlap ratio between the estimated minimum area rectangles and the ground truth rectangles. Since texts could be arbitrarily oriented, these rectangles were rotated to be axis-aligned and then the overlap-ratio was computed ([8]). For the estimated and the ground truth rectangles, if the angle between them is less than 30 degree and the overlap ratio is bigger than 0.6, detection of the estimated rectangle was evaluated as True, otherwise False. The Precision, Recall and F-measure performance metrics are also defined by

                           
                              (6)
                              
                                 
                                    P
                                    r
                                    e
                                    c
                                    i
                                    s
                                    i
                                    o
                                    n
                                    =
                                    
                                       
                                          |
                                          T
                                          P
                                          |
                                       
                                       
                                          |
                                          E
                                          |
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    R
                                    e
                                    c
                                    a
                                    l
                                    l
                                    =
                                    
                                       
                                          |
                                          T
                                          P
                                          |
                                       
                                       
                                          |
                                          T
                                          |
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    F
                                    -
                                    m
                                    e
                                    a
                                    s
                                    u
                                    r
                                    e
                                    =
                                    2
                                    *
                                    
                                       
                                          p
                                          r
                                          e
                                          c
                                          i
                                          s
                                          i
                                          o
                                          n
                                          *
                                          r
                                          e
                                          c
                                          a
                                          l
                                          l
                                       
                                       
                                          p
                                          r
                                          e
                                          c
                                          i
                                          s
                                          i
                                          o
                                          n
                                          +
                                          r
                                          e
                                          c
                                          a
                                          l
                                          l
                                       
                                    
                                 
                              
                           
                        where TP is the set of true positive detections and E and T are the sets of estimated and ground truth rectangles respectively.

The proposed method was also evaluated on the public datasets such as ICDAR 2011, ICDAR 2013 and Epshtein ones. The ICDAR 2011 dataset [3] is an extension to the dataset used for the text locating competitions of ICDAR 2003 and 2005 and includes 485 natural images. All the images in this database are from the dataset of ICDAR 2003 and 2005, except for a few extra images added in the ICDAR 2011 Robust Reading Competition Challenge. The ICDAR 2013 dataset [44] includes 462 images and is almost the same as the the ICDAR 2011 dataset with small differences including making a revision of ground-truth texts at several images and removing the duplicated images over training and test sets. The other difference is that don’t care regions are introduced in the ground-truth. It is worth noting that these datasets contain mostly focused scene text images. Epshtein et al.
                           1
                        
                        
                           1
                           
                              http://research.microsoft.com/en-us/um/people/eyalofek/text_detection_database.zip .
                         proposed a more difficult database containing 307 natural images. Images in this dataset contain many repetitive structures like windows and bricks and low contrast and blurred scene texts and also unlike ICDAR 2011 and 2013 images they are mostly incidental scene text ones. In order to evaluate the algorithm on ICDAR and Epshtein datasets, we followed the methodology described in [3] to obtain the Recall, Precision and F-measure metrics.

The algorithm was implemented in Matlab 8.2 and experiments were performed on a PC with Intel Core i7 machine (2.7 GHz). Evaluating on our and ICDAR2011 datasets, the radius, 
                           
                              
                                 T
                                 
                                    F
                                    _
                                    I
                                 
                              
                              ,
                           
                         
                        Tfilter
                         parameters which were used in WMF technique in the Section 3.3, FIS-based classification in Sections 3.2 and 3.4 respectively were set experimentally to 
                           
                              r
                              a
                              d
                              i
                              u
                              s
                              =
                              15
                              ,
                           
                        
                        
                           
                              
                                 T
                                 
                                    F
                                    _
                                    I
                                 
                              
                              =
                              0.5
                              ,
                           
                        
                        
                           
                              
                                 T
                                 
                                    f
                                    i
                                    l
                                    t
                                    e
                                    r
                                 
                              
                              =
                              0.7
                           
                        . As can be seen in Fig. 10
                         the optimized Δ parameter used in the Section 3.3 is the one achieving the best performance in terms of F-measure and therefore was set as 
                           
                              Δ
                              
                              =
                              
                              15
                           
                         and 
                           
                              Δ
                              
                              =
                              
                              13
                           
                         for our and ICDAR2011 datasets respectively.


                        Our Dataset. First, performance of the proposed method and some of the state-of-the-art methods designed for Latin or Latin/Chinese scene text localization task were compared over our dataset. The algorithms in [15] and [45] were re-implemented in Matlab. The Matlab based implementation of the algorithm in [20] has been made available by the authors in their website. In order to have a fair comparison on our dataset, this implementation was used with an intensive manual parameter tuning. Since English and Farsi/Arabic texts have different characteristics and our dataset contains both, the proposed method which could handle both Latin and Farsi/Arabic scene texts at the same time, outperformed the state-of-the-art methods designed for Latin or English/Chinese scripts (Table 6
                        ). Some of the successful scene text localization results on our dataset images are shown in the first row of Fig. 11
                        .


                        ICDAR datasets. Since ICDAR performance measures are based on the word level evaluation, text lines localized by the proposed algorithm were further partitioned into words by modifying the text line extraction stage of the algorithm as follows: First, value of the threshold used for measuring the closeness of the CCs was modified. Second, the arbitrarily oriented projection profile analysis for the text groups along the major axis of the minimum area rectangles was also applied via radon transform. Although these datasets do not contain any Farsi/Arabic texts and only have Latin texts the proposed method compared with the other methods achieved a very good performance (second rank; Tables 7 and 8). It seems that the proposed method works better than the other methods for two types of few images in these datasets and therefore leads to the good performance despite the monolingual property of these datasets. First type is the low resolution images which the proposed method tries to handle them via applying the weighted median filtering before an MSER detector with a high Δ parameter (Fig. 4). Second type is the incidental scene text images which the proposed method performs the LRMR based non-text filtering process.


                        Epshtein’s dataset. Since this dataset contains mostly incidental scene text images, the proposed method due to the saliency-based non-text filtering which is implemented by LRMR model, discards many of the false-positive encouragingly leading to a dramatic increase in precision metric. The proposed method also achieved the best performance in terms of F-measure on this dataset (Table 9).

In this experiment effects of the weighted median filtering (WMF) on the system performance was evaluated. According to the use of relatively large Δ value in the MSER extraction process, some of the inhomogeneous text regions which are not extracted by the MSER feature detector were smoothed by WMF technique leading to a better performance in terms of the recall rate (Table 10). In addition, WMF pre-processing added extra 0.2 s to the average processing time for Our and ICDAR2011 datasets (last column of the Table 10). However, since C++ based implementation of the algorithm could increase its speed dramatically, it seems that this extra computational load could be acceptable for now.

As can be seen in Fig. 12
                        
                        
                        
                        
                         when the LRMR-based non-text filtering algorithm was applied to a sample focused scene text image from ICDAR dataset with a relatively high quality, some of the true positives (text regions that are missed by the algorithm) likely to be filtered by the LRMR-based non-text filtering process. One reason may be due to the sparser MSER candidate regions used to generate the prior map in the LRMR model. In Table 11 effect of the input image classification block on the performance of the system for our, ICDAR and Epshtein datasets were evaluated. In the first experiment, performance of the proposed method was evaluated. For the second and third experiments the focused/incidental image classification (FIC) stage was not exploited. For the second experiment, only incidental path, the LRMR model and FIS algorithm were applied to both focused and incidental scene text images and for the third one, only focused path, the LRMR model was not used and only FIS algorithm was applied to both focused and incidental scene text images. Therefore here for each dataset, effect of early image classification stage as well as LRMR model on the system were evaluated. Since ICDAR datasets unlike our and Epshtein ones contain mostly focused scene text images, for the second experiment, applying LRMR-based non-text filtering algorithm without any discrimination for all the images makes a lot of true-positive to be removed and degrades the performance and efficiency dramatically. Therefore compared to our and Epshtein datasets, role of the input image classification block seems to be more important for these datasets. Since Epshtein dataset has no focused scene text image, therefore performance and efficiency for this dataset did not differ from the proposed method. Also since our dataset compared with the Epshtein’s dataset have more focused scene text images, performance and efficiency for our dataset degrades a little bit in the experiment.

For the third experiment, as can be seen in Table 11 when image classification stage was not exploited and system tries to filter the non-text candidates only via the focused path, the performance metrics for our and Epshtein datasets became worse than the results of the second experiment. However for ICDAR datasets because of the large amount of focused scene text images, results of the third experiment were better than the second ones.

As a result among the three experiments for all of the datasets the best results were achieved for the first experiment. Therefore these experiments prove that LRMR model plus FIS should be used for incidental scene text images and FIS should be used for focused scene text images.

The proposed LRMR-based filtering process with its text-based prior works as a text-specific saliency detection. In this experiment performances of some state-of-the-art saliency detection methods were compared with the LRMR-based model in terms of scene text localization task. In other words, these methods were used instead of the LRMR-based saliency detection method in the non-text filtering process which was designed for incidental scene text images. All of the these methods except the proposed LRMR and LR [40] methods are task-independent and only LRMR is the text-specific method. As can be seen in Fig. 13
                        
                        , except the proposed LRMR-based method, the other methods extracted the salient regions without any specific bias for the text regions. While the salient regions are mostly unique, the inverse might not necessarily be true [40]. Some of the regions with a remarkable uniqueness and high local contrast might be meaningless for human and considered as a noise. Therefore in the LRMR model as a merit, integration of the high level prior about the probable location of texts with the low level saliency could distinguish between the general unique/high-contrast regions and the real texts in the image. Therefore the proposed LRMR model unlike the other saliency detection methods which are task independent ones could learn the text saliency within a single image and could be considered as a data-driven algorithm. This is a major merit of the proposed LRMR model for scene text localization task.

In [25] usefulness of the four state-of-the-art saliency detection methods on the scene text localization application was investigated and among them, Torralba et al.’s method [31] was selected as the best one. Context-aware saliency detection (CA) [55] and global contrast-based saliency detection (RC and HC) [56] methods are also the two state-of-the-art approaches. In this experiment, performance of the system when the aforementioned approaches (LR, Torralba’s method, CA, RC and HC) were used instead of the proposed LRMR-based algorithm was evaluated. The implementations of the above algorithms were publicly available by their authors. As can be seen in Table 12
                         among these approaches, the proposed LRMR-based algorithm due to its text-specific property achieved the best performance in terms of the F-measure metric. Also due to the fact that ICDAR datasets contain a few incidental scene text images, input image classification block prevents the saliency detection algorithm to be performed. Therefore substituting the LRMR model with the other saliency detection methods did not change the performance so much compared with our and Epshtein datasets.

@&#CONCLUSIONS AND FUTURE WORK@&#

In this paper we have presented an effective approach to localize Latin and Farsi/Arabic scene text with different fonts, sizes and orientations in the natural images. First, two algorithm blocks have been performed at the same time. The first one extracts the candidate text regions via the MSER feature detector enhanced by weighted median filtering. In the second algorithm block, input image has been classified into focused and incidental scene text images via fuzzy inference system (FIS). Afterward for the focused scene text images, the candidate text regions extracted in the first block have been given to the non-text filtering algorithm which is also based on FIS. On the other hand for the incidental scene text images containing relatively complex background, the aforementioned candidates have been given to an extra filtering process based on low rank matrix recovery (LRMR) model before the FIS algorithm. In fact the LRMR model acts as a text-specific saliency detection method discarding the non-text regions as the non-salient regions. After filtering the non-text regions, the remaining ones have been clustered exploiting the stroke width and color features of the regions. In each cluster, close regions have been grouped together to make the bounding boxes with the help of minimum area rectangle technique. To evaluate the proposed method a dataset containing Farsi/Arabic and Latin scene texts have been provided. The experimental results show that the proposed method outperforms the state-of-the-art methods on our dataset and despite the lack of Farsi/Arabic scene texts on Epshtein dataset and also has achieved the second place for ICDAR 2011 and ICDAR 2013 datasets. Also results showing the superiority of the proposed LRMR-based saliency detection compared with the state-of-the-art saliency detection methods in terms of scene text localization task. As a future work the proposed method could be exploited for the other scripts such as Chinese, Devanagari and Bangla.

@&#REFERENCES@&#

