@&#MAIN-TITLE@&#Prediction of mortality after radical cystectomy for bladder cancer by machine learning techniques

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Machine learning methods are used to predict the mortality after radical cystectomy.


                        
                        
                           
                           Extreme learning machine (ELM) based algorithms outperform in speed and accuracy.


                        
                        
                           
                           ELM and regularized ELM can identify the predictors of mortality after the surgery.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Bladder cancer

Radical cystectomy

Mortality

Prediction

Prognosis

Machine learning

@&#ABSTRACT@&#


               
               
                  Bladder cancer is a common cancer in genitourinary malignancy. For muscle invasive bladder cancer, surgical removal of the bladder, i.e. radical cystectomy, is in general the definitive treatment which, unfortunately, carries significant morbidities and mortalities. Accurate prediction of the mortality of radical cystectomy is therefore needed. Statistical methods have conventionally been used for this purpose, despite the complex interactions of high-dimensional medical data. Machine learning has emerged as a promising technique for handling high-dimensional data, with increasing application in clinical decision support, e.g. cancer prediction and prognosis. Its ability to reveal the hidden nonlinear interactions and interpretable rules between dependent and independent variables is favorable for constructing models of effective generalization performance. In this paper, seven machine learning methods are utilized to predict the 5-year mortality of radical cystectomy, including back-propagation neural network (BPN), radial basis function (RBFN), extreme learning machine (ELM), regularized ELM (RELM), support vector machine (SVM), naive Bayes (NB) classifier and k-nearest neighbour (KNN), on a clinicopathological dataset of 117 patients of the urology unit of a hospital in Hong Kong. The experimental results indicate that RELM achieved the highest average prediction accuracy of 0.8 at a fast learning speed. The research findings demonstrate the potential of applying machine learning techniques to support clinical decision making.
               
            

@&#INTRODUCTION@&#

Bladder cancer is a common cancer in genitourinary tract [1], affecting mainly the elderly population. In Hong Kong, a survey reported in 2011 that bladder cancer constituted 1.4% of all new cases of cancers [2]. Among various types of bladder cancer, muscle invasive bladder cancer has poor prognosis, with high tendency of metastasis and mortality that necessitate prompt treatment [3]. The most effective treatment approach is the surgical excision of bladder and the surrounding lymphatic tissue, which is known as radical cystectomy. Radical cystectomy is a major surgery that has significant morbidity and mortality [2]. The early postoperative complication rate is between 25% and 57% [4,5], and the early mortality rate is around 3%. The rates are higher for elderly patients [6–8]. Efforts have been made to identify the risk factors in order to maximize the operative outcomes, particularly the long-term survival after surgery. In a retrospective review, the association between clinicopathological factors and mortality for 117 patients treated with radical cystectomy for bladder cancer was investigated from statistical inference perspective [2]. In this study, instead of statistical inference, seven machine learning methods – back-propagation neural network (BPN), radial basis function network (RBFN), extreme learning machine (ELM), regularized ELM (RELM), support vector machine (SVM), naive Bayes (NB) classifier and k-nearest neighbour (KNN) algorithm-are exploited as alternative approaches to predict the overall 5-year survival based on the same clinicopathological dataset of 117 patients treated with radical cystectomy.

The indications for radical cystectomy include treatment failures in non-muscle invasive bladder cancer, and T2-T4aN0M0 muscle invasive bladder cancer [9,10]. Radical systectomy includes the excision of bladder, prostate (in case of men), urethra, part of distal ureter, and lymphatic tissue of pelvis. After the removal of urinary bladder, urinary diversion is performed to divert the urine produced from the kidneys outside of body. Urinary diversion can either be continence diversion by a neo-bladder or continence pouch, or urinary conduit and ureterocutaneostomy. Radical cystectomy and urinary diversion can be performed by conventional open surgery or minimally invasive surgery.

In the fields of medicine, clinical data are essential for marking diagnosis, formulating treatment and predicting prognosis. Clinicians make use of the knowledge in different specialties to analyze the histological (cell-based), clinical (patient-based), and demographic (population-based) information [11], where statistical methods such as Cox regression, logistic regression and Kaplan–Meier estimator are conventional employed in the analyses. For example, Kaplan–Meier method and Cox proportional hazards model were used to evaluate the prognostic factors of recurrence, progression and disease mortality in patients with bladder cancer [12]. Logistic regression based on 12 variables was used to identify the predictors of overall 5-year survival of patients who had undergone radical cystectomy for bladder cancer [13]. However, with rapid development of health technologies and informatics, medical data of high dimensionality are made available in both volume and variety. The accuracy of outcome prediction depends heavily on effective information integration of data acquired from various sources, clinically or pathologically, making the conventional statistical analyses that rely on clinicians׳ knowledge and experience a difficult task. The weakness of statistical methods is more apparent when handling medical data with high variability, nonlinear interactions among the variables, and heterogeneous distributions. For example, regression model, a common statistical technique, often requires some explicit assumptions on the relationships among the data that may be practically invalid [13]. Hence, researchers have begun to investigate alternative techniques for clinical outcome prediction, where computational approaches are a main focus. In particular, machine learning has been introduced into the medical domain to overcome the problems with statistical methods and uncover the knowledge hidden in the complex clinical data.

Machine learning is a field in computer science leveraging knowledge from artificial intelligence, optimization and statistics to develop algorithms based on the available data. The approach is to build a model by learning from experience (i.e. the existing data, or the known samples acquired) and use the model to make predictions for the new samples [14]. While the quality and size of the samples can affect the prediction performance, machine learning methods are able to handle large, noisy and complex datasets, rendering it a promising technique for broad application in various areas. They have been explored as a more powerful alternative to statistical methods for classifying patterns and making predictions using techniques such as unconventional optimization strategies, conditional probabilities or absolute conditionality [15].

In medicine and healthcare, machine learning has been applied for personalized and predictive medicine [16], cancer diagnosis and detection [15], and for the study of prevention and treatment policy [11]. For bladder cancer, robust outcome predictions of patients undergoing radical cystectomy was achieved using artificial neural work (ANN) prediction model, with configuration optimized by genetic algorithm (GA) [17]. The system was user-friendly and had potential for widespread use for medical decision support. Histology type and bilharziasis datasets were employed to construct a model using ANN and radial basis function network to predict the survival of bladder cancer patients after diagnosis [18]. Besides, clinicopathological and molecular markers were also used to create an ANN model for predicting one-year survival of patients with muscle invasive bladder cancer [19].

As there is never a best algorithm for all problems, it is necessary to test the performance of different algorithms on a specific problem and identify the optimal one [20]. In this paper, seven machine learning methods are investigated to evaluate their performance in predicting bladder cancer mortality after radical cystectomy for the purpose of prognosis. The seven methods – BPN, RBFN, ELM, RELM, SVM, NB classifier and KNN – were selected because they are representatives among the algorithms in their respective domains. Details of these methods will be provided in Section 3. The implementation of these methods included two major processes. In the training process, the learning methods made use of the training dataset, e.g. the supervised input-output pairs, to identify the relationship directly from the clinicopathological data and built the corresponding model. In the testing process, the classification ability of the model is evaluated using the testing dataset. The predicted outputs were compared with the actual outputs of the testing dataset to measure the performance in terms of accuracy, sensitivity, specificity and precision.

The reminder of this paper is organized as follows. Section 2 describes the clinicopathological data adopted in the study. Section 3 briefly reviews the principles of the seven machine learning methods. In Section 4, the 10-fold cross validation strategy and performance indices used in the experiment are introduced. In Sections 6 and 7, the prediction results are presented and discussed. A conclusion is given in Section 7.

The dataset employed in this study originated from a retrospective review on the 5-year survival of patients treated with radical cystectomy for bladder cancer [2]. The data were retrieved from computerized clinical records of 117 patients who had undergone radical cystectomy within the period from 2003 to 2011 in a urology unit in Hong Kong. The purpose in the retrospective study was to examine whether age, tumor stage and preoperative serum albumin level are independent predictors of survival after radical cystectomy. Ninety-nine of the patients were male. The mean age was 68 years old (SD=10 years). There was no loss of follow-up. The mean follow-up time was 31 months (SD=29 months). The 30-day mortality, 5-year cancer-specific mortality, other-cause mortality, and the overall mortality rates were 3%, 33%, 22% and 55% respectively. Open radical cystectomy was performed in 71 cases and laparoscopic/robotic-assisted radical cystectomy was conducted for the rest. 96 patients had ileal conduit and 21 patients had continent diversion. Other data includes hospital stay duration, preoperative serum albumin level, Charlson comorbidity index, tumor grade, tumor stage and pathological lymph node status. Further details about the dataset can be found in [2].

For those attributes covering wide numerical range when compared with the others, i.e., age, preoperative serum albumin level and follow-up time, pre-processing was performed to normalize them into the range of [0, 1]. With the advice from physicians, irrelevant data were ignored for the study (e.g. date of operation, date of death) and the final dataset used in the study contained 10 attributes (input) and 1 class attribute (output), as shown in 
                     Table 1.

In this section, the seven machine learning methods adopted in the study are briefly presented. The setting of the model parameters in the experiment is also discussed.

ANN emulates the learning ability of biological neural networks where appropriate interconnections (i.e., nodes) are made between the neurons for information transmission and parallel processing at the neurons. BPN is the most prevailing algorithm developed to train the ANN [21]. In BPN, the multilayer perception (MLP) architecture is usually used for data classification and prediction. As shown in 
                        Fig. 1, the architecture contains one input layer, one output layer, and one or more hidden layers. The nodes in the ANN are connected by links, each associated with a weight, to model the nonlinear relationship between the input and output layers.

The network is trained using supervised learning with known input–output pairs. In the training process, data fed forward into the input layer pass through the nodes in the hidden layer via the interconnections to produce the predicted results at the nodes in the output layer. The value at each node is computed using a mathematical function, known as activation function [22]. The differences between the prediction and the actual outcome are back-propagated to optimize the network structure (i.e., to learn the nonlinear relationship). In this study, the value H
                        
                           j
                         of a hidden node j is expressed as
                           
                              (1)
                              
                                 
                                    
                                       H
                                    
                                    
                                       j
                                    
                                 
                                 =
                                 f
                                 
                                    (
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          
                                             
                                                w
                                             
                                             
                                                i
                                                j
                                             
                                          
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          −
                                          
                                             
                                                b
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                    )
                                 
                                 
                                 j
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 l
                                 ,
                              
                           
                        where 
                           
                              
                                 w
                              
                              
                                 i
                                 j
                              
                           
                         is the weight of the link connecting the input node 
                           i
                         and the hidden node 
                           j
                        , 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                         is the value of the input node 
                           i
                        , 
                           
                              
                                 b
                              
                              
                                 j
                              
                           
                         is the bias, 
                           n
                         is the number of input nodes, 
                           l
                         is the number of hidden nodes, and 
                           f
                         is the activation function. In this study, 
                           f
                         is given by
                           
                              (2)
                              
                                 f
                                 (
                                 x
                                 )
                                 =
                                 
                                    2
                                    
                                       (
                                       1
                                       +
                                       exp
                                       (
                                       −
                                       2
                                       x
                                       )
                                       )
                                    
                                 
                                 −
                                 1
                                 .
                              
                           
                        
                     

There was only one output node in the study, which is to predict 5-year survival after surgery. The value 
                           O
                         of the node in the output layer is given by
                           
                              (3)
                              
                                 O
                                 =
                                 g
                                 
                                    (
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          l
                                       
                                       
                                          
                                             
                                                H
                                             
                                             
                                                j
                                             
                                          
                                       
                                       
                                          
                                             w
                                          
                                          
                                             j
                                          
                                       
                                       −
                                       b
                                    
                                    )
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 w
                              
                              
                                 j
                              
                           
                         is the weight of the link connecting the hidden node 
                           j
                         and the output node, 
                           b
                         is the bias, and 
                           g
                         is the activation function of the output node, which is given by
                           
                              (4)
                              
                                 g
                                 
                                    (
                                    x
                                    )
                                 
                                 =
                                 x
                                 .
                              
                           
                        
                     

If the predicted result is unacceptable, the error e between the computed output O and the target output Y, i.e.,
                           
                              (5)
                              
                                 e
                                 =
                                 Y
                                 −
                                 O
                              
                           
                        will be back-propagated through the network, so that all the weights and the biases will be re-adjusted to minimize the error. The weight update rules are given by
                           
                              (6)
                              
                                 
                                    
                                       w
                                    
                                    
                                       i
                                       j
                                    
                                 
                                 =
                                 
                                    
                                       w
                                    
                                    
                                       i
                                       j
                                    
                                 
                                 +
                                 η
                                 
                                    
                                       H
                                    
                                    
                                       j
                                    
                                 
                                 
                                    (
                                    
                                       1
                                       −
                                       
                                          
                                             H
                                          
                                          
                                             j
                                          
                                       
                                    
                                    )
                                 
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       w
                                    
                                    
                                       i
                                       j
                                    
                                 
                                 e
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 ..
                                 .
                                 ,
                                 n
                                 ;
                                 j
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 ..
                                 .
                                 ,
                                 l
                                 ,
                                 
                                 and
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                 
                                 =
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                 
                                 +
                                 η
                                 
                                    
                                       H
                                    
                                    
                                       j
                                    
                                 
                                 e
                                 ,
                                 
                                 j
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 ..
                                 .
                                 ,
                                 l
                              
                           
                        where 
                           η
                         is the learning rate. The bias update rules are
                           
                              (8)
                              
                                 
                                    
                                       a
                                    
                                    
                                       j
                                    
                                 
                                 =
                                 
                                    
                                       a
                                    
                                    
                                       j
                                    
                                 
                                 +
                                 η
                                 
                                    
                                       H
                                    
                                    
                                       j
                                    
                                 
                                 
                                    (
                                    
                                       1
                                       −
                                       
                                          
                                             H
                                          
                                          
                                             j
                                          
                                       
                                    
                                    )
                                 
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                 
                                 e
                                 ,
                                 
                                 j
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 ..
                                 .
                                 ,
                                 l
                                 ,
                                 
                                 and
                              
                           
                        
                        
                           
                              (9)
                              
                                 b
                                 =
                                 b
                                 +
                                 e
                              
                           
                        
                     

The BPN algorithm executes iteratively until an acceptable error is reached or the maximum iteration limit is met.

In this study, the BPN had 10 input nodes and 1 output node, corresponding to attributes of the clinical data listed in Table 1. The number of hidden nodes was determined through repeated testing using the data to obtain the optimal number. While under-fitting of the data by the network model can be avoided by using more hidden nodes, the computational time and the model generalization capability are adversely affected. In addition, the learning rate was also varied in the experiment to identify the optimal value, and thus the corresponding weights and bias values of the resulting BPN. Otherwise, it would result in oscillations and instability [23] if the rate is too high, or leading to a slow training process if the rate is too low. The momentum of the learning process, a parameter to control the ability to get rid of suboptimal solutions during the learning process, is also set appropriately so that the network can converge to the optimal structure in a stable and swift manner.

In the experiment, the number of hidden neurons, the momentum and the learning rate were varied by taking a value from the sets 
                           {
                           
                              3
                              ,
                              5
                              ,
                              7
                              ,
                              9
                              ,
                              11
                              ,
                              13
                              ,
                              15
                              ,
                              17
                              ,
                              19
                              ,
                              21
                              ,
                              23
                              ,
                              25
                              ,
                              27
                              ,
                              29
                           
                           }
                        , 
                           {
                           
                              0.9
                              ,
                              0.5
                              ,
                              0.2
                              ,
                              0
                           
                           }
                         and 
                           {
                           
                              0.01
                              ,
                              0.05
                              ,
                              0.09
                           
                           }
                         respectively in each trial. Different combinations were used to evaluate the performance of the BPN obtained, thereby identifying the optimal values and the best BPN structure.

In addition to BPN, another algorithm for training ANN is also investigated in the study, namely, the RBFN [24]. The structure of RBFN adopted in this study was similar to that of the BPN, as shown in Fig. 1. In RBFN, radial basis function is used as the activation function for the hidden nodes. Gaussian function is typically adopted as the radial basis function since it allows for factorization and linear optimization in the formulation. In RBFN, each hidden node j is parameterized with the center vector 
                           
                              
                                 c
                              
                              
                                 j
                              
                           
                        , and the width 
                           
                              
                                 σ
                              
                              
                                 j
                              
                           
                         
                        [25]. When the Gaussian function is adopted, the output of the network 
                           O
                         is given by
                           
                              (10)
                              
                                 O
                                 =
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    l
                                 
                                 
                                    
                                       
                                          w
                                       
                                       
                                          j
                                       
                                    
                                 
                                 exp
                                 
                                    (
                                    
                                       −
                                       
                                          1
                                          
                                             2
                                             
                                                
                                                   σ
                                                
                                                2
                                             
                                          
                                       
                                       
                                          
                                             ‖
                                             
                                                x
                                                −
                                                
                                                   
                                                      c
                                                   
                                                   
                                                      j
                                                   
                                                
                                             
                                             ‖
                                          
                                          2
                                       
                                    
                                    )
                                 
                              
                           
                        where 
                           x
                           =
                           
                              
                                 (
                                 
                                    
                                       
                                          x
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                    ..
                                    .
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          n
                                       
                                    
                                 
                                 )
                              
                              T
                           
                         is the input vector, 
                           
                              
                                 c
                              
                              
                                 j
                              
                           
                         is the center vector for the hidden node 
                           j
                        , 
                           ‖
                           
                              x
                              −
                              
                                 
                                    c
                                 
                                 
                                    j
                                 
                              
                           
                           ‖
                         is the Euclidean distance between the input vector and the center vector, 
                           
                              
                                 w
                              
                              
                                 j
                              
                           
                         is the weight of the connection between node j and the output node, and 
                           l
                         is the number of hidden neurons. The width 
                           
                              
                                 σ
                              
                              
                                 j
                              
                           
                         of the 
                           j
                        th hidden node, also called the spread, is defined as
                           
                              (11)
                              
                                 
                                    
                                       σ
                                    
                                    
                                       j
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             c
                                          
                                          
                                             max
                                          
                                       
                                    
                                    
                                       
                                          
                                             2
                                             l
                                          
                                       
                                    
                                 
                                 
                                 j
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 ..
                                 .
                                 ,
                                 l
                              
                           
                        where 
                           
                              
                                 c
                              
                              
                                 max
                              
                           
                         is the maximum of the Euclidean distances between the centers [26].

It can be seen from Eq. (10) that the output of the RBF network is close to 1 if the distance between the input vector and the center vector is small. Geometrically, the hidden layer of the RBFN maps the input vector in the low dimensional space into the high dimensional space. In other words, the problem in the input space becomes linearly separable with a hyperplane in the high dimensional space.

In the experiment, the spread of the RBFN was varied to identify the optimal value. An RBFN with large spread is prone to misclassification while a small spread has poor generalization capability [27]. The spread of the RBF network in the experiment was selected from 25 different values ranging from 
                           2
                           e
                           −
                           12
                         to 
                           2
                           e
                           12
                        , at powers of 10, to compare the performance and find out the one yielding the highest accuracy.

ELM is a fast learning method for single-hidden layer neural network [28,29]. A key feature of ELM is that the weights and the bias between the input and the hidden layers are randomly assigned, whereas the weights between the hidden and the output layers are analytically determined by utilizing Moore–Penrose (MP) generalized inverse operation of the hidden output matrices [30]. In this study, the algorithm used to implement the ELM is summarized as follows.
                           
                              Step 1: Randomly assign the input weight 
                                    
                                       
                                          w
                                       
                                       
                                          i
                                          j
                                       
                                    
                                  between the input and the hidden layers, and the bias 
                                    
                                       
                                          b
                                       
                                       
                                          j
                                       
                                    
                                 . The nonlinear system is then transformed into a linear system and can be expressed as
                                    
                                       (12)
                                       
                                          H
                                          β
                                          =
                                          T
                                       
                                    
                                 where 
                                    H
                                  is the hidden-layer output matrix, 
                                    β
                                  is the matrix of output weights and 
                                    T
                                  is the matrix of the desired output. Further details about the formulation can be found in [30].

Step 2: Calculate the hidden layer output matrix 
                                    H
                                 .

Step 3: Calculate the output weights vector 
                                    β
                                  by obtaining the least-square solution of the linear system in Eq. (13).

A variation of ELM, RELM, was also investigated in this study. RELM inherits the fast learning feature of ELM while its generalization performance is enhanced by using the least squares regression methods to identify the degree of relevance of the weight linking a hidden node to the output layer, where penalties are applied to the coefficient vectors [31]. In RELM, the regularization parameter 
                           γ
                         is introduced to improve the controllability [32]. In order to reduce the effect of noise, RELM introduces a weighting factor 
                           
                              
                                 v
                              
                              
                                 i
                              
                           
                         to weigh the error between the output of RELM and the actual output of the ith input sample. Hence, the output weight 
                           β
                         in Eq. (13) can be expressed as
                           
                              (14)
                              
                                 β
                                 =
                                 
                                    
                                       (
                                       
                                          
                                             I
                                             γ
                                          
                                          +
                                          
                                             
                                                H
                                             
                                             T
                                          
                                          
                                             
                                                D
                                             
                                             2
                                          
                                          H
                                       
                                       )
                                    
                                    +
                                 
                                 
                                    
                                       H
                                    
                                    T
                                 
                                 
                                    
                                       D
                                    
                                    2
                                 
                                 T
                                 ,
                              
                           
                        where 
                           D
                           =
                           dialog
                           
                           
                              (
                              
                                 
                                    
                                       v
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       v
                                    
                                    
                                       2
                                    
                                 
                                 ,
                                 ..
                                 .
                                 
                                    
                                       v
                                    
                                    
                                       N
                                    
                                 
                              
                              )
                           
                         and 
                           γ
                         is the regularized factor. In the experiment, the number of hidden nodes in both the ELM and RELM was set experimentally by selecting a value from 
                           {
                           
                              20
                              ,
                              22
                              ,
                              24
                              ,
                              26
                              ,
                              28
                              ,
                              30
                              ,
                              32
                              ,
                              34
                              ,
                              36
                              ,
                              38
                           
                           }
                        .

SVM is a typical kernel-based technique for supervised data classification. The basic principle is to create a hyperplane as the decision surface for classification, where the edge of the isolation between different categories of data is maximized. In the process, the input data vectors are first mapped to a high-dimensional space [33]. The SVM algorithm then searches for a hyperplane with the largest margin in order to achieve the best generalization ability. 
                        Fig. 2 gives an example with two linearly separable classes and the data points are denoted by crosses and triangles respectively. The points closest to the decision surface are the supporting vectors and the distance between support vectors and surface is the margin.

Consider a given training dataset 
                           T
                           =
                           
                              {
                              
                                 
                                    (
                                    
                                       
                                          
                                             x
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             y
                                          
                                          
                                             1
                                          
                                       
                                    
                                    )
                                 
                                 ,
                                 ..
                                 .
                                 ,
                                 
                                    (
                                    
                                       
                                          
                                             x
                                          
                                          
                                             N
                                          
                                       
                                       ,
                                       
                                          
                                             y
                                          
                                          
                                             N
                                          
                                       
                                    
                                    )
                                 
                              
                              }
                           
                           ∈
                           
                              (
                              
                                 X
                                 ×
                                 Y
                              
                              )
                           
                         with 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ∈
                           X
                           =
                           
                              
                                 R
                              
                              n
                           
                           ,
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           ∈
                           Y
                           =
                           
                              {
                              
                                 1
                                 ,
                                 −
                                 1
                              
                              }
                           
                           
                              (
                              
                                 i
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 ..
                                 .
                                 ,
                                 N
                              
                              )
                           
                        , where the training data matrix 
                           X
                         has two separable classes with the class labels −1 and +1 stored in the vector 
                           Y
                        . Applying the Lagrangian multiplier with the kernel function 
                           K
                           
                              (
                              
                                 x
                                 ,
                                 x
                                 ′
                              
                              )
                           
                         and the regularization parameter
                           C
                        , the dual formulation of SVM is written as follows,
                           
                              (15)
                              
                                 
                                    
                                       
                                          min
                                          
                                          
                                             1
                                             2
                                          
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          
                                          
                                             
                                                y
                                             
                                             
                                                j
                                             
                                          
                                          
                                             
                                                α
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                α
                                             
                                             
                                                j
                                             
                                          
                                          K
                                          
                                             (
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ,
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      j
                                                   
                                                
                                             
                                             )
                                          
                                          −
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             
                                                
                                                   α
                                                
                                                
                                                   j
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          s
                                          .
                                          t
                                          .
                                          
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             
                                                
                                                   y
                                                
                                                
                                                   i
                                                
                                             
                                             
                                                
                                                   α
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          =
                                          0
                                          ,
                                          
                                          0
                                          ≤
                                          
                                             
                                                α
                                             
                                             
                                                i
                                             
                                          
                                          ≤
                                          C
                                          ,
                                          i
                                          =
                                          1
                                          ,
                                          ..
                                          .
                                          ,
                                          N
                                       
                                    
                                 
                              
                           
                        where the optimal solution 
                           
                              
                                 α
                              
                              ⁎
                           
                           =
                           
                              
                                 (
                                 
                                    
                                       
                                          
                                             α
                                          
                                          
                                             1
                                          
                                       
                                       ⁎
                                    
                                    ,
                                    ..
                                    .
                                    ,
                                    
                                       
                                          
                                             α
                                          
                                          
                                             N
                                          
                                       
                                       ⁎
                                    
                                 
                                 )
                              
                              T
                           
                         and threshold value 
                           
                              
                                 b
                              
                              ⁎
                           
                         can be obtained. Thus the decision function for the classification is given by
                           
                              (16)
                              
                                 f
                                 
                                    (
                                    x
                                    )
                                 
                                 =
                                 sgn
                                 
                                    (
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          N
                                       
                                       
                                          
                                             
                                                
                                                   α
                                                
                                                
                                                   i
                                                
                                             
                                             ⁎
                                          
                                       
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       K
                                       
                                          (
                                          
                                             x
                                             ,
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          )
                                       
                                       +
                                       
                                          
                                             b
                                          
                                          ⁎
                                       
                                    
                                    )
                                 
                                 .
                              
                           
                        
                     

The summation in Eq. (16) is performed only on a small group of support vectors with non-zero 
                           
                              
                                 α
                              
                              
                                 i
                              
                           
                        .

In this study, a polynomial kernel function was used in the experiment, where the optimal value of the regularization parameter C is determined using the 10-fold cross-validation strategy (to be discussed in Section 4.1) to achieve a balance between classification accuracy (with a larger C) and generalizabity (with a smaller C, i.e. larger margin). The value of C in the experiment was chosen from 
                           {
                           
                              150
                              ,
                              200
                              ,
                              250
                           
                           }
                        .

The KNN algorithm was also investigated in this study for its simplicity in implementation [34]. In KNN, the training dataset is reserved and used to classify a new unclassified testing dataset. Classification is achieved by comparing the testing dataset with the groups in the training set to identify the most similar one based on a distance function [35]. In this study, the similarity between two neighboring data points x and y is measured by the Euclidean distance
                           
                              (17)
                              
                                 
                                    
                                       
                                          ∑
                                          i
                                       
                                       
                                          
                                             
                                                (
                                                
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   −
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                                )
                                             
                                             2
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where 
                           x
                           =
                           
                              (
                              
                                 
                                    
                                       x
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       2
                                    
                                 
                                 ,
                                 ..
                                 .
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       n
                                    
                                 
                              
                              )
                           
                         and 
                           y
                           =
                           
                              (
                              
                                 
                                    
                                       y
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       2
                                    
                                 
                                 ,
                                 ..
                                 .
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       n
                                    
                                 
                              
                              )
                           
                         are vectors in n-dimensional space.

The number of neighbors 
                           k
                         has to be set appropriately to reduce the effect of noise and outliers on the classification (k too small) and to avoid the dominance of local behavior (k too large) [35]. In the study, the value of 
                           k
                         was chosen experimentally with values in 
                           {
                           
                              10
                              ,
                              12
                              ,
                              15
                              ,
                              18
                              ,
                              20
                           
                           }
                         to identify the one with the best performance.

Lastly, the NB classifier was employed in the study by assuming that all the attributes were conditionally independent of the class labels [36,37]. In the training stage, the NB classifier estimated the parameters of the class priors and the probability distribution of the attributes by analyzing the training samples. In the testing stage, the method calculated the posterior probability of every sample belonging to each class. The NB classifier then selected the class with the largest posterior probability as the output of the testing samples.

@&#EVALUATION@&#

In supervised learning, for a given sample with known input and output, an ideal prediction model taking the same input is expected to produce a result consistent with the known output of the sample. In other words, the 5-year mortality suggested by the prediction model is “alive” for a sample indicating that the bladder cancer patient was alive after radial cystectomy (i.e., true positive). The output of the prediction model is “dead” for a sample recording that the bladder cancer patient had been dead after the surgery (i.e., true negative). In the experiment, the reliability of the prediction models was quantified by the four standard performance measures, namely, accuracy, sensitivity, specificity and precision, which are given by
                           
                              (20)
                              
                                 Accuracy
                                 =
                                 (
                                 T
                                 P
                                 +
                                 T
                                 N
                                 )
                                 /
                                 (
                                 T
                                 P
                                 +
                                 T
                                 N
                                 +
                                 F
                                 P
                                 +
                                 F
                                 N
                                 )
                                 ,
                              
                           
                        
                        
                           
                              (21)
                              
                                 Sensitivity
                                 =
                                 T
                                 P
                                 /
                                 
                                    (
                                    
                                       T
                                       P
                                       +
                                       F
                                       N
                                    
                                    )
                                 
                                 ,
                              
                           
                        
                        
                           
                              (22)
                              
                                 Specificity
                                 =
                                 T
                                 N
                                 /
                                 
                                    (
                                    
                                       T
                                       N
                                       +
                                       F
                                       P
                                    
                                    )
                                 
                                 
                                 
                                 and
                              
                           
                        
                        
                           
                              (23)
                              
                                 Precison
                                 =
                                 T
                                 P
                                 /
                                 
                                    (
                                    
                                       T
                                       P
                                       +
                                       F
                                       P
                                    
                                    )
                                 
                                 ,
                              
                           
                        where 
                           T
                           P
                        , 
                           T
                           N
                        , 
                           F
                           P
                        , 
                           F
                           N
                        denotes the number of true positives, true negatives, false positives and false negatives respectively.

In this study, the training and testing datasets required for evaluating the machine learning methods were generated randomly using the clinicopathological dataset of the bladder cancer patients. To reduce the biasing effect that may be introduced in the generation of the training and testing datasets, which can lead to over-fit or under-fit models, the 10-fold cross-validation strategy was used in the experiment. Here, the original clinicopathological dataset was randomly divided into 10 subsets. The subsets had a class distribution similar to that of the original dataset. Cross validation was performed by alternately using one subset as the testing dataset to evaluate the model built using the remaining 9 subsets as the training dataset. This process was repeated 10 times so that every subset took turn to serve as the testing dataset, thereby resulting in 10 models. The mean and standard deviation of the performance indices of the 10 models created by the cross-validation strategy were calculated [38].

@&#EXPERIMENTS@&#

The models for predicting the 5-year mortality after radical cystectomy were experimentally studied by using the clinicopathological data described in Section 2 and the seven machine learning methods discussed in Section 3. Three experiments were conducted.

First, experiments were conducted to identify the optimal parameters for these models, except the NB classifier where the model was built based on prior knowledge deduced from the clinicopathological data. The parameters are shown in 
                        Table 2. The seven prediction models were evaluated and the experimental results are given in 
                        Table 3.

Second, as discussed in Section 2, the dataset used here was originally collected to examine the association between patient age, tumor stage, preoperative serum albumin level and mortality following radical cystectomy [2]. In this study, experiments were designed to serve the same purpose using machine learning techniques. This was achieved by using the random permutation strategy [39] in which an attribute can be evaluated by randomly permuting the values of that attribute with the data in the same dataset, and then observing any reduction in accuracy of the machine learning methods before and after random permutation. The larger the decrease is in accuracy, the stronger the association of that parameter is with the outcome. In experiment, we first randomly permuted the values of the attribute “age” and then run the machine learning models on the randomly permuted dataset. The experiment was repeated ten times to calculate the average accuracy. The experiments were conducted in the same way for the attributes “tumor stage” and “preoperative serum albumin level” respectively. As it is evident from Table 3 that the performance of ELM and RELM is outstanding among the seven machine learning methods, emphasis was put on these two methods and the experimental results presented in 
                        
                        
                        Tables 4, 5 and 6.

Third, after the seven machine learning models were built by using the clinicopathological data described in Section 2, experiments were conducted to do comparison with the common-used nomogram prediction method designed by Lughezzani [40]. The method, which is also available on-line [41], is developed using the smoothed Poisson regression model according to disease stage and patient age [40,41]. A dataset with attributes required in both the nomogram prediction method and the machine learning models here was acquired in order to carry out the comparisons. Records of 30 patients were contained in the dataset. For fair comparison, we set the prediction output of this common nomogram to 1 if its probabilistic output is bigger than 0.5, 0 otherwise. Similarly, emphasis was put on the two outperforming machine learning methods ELM and RELM. The experimental results are shown in 
                        Table 7.

@&#DISCUSSION@&#

The paper investigated the use of seven machine learning methods to predict the mortality of after the mortality after radical cystectomy. The performance of the seven prediction models could be broadly classified into three tiers, refer to experimental results in Table 3. The prediction models developed using BPN, NB classifier and KNN exhibited relatively low performance, with a mean classification accuracy of 0.7222, 0.7333 and 0.7222 respectively. The middle tier includes the prediction models developed using SVM and RBFN. Their mean classification accuracy was 0.7556 and 0.7667 respectively. For ELM and RELM, the resulting prediction models developed showed the best classification performance, with RELM achieving the highest mean accuracy of 0.8000. While their mean classification accuracy obtained using ELM and RBFN was the same, the maximum value achieved by the former was higher. In this practical medical application, ELM and RELM, as theoretically well-founded machine learning techniques, were found to outperform the other five methods.

Regarding the low performers, while the principles of the NB classifier is relatively more intuitive to clinicians, the assumption of conditional independence of the attributes is practically difficult to be fully satisfied in many medical applications. The probabilities of the events are usually obtained individually and their associations and interactions are unclear, which affect the validity of the assumption of the NB classifier and thus the classification accuracy. If the assumption is hold, the NB classifier can converge quickly and require less training data. It can be seen from the results in Table 3 that the NB classifier slightly outperformed the KNN method, in the terms of mean and maximum accuracy. As the size of the dataset in this study was relative small, the NB classifier, which is a high-bias-low-variance approach, was advantageous over low-bias-high-variance KNN method. This is mainly because the NB classifier is a linear method that constructs only a hyperplane, whereas KNN is a nonlinear method that builds variable boundaries between classes. The latter can adversely increase the probability of misclassification, particularly for noisy data. Moreover, the value of 
                        k
                      in KNN is case dependent and the method is prone to over-fitting [42].

Among the seven prediction models, the models developed using RELM and ELM exhibited the highest mean sensitivity and specificity respectively (over 0.8), while the ones built with ELM and BPN had the lowest mean sensitivity and specificity respectively. For all the prediction model, the mean values of sensitivity and specificity values were all above 0.7, except the mean specificity achieved using BPN (i.e. 0.6811). For the models built using ELM and SVM, the mean sensitivity was lower than the mean specificity, the reverse was true for the other machine learning methods, except the NB classifier, where the mean sensitivity and specificity were about the same. The difference between the mean sensitivity and mean specificity was relatively large for the model developed using regularized ELM (0.8599 vs. 0.7236). Similar situations were also observed from the models built using the ELM and BPN. Ideally, prediction models with high sensitivity and high specificity are desired. The small sample size in this study may be an obstacle towards an ideal model. Taking precision into consideration, the models developed using ELM and RELM exhibited both high mean accuracy and high mean precision, which is desirable for outcome prediction.

On the other hand, the neural networks constructed using ELM were found to contain more hidden nodes when compared to that using BPN [30], which complicated the network architecture and incurred more computations. Nevertheless, the timing performance of the machine learning methods ELM and RELM was indeed outstanding whereas the learning speed of the models based on BPN and RBFN was relatively slow. The slow learning was attributed to the fact that the conventional gradient-descent based methods were used in BPN and RBFN [43], where multiple iterations were required to adjust and optimize the weights and bias of the neural network to avoid sub-optimal solutions resulting from the problem of local minima. Besides, manual setting of parameters was also required in the gradient-descent methods.

Moreover, by comparing Tables 4, 5 and 6 with Table 3, we can easily find that the accuracy performance obtained by both ELM and RELM degrades a lot after random permutation of ‘age’, ‘tumor stage’ and ‘preoperative serum albumin level’. The accuracy was decreased by 11.96%, 16.67% and 14.49% respectively for ELM; and 10.42%, 13.54% and 6.04% respectively for RELM. Therefore, we can conclude by using the machine learning approach that these three attributes are predictors of survival after radical cystectomy. The findings here are in line with the claim in [2], demonstrating the feasibility of using ELM and RELM to identify the predictors of survival after radical cystectomy.

Finally, referring to Table 7, it is found that the proposed ELM and RELM models have better accuracy than the adopted nomogram prediction method, i.e. greater than 0.7 for ELM and RELM versus 0.63 for the nomogram prediction method. In other words, the results demonstrate the advantage of using machine learning technology in predicting mortality after radical cystectomy.

In summary, the findings in the experiments suggested that ELM and RELM were well-suited for the prediction of mortality after radical cystectomy concerned in this study.

@&#CONCLUSION@&#

Seven machine learning methods were explored in this study for the prediction of mortality of radical cystectomy for bladder cancer. Real clinicopathological data of 117 patients who had undergone the surgery were adopted. Prediction models were developed using the machine learning methods and the performance was evaluated in terms of accuracy, sensitivity, specificity and precision.

The experimental results indicated that ELM and RELM demonstrated competitive performance among the machine learning methods investigated. The running time was also among the lowest. The findings suggest that the ELM-based algorithms are relatively more effective in the prediction of mortality of radical cystectomy. In addition to superiority in accuracy and speed, ELM is also advantageous over the gradient-descent algorithms in machine learning that require manual parameter tuning and suffer from the problem of local minima. Moreover, the feasibility of using ELM and RELM to identify the predictors of survival after radical cystectomy was validated. The proposed ELM-based algorithms exhibited better accuracy than one common-used nomogram prediction method.

In addition to these seven models, it is also worthwhile to evaluate the performance of other regression models like least square support vector regression on the bladder cancer data set, which is a future work of the study. Furthermore, it is also noted that the classification accuracy of the seven models investigated in the study was indeed not high enough (less than 0.8) to offer a reliable reference for clinical decision support. This may be caused by the small sample size in the study, which calls for the need to establish a centralized data repository to collect the clinicopathological data obtained from the urology units of multiple hospitals for bladder cancer research and to facilitate the sharing of the data for the development of reliable prediction models. Further work on external validation will be conducted through prospective collection of the test data with the attributes required by the methods of the Bladder Cancer Consortium or the SEER-Medicare database, and also the machine learning models presented here. To facilitate data collection, a clinical data repository is being set up to consolidate the data from the urology unit of multiple hospitals.

None declared.

@&#ACKNOWLEDGMENTS@&#

This work was supported in part by the Research Grants Council of the Hong Kong SAR (PolyU5134/12E), the Hong Kong Polytechnic University (G-UC93), and the scholarship donated by Nelson Y.C. Yu.

@&#REFERENCES@&#

