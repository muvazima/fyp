@&#MAIN-TITLE@&#Cross-pose face recognition based on multiple virtual views and alignment error


@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Cross-pose face recognition with only a single frontal gallery face is a challenging task.


                        
                        
                           
                           Multiple virtual views are generated from a single frontal gallery face in advance.


                        
                        
                           
                           Alignment error of two faces is calculated after a two-phase (pose and individual) alignment.


                        
                        
                           
                           Correlation between patches is considered by overlapping and covariance.


                        
                        
                           
                           Cross-pose face recognition accuracy is significantly improved than compared methods.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Face recognition

Virtual views

Alignment error

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           Image, graphical abstract
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Face recognition has been investigated for several decades. According to the 2009 NIST MBGC report [1], face recognition remains a challenging endeavor on account of variations in poses, illumination, occlusion, and aging. Among these, pose variance is the most difficult to address. Face recognition algorithms can be used to identify criminals from surveillance systems for public security. In addition, they can be applied to automatically annotate digital photos for individuals. Moreover, commercial face recognition software is publicly available, such as Google Picasa and Apple iPhoto [2].

Discriminative facial features are important for both accuracy and speed. Local features, such as local Gabor binary patterns (LGBP) [3] and high-dimensional local binary patterns (LBP) [4], are effective for face recognition. Wright et al. proposed use of sparse representation for face recognition; it can handle illumination, expression variance, and occlusion [5]. The face recognition problem can be divided into two categories: face identification and face verification. Face identification serves to identify a probe face from a set of gallery faces with known identities. Face verification is used to determine whether two images belong to the same subject.

Face verification is a useful branch of the face recognition problem. Recently, a joint Bayesian model trained from a large dataset of labeled faces was successfully used for face verification [6]. Based on this model, a transfer learning method was proposed for combining ample cross-domain source data [7]. In [8], the Facebook AI Research group proposed the DeepFace framework, which uses deep networks for face verification. The system reached a state-of-the-art accuracy of over 97% on the Labeled Faces in the Wild (LFW) database.

Face identification is another challenging endeavor for face recognition. Under controlled settings, such as a frontal face with little illumination variance, the face identification performance has approached human capacity. While pose and illumination variances exist in most applications, face identification accuracy significantly decreases when test faces are non-frontal. Most facial image databases contain only frontal faces, such as driver licenses. The Department of Motor Vehicles collects frontal view images of each driver. Thus, it is necessary to process cross-pose matching to identify a randomly posed face from a frontal view database. Cross-pose identification remains a challenging problem. Moreover, in cases in which only a single frontal face is available, cross-pose matching becomes more difficult. The difficulty in identifying a face with different poses is that the ‘between-subject’ differences are less than the ‘between-pose’ differences. There are two solutions for handling this problem: geometry-based methods and pose-invariant-based methods.

The geometry-based method uses an alignment method to build correspondences among different poses. Based on these correspondences, a probe face with different poses can be normalized to a frontal face. This method can be performed in both 2D and 3D cases. Regarding 2D methods, a Markov random field (MRF) is used to find correspondences between a frontal face and profile faces [9,10]. The 2D displacement is captured using MRF by minimizing the energy, which includes the residual of two corresponding nodes and smoothness between neighboring nodes. MRF is effective on some databases; however, it incurs lengthy computation times. Lucas–Kanade is another effective 2D normalization method [11]; it calculates the transformation parameters for each of the correspondences of two images.

The 3D morphable method is an effective normalization method [12] that builds a 3D general model and fits it to a probe 2D face. The fitted shape and texture coefficients are used for face identification. Li et al. [13] synthesized a probe face by estimating 3D displacement fields from a 3D face database. It has been reported that 3D techniques can achieve impressive results on many databases. However, 3D face databases are required for these methods. Furthermore, recovery of a 3D virtual face from a 2D image is difficult because of insufficient information. Moreover, fitting a 3D model to a 2D image is sensitive to factors such as illumination and expression.

The pose-invariant-based method employs pose-invariant features or pose-insensitive classifiers to eliminate the pose influence. Tied-factor analysis has been proposed for representing a non-frontal face by a pose-contingent linear transformation of identifiers [14]. The resultant pose-invariant identity subspace is used for identification. Another subspace, called the discriminant-coupled latent subspace, has been proposed [15]. It is used to find projections of the same subject from different poses that are maximally correlated in the latent subspace. One-shot similarity (OSS) and two-shot similarity (TSS) are pose-insensitive classifiers [16]; they require a third-party dataset with no probe and gallery faces in it. Each subset can be of the same subject with different poses. Similarities between two faces are calculated by models built from these faces and the subsets using linear discriminant analysis (LDA) or support vector machine (SVM). Similarly, cross-pose face recognition likewise requires a third-party dataset [17]. Faces from different poses are linearly represented by the third-party dataset based on a subspace method. The obtained linear coefficients are used for face identification. Recently, neural networks [18] and deep learning [19] have been applied for calculating the pose-invariant features. The networks are trained by converting a non-frontal face to a frontal face; the pose-invariant features are obtained in a specific layer.

In this paper, we propose a novel algorithm based on multiple virtual views and alignment error (MVV–AE) for face recognition under large pose changes with only a single frontal face available for each subject in a gallery. The main contributions of this paper are as follows: (1) scale-invariant feature transform (SIFT)-matching score based on multiple virtual views is proposed to improve the performance of SIFT for the large pose change. A frontal face is transformed into multiple virtual views using learning warps across poses by the Lucas–Kanade algorithm [11]. SIFT is used to calculate the keypoints from these virtual views and to match them to a probe face; (2) a two-phase alignment method is proposed to calculate the alignment error between a probe face and a gallery face. Offline alignment is used to calculate pose differences, while online alignment is used to calculate individual differences. Overlapping and covariance are adopted to capture correlations between patches; (3) a hybrid similarity between probe and gallery faces is obtained by the number of matched keypoints from SIFT and the two-phase alignment error.

The remainder of this paper is organized as follows. In Section 2, we describe the framework of the proposed face recognition algorithm based on MVV–AE. We describe the MVV generation method in Section 3. In Section 4, we introduce the proposed two-phase AE with the correlation method. The combination of these two methods is used for the similarity calculation in Section 5. In Section 6, we apply the above algorithm to the FERET [23] database and present the experimental results. Finally, we present our conclusions in Section 7.

Local features, such as LBP and SIFT, are effective for face recognition with small pose changes. However, the performance significantly decreases with large pose variations. To enable the effectiveness of these local features for large pose changes, we propose a novel framework based on MVV–AE. The challenge of this objective is that gallery faces and probe faces are of different poses and only a single frontal face is stored in the gallery. Fig. 1
                      shows the framework of the proposed MVV–AE method, whereby warps between poses are learned from numerous face pairs of different poses. Given a gallery face and a probe face, we can calculate their similarity based on two parts: the number of matched keypoints and the alignment error. Since pose estimation is out of the scope of this paper, we assume the pose of a probe face is annotated with the ground truth. The input data for a test consist of a probe face image and its ground-truth pose.

The number of matched keypoints is obtained from the SIFT matching algorithm between the probe face and generated virtual views of a gallery face. For each gallery face, we generate multiple virtual views in advance using the learned warps between poses. SIFT keypoints are detected from these virtual views and matched with the probe face.

The alignment error is calculated by a hybrid alignment method with consideration of the correlations between patches. The differences between two face images are primarily from the pose variance and identity differences. In this study, we consider both of these differences in our hybrid alignment, which contains a pose and individual alignment.

Without 3D information, a probe face with a profile angle is difficult to transform into a frontal face because of occlusion, especially in the nose area. However, frontal faces contain intact information with no occlusion. This motivates us to transform a frontal face into multiple profile views instead of normalizing a profile face into a frontal view. In this scheme, transformation of a frontal face into virtual views is performed in advance, while normalization of a profile face into a frontal view should be simultaneously conducted with the face recognition.

To transform a frontal face into multiple virtual views, we must learn warps among multiple poses. As a simple and efficient warp, the affine transformation is used in this study. The human face contains significant 3D depth information; a single affine warp for the entire face is insufficient for capturing transformations between poses. Thus, we divide a face into multiple subregions or patches; a warp is learned for each patch. The Lucas–Kanade algorithm [11] is effective for learning warps between poses, as shown in Fig. 2.
                  

To obtain generic warps, numerous face pairs are used to learn warps between poses. This can be performed by averaging two sets of faces; the averaged face pairs are used to learn warps. However, an averaging of faces results in neutralization, which is too generalized to represent a specific individual. The stack flow method [11] provides a better solution by calculating warps that minimize all face pairs from two poses as follows:

                        
                           (1)
                           
                              
                                 
                                    E
                                    
                                       r
                                       
                                          (
                                          
                                             s
                                             t
                                             k
                                          
                                          )
                                       
                                    
                                 
                                 =
                                 
                                    ∑
                                    j
                                 
                                 
                                    ∑
                                    x
                                 
                                 
                                    
                                       (
                                       
                                          
                                             I
                                             
                                                j
                                                ,
                                                r
                                             
                                          
                                          
                                             (
                                             
                                                W
                                                
                                                   (
                                                   
                                                      X
                                                      ,
                                                      P
                                                   
                                                   )
                                                
                                             
                                             )
                                          
                                          −
                                          
                                             T
                                             
                                                j
                                                ,
                                                r
                                             
                                          
                                          
                                             (
                                             X
                                             )
                                          
                                       
                                       )
                                    
                                    2
                                 
                              
                           
                        
                     where I and T are images from two poses, and j and r are the jth pair of images and the patch index, respectively. W(X, P) is the warp function, which is the affine transformation in this study, and

                        
                           (2)
                           
                              
                                 W
                                 
                                    (
                                    
                                       X
                                       ,
                                       P
                                    
                                    )
                                 
                                 =
                                 P
                                 X
                                 =
                                 
                                    (
                                    
                                       
                                          
                                             
                                                1
                                                +
                                                
                                                   p
                                                   1
                                                
                                             
                                          
                                          
                                             
                                                p
                                                3
                                             
                                          
                                          
                                             
                                                p
                                                5
                                             
                                          
                                       
                                       
                                          
                                             
                                                p
                                                2
                                             
                                          
                                          
                                             
                                                1
                                                +
                                                
                                                   p
                                                   4
                                                
                                             
                                          
                                          
                                             
                                                p
                                                6
                                             
                                          
                                       
                                       
                                          
                                             0
                                          
                                          
                                             0
                                          
                                          
                                             1
                                          
                                       
                                    
                                    )
                                 
                                 
                                    (
                                    
                                       
                                          
                                             x
                                          
                                       
                                       
                                          
                                             y
                                          
                                       
                                       
                                          
                                             1
                                          
                                       
                                    
                                    )
                                 
                                 .
                              
                           
                        
                     The Lucas–Kanade algorithm provides a solution for Eq. (1) by iterating the update of P with ΔP as

                        
                           (3)
                           
                              
                                 Δ
                                 P
                                 =
                                 
                                    H
                                    
                                       
                                          (
                                          
                                             s
                                             t
                                             k
                                          
                                          )
                                       
                                    
                                    
                                       −
                                       1
                                    
                                 
                                 
                                    ∑
                                    j
                                 
                                 
                                    ∑
                                    X
                                 
                                 
                                    
                                       (
                                       
                                          ∇
                                          
                                             I
                                             
                                                j
                                                ,
                                                r
                                             
                                          
                                          
                                             
                                                ∂
                                                W
                                             
                                             
                                                ∂
                                                P
                                             
                                          
                                       
                                       )
                                    
                                    T
                                 
                                 
                                    (
                                    
                                       
                                          T
                                          r
                                       
                                       
                                          (
                                          X
                                          )
                                       
                                       −
                                       
                                          I
                                          r
                                       
                                       
                                          (
                                          
                                             W
                                             
                                                (
                                                
                                                   X
                                                   ,
                                                   P
                                                
                                                )
                                             
                                          
                                          )
                                       
                                    
                                    )
                                 
                              
                           
                        
                     where 
                        
                           ∇
                           
                              I
                              r
                           
                           =
                           
                              (
                              
                                 
                                    
                                       ∂
                                       
                                          I
                                          r
                                       
                                    
                                    
                                       ∂
                                       x
                                    
                                 
                                 ,
                                 
                                    
                                       ∂
                                       
                                          I
                                          r
                                       
                                    
                                    
                                       ∂
                                       y
                                    
                                 
                              
                              )
                           
                           ,
                        
                     
                     
                        
                           
                              
                                 ∂
                                 W
                              
                              
                                 ∂
                                 P
                              
                           
                           ,
                           
                        
                     and H
                        img
                      are the gradient of Ir
                     , the Jacobian of the warp and a pseudo Hessian matrix, respectively.

Let N and M be the number of patches in an image and the number of poses, respectively. Let 
                        
                           Φ
                           =
                           (
                           
                              
                                 P
                                 1
                              
                              ,
                              
                                 P
                                 2
                              
                              ,
                              …
                              ,
                              
                                 P
                                 N
                              
                           
                           )
                        
                      be the warps between two poses. A series of warps, 
                        
                           
                              Φ
                              1
                           
                           ,
                           
                              Φ
                              2
                           
                           ,
                           …
                           ,
                           
                              Φ
                              M
                           
                        
                     , are learned between the frontal view and profile poses. Based on these warps, a frontal face is transformed into multiple virtual poses. The SIFT keypoints are detected from these virtual views and matched with the probe face. Accordingly, the number of matched keypoints between the probe face and gallery faces is obtained.

The probe face is compared with all generated virtual views because it is not easy to accurately estimate the pose; moreover, images from other poses provide additional facial information. However, considering computation time, an interval is required to maintain sparsity. SIFT is chosen for the features on account of its scale-invariant characteristic and good performance for pose variance within 25°, according to our experiments. In this regard, poses are divided into four quadrants, we define the poses in the same quadrant as the same orientation. As for the FERET database, it consists of poses from left to right captured at 60°, 40°, 25°, 15°, −15°, −25°, −40° and −60°. We divided them into two orientations: left and right. Poses at 60°, 40°, 25°, and 15°belong to the same orientation. We generate a virtual view for each pose, and based on our experiments, we found that the virtual views from different orientations of the probe face had a negative impact on the face recognition accuracy (see Section 6.1). Thus, we estimate the orientations (up-left, up-right, down-left, down-right) of the probe face. The virtual views of the same orientation of the probe face are used to calculate the matched keypoints. In this way, the compared virtual views are significantly reduced, and the estimation of orientations is much easier than that of poses.

Learned warps are the correspondences between poses and are trained from numerous face pairs. When directly applying the learned warp to the probe face, it is not sufficiently accurate on account of individual differences. Once exact warps between faces are available, we can easily calculate the difference between these two faces. The difference of two faces is derived from the pose and individual difference. The learned warps described in Section 3 demonstrate the pose alignment process. To obtain more accurate warps, we propose a two-phase alignment process, which is comprised of the pose alignment and individual alignment. The pose alignment is fulfilled by the learned warps between poses, as outlined in Section 3; it is performed in an offline manner without incurring much computation time. Individual alignment, which is conducted in an online approach, converges quickly because of the pose alignment performed prior to it. Let Pp
                      and Pi
                      be the pose warps and individual warps matrices, respectively. The two-phase alignment error (AE) is used to minimize the following error:

                        
                           (4)
                           
                              
                                 
                                    E
                                    r
                                 
                                 =
                                 
                                    ∑
                                    x
                                 
                                 
                                    
                                       (
                                       
                                          
                                             I
                                             r
                                          
                                          
                                             (
                                             
                                                W
                                                
                                                   (
                                                   
                                                      X
                                                      ,
                                                      
                                                         P
                                                         p
                                                      
                                                      ,
                                                      
                                                         P
                                                         i
                                                      
                                                   
                                                   )
                                                
                                             
                                             )
                                          
                                          −
                                          
                                             T
                                             r
                                          
                                          
                                             (
                                             x
                                             )
                                          
                                       
                                       )
                                    
                                    2
                                 
                              
                           
                        
                     where 
                        
                           W
                           
                              (
                              
                                 X
                                 ,
                                 
                                    P
                                    p
                                 
                                 ,
                                 
                                    P
                                    i
                                 
                              
                              )
                           
                           =
                           
                              P
                              i
                           
                           
                              P
                              p
                           
                           X
                        
                     , and the multiplication of the two-warp matrices, Pp
                      and Pi
                     , corresponds to the pose alignment and individual alignment, respectively. Pp
                      is learned by numerous face pairs of frontal faces and non-frontal faces with ground-truth pose p in advance. For each probe face, an online alignment is performed with each gallery face i using the Lucas–Kanade method by the iteration of update Pi
                     . The online alignment converges quickly due to the pose alignment prior to it. In our experiments, Pi
                      converged after approximately 10–15 iterations, which indicates no significant increase in computational time.

The block effect is incurred during the division of patches. Moreover, the Lucas–Kanade method is performed for each patch separately. The method readily warps adjacent patches to much different areas. Thus, we propose the use of overlapping of patches. Four neighboring patches are overlapped, as shown in Fig. 3
                     . The small patch at the center is overlapped by four neighboring patches; the warp parameter of the center patch is the average of these four patches.

After Pi
                      is obtained, the alignment error can be calculated by Eq. (4) for each patch. Because overlapping is used in our scheme, there is a correlation among patches. Considering these correlations, we multiply the covariance of the alignment error of each patch to calculate the alignment error of the whole face as

                        
                           (5)
                           
                              
                                 
                                    E
                                    
                                       m
                                       a
                                       h
                                    
                                 
                                 =
                                 
                                    
                                       (
                                       
                                          E
                                          −
                                          
                                             E
                                             ¯
                                          
                                       
                                       )
                                    
                                    T
                                 
                                 
                                    Cov
                                    
                                       −
                                       1
                                    
                                 
                                 
                                    (
                                    
                                       E
                                       −
                                       
                                          E
                                          ¯
                                       
                                    
                                    )
                                 
                              
                           
                        
                     where 
                        
                           E
                           =
                           (
                           
                              
                                 E
                                 1
                              
                              ,
                              
                                 E
                                 2
                              
                              ,
                              …
                              ,
                              
                                 E
                                 N
                              
                           
                           )
                        
                     , N is the number of patches, Cov is the covariance of E, 
                        
                           E
                           ¯
                        
                      is the mean of vector E, Cov and 
                        
                           E
                           ¯
                        
                      are statistics of alignment errors calculated across the entire gallery faces, Emah
                      is the alignment error of a gallery face and a probe face. Eq. (5) shows that Emah
                      is actually the square of the Mahalanobis distance of E. By using the Mahalanobis distance, the correlations between patches are taken into account in the alignment error.

The number of matched keypoints calculated in Section 3 represents the similarity of two faces, while the alignment error in Section 4 corresponds to the dissimilarity between two faces. We combine these two factors to calculate the similarity index of two faces as follows:

                        
                           (6)
                           
                              
                                 
                                    S
                                    i
                                 
                                 =
                                 λ
                                 
                                    
                                       M
                                       i
                                    
                                    
                                       
                                          max
                                          i
                                       
                                       
                                          (
                                          
                                             M
                                             i
                                          
                                          )
                                       
                                    
                                 
                                 −
                                 
                                    (
                                    
                                       1
                                       −
                                       λ
                                    
                                    )
                                 
                                 
                                    
                                       E
                                       i
                                    
                                    
                                       
                                          max
                                          i
                                       
                                       
                                          (
                                          
                                             E
                                             i
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        
                     where Mi
                      and Ei
                      are the number of matched keypoints and the alignment error between the probe face and gallery face, i, respectively. Mi
                      and Ei
                      are normalized to [0,1] by dividing them with the maximum value of Mi
                      and Ei
                      among all the subjects. λ is the weight for these two factors. In general, the proposed MVV–AE face recognition method that is based on matching and the alignment error can be summarized as follows:

                        
                           
                              
                              
                              
                                 
                                    Algorithm: MVV–AE similarity
                                 
                              
                              
                                 
                                    
                                       Pre-computation:
                                    
                                 
                                 
                                    1.
                                    Learn a series of warp parameters, 
                                          
                                             
                                                Φ
                                                1
                                             
                                             ,
                                             
                                                Φ
                                                2
                                             
                                             ,
                                             …
                                             ,
                                             
                                                Φ
                                                M
                                             
                                          
                                       , between 
                                          M
                                        different poses from a frontal face using 
                                          M
                                        stack images; each stack image is from the same pose.
                                 
                                 
                                    2.
                                    For each frontal face in the gallery, we generate 
                                          M
                                        virtual views based on the learned warp parameters.
                                 
                                 
                                    3.
                                    Compute the SIFT keypoints of these 
                                          M
                                        virtual views and store them in a keypoint database.
                                    
                                 
                                 
                                    
                                       Recognition:
                                    
                                 
                                 
                                    1.
                                    For each probe face, detect the SIFT keypoints and compare these keypoints with keypoints of each subject with the same orientation in the keypoint database. The number of matched keypoints is obtained.
                                 
                                 
                                    2.
                                    For each probe face, calculate the alignment error with each subject using the correlated two-phase alignment error by Eq. (5).
                                 
                                 
                                    3.
                                    The similarity index between a probe face and gallery faces is calculated using both the number of matched keypoints and the alignment error using Eq. (6).
                                 
                                 
                                    4.
                                    The gallery face that has the maximum similarity index in relation to the probe face is considered a matched face.
                                 
                              
                           
                        
                     
                  

@&#RESULTS@&#

In our experiments, the FERET database was used to evaluate the performance of the proposed algorithms. The database contains more than 14,000 faces, which are classified into several categories for different research purposes. In this study, we used the pose subset of FERET. This subset is widely used to evaluate cross-pose face recognition algorithms [11–13]. This dataset contains 200 subjects with nine poses for each subject. These poses are captured at 60°, 40°, 25°, 15°, −15°, −25°, −40° and −60°. Throughout the experiments, we used the cross-validation protocol. The database was randomly divided into two groups, Group A and Group B, which each contained 100 subjects, one group for training warps and the other for test. This process repeated 10 times and the averaged performance is used to measure the face recognition accuracy. Face images were cropped from the original FERET database to exclude hair and the background. These cropped images were then resized to a resolution of 200 × 200; the ratio of the nearest neighbor for determining the matching SIFT keypoints was set to 0.8. The specifications of the computer used to perform all experiments are as follows. All experiments were performed on an Intel Core i7-4790 with 3.6 G Hz, and 8GB of RAM running on 64-bit Windows 7 Enterprise SP1.

Multiple virtual views were generated from a single frontal face to enable non-frontal probe face matching with the frontal gallery face. Fig. 4
                         shows some of the multiple virtual views generated from a single frontal face in Group B based on the warps learned from Group A. The two subjects are presented in Fig. 4. The first and third rows are the ground truth of various poses for these two subjects from the FERET dataset; the second and fourth rows are the virtual views generated using the learned warps. Table 1
                         shows the quantitative comparison of virtual views and corresponding ground truth. The average number of matched keypoints is evaluated over the entire dataset between virtual views and ground truths. The accuracy illustrates the performance of face recognition using virtual views of the same pose with probe faces. Keypoints from generated virtual views behaves similarly to ones from the ground truth images. This table exhibits that our virtual views are close to the ground truth for poses within 25°, and provide discriminative feature for poses larger than 25°.

Nine virtual views were generated for each subject in the dataset. It should be noted that there are left and right orientations in the FERET dataset. As discussed in Section 3, we assumed that only the virtual views that have the same orientation as the probe face would have a positive impact on the face recognition accuracy. We compared four scenarios to verify this assumption. (1) MVV-ORIENT: a probe face was compared with virtual views that had the same orientation as the probe face. (2) MVV-SINGLE: a probe face was compared with a single virtual view that had the same pose as the probe face. (3) MVV-FULL: a probe face was compared with all generated virtual views. (4) SIFT: a probe face was compared with only a frontal face using the SIFT matching. Tables 2 shows the performance comparison of these four scenarios using the FERET database; Groups A was used for training warps and Group B was used for testing in Table 2
                        . This table illustrates that the accuracy relationship of each scenario is as follows: MVV-ORIENT>MVV-FULL>MVV-SINGLE>SIFT.

This result verifies that our assumption is reasonable. Only virtual views with the same orientation as the probe face contributed to the accuracy of face recognition (MVV-ORIENT>MVV-SINGLE), whereas virtual views with other orientations not only reduced the accuracy (MVV-FULL<MVV-ORIENT) but increased the computation time. Thus, in our experiments, a probe face was compared with virtual views that had the same orientation as the probe face using SIFT. It should be additionally noted that the face recognition rate with multiple virtual views increased by approximately 10% compared to the SIFT algorithms when the pose difference was greater than 25°. The SIFT algorithm was effective when the pose difference was within 25°.

To evaluate the tolerance of the proposed method to different subsets of training face poses (corresponding to virtual views), we measure the performance of face recognition with different combinations of virtual views. As for the FERET database, it contains eight virtual views. Considering that the probe face is only compared with virtual views of the same orientation, we generated pairs of virtual views at 15°, 25°, 40°, and 60° angles for both orientations. Different combinations of these pairs of virtual views are used to test the probe face with poses ranging from −60° to 60°. In this case, the multiple virtual views do not encompass all poses in the test set. As a result, the average performance of face recognition with different subsets of virtual views is shown in Table 3
                        . The table indicates that the efficacy of the poses is as following rank: 25°>15°>40°>60°. The virtual view of 25° is critical for both small and large angle poses. When two virtual views are used, it is recommended to include 25°, while three or more virtual views usually produce acceptable results. Virtual views from large angles, such as 60°, are generally affected by severe artifacts incurred by patch division, which degrade the performance of the SIFT algorithm.

The patch division process inevitably introduces the artifacts of the virtual views. Overlapping is used to reduce these artifacts. The correlation between patches was considered by using four neighboring overlapped patches and the covariance of the alignment error. Table 4 shows face recognition rates using the two-phase alignment error with correlation (AE-COR) and without correlation (AE-NCOR). This table illustrates that the correlations between patches significantly improved the face recognition accuracy. This was especially the case when the pose difference was greater than 15°; the recognition rate increased by more than 10% for pose differences of 25°, and by more than 20% for pose differences of 40°.

The MVV or AE independently applied are still not sufficiently accurate for cross-pose face recognition. Thus, we propose the MVV–AE framework to combine the number of matched keypoints of MVV and AE. To obtain an optimal weight, we measure the performance with various λ from 0 to 0.7 with an interval of 0.1. Table 5
                        
                        
                        
                         shows the face recognition accuracy with various λ. A λ of 0.2 produced the optimal performance among these weights. The small λ illustrates the importance of AE with regard to the accuracy even separately the MVV algorithm is more discriminative than AE as it can be seen from Table 2 and Table 4. By increasing the weight of AE, the accuracy is increased accordingly.


                        Fig. 5 shows the face recognition accuracy comparison of MVV, AE, and MVV–AE. This figure illustrates that MVV–AE performed better than both MVV and AE, especially when the pose difference was greater than 25°. Accuracy increased by more than 10% under these poses. The MVV and AE algorithms complement each other; accordingly, a hybrid similarity of MVV and AE achieved better results. Table 6
                         shows the standard deviation of accuracies based on the multiple experiments using cross-validation protocol. This result exhibits that MVV–AE not only increases the accuracy of face recognition but also gains the robustness towards different data.

To evaluate the effectiveness of the proposed MVV–AE, we compared it with the weighted LBP [21], LGBP [3], SIFT [20], Affine-SIFT [22], stack flow [11], and OSS [16]. Local Binary Patterns (LBP) and SIFT are widely used for effective local features in many face recognition studies. The LGBP combines LBP and Gabor features, and Affine-SIFT makes SIFT features affine-invariant. A third-party dataset is required for OSS to calculate the similarity between two faces. In our experiments, OSS between the probe face and gallery face in Groups A/B was calculated by subsets in Groups B/A; each subset consisted of nine pose faces of a single subject. LDA was used to calculate the similarity between the face and a subset for the OSS algorithm.


                        Table 7 shows the comparison of face recognition rates and computational time with other studies. The proposed MVV–AE achieved the best performance for all poses among the algorithms to which it was compared. The LGBP demonstrated approximately 15% higher accuracy than the LBP algorithm. SIFT achieved a perfect performance for pose differences within 25°. Affine-SIFT showed only a slight improvement over SIFT. This result was due to the fact that the face is not planar; therefore, an affine transform for an entire face cannot capture the appropriate transform between poses. One-shot similarity (OSS) achieved better results than LBP for small pose changes, such as within 25°. However, it was not capable of dealing with large pose changes. The stack flow algorithm, which is based on the Lucas–Kanade method, achieved good results for small pose changes. In general, our proposed MVV–AE achieved an approximately 20% face recognition accuracy increase over LBP-based algorithms (LBP, LGBP, OSS) and a 10% increase over SIFT-based algorithms (SIFT, Affine-SIFT) and Lucas–Kanade algorithms (stack flow). In particular, our proposed MVV–AE algorithm achieved 95% face recognition accuracy when pose differences were less than 40°. The computational time shows the time for testing with a gallery face. The proposed MVV–AE exhibited the worst time, while it is worthy and acceptable due to the higher accuracy.

@&#CONCLUSION@&#

In this paper, a novel face recognition framework based on multiple virtual views and alignment error was developed and implemented. The method enables a non-frontal probe face to be compared with a frontal face in a gallery. To overcome the problem of differing poses, a frontal face is transformed into multiple virtual views using learned warps between poses. SIFT is used to calculate the number of matched keypoints between the probe face and virtual views. Furthermore, a two-phase alignment error was proposed to capture the pose and individual alignment error between faces, while correlations between patches are used to improve accuracy. Finally, the number of matched keypoints and the alignment error between the probe face and gallery face are combined to calculate the similarity between them. Experimental results showed that multiple virtual views significantly increased the cross-pose face recognition rate compared to a single SIFT algorithm. Furthermore, combining the alignment error with correlation, our proposed method achieved impressive results when it was compared to other algorithms.

@&#ACKNOWLEDGMENTS@&#

This work was also supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MEST; no. 2012R1A2A2A03).

@&#REFERENCES@&#

