@&#MAIN-TITLE@&#Public street lighting remote operation and supervision system

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Public street lighting system consists of points of light and a supervision.


                        
                        
                           
                           The architecture of the proposed system is modular and expandable.


                        
                        
                           
                           The system follows standard technologies.


                        
                        
                           
                           The results obtained attests that this applied methodology is feasible.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Remote operation

Lighting system

Supervision

XML

@&#ABSTRACT@&#


               
               
                  Public street lighting system consists of devices distributed in points of light and a supervision and control application. The system architecture is modular and expandable. In developing the work the C# language is adopted to develop the operation and monitoring via standard CyberOPC and XML file types are applied to the device description and definition of the network topology. This paper describes the validation proposed and the results obtained attests that this applied methodology is feasible and can be applied to other public lighting systems.
               
            

@&#INTRODUCTION@&#

This work is part of the project that analyzes the development of technologies for street lighting system, which is a wireless sensor network system that monitors and controls electrical variables. The system consists of devices attached to lighting points, which are interconnected via a network, and software tools for monitor and control. Moreover, the low levels of Wireless Sensor Networks (WSN) used refer to the IEEE 802.15.4 standard operating in mesh topology, through multi-hop communication based on low-range communication.

The main contribution from this paper is proposing a developing methodology and architecture for Public Street Lighting Remote Operation and Supervision System intended for sensor wireless urban networks [1]. The motivation is the search for a way to architecture recent and advanced features related to standard services and software components.

The next section describes the proposed public lighting system, as well as the specified requirements. Section 3 presents the requirements for public lighting system operation. Section 4 introduces the proposed system and details its architecture. Section 5 presents the results as well as the proposed evaluation strategies and, finally, the last section draws the conclusions.

The proposed public lighting system consists of an urban network that, according to the document RFC 5548 [1] — Routing Requirements for Urban Low-Power and Lossy Networks, is defined as: “Sensing and actuating nodes placed outdoors in urban environments so as to improve people's living conditions as well as to monitor compliance with increasingly strict environmental laws. These field nodes are expected to measure and report a wide gamut of data (for example, the data required by applications that perform smart-metering or that monitor meteorological, pollution, and allergy conditions). The majority of these nodes are expected to communicate wirelessly over a variety of links such as IEEE 802.15.4, low-power IEEE 802.11, or IEEE 802.15.1 (Bluetooth), which given the limited radio range and the large number of nodes requires the use of suitable routing protocols”.

The recent document proposed by IETF [1], which describes requirements for routing on urban networks, also affirms that these networks must attend convergent traffic, where one node (usually a gateway or sink) receives messages from several low frequency meters (a maximum of one measurement per hour, and a minimum of one measurement per day). The number of nodes must be in the order of 102 to 107, distributed in areas varying from hundreds of meters to one square kilometer, considering that nodes are commissioned in groups, and the battery shelf life is usually in the order of 10 to 15years. The frequency band must be ISM (Industrial, Scientific and Medical), and the nodes will probably have from 5 to 10 immediate neighbors to communicate. The routing protocol must enable the network to be autonomous and organize itself, requiring a minimum energetic cost for maintenance functions. The protocol must also ensure that any diagnostic or failure information from the nodes is communicated without interfering on the network operational mode, respecting the time limits. Specifically to public lighting system, according Brazilian Energy concessionaire, each node is positioned to the distance of 30m. The number of neighbors can vary from 3 to 34 for public street lighting.

In the literature, as in the solutions found in the industry, it is seen that the existence of a standard on the functionalities needed to manage the public street lighting system. In general, Denardin et al. [2] affirm that a public street lighting system have to be able to execute the basic tasks such as, switch on/off, control the luminosity (when it is possible), measure and control the lamp long life. From the analysis made in correlate works, it is possible to divide the requirements into the following.
                        
                           •
                           Supervision: Lee et al. [3] illustrate a typical example of how a management app can dispose the information about lamp condition in real-time through a graphical perspective, in which the user can easily identify the operation conditions of a branch into the network. The Streetlight Vision company app, for instance, monitors and anticipates the end of a lamp working life by supervising its state. Denardin et al. [2], Leccese and Leonowicz [5] and Long et al. [6] affirm that, besides supervising the operation condition of lighting points in real-time, the system was also able of monitoring the variables of lamp consumption.

Operation: operations such as switch on/off and control of lamp luminosity are resources met in the companies Central Software apps — Street intelligence Inc. [7], EpiSensor [8], Strategic Telemetry [9] and Streetlight Vision [4]. Such operations are made individually, meaning that only one node sensor is selected and the command is sent only to that device, previously specified. Streetlight Vision [4], Leccese and Leonowicz [5] and Long et al. [6] emphasize that the public lighting system can, according to the user need, set the unitary or group control of node sensor, it means that a determined command is applied to many nodes simultaneously, which enables a higher productivity to the system operator

Alarms: Long et al. [6] affirm the system management app installed in the control center must be able to bear alarms for abnormal operation conditions. The company Streetlight Vision [4] suggests that the resource of alarm manipulation such as its configuration be stored in a data base. Our purpose has a limitation, since there is no a notification mechanism to the user; due to that, reports or screen alarms must be consulted in order to check the occurrence of any alarm.

Remote operation: in the work of Chunguo et al. [10] the limitation on the using of ActiveX controls in Internet browsers, which is considered an unsafe solution as it enables external invasion to the computer through the network, can be cited. ActiveX is a technology that belonged to Microsoft that enables its browser Internet Explorer to show and dospose contents in HTML pages. The apps by Central Software — Streetlight intelligence Inc. [7] and Street Vision [4] dispose the complete system control by Internet, enabling the authorized users to operate it remotely. On the other hand, Zotos et al. [11] has the system conception management and monitoring by Internet.

Supportability: the functions available in a node sensor are distinct among manufacturers. In order for a management app to work fully and dospose the functionalities of devices to the user, it is necessary for the manufacturer to describe the functionalities in a common standard. The integration of new sensor node models enables the management app to supervise/control sensor nodes with different functionalities. Sensor node manufacturers can develop their equipment and integrate them in solutions without any need of additional modifications in the management app. This requirement is normally implemented through the description files, which describe all the device information to be integrated in the system.

Follow below all the requirements proposed for public lighting system operation.
                        
                           •
                           Remote operation: the user must operate the system through an Internet browser, named as remote operation app.

Alarms: the armed sensor nodes must be listed and positioned in maps. A burnout light is a typical alarm. The user will be able to configure the alarm visualization, hiding the unwanted alarms. Moreover, all the occurrences must be listed in report papers.

Operating state: the remote operation app must list the operating state of all lamps (switched on/off) and position each of them in the map. The user will be able to configure the operating state visualization. For instance, the user can filter out from the visualization all the switched on lamps so, by doing that, the only lamps shown, will be the switched on ones or vice versa. As a result, it is possible during the day light to detect which lighting points need intervention to be reset to their regular working.

Individual operating: when selecting a node sensor, it enables to supervise the values of all their parameters and configure them. An example of a configurable parameter is the node sensor operation; be it automatic or manual. In the automatic mode, the photocell relay controls when the lamp is switched on/off, in function of the presence of luminosity or not. However, in the manual mode, the user can specify a timetable for the lamps to be switched on/off.

Group operating: the group operating is a requirement to configure multiple sensor nodes simultaneously. A geometric figure (circle or rectangle) is drawn in the graphical interface and all the sensor nodes in the area will be configured. A typical application case is when it is wanted to switch on/off the lamps of a street/block. Without this requirement, the user would have to configure individually several street/block sensor nodes;

Consumption: measures and presents the consumption in a graphical format daily and monthly;

Supportability: in the fieldbus protocols scenery, the capability of bearing different models from different sensor node manufactures (interoperability) is a real necessity. Analogically, this requirement needs to be incorporated to the system. The solution would be the manufacture to describe all the node sensor features in a description file. The supportability would happen when the description file is transferred and integrated to the system, which would pass all the information needed from the device by offline mode.

Standardization: correlated works in their majority, do not apply standards for supervision, which would guarantee the interoperability among the apps and systems. However, the authors Atici et al. [12] present a centralized architecture in the server OPC-DA (OPC Data Access) [13,14], multiple segment of sensor nodes communicate to the server OPC-DA, disposing their variables. The server OPC-DA facilitates the sensor nodes supervision maintenance. Moreover, another advantage of adopting an OPC-DA server is the ability of expanding the system, so multiple servers can be managed by only one management app, centralizing the system management process. The standardization of communication between the management app and the communication controller permits architecture: open, distributed, easy to maintain and expansible.

Information exchanged between the app and the OPC-DA is in binary format, which limits the application in the Internet because the firewalls restrict this kind of information. Therefore, posteriorly, the following standards were created OPC XML-DA (OPC XML Data Access) and the CyberOPC [15], especially gotten for the data exchange remotely, in unsafe environments.

Torrisi [16] presents a comparative study between OPC XML-DA and CyberOPC. He concludes that the CyberOPC provides similar services (reading and writing) to OPC XML-DA. However, the CyberOPC offers performance superior in data exchanges through Internet by using a safer method due to the SSL protocol (Secure Socket Layer).

One function not mentioned in the previous correlate works, is the ability of grouping the node sensor variables. The lamp consumption can be classified as a process variable, while variables that indicate the lamp operation conditions are grouped as alarm. This type of approach enables the management app to be able to separate alarms and values of process in distinctive view.

Another ability not related previously, is the support to multiple languages by disposing the sensor functionalities in many different languages to enable the operators to select the native language or the most convenient.

In this section, architectural models are called as an interconnection of computers, communication networks, sensors and computers to provide the supervision and operation of a public lighting system.

In the works of Atici et al. [12], Yao et al. [17], Long et al. [6], Chunguo et al. [10], Chen and Liu [18], Iordache et al. [19] and Hong et al. [20], it is seen a standard, referred to the conception of the architecture. The sensor nodes are grouped by communication controllers that make a cell. Moreover, many cells are made in order to, geographically an area, be monitored and controlled. The communication controller coordinates the communication between the cell sensor nodes by a management app.

The cell is made by sensor nodes interconnected in network and they are grouped into two models, one by segment and another by region. The model by segment is used when the physical environment of communication between the sensors and communication controller is cabled. Typically the communication controller is installed into the transformation unit. Some examples of these models are illustrated by Yao et al. [18], Atici et al. [13], Chunguo et al. [10], Chen and Liu [19], Iordache et al. [20] and Hong et al. [12].


                     Fig. 1
                      shows the architectural model by cell typical, in which the cell is made by sensor nodes and the communication controller. Each cell is connected to the computer in order to have a full coverage of a city.

The works elaborated by Atici et al. [12], Yao et al. [17], Long et al. [6], Chunguo et al. [10], Chen and Liu [18], Iordache et al. [19], Mendalka et al. [21] and Hong et al. [20] present distributed solutions in which the communication controller and the management app can communicate to themselves remotely through the Ethernet net. Fig. 2
                      presents a typical solution found in the literature for distribution of communication controllers through the Ethernet network.

The architecture proposed in this work is based on the cell model, applying the architectural model by region. The architectural model by segment is not applicable to this one, because the work adopts a sensor wireless network. The studies about architectural models present the possibility of distribution of communication controllers via Ethernet network, which permits the distribution of communication controllers geographically in a city.

In the architecture proposed, the cell has the function of providing the access to supervision and writing for a determined number of sensor nodes. Besides, it can be accessed remotely through Ethernet network by the management apps, allowing the cell physical distribution. Although the communication controllers are not directly coupled to the Ethernet network but to the computer, that allows the remote communication.

A cell is made by a cabin, an industrial computer, a GPS antenna, a communication controller and sensor nodes. Countless cells can be made for a full coverage of a city.

The industrial computer is installed in the cabin and it is connected to a communication controller, which communicates by using a serial protocol, while the communication controller exchanges information with the sensor nodes by the wireless network.

The communication controller still has the ability to read periodically the Global Positioning System time, aiming to maintain itself as the base unique of time among many cells. Therefore, the alarms that occurred in different cells can be ordained. The management apps (configuration/topology, remote operation server and monitoring) are installed in computers for operation and they communicate with the cells by the Ethernet network.

The remote access is made by the Internet browser from the computer app of remote operation to the remote operation server. The Google map library [22] is adopted to plot the lighting points in the maps.

The Fig. 3
                      presents the system architecture and it illustrates cell 01, the one responsible to group some sensors. Besides, it shows the ability to add new cells until all the lighting points in the city are mapped.


                     Fig. 4
                      presents a diagram of the system components. The remote operation system includes the server HTTP, the configuration of topology sensors and the CyberOPC client that is an OPC server. The cell includes the CyberOPC Server, which is responsible for reading the device variable values.

Communication works as the following: the management apps available in the operation computer require supervision data through the Ethernet network to the industrial computer installed in the cabin. The request is accepted by the cabin computer and retransmitted to the communication controller connected to a serial port.

The communication controller, in its turn, transmits through wireless network the request, identifying it. The sensor nodes receive the notifications then process them and transmit the response through wireless network to the controller. The response is received by the controller through wireless network and retransmitted through serial port to the computer that sends the response to the operation computer through the Ethernet network.

The supportability is applied by adopting for each node sensor, an XML file, named as node sensor description file or description file. In the file, the node sensor manufacture, describes the collection of parameters, variable groups of alarms/process/operation and optionally IHM groups. Fig. 5
                         presents the description file format.

The sensor node description files have some features and services that are consumed by the remote operation server, CyberOPC and configuration/topology app. In this scenery, each application must know how the file is structured and codify the description file manipulation individually. In order to avoid each app to manipulate the description files in different ways, the XMLLibrary component centralizes the file manipulation and disposes services to the management apps. Thus, the management apps do not need to manipulate the description files directly. The management apps apply the component without knowing how the file is structured and must know only how the XMLLibrary services are arranged.

In this paper, XML is used to describe the features and functionalities of a sensor node in order to achieve interoperability across different sensors to public street lighting. This description is used by traditional systems in the area of industry, Foundation Fieldbus protocols, for example. In this case, XML would be included in the monitoring system in order to integrate support for the physical device. For this use and for public lighting, no other studies in the literature were found.

For more information of Foundation Fieldbus Device Description, the following work can be consulted [23].

The remote operation works as the following: with the operation server app activated, the client opens the Internet browser and requires a HTML page stored in the remote operation server. As response, the server sends the page to the client browser, which is provided to the client with the system features.

The remote operation server has a HTTP server. The client HTTP (Internet browser) requires the page to the remote operation server, which sends the page as a response to the client.

Pages in HTML are static, once required to the server, they are processed and their content is not updated anymore. If any change happens to the operation state of the sensor nodes, the user will not be notified. For the notification process to be automatized, the HTML pages incorporate JavaScript codes, these ones enable the HTTP requests to be effectuated into the server. Object like XMLHttpRequest is created and configured in JavaScript and its function is to send HTTP requests periodically to the remote operation sensor.

Services are available in the remote operation server for consumption of the object type XMLHttpRequest. The server formats the response in the XML standard in order to indicate the changes. The JavaScript code manipulates the response sent by the server and updates the graphical interface.

This way, once required to the HTML page to the remote operation server, the user will be notified about the changes occurred in the sensor nodes. Fig. 6
                         illustrates the working of the update verification process, based on the HTTP requests. The signs that started in a remote operation app towards the server are the requests. The signs on the opposite direction are responses in the XML format coming from the server to the app. The process is ended when the app is closed.

Once the remote operation server is started, the monitoring services are started automatically. There are three monitoring services: alarms, operation and consumption. Clarifying it, when the monitoring service is started the following steps must be followed: the reading of the topology file, in order to identify the cells and their sensors; and to perform the reading of the description files in order to obtain the parameter equivalent to each type of monitoring service (and so to perform the monitoring of parameters in the CYberOPC server).

The alarm monitoring service maintains an information base with the armed sensors and to keep it updated, reading requisition is performed periodically to the CyberOPC server. This base is shared with the remote operation server. The remote operation server uses it when receiving a requisition of service from the page for checking if there was any change in the armed sensor nodes. Another functionality for the alarm monitoring service is to store the occurrences of alarm in XML files, enabling to generate reports about it.

The monitoring service of the operation state is an analog to the alarms service. The states of node sensor operation are held in an information base by the periodical reading and shared with the remote operation server that, in its turn, uses it to maintain the graphical interface updated. Moreover, the service stores changes of the operation states in XML files in order to generate the reports.

The consumption monitoring service is programmed to perform the power consumption reading of all the sensor nodes. The reading is scaled along the day, with a predetermined nine timetable: 09:00am, 10:00am, 11:00am, 12:00pm, 13:00pm, 14:00pm, 15:00pm, 16:00pm, and 17:00pm.

This section was divided into five (5) subsections, one for each experiment. Subsection A is designed for the initial evaluation, it is called the pilot test; subsection B presented an analysis about the system behavior considering latency and the non-delivered messages; subsection C detailed the start-up performance when to obtain the initial operation state and alarms of a cell; in subsection D the command in individual operation is analyzed; finally, in subsection E an experiment is described in order to validate the architecture in cells.

A pilot test was configured for the result analysis. Two computers were used: in the first one a system with all infrastructures was installed in order to establish the communication with the communication controller installed in the serial port, the remote operation system and public lighting supervision. In order to get integration, an XML description file was developed for the node sensor model and a topology file compounded only by a cell and three sensor nodes.

The node sensor developed has just two parameters: LampOperation and LampConsumption. The first is used for switching on/off a LED, simulating a lamp and the second is used for indicating the node sensor consumption.

In the second computer, a device was installed “MC1322 UBS Dongle” by FreeScale company coupled to the USB port, with ability of capturing messages that traffic in the wireless network, between the communication controller and the sensor nodes. The software Wireshark [24] is also installed in order to visualize the captures performed.

The capture device aims to assure that a command required by the system is transmitted to the node sensor destination through the communication controller, and also if the node sensor responds to the request command.

The address adopted for the communication controller was “1”. The sensor nodes were configured with the address “5”, “6” and “7”. Fig. 7
                         presents the experiment configuration on integration test of communication between the remote operation system and the lighting public supervision and the communication controller and the real sensor nodes.

Activating the system some reading requirements are sent to the communication controller and, this in turn, retransmits these requirements to the wireless sensor network. This moment, it was possible to visualize (Fig. 8
                        ) through the captures the controller request in the direction of the destination node sensor with the address “7”.

In Fig. 8 it is possible to visualize in the first line, the package sent by the controller with the address “1” to the network (the address is available in the third byte in column Source). Then, the node sensor with the address “7” provides the response and, successive requests are made by the controller to some of the node sensor. In some cases it is possible to visualize two followed responses for one request only, which is the result of retransmissions of a node.

The final integration product is shown in Fig. 9
                        , with the sensor nodes positioned in map and the indication of their operation state in a list format. It is still possible to switch on/off the LED that simulated the lamp, through the individual operation. The requirement of consumption measuring was not tested, due to such feature was not fully implemented by the node sensor.

The latency requirements and the reading or writing non-delivery are present in the public lighting systems that use non-deterministic networks. These tests aim to ascertain the behavior of operation and supervision system proposed before such conditions and besides detecting possible problems in the proposed requirements.

In order to perform the proposed tests, the result in simulation gotten by Pantoni and Brandão [25] with a network of 1000 sensor nodes is adopted as parameters of utilization. The results presented were 80% (worst case) for the delivery rate and 35s in average for the latency for divergent traffic. The divergent traffic is defined as reading and writing requirements by the system with destination to sensor nodes. The latency in convergent traffic which is defined as notification delivery of the sensor nodes to the communication controller is still presented. The average delay presented, in the worst case, of simultaneous transmissions, is in the maximum 110s with 200 sensor nodes transmitting simultaneously and 70s for non-simultaneous transmissions. The protocol GGPSR adopts routing strategies in order to deliver the messages in an independent way.

According to the study [25], the latency value meets the requirements specified in RFC 5548 [1] in the order of tens of seconds and the delivery rate is satisfactory when compared to correlate studies in the literature.

The latency simulation of the convergent traffic is specific with protocol and this work does not aim to detail all the GGPSR protocol features. However, it is notorious that the latency of the convergent traffic must be tested in order to fully validate the solution and for such a thing, it is suggested that a future integration between the presented simulator and the GGPSR protocol simulator.

For the experiment, the simulator was configured with 1018 sensor nodes, representing a cell, with a delivery rate of 80% of latency of 35s.

The test indicates the initial reading performance of operation and alarms conditions of a cell. Two variables are correlated in this test: the number of alarm/operation parameters and also if the start-up occurs in the period of sunset or dawn.

The number of alarm and operation parameters has direct connection to the performance, the bigger the number of parameters is, the bigger will be the number of reading requirements to the sensor nodes.

In case if the start-up occurs at sunset or dawn, the performance is optimized because the alarms are automatically sent in these periods, indicating the alarm/operation state. An alarm coming from the simulator in direction to CyberOPC indicates a single package, the information about the operation state and also the alarm conditions, saving the need of sending the reading commands to ascertain such conditions. Exemplifying it, a portion of lamps will be on in a determined time and the alarms generated for each correspondent node sensor. As the alarms are received, the CyberOPC will not require the readings to the sensor nodes that had already sent such alarm notification, which optimizes the requirement flow.

Besides the performance, three results were measured. The number of requirements sent, the number of package received and the number of requirements refused. In total, about four tests were performed; Table 1
                         summarizes the experiment with the tests sceneries and their results.

In the first and second tests, the node sensor was described with two parameters of alarm and one of operation, totalizing three parameters of reading per sensor. The difference between both tests is that, the first one was performed apart from the periods of sunset or dawn. Comparing both tests, the result gotten in the second performance was superior to the first, as the start-up in the second test was approximately 45min and in the first was about 1h and 30min. It was noticed that in the second test, the number of requirements sent is lower than in the first, which indicates the influence of alarm notifications in the process, as for each notification gotten, three readings are refused.

In tests three and four, the sensor nodes included just an alarm parameter. In the case of test three specifically, it was performed apart from the periods of sunset and dawn. Test three was 7 (seven) minutes slower when compared to test 4, which indicates the influence of the alarm notifications in the tests.

The tests performance is illustrated graphically in Fig. 10
                        .

It was concluded that the start-up performance is related directly to the number of alarm and operation parameters described in the node sensor, when compared to other group of tests 1 and two to the group of tests 3 and 4, having the second group of tests the most efficient performance. In order to meet a better performance, a new communication protocol service between the communication controller and the CyberOPC is suggested. This new service would send only one requirement, so it would have as response an alarm notification that would be sent with all the information about alarm and/or operation state of the node sensor. Thus, in tests 1 and 2, just a command per node would be sent not three, decreasing drastically the number of requirements to the sensor nodes.

When the start-up occurs in the period of sunset or dawn, the performance results are better, as the alarm notifications make the reading process more effective. However, the performance results gotten are still seen as vital importance for the system operators, because in case of reestablishment of a cell, the time must be known.

The individual operation test was performed to ascertain the performance of request under latency conditions and delivery rate, this test being subdivided into two, supervision and writing. Table 2
                         summarizes the results of the test performed.

In the supervision test, when a node sensor was selected the total time was taken, up to the values of all the 13 parameters were filled totally in the graphical interface, obtaining the average of 4 (four) minutes and 50 (fifty) seconds in four samples.

The remote operation app presents during the supervision process the progress, indicating the values read and the ones not-read. Fig. 11
                         highlights two parameters (indexes 6 and 7) that are still not read and the other ones already available to the users, having the indication of “Reading…” used to mark the non-read parameters.

In the writing test, when submitted to the writing of a value to the parameter, the total time was taken until the new value was updated in the graphical interface, obtaining an average of 2min and 47s.

It was concluded that the system behaves suitably before the latency and do not deliver the packages when processing the reading, because independently from the time the requirements are performed by the simulator, the information is provided to the operator.

In order to have the architectural validation based on cells, the experiment was configured according to Fig. 12
                        , using three computers. Two cells in simulation were created, one with 976 sensor nodes and the other with 255 sensor nodes, totalizing 1231 sensor nodes, each cell being represented by a computer. In the third computer, the remote operation server was installed and activated. Thus, the supervision and operation of system were provided to the operators. For a division into two cells, one topology with two communication controllers was established.

After configuring the experiment and activating the cells, the first test was to ascertain the communication between the remote operation server and the cells. For this, a usual CyberOPC client available in the server was used for sending the reading commands to cells.


                        Fig. 13
                         presents the client CyberOPC configured in order to have access to the value of a parameter in cell 1, so:
                           
                              -
                              (1): computer address for Cell 1;

(2): certificate used for authentication between client and CyberOPC server;

(3): connection port for CyberOPC server;

(4): parameter that will have the reading requested;

“IPGateway.2368.11”, being the IPGateway the connector in which will be the reading “2368” driven: node sensor identifier;

“11”: parameter index;

(5) response to the reading command.

The response is compounded by the fields ReceiveTime (time the CyberOPC server received the request), ReplyTime (time the server replied the request), ServerState (indicates the operation state of CyberOPC server), ItemID (node sensor Identifier and parameter compound separated by a point), TimeStamp (indicates the last parameter update time) and Value (parameter value).

Finally, Fig. 14
                         presents the remote operation app screen, which is highlighting the two cells in the geographic map.

@&#CONCLUSION@&#

Characteristics of academic proposals and industry solutions were researched and analyzed, so it was concluded that the solution proposed attempt all requirements for operation and supervision of a lighting public system. In all the proposals studied, any of them present the possibility for the sensor node integration by using the file XML for the sensor node supportability, which makes a contribution of this work. Besides that, the architecture was designed to street lighting application with CyberOPC Server and integration capability of the sensors to the system described by XML file (as done in fieldbus systems).

The simulator enabled the creation of scenery near to real public lighting operation system scenery, in which it was possible to measure the performance and ascertain the points for improvement.

Best results can be obtained with minor amounts of sensor nodes in a cell due to CyberOPC bottleneck. Thus, a greater number of cells can be designed to cover a large area, reaching up to serve a city or a country.

Thus, the system presented is viable, as the results validate the methodology and the architecture proposed.

@&#REFERENCES@&#

