@&#MAIN-TITLE@&#Classification of gene expression data: A hubness-aware semi-supervised approach

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A semi-supervised hubness-aware classifier is proposed.


                        
                        
                           
                           The classifier is evaluated on publicly available real gene expression data.


                        
                        
                           
                           We made the implementation of hubness-aware machine learning techniques available in the PyHubs software package.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Gene expression

Machine learning

Semi-supervised classification

High dimensionality

@&#ABSTRACT@&#


               
               
                  Background and objective
                  Classification of gene expression data is the common denominator of various biomedical recognition tasks. However, obtaining class labels for large training samples may be difficult or even impossible in many cases. Therefore, semi-supervised classification techniques are required as semi-supervised classifiers take advantage of unlabeled data.
               
               
                  Methods
                  Gene expression data is high-dimensional which gives rise to the phenomena known under the umbrella of the curse of dimensionality, one of its recently explored aspects being the presence of hubs or hubness for short. Therefore, hubness-aware classifiers have been developed recently, such as Naive Hubness-Bayesian k-Nearest Neighbor (NHBNN). In this paper, we propose a semi-supervised extension of NHBNN which follows the self-training schema. As one of the core components of self-training is the certainty score, we propose a new hubness-aware certainty score.
               
               
                  Results
                  We performed experiments on publicly available gene expression data. These experiments show that the proposed classifier outperforms its competitors. We investigated the impact of each of the components (classification algorithm, semi-supervised technique, hubness-aware certainty score) separately and showed that each of these components are relevant to the performance of the proposed approach.
               
               
                  Conclusions
                  Our results imply that our approach may increase classification accuracy and reduce computational costs (i.e., runtime). Based on the promising results presented in the paper, we envision that hubness-aware techniques will be used in various other biomedical machine learning tasks. In order to accelerate this process, we made an implementation of hubness-aware machine learning techniques publicly available in the PyHubs software package (http://www.biointelligence.hu/pyhubs) implemented in Python, one of the most popular programming languages of data science.
               
            

the set of k-nearest neighbors of x.

probability that x belongs to class C given its nearest neighbors

the probability of the event that x appears as one of the k-nearest neighbors of any labeled training instance belonging to class C
                     

the prior probability of the event that an instance belongs to class C
                     

how many times x occurs as one of the k-nearest neighbors of labeled training instances belonging to class C
                     

how many times x occurs as one of the k-nearest neighbors of other instances when considering 
                           
                              D
                              lab
                           
                           ∪
                           {
                           x
                           }
                        
                     

@&#INTRODUCTION@&#

Various tissues are characterized by different gene expression patterns. Additionally, a number of diseases and disease subtypes may be associated with characteristic gene expression patterns. Therefore, recognition tasks related to gene expression data may contribute to the diagnosis of various diseases such as colon cancer, lymphoma, lung cancer and subtypes of breast cancer [9]. Due to the large amount of data (e.g., even if we consider just a single patient, expression levels of thousands of genes may be measured), such recognition tasks are typically solved by computers, and state-of-the-art solutions are based on machine learning.

In case of supervised machine learning, a previously collected dataset (e.g., gene expression levels measured for a set of patients) together with evidence or indication (e.g., the presence, absence or subtype of a particular disease for each patient) is used to induce a decision model, called classifier. Once the classifier is induced, it will be able to solve the recognition task for new data instances (e.g., the classifier will be able to recognize the subtype of cancer for new patients). With training the classifier we refer to the induction of the model, while the data used to induce the model is called training data. If the data is associated with evidence, it is called labeled data, e.g., a labeled dataset may contain gene expression levels together with the information describing which patient has which subtype of cancer, in contrast, if only the gene expression levels are available without knowing the subtype or presence of the disease, the dataset is unlabeled. The value of the evidence is called label, e.g., if a patient has estrogen receptor positive (ER+) subtype of breast cancer, we say its label is “ER+” (at the technical level, labels are usually coded by integer numbers, such as 0 for “ER+” and 1 for “ER−”).

The classification task is challenging for several reasons. Usually, the expression levels of several thousands of genes are measured, therefore, the data is high-dimensional which gives rise to the phenomena known under the umbrella of the curse of dimensionality 
                     [3]. While well-studied aspects of the curse are the sparsity and distance concentration, see e.g., [19], a recently explored aspect of the curse is the presence of hubs [14], i.e., instances that are similar to surprisingly many other instances. According to recent observations, the presence of hubs characterizes gene expression datasets [10,15]. A hub is said to be bad if its class label differs from the class labels of those instances that have this hub as one of their k-nearest neighbors. In the context of k-nearest neighbor classification, bad hubs were shown to be responsible for a surprisingly large portion of the total classification error.

Recently, algorithms have been developed under the umbrella of hubness-aware data mining, see e.g., [5,12,13,15,22,25,26,20,21]. These algorithms try to recognize bad hubs and reduce their influence on classifications of unlabeled instances.

It may be expensive (or even impossible in case of rare diseases) to collect large amount of labeled gene expression data, therefore, we have to account for the fact that only relatively few labeled instances are available which may not reflect the structure of the classes well enough. Therefore, while training the classifier, in addition to learning from labeled data, the classifier should be able to use unlabeled data too in order to discover the structure of the classes.

In this paper we introduce a semi-supervised hubness-aware classifier, i.e., a classifier that uses both labeled and unlabeled data for training. In particular, our approach is an extension of the Naive Hubness Bayesian k-Nearest Neighbor, or NHBNN for short [23], which is one of the most promising hubness-aware classifiers. As we will show, straightforward incorporation of semi-supervised classification techniques with NHBNN leads to suboptimal results, therefore, we develop a hubness-aware inductive semi-supervised classification scheme. We propose to use our classifier for recognition tasks related to gene expression data. To our best knowledge, this paper is the first that studies hubness-aware semi-supervised classification of gene expression data.

@&#METHODS@&#

Semi-supervised classification, often in a general data mining context, i.e., without special focus on the analysis of genetic data, has been studied intensively, see e.g., [6,11] and the references therein for related works on semi-supervised classification. In order to ensure that our study is self-contained, we begin this section by reviewing the Naive Hubness Bayesian k-Nearest Neighbor (NHBNN) classifier [23] and the self-training semi-supervised learning technique in Sections 2.1 and 2.2. The presentation of NHBNN and self-training is based on [21,11]. Subsequently, we describe our proposed semi-supervised approach in Section 2.3, which is followed by the methods used for the experimental evaluation in Section 2.4.

We aim at classifying instance x
                        *, i.e., we want to determine its unknown class label y
                        *. We use 
                           
                              N
                              k
                           
                           (
                           
                              x
                              *
                           
                           )
                         to denote the set of k-nearest neighbors of x
                        *. For each class C, Naive Hubness Bayesian k-Nearest Neighbor (NHBNN) estimates 
                           P
                           (
                           
                              y
                              *
                           
                           =
                           C
                           |
                           
                              N
                              k
                           
                           (
                           
                              x
                              *
                           
                           )
                           )
                        , i.e., the probability that x
                        * belongs to class C given its nearest neighbors. Subsequently, NHBNN selects the class with highest probability.

NHBNN follows a Bayesian approach to assess 
                           P
                           (
                           
                              y
                              *
                           
                           =
                           C
                           |
                           
                              N
                              k
                           
                           (
                           
                              x
                              *
                           
                           )
                           )
                        . For each labeled training instance x, one can estimate the probability of the event that x appears as one of the k-nearest neighbors of any labeled training instance belonging to class C. This probability is denoted by 
                           P
                           (
                           x
                           ∈
                           
                              N
                              k
                           
                           |
                           C
                           )
                        . While calculating nearest neighbors, throughout this paper, an instance x is never treated as the nearest neighbor of itself, i.e., 
                           x
                           ∉
                           
                              N
                              k
                           
                           (
                           x
                           )
                        .

Assuming conditional independence between the nearest neighbors given the class, 
                           P
                           (
                           
                              y
                              *
                           
                           =
                           C
                           |
                           
                              N
                              k
                           
                           (
                           
                              x
                              *
                           
                           )
                           )
                         can be assessed as follows:
                           
                              (1)
                              
                                 P
                                 (
                                 
                                    y
                                    *
                                 
                                 =
                                 C
                                 |
                                 
                                    N
                                    k
                                 
                                 (
                                 
                                    x
                                    *
                                 
                                 )
                                 )
                                 ∝
                                 P
                                 (
                                 C
                                 )
                                 
                                    ∏
                                    
                                       
                                          x
                                          i
                                       
                                       ∈
                                       
                                          N
                                          k
                                       
                                       (
                                       
                                          x
                                          *
                                       
                                       )
                                    
                                    
                                 
                                 
                                    P
                                    (
                                    
                                       x
                                       i
                                    
                                    ∈
                                    
                                       N
                                       k
                                    
                                    |
                                    C
                                    )
                                 
                                 .
                              
                           
                        where P(C) denotes the prior probability of the event that an instance belongs to class C. From the labeled training data, P(C) can be estimated as
                           
                              (2)
                              
                                 P
                                 (
                                 C
                                 )
                                 ≈
                                 
                                    
                                       |
                                       
                                          D
                                          C
                                          lab
                                       
                                       |
                                    
                                    
                                       |
                                       
                                          D
                                          lab
                                       
                                       |
                                    
                                 
                                 ,
                              
                           
                        where 
                           |
                           
                              D
                              C
                              lab
                           
                           |
                         denotes the number of labeled training instances belonging to class C and 
                           |
                           
                              D
                              lab
                           
                           |
                         is the total number of labeled training instances. The maximum likelihood estimate of 
                           P
                           (
                           
                              x
                              i
                           
                           ∈
                           
                              N
                              k
                           
                           |
                           C
                           )
                         is the fraction
                           
                              (3)
                              
                                 P
                                 (
                                 
                                    x
                                    i
                                 
                                 ∈
                                 
                                    N
                                    k
                                 
                                 |
                                 C
                                 )
                                 ≈
                                 
                                    
                                       
                                          N
                                          
                                             k
                                             ,
                                             C
                                          
                                       
                                       (
                                       
                                          x
                                          i
                                       
                                       )
                                    
                                    
                                       |
                                       
                                          D
                                          C
                                          lab
                                       
                                       |
                                    
                                 
                                 ,
                              
                           
                        where N
                        
                           k,C
                        (x
                        
                           i
                        ) denotes the (k, C)-occurrence of an instance x
                        
                           i
                        , i.e., how many times x
                        
                           i
                         occurs as one of the k-nearest neighbors of labeled training instances belonging to class C.


                        
                           Example
                           
                              Fig. 1
                               shows a simple two-dimensional example, i.e., instances, denoted from now on as x
                              1, …, x
                              11 in text, correspond to points of the plane. In this example, we use k
                              =1. In Fig. 1, a directed edge points from each labeled training instance to its first nearest neighbor among the labeled training instances. In other words: the nearest neighbor relationships shown in the Fig. 1 are calculated solely on the labeled training data.

Out of the ten labeled training instances, six belong to the class of circles (C
                        1) and four belong to the class of rectangles (C
                        2). Thus: 
                           |
                           
                              D
                              
                                 
                                    C
                                    1
                                 
                              
                              lab
                           
                           |
                           =
                           6
                        , 
                           |
                           
                              D
                              
                                 
                                    C
                                    2
                                 
                              
                              lab
                           
                           |
                           =
                           4
                        , P(C
                        1)=0.6 and P(C
                        2)=0.4. Next, we calculate N
                        
                           k,C
                        (x
                        
                           i
                        ) for both classes and classify x
                        11 using its first nearest neighbor, i.e., x
                        6. In particular, Eq. (3) leads to
                           
                              
                                 P
                                 (
                                 
                                    x
                                    6
                                 
                                 ∈
                                 
                                    N
                                    1
                                 
                                 |
                                 
                                    C
                                    1
                                 
                                 )
                                 ≈
                                 
                                    
                                       
                                          N
                                          
                                             1
                                             ,
                                             
                                                C
                                                1
                                             
                                          
                                       
                                       (
                                       
                                          x
                                          6
                                       
                                       )
                                    
                                    
                                       |
                                       
                                          D
                                          
                                             
                                                C
                                                1
                                             
                                          
                                          lab
                                       
                                       |
                                    
                                 
                                 =
                                 
                                    0
                                    6
                                 
                                 =
                                 0
                              
                           
                        and
                           
                              
                                 P
                                 (
                                 
                                    x
                                    6
                                 
                                 ∈
                                 
                                    N
                                    1
                                 
                                 |
                                 
                                    C
                                    2
                                 
                                 )
                                 ≈
                                 
                                    
                                       
                                          N
                                          
                                             1
                                             ,
                                             
                                                C
                                                2
                                             
                                          
                                       
                                       (
                                       
                                          x
                                          6
                                       
                                       )
                                    
                                    
                                       |
                                       
                                          D
                                          
                                             
                                                C
                                                2
                                             
                                          
                                          lab
                                       
                                       |
                                    
                                 
                                 =
                                 
                                    2
                                    4
                                 
                                 =
                                 0.5
                                 .
                              
                           
                        According to Eq. (1) we calculate
                           
                              
                                 P
                                 (
                                 
                                    y
                                    11
                                 
                                 =
                                 
                                    C
                                    1
                                 
                                 |
                                 
                                    N
                                    2
                                 
                                 (
                                 
                                    x
                                    11
                                 
                                 )
                                 )
                                 ∝
                                 0.6
                                 ×
                                 0
                                 =
                                 0
                              
                           
                        and
                           
                              
                                 P
                                 (
                                 
                                    y
                                    11
                                 
                                 =
                                 
                                    C
                                    2
                                 
                                 |
                                 
                                    N
                                    2
                                 
                                 (
                                 
                                    x
                                    11
                                 
                                 )
                                 )
                                 ∝
                                 0.4
                                 ×
                                 0.5
                                 =
                                 0.2
                                 .
                              
                           
                        As 
                           P
                           (
                           
                              y
                              11
                           
                           =
                           
                              C
                              2
                           
                           |
                           
                              N
                              2
                           
                           (
                           
                              x
                              11
                           
                           )
                           )
                           >
                           P
                           (
                           
                              y
                              11
                           
                           =
                           
                              C
                              1
                           
                           |
                           
                              N
                              2
                           
                           (
                           
                              x
                              11
                           
                           )
                           )
                        , x
                        11 will be classified as a rectangle.

The previous example also illustrates that estimating 
                           P
                           (
                           
                              x
                              i
                           
                           ∈
                           
                              N
                              k
                           
                           |
                           C
                           )
                         according to (3) may simply lead to zero probabilities. In order to avoid this, we can use a simple Laplace-estimate for 
                           P
                           (
                           
                              x
                              i
                           
                           ∈
                           
                              N
                              k
                           
                           |
                           C
                           )
                         as follows:
                           
                              (4)
                              
                                 P
                                 (
                                 
                                    x
                                    i
                                 
                                 ∈
                                 
                                    N
                                    k
                                 
                                 |
                                 C
                                 )
                                 ≈
                                 
                                    
                                       
                                          N
                                          
                                             k
                                             ,
                                             C
                                          
                                       
                                       (
                                       
                                          x
                                          i
                                       
                                       )
                                       +
                                       m
                                    
                                    
                                       |
                                       
                                          D
                                          C
                                          lab
                                       
                                       |
                                       +
                                       mq
                                    
                                 
                                 ,
                              
                           
                        where m
                        >0 and q denotes the number of classes. Informally, this estimate can be interpreted as follows: we consider m additional pseudo-instances from each class and we assume that x
                        
                           i
                         appears as one of the k-nearest neighbors of the pseudo-instances from class C. We use m
                        =1 in our experiments.

Even though (k, C)-occurrences are highly correlated, as shown in [21,23], NHBNN offers improvement over the basic kNN. This is in accordance with other results from the literature that state that Naive Bayes can deliver good results even in cases with high independence assumption violation [16].

Self-training is one of the most commonly used semi-supervised algorithms. Self-training is a wrapper method around a supervised classifier, i.e., one may use self-training to enhance various classifiers. To apply self-training, for each instance x
                        * to be classified, besides its predicted class label, the classifier must be able to output a certainty score, i.e., an estimation of how likely the predicted class label is correct.

Self-training is an iterative process during which the set of labeled instances is grown until all the instances become labeled. Let L
                        
                           t
                         denote the set of labeled instances in the t-th iteration (t
                        ≥0) while U
                        
                           t
                         shall denote the set of unlabeled instances in the t-th iteration. L
                        0 denotes the instances that are labeled initially, i.e., the labeled training data, while U
                        0 denotes the set of initially unlabeled instances. In each iteration of self-training, the base classifier is trained on the labeled set L
                        
                           t
                        . Then, the base classifier is used to classify the unlabeled instances. Finally, the instance with highest certainty score is selected. This instance, together with its predicted label 
                           
                              y
                              ˆ
                           
                        , is added to the set of labeled instances in order to construct L
                        
                           t+1 the set of labeled instances in the next iteration. We refer to [11] for the pseudocode and an illustration of the self-training algorithm.

If an unlabeled instance is classified incorrectly and this instance is added to the training data of the subsequent iterations, this may cause a chain of classification errors. Therefore, as noted in [8], it may be worth to stop self-training after a moderate number of iterations and use the resulting model to label all the remaining unlabeled instances.

In order to allow NHBNN to be used in self-training mode, we only need to define an appropriate certainty score. A straightforward certainty score may be based on the probability estimates as follows:
                           
                              (5)
                              
                                 certainty
                                 (
                                 
                                    x
                                    *
                                 
                                 )
                                 =
                                 
                                    
                                       P
                                       (
                                       
                                          C
                                          ′
                                       
                                       )
                                       
                                          ∏
                                          
                                             
                                                x
                                                i
                                             
                                             ∈
                                             
                                                N
                                                k
                                             
                                             (
                                             
                                                x
                                                *
                                             
                                             )
                                          
                                       
                                       
                                          P
                                          (
                                          
                                             x
                                             i
                                          
                                          ∈
                                          
                                             N
                                             k
                                          
                                          |
                                          
                                             C
                                             ′
                                          
                                          )
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             
                                                C
                                                j
                                             
                                             ∈
                                             C
                                          
                                       
                                       (
                                       P
                                       (
                                       
                                          C
                                          j
                                       
                                       )
                                       
                                          ∏
                                          
                                             
                                                x
                                                i
                                             
                                             ∈
                                             
                                                N
                                                k
                                             
                                             (
                                             
                                                x
                                                *
                                             
                                             )
                                          
                                       
                                       
                                          P
                                          (
                                          
                                             x
                                             i
                                          
                                          ∈
                                          
                                             N
                                             k
                                          
                                          |
                                          
                                             C
                                             j
                                          
                                          )
                                       
                                       )
                                    
                                 
                                 .
                              
                           
                        where C′ denotes the class with maximal estimated probability and 
                           C
                         denotes the set of all the classes. In the example shown in Fig. 1, the above certainty estimate gives
                           
                              
                                 
                                    0.2
                                    
                                       0
                                       +
                                       0.2
                                    
                                 
                                 =
                                 1
                              
                           
                        when classifying x
                        11.

However, this certainty estimate does not take into account that, usually, unlabeled instances appearing as nearest neighbors of many labeled instances can be classified more accurately as these instances are expected to be located “centrally” in the dataset, i.e., they appear in relatively dense regions of the data, see e.g., [25]. Therefore, we propose to use the following hubness-aware certainty score:
                           
                              (6)
                              
                                 hc
                                 (
                                 
                                    x
                                    *
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             (
                                             
                                                N
                                                k
                                                ′
                                             
                                             (
                                             
                                                x
                                                *
                                             
                                             )
                                             )
                                          
                                          α
                                       
                                       P
                                       (
                                       
                                          C
                                          ′
                                       
                                       )
                                       
                                          ∏
                                          
                                             
                                                x
                                                i
                                             
                                             ∈
                                             
                                                N
                                                k
                                             
                                             (
                                             
                                                x
                                                *
                                             
                                             )
                                          
                                       
                                       
                                          P
                                          (
                                          
                                             x
                                             i
                                          
                                          ∈
                                          
                                             N
                                             k
                                          
                                          |
                                          
                                             C
                                             ′
                                          
                                          )
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             
                                                C
                                                j
                                             
                                             ∈
                                             C
                                          
                                       
                                       (
                                       P
                                       (
                                       
                                          C
                                          j
                                       
                                       )
                                       
                                          ∏
                                          
                                             
                                                x
                                                i
                                             
                                             ∈
                                             
                                                N
                                                k
                                             
                                             (
                                             
                                                x
                                                *
                                             
                                             )
                                          
                                       
                                       
                                          P
                                          (
                                          
                                             x
                                             i
                                          
                                          ∈
                                          
                                             N
                                             k
                                          
                                          |
                                          
                                             C
                                             j
                                          
                                          )
                                       
                                       )
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              N
                              k
                              ′
                           
                           (
                           
                              x
                              *
                           
                           )
                         denotes how many times instance x
                        * appears as one of the k-nearest neighbors of other instances when considering the labeled training data 
                           
                              D
                              lab
                           
                         together with the unlabeled instance x
                        *, i.e., 
                           
                              D
                              lab
                           
                           ∪
                           {
                           
                              x
                              *
                           
                           }
                         and α is a hyper-parameter that controls the contribution of 
                           
                              N
                              k
                              ′
                           
                           (
                           
                              x
                              *
                           
                           )
                         to the value of certainty score. Please note that in order to calculate hc(x
                        *), we do not take other unlabeled instances into account.

According to our empirical results (see Section 3), the above certainty estimation works well with α
                        =0.2 in various domains ranging from breast cancer over colon cancer to lung cancer, therefore we use α
                        =0.2 by default. In the example shown in Fig. 1, the above certainty estimate gives
                           
                              
                                 
                                    
                                       
                                          2
                                          0.2
                                       
                                       ×
                                       0.2
                                    
                                    
                                       0
                                       +
                                       0.2
                                    
                                 
                                 ≈
                                 1.149
                              
                           
                        when classifying x
                        11, as x
                        11 appears as nearest neighbor of x
                        6 and x
                        9 when considering all the eleven instances for the computation of the nearest neighbor relationships (we assume that the distance between x
                        11 and x
                        9 is lower than the distance between x
                        9 and x
                        6, therefore, x
                        11 will be the nearest neighbor of x
                        9 when considering all the instances).


                        Datasets. We used publicly available gene expression data of breast cancer tissues [18], colon cancer tissues [1], and lung cancer tissues [2]. In these datasets, the expression levels of 7650, 6500 and 12,600 genes have been measured for 95, 62 and 203 patients respectively. The breast and colon cancer datasets had two classes, while the lung cancer dataset had five classes. In all the cases, classes correspond to subtypes of the disease or healthy tissues, see [9] for details. Out of the five classes of the lung cancer dataset, we ignored one because extraordinarily few instances (in particular, only six instances) belonged to that class.


                        Experimental protocol. We simulated two scenarios in which the available training data is not fully representative. In both scenarios, we selected a few instances as labeled training data while the remaining instances were considered as unlabeled data. The classifiers were evaluated on this unlabeled data. The true class labels of the “unlabeled instances” were given in the datasets, however, these true class labels were only used for evaluation, i.e., the labels of the “unlabeled instances” were unknown to the classifier.

In the first scenario, denoted as BreastCancer-B, ColonCancer-B and LungCancer-B we considered five randomly selected instances per class as labeled training data. This results in balanced distribution of classes in the labeled training data whereas the entire datasets were class-imbalanced [9].

In the second scenario, denoted as BreastCancer-I, ColonCancer-I and LungCancer-I, we considered an imbalanced sample as labeled training data. In order to ensure a challenging classification task in which the labeled training data is not representative, we selected 5 instances from the majority class and 10 instances from the minority class(es) as labeled training data. By default, we report results observed in the first (balanced) scenario, unless the opposite is stated explicitly.

We repeated all the experiments 100 times with 100 different initial random selections of the labeled training instances. We measured the performance of the classifiers in terms of classification accuracy, i.e., the fraction of correctly classified unlabeled instances, macro-averaged F1-score and Matthews correlation coefficient (MCC). Both F1-score and MCC were aggregated over the runs and classes. We report the average and standard deviation of the accuracies achieved in the aforementioned 100 runs. Additionally, we used binomial test as suggested in [17], in order to judge if the differences between our approach and the baselines are statistically significant. We performed the aforementioned binomial test in each of the 100 runs and considered the difference to be statistically significant if the median of the resulting p-values was less than 0.05.


                        Compared methods. We focus on the comparison of the following approaches:
                           
                              •
                              NHBNN-HS, i.e., NHBNN in self-training mode with the proposed hubness-aware certainty score according to Formula (6),

NHBNN-Simple, i.e., NHBNN in self-training mode with the straightforward certainty score according to Formula (5),


                                 k-NN in self-training mode with the proposed hubness-aware certainty score according to Formula (6),

NHBNN-SV, i.e., supervised NHBNN that uses only the labeled training instances but does not learn from the unlabeled data,

HFNN, i.e., Hubness-aware Fuzzy Nearest Neighbors, which is a hubness-aware supervised classifier, therefore, it uses only the labeled training instances but does not learn from the unlabeled data, see [26] for more details,

GRF, i.e., semi-supervised classification with Gaussian Random Fields
                                    1
                                 
                                 
                                    1
                                    We predicted class labels according to Formula (5) in [29]. We note that in order to avoid numerical problems, we set GRF's length scale hyperparameters σ
                                       
                                          d
                                        as 100-times the standard deviation of the d-th “component”, which is the expression level of the d-th gene, in our case. In case of the binary classification tasks, we used the “default” decision threshold of 0.5. In case of the non-binary classification tasks, LungCancer-B and LungCancer-I, we used the one-vs-rest protocol with GRF.
                                  based on [29].

In accordance with [22], by default, we used k
                        =5 for all the aforementioned variants of NHBNN and k-NN. Note, however, that we performed experiments with other k values as well and we observed similar trends. As distance measure, we used the Cosine distance with all the aforementioned classifiers.

For semi-supervised classifiers, by default, we report results for 20 iterations of self-training, i.e., 20 instances were labeled and added to the training set iteratively (one instance was labeled in each iteration) and then the model resulting after the 20th iteration was used to label all the remaining unlabeled instances.

@&#RESULTS@&#


                     Tables 1 and 2
                     
                      show the accuracy and F1-score of our approach and the baselines. Our approach, NHBNN-HS, consistently outperforms all the examined baselines on all the three datasets in both scenarios. The only exception is in case of BreastCancer-I when NHBNN-HS performs slightly worse than NHBNN-Simple, although the difference is not significant statistically. We note that even in this case, NHBNN-HS significantly outperforms k-NN, HFNN and GRF. We observed similar trends when we evaluated our approach and the baselines in terms of MCC. Fig. 2
                      shows that NHBNN-HS systematically outperforms its competitors for various k values, except for k
                     =1. The diagrams in the top of Fig. 3
                      show the accuracy of our approach as function of α, i.e., the exponent of 
                        
                           N
                           k
                           ′
                        
                        (
                        
                           x
                           *
                        
                        )
                      in Formula (6). As one can see, α
                     =0.2 can be considered as a reasonable “default” setting of α. The diagrams in the bottom of Fig. 3 show the accuracy of our approach, NHBNN-HS, and NHBNN-Simple as function of the number of self-training iterations. For comparison, the accuracy of the NHBNN-SV is shown as well. As one can see, NHBNN-HS systematically outperforms NHBNN-Simple for various settings of the number of iterations.

Additionally, we tried (a) supervised k-NN and (b) support vector machines from the Weka software package [28] with polynomial and RBF kernels with various settings of the complexity constant and the exponent of the polynomial kernel. According to our observations, self-training was not able to substantially improve the performance of SVMs overall: SVMs without self-training performed as well as (or sometimes even better than) SVMs with self-training. More importantly, NHBNN-HS was competitive to SVMs, too: for example on the Breast Cancer and Colon Cancer datasets, best performing SVMs achieved classification accuracy of 0.781 and 0.705 respectively.

Despite the fact that cancer is a multifactorial disease, and therefore it is inherently difficult, if not impossible, to determine the reason why an individual patient got the disease, we argue that the model built by NHBNN, i.e., the conditional probabilities describing how often characteristic patients (hubs) appear as nearest neighbors of patients from different classes, may be more interpretable to human experts than the model built by SVMs. Regarding supervised k-NN, we note that NHBNN-HS outperformed supervised k-NN as well which is in accordance with the previous results.

@&#DISCUSSION@&#

As one can see from Table 1, both the algorithm and the certainty score are relevant: both NHBNN in self-training mode with the straightforward certainty score and k-NN with the hubness-aware certainty score achieve suboptimal accuracy compared with our approach NHBNN-HS. Furthermore, as we expected, semi-supervised classification outperforms supervised classification as it can be seen from the comparison against NHBNN-SV. These observations are confirmed by the results in case of various k values as shown in Fig. 2.

As one can see in the bottom of Fig. 3, on the Breast Cancer and Colon Cancer datasets NHBNN-HS and NHBNN-Simple converge to similar accuracies. In contrast, the proposed approach, NHBNN-HS converges to a much better solution on the Lung Cancer dataset.

Based on the observations above, we note that even in cases in which NHBNN-HS and NHBNN-Simple converge to the same solution, NHBNN-HS is preferable to NHBNN-Simple as (i) the former may lead to more accurate results if the number of self-training iterations is fixed or (ii) the same accuracy may be achieved in fewer self-training iterations. For example, on the Breast Cancer dataset, NHBNN-HS achieves an accuracy of 0.84 in just 13 iterations, whereas NHBNN-Simple requires 31 iterations to achieve the same accuracy, while on the Lung Cancer dataset, NHBNN-HS achieves an accuracy of 0.75 in 13 iterations, whereas NHBNN-Simple requires 36 iterations to achieve the same accuracy.

As shown in the top of Fig. 2, hyper-parameter α that controls the contribution of 
                        
                           N
                           k
                           ′
                        
                        (
                        
                           x
                           *
                        
                        )
                      to the value of the certainty score effects the performance of the proposed approach which is in accordance with our expectations: setting α
                     =0, the certainty scores of Formula (6) reduces to the straightforward certainty score of Formula (5). On the other hand, higher values of α result in increased influence of the 
                        
                           N
                           k
                           ′
                        
                        (
                        
                           x
                           *
                        
                        )
                     . While it is important to take 
                        
                           N
                           k
                           ′
                        
                        (
                        
                           x
                           *
                        
                        )
                      into account in the certainty score, as our observations show, the balance between the hubness-score 
                        
                           N
                           k
                           ′
                        
                        (
                        
                           x
                           *
                        
                        )
                      and the straightforward certainty scores leads to the overall best results.

Assuming that the distances between instances can be pre-calculated and cached, NHBNN-HS can be implemented with minimal additional computational costs compared with NHBNN-Simple. For each labeled instance, we only need to record the distance to its k-th nearest neighbor among the labeled instances. Let us call this distance the k-distance of a labeled instance. Let us consider a labeled instance x and an unlabeled instance x
                     *. By comparing the k-distance of x and the distance between x and x
                     *, one can simply decide if x
                     * appears as one of the nearest neighbors of x when considering 
                        
                           D
                           lab
                        
                        ∪
                        {
                        
                           x
                           *
                        
                        }
                     . This way, 
                        
                           N
                           k
                           ′
                        
                        (
                        
                           x
                           *
                        
                        )
                      can be calculated quickly. At the end of each self-training iteration, k-distances are to be updated based on the instance(s) that became labeled in that iteration. As these operations require minimal additional computational costs compared to other costs of the learning algorithm (such as distance calculations), for the same number of self-training iterations, the computational costs of NHBNN-HS and NHBNN-Simple are approximately the same. Taking the previous observations into account, we conclude that NHBNN-HS may achieve more accurate results with (approximately) the same computational costs, or the same accuracy with remarkably less computational costs.

While instances may influence classification decisions in many ways, hubs are generally known to play a crucial role in classification decisions. Specifically, in case of NHBNN, hubs influence the neighbor occurrence profiles of many instances, i.e., they affect the conditional probabilities 
                        P
                        (
                        
                           x
                           i
                        
                        ∈
                        
                           N
                           k
                        
                        |
                        C
                        )
                      of many instances.

To demonstrate that the proposed approach is indeed able to label hubs correctly, we selected two patients from the BreastCancer dataset, identified by X21600 and X21621 respectively. X21600 has ER+ subtype of breast cancer and appears as one of the k-nearest neighbors (k
                     =5) of 24 other ER+ patients, while it appears as one of the nearest neighbors of only one ER− patient. X21621 has ER− subtype of breast cancer and appears as one of nearest neighbors of 11 other patients, each of them having ER− subtype of breast cancer. The expression levels of the genes with descriptions containing “BRCA” is depicted in Fig. 4
                      for these two patients. We considered the runs when these instances were not among the initially labeled instances and we observed that NHBNN-HS labeled X21621 always correctly, while it labeled X21600 in 97% of the aforementioned runs correctly. This illustrates that NHBNN-HS performs well in terms of labeling of the “most important” instances.

Next, we discuss the performance of GRF. One of the most important hyperparameters of GRF, which may affect its performance, is the decision threshold. In our experiments, we used the “default” value of 0.5, which is called harmonic threshold in [29]. This selection is in accordance with our assumption that only a small set of labeled instances is given and this set is not a fully representative sample of the unlabeled data. On the other hand, in several practical applications, additional information might be available which allows to set GRF's decision threshold in a more informed way.

In many applications, obtaining reliable class labels for large training samples may be difficult or even impossible. Therefore, semi-supervised classification techniques are required as they are able to take advantage of unlabeled data. Some of the most prominent recent methods developed for the classification of high-dimensional data follow the paradigm of hubness-aware data mining. However, hubness-aware classifiers have not been used for semi-supervised classification tasks previously. Therefore, in this paper, we introduced a semi-supervised hubness-aware classifier and we showed that it outperforms all the examined relevant baselines on the classification of gene expression data.

Based on the promising results presented in the paper, we envision that hubness-aware techniques will be used in further biomedical recognition tasks such as ECG-based person identification [7], diagnosis of schizophrenia [4] or link prediction in biomedical networks [27]. In order to accelerate this process, we made an implementation of hubness-aware machine learning techniques publicly available in the PyHubs software package on our website.
                           2
                        
                        
                           2
                           
                              http://www.biointelligence.hu/pyhubs.
                         The PyHubs software package is implemented in Python, one of the most popular programming languages of data science. PyHubs may be seen as complementary to HubMiner [24] which is a Java-based implementation of hubness-aware machine learning techniques.

@&#ACKNOWLEDGMENTS@&#

The author thanks the anonymous reviewers for their insightful comments. The PyHubs software package was implemented by Mararu-Nicoarˇa Vlad under the supervision of the author of the current study. This research was performed within the framework of the grant of the Hungarian Scientific Research Fund – OTKA 111710 PD. This paper was supported by the János Bolyai Research Scholarship of the Hungarian Academy of Sciences.

@&#REFERENCES@&#

