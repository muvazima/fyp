@&#MAIN-TITLE@&#Dual many-to-one-encoder-based transfer learning for cross-dataset human action recognition

@&#HIGHLIGHTS@&#


               
                  
                     
                        
                           
                           Proposed a new transfer-learning method for cross-dataset action recognition.


                        
                        
                           
                           A new dual many-to-one encoder method for feature extraction across action datasets.


                        
                        
                           
                           Achieved over 10% increase in recognition accuracy over recent work.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Cross-dataset

Action recognition

Neural network

Transfer learning

Domain adaptation

@&#ABSTRACT@&#


               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Human action recognition has drawn immense interests over the years, with its applications in a wide range of fields, including video labeling, video content retrieval, video surveillance, and sports video analysis. With the growing convenience of capturing and sharing videos, the computer vision community has seen a growing variety of human action datasets with substantial amount of videos. While the majority of these video data do not have annotations on them and hand labeling large datasets requires considerable amount of human efforts, researchers are interested in developing mechanisms to automatically generate annotations to these video data. Considering the fact that large-scale datasets always exhibit high intra-class variations, the requirement for a rational number of training data can easily go beyond the number of existing labeled data. Thus, researchers are thinking about the possibility of employing previously annotated datasets to facilitate automatic labeling of new datasets. For the human action recognition problem, different datasets share common actions. If it is possible to transfer knowledge between source and target action datasets, the annotated source dataset can serve as an augmentation to the training data, based on which effective labeling of the target dataset can be carried out. However, the auxiliary domain data may suffer from the serious domain-shift problem. For example, the action ‘run’ in one dataset may consist of videos of athletes running on field tracks while the same action from another dataset may contain videos of people running on the streets. In order to alleviate such problems, an algorithm that can reduce cross-domain variance is required. Algorithms of this type belong to transfer learning, which is a particular branch of machine learning that aims to utilize knowledge from one source or task to assist the same or a different task on another source.

In this paper, we tackle the problem of action recognition across four benchmark datasets. The major challenge comes from the significant cross-dataset variations. For example, Diving sequences in UCF Sports dataset [1] consist of TV broadcast videos with steady camera movements, controlled lighting, and trivial viewpoint changes, while videos of the same action class in HMDB 51 dataset [2] exhibit large variances in lighting, background, and viewpoints. Considering the significance of cross-dataset variations, hand-crafted features, which are designed to capture the discriminative properties of images or videos, are incapable of producing domain-invariant features for cross-domain datasets. Thus, we are interested in learning generalized feature representations across datasets, so that the resulting features from the source dataset can be used in the recognition of unseen instances in the target dataset. Our proposed method learns cross-dataset generalized features by training two many-to-one encoders on the source and target datasets in parallel, and maps raw features of instances from the two datasets to the same feature space. To our knowledge, this is the first time a dual many-to-one encoder architecture is used in cross-dataset action recognition. Contributions in this work are as follows:


                     
                        
                           •
                           We have proposed a new transfer-learning method for cross-dataset action recognition. Our method can be easily generalized to other recognition tasks.

We have designed a novel dual many-to-one encoder architecture for extracting generalized features across action datasets.

We achieved over 10% increase in recognition accuracy over recent work in cross-dataset action recognition.

The organization of the paper is as follows: in Section 2 we present some background and related work; in Section 3 we present details of the proposed method; in Section 4 we describe our experimental results, and we conclude the paper in Section 5.

In this section, we present the background and related works that are essential to understanding the objective and methodologies behind our proposed method. We briefly review the concepts and methods in transfer learning and its benefits in cross-dataset human action recognition.

Traditional machine learning methods follow the convention that the training data and the testing data are from the same distribution. However, this assumption is not always satisfied in real world applications. For example, when classifying web documents into several predefined categories, the test web documents may contain categories that are not sufficiently represented in the training data. Therefore, traditional learning methods may fail in such cases [3,4]. The objective of transfer learning is to transfer the knowledge from a source domain to the target domain. More specifically [5] gives the following definition:


                        
                           Definition
                           Transfer learning


                           Given a source domain D
                              
                                 s
                               and learning task T
                              
                                 s
                              , a target domain D
                              
                                 T
                               and learning task T
                              
                                 T
                              , transfer learning aims to help improve the learning of the target predictive function f
                              
                                 T
                              (⋅) in D
                              
                                 T
                               using the knowledge in D
                              
                                 S
                               and T
                              
                                 S
                              , where D
                              
                                 S
                              ≠D
                              
                                 T
                              , or T
                              
                                 s
                              ≠T
                              
                                 T
                              .

Surveys like [5] provides taxonomy related to transfer learning. Transfer learning can be further divided into three categories depending on the presence of labeled data in target and source tasks: 1) Inductive transfer learning, when labeled data is available in the target domain, 2) Transductive transfer learning, when labeled data is available only in source domain, and 3) Unsupervised transfer learning, when no labeled data is available in either source or target domains. Inductive transfer learning is closely related to multitask learning, its purpose is to learn both source and target tasks simultaneously, and transfer knowledge between the two tasks to improve performance on both [5]. Our proposed method falls into this category by learning two many-to-one encoders in parallel.

The type of knowledge being transferred can be divided into four categories: 1) instance transfer, 2) feature-representation transfer, 3) parameter transfer and 4) relational-knowledge transfer. Instance transfer uses source instances to train a classifier on target instances, usually by selecting or weighting source instances according to a certain metric. Feature-representation transfer reduces the differences between the source and target datasets by mapping one representation to the other, or by mapping both source and target representation to a common representation. Parameter transfer learns parameters shared by the source and target datasets. Finally, relational knowledge transfer uses a network or graph to explore the relationships within a dataset, and transfer the relationships from the source to the target [5].

Transfer learning has been adopted by the computer vision community in areas including object detection [6,7], object classification [8--11] and video event detection [12--16]. For example, Duan et al. [14] proposed an adaptive multiple kernel learning method by adapting classifiers based on base kernels and pre-learned average classifiers. Their method was applied to event recognition in consumer videos on the web. Lim et al. [7] proposed an object detection model that augments training data by borrowing and transforming instances from other classes. Gao et al. [17] designed a low-level feature model that could be used as a prior for learning new object class models from scarce training data. Tommasi and Caputo [18] performed domain adaptation of object classification inside the naive Bayes nearest neighbor framework by iteratively learning class metrics that can induce large between-class variance. Kulis et al. [11] addressed domain adaptation between different types of features and dimensionality for object classification by using asymmetric kernel transformation.

In human action recognition in particular, transfer learning has received increased interest over the years [19--23]. We will expand on the application of transfer learning in action recognition in Section 2.4.

With the revival of neural networks in recent years, many researchers have applied neural networks to transfer learning. Zhang [4] tackled the problem of cross-domain document classification using restricted Boltzman machine to learn a set of hierarchical features from source domain, then select a subset of the features by kernel-task alignment. Girshick et al. [24] adopted region proposal Convolutional Neural Networks in a two-stage object detection framework, where regions of interests were first detected and then semantics were learnt by fine-tuning. Oquab et al.
                        [25] designed a method to reuse CNN layers trained on ImageNet, and transferred parameters for object classification in the PASCAL VOC dataset.

Different from the methods used in prior works, we propose a new method by training a pair of many-to-one encoders in parallel, and then map raw features from the source and target datasets to the same feature space. We will expand on the details of our method in Section 3.

Human action recognition is a widely studied topic. The goal of human action recognition is to correctly classify actions performed by one or more persons into a pre-defined category. Survey papers like [26] provide taxonomy for this topic.

Efficacy of any action recognition method depended on the feature representation method used. Among recent work, Shao et al. [27] proposed a novel descriptor based on spatio-temporal Laplacian pyramid coding, which effectively extracts holistic representation without loss of information. Yu et al. [28] developed a structure-preserving binary representation for videos with depth information. Shao et al. [29] presented a method that extracts discriminative features by efficient spatio-temporal localization of human actions. Inspired by evolutionary method, Liu et al. [30] proposed a genetic programming approach toward learning spatio-temporal representations. To fuse different types of representations into one, Shao et al. [31] designed a spectral coding algorithm based on kernel multiview projections.

Although the above methods achieved satisfactory performance in action recognition, they were designed with the assumption that the training data and the testing data came from the same dataset. Unlike these methods, we propose a novel feature extraction method that can handle cross-dataset classification by learning generalized features. We show its efficacy in labeling unseen videos from the target dataset. Though our method was developed for action recognition, we argue that the proposed method can be generalized to other cross-dataset applications as well.

There has been an exploding interest in applying transfer learning techniques to action recognition [19,23,32,33]. There are interests in applying transfer learning to datasets generated by different sensors, e.g., cameras, wearable sensors, or other sensor modalities [20,34]. Others are interested in applying transfer learning to datasets of the same domain, and most commonly from video sequences. While video sequences are the most common type of action datasets, the use of cameras to capture actions introduces complex issues; for example, different viewing angles, cluttered background, or changes in illumination, all contribute to significant variance in the captured videos. Therefore, action recognition in videos is a challenging problem.

Some researchers focus on transferring knowledge for action recognition between camera views [32]. For example, Liu et al. [10] extracted high level features shared across views by using bipartite graph partitioning on two view-dependent vocabularies. Zheng et al. [35] proposed to learn a pair of dictionaries on videos taken from different views and build a view-invariant sparse representation. Zhang et al. [36] used linear transformation to transform source view to target view via virtual path. Zhu and Shao [37] proposed to learn a sparse representation based on dense trajectory features by learning a view-independent basis dictionary and by forcing the same actions to have an identical representation. For applications in smart homes, Wu et al. [38] presented a multiview activity recognition technique by performing spatio-temporal feature fusion.

Other researchers tackled problems that are related to event search and abnormality recognition. Lam et al. [13] integrated transfer learning with relevance feedback to aid user's event query with known classification problems. Nater et al. [39] addressed the problem of abnormal event detection by adapting a Least-Square Support Vector Machine learnt from normal events to unseen events.

Some researchers proposed new adaptive transfer learning models. Yang et al. [40] proposed an adaptive support vector machine that transforms a classifier trained on a source dataset to work on a target dataset. Lin adn Li [41] extended the boosting-based learning method which allows classifiers built on a source action dataset to adapt to a new dataset.

In this paper, we tackle the problem of cross-dataset action recognition. In recent work, Cao et al. [42] used a probabilistic approach where a Gaussian mixture model learnt from a source dataset serves as a prior and is iteratively adapted to a target dataset for action detection. Sultani and Saleemi [43] proposed to use a weighted histogram by putting more weights on areas with high foreground confidence. This alleviates the problem in cross-dataset action recognition when the background may obscure recognition results. Different from these approaches, our method transfers knowledge across datasets by simultaneously learn a pair of many-to-one encoders. Using this model, we implicitly learn the distribution of raw features from the two datasets and map them to the same feature space.

@&#BACKGROUND@&#

In this section, we present background for the two components essential to our method: action bank features and neural networks.

Feature extraction for action recognition from videos is a well-studied topic. Many established action recognition methods make use of low/mid-level features; e.g., local space-time features [44,45,46], 3D dense point trajectories [47], and 3D gradient histograms [48,49]. Although these features achieve promising results on benchmark datasets [50], they lack robustness with respect to the range of semantics and the amount of intra-class variances they can handle. These drawbacks of low/mid-level features are particularly acute when used in cross-dataset recognition, where the capability of high level semantic representation and adaptivity toward large intra-class variances are needed.

Recently, Sadanand and Corso [51] proposed action bank, an action feature which is capable of learning high level semantic representation and is immune to a certain amount of intra-class variance. Action bank feature is essentially a template-based feature, where the templates, or ‘banks', are extracted from video sequences across a range of actions and viewpoints. Given a video sequence, the banks are used as detectors, which decompose the video sequence into a conglomeration of responses at various spatiotemporal orientations. Then max pooling is applied to the responses, resulting in robustness to local displacements. Because of its high-level semantic representation and embedded view-invariance, action bank feature is an ideal candidate for use in our cross-dataset action recognition method. We will discuss the details of how we generate a generalized high-level feature from the action bank feature for cross-dataset action recognition in Section 3.

After a long plateau, neural network research has been revived in recent years with the emergence of deep learning [52,53]. The computer vision community has adopted deep learning in many applications; most notably object detection, object classification, hand-written digit recognition, and action recognition. In general, a neural network can be trained to approximate a function f : X → Y, where X is the input and Y is the target output. A typical neural network consists of one or multiple layers of neurons whose inputs are determined by certain weights and biases, and outputs, or activations, are determined by a function of choice and passed among the neurons [54]. The layered architecture of neural networks, together with the adaptivity of tuning inherent weights and bias to fit specific tasks, make it a powerful model which can generalize to the mapping of input X to output Y.

In this work, we use a type of neural network called many-to-one encoders. Unlike auto-encoders, which force target outputs to be identical to the inputs, many-to-one encoders force target outputs to be identical among certain subsets of the inputs. In our case, the target outputs are set to be the same for input instances that belong to the same action class. The identical representation of same action class serves as a guide for encoder training, where intra-class variances are minimized across datasets. The trained encoders are used for feature extraction: given raw feature vectors as inputs, the encoders' outputs are a set of new high-level features which generalize across datasets. We will follow up with the details in Section 3.

@&#PROPOSED METHOD@&#

In this section, we describe our proposed method in details. An overview of our method is presented in Fig. 1
                     . Our method consists of three stages. In the first stage, preprocessing is done on both datasets, where action bank features are extracted and reduced to a smaller subspace. In the second stage, centroids of training instances from each class are extracted, which are then used to guide encoder training. In the third stage, a pair of many-to-one encoders are trained on the source and target datasets in parallel, after which the activations of the output layer are extracted as the final features. Finally, a linear support vector machine classifier is built on the encoded features extracted from the source dataset, and then tested on the new features extracted from unseen samples of the target dataset.

We first process the datasets by extracting their action bank features [51]. Unlike low- or mid-level feature extraction methods [50], action bank method aims at extracting high level representation through semantic transfer. Given a test video clip, the action bank feature is extracted by first decomposing the video to energy responses at various spatiotemporal orientations. The responses are then correlated with a set of video clips across various semantic and viewpoints. More specifically, decomposition of videos are done via 3D Gaussian third derivative filtering 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          3
                                       
                                       
                                          
                                             
                                                θ
                                             
                                             ^
                                          
                                       
                                    
                                 
                              
                              (
                              p
                              )
                           , where 
                              
                                 
                                    θ
                                 
                                 ^
                              
                            captures the 3D direction of the filter symmetry axis and p denotes space-time position. Then, point-wise responses of video sequence I to this filter are squared and summed over spatiotemporal neighborhood Ω, s.t. 
                              
                                 (1)
                                 
                                    
                                       
                                          E
                                       
                                       
                                          
                                             
                                                θ
                                             
                                             ^
                                          
                                       
                                    
                                    (
                                    p
                                    )
                                    =
                                    
                                       
                                          ∑
                                       
                                       
                                          p
                                          ∈
                                          Ω
                                       
                                    
                                    
                                       
                                          (
                                          
                                             
                                                G
                                             
                                             
                                                
                                                   
                                                      3
                                                   
                                                   
                                                      
                                                         
                                                            θ
                                                         
                                                         ^
                                                      
                                                   
                                                
                                             
                                          
                                          *
                                          
                                          I
                                          )
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           Seven types of spatiotemporal energies are evaluated, including static E
                           
                              s
                           , leftward E
                           
                              l
                           , rightward E
                           
                              r
                           , downward E
                           
                              d
                           , upward E
                           
                              u
                           , flicker E
                           
                              f
                            and lack of structure E
                           
                              o
                           . Each type of energy is computed as a sum over 4 basis third-order filters; i.e., 
                              
                                 
                                    E
                                 
                                 
                                    
                                       
                                          n
                                       
                                       ^
                                    
                                 
                              
                              (
                              p
                              )
                              =
                              
                                 
                                    ∑
                                 
                                 
                                    i
                                    =
                                    0
                                 
                                 
                                    3
                                 
                              
                              
                                 
                                    E
                                 
                                 
                                    
                                       
                                          
                                             
                                                θ
                                             
                                             
                                                i
                                             
                                          
                                       
                                       ^
                                    
                                 
                              
                              (
                              p
                              )
                           , where each 
                              
                                 
                                    
                                       
                                          θ
                                       
                                       
                                          i
                                       
                                    
                                 
                                 ^
                              
                            is a basis computed according to the conventional steerable filters as follows [55]
                           
                              
                                 (2)
                                 
                                    
                                       
                                          
                                             
                                                θ
                                             
                                             
                                                i
                                             
                                          
                                       
                                       ^
                                    
                                    =
                                    cos
                                    ⁡
                                    
                                       
                                          
                                             
                                                π
                                                i
                                             
                                             
                                                4
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                θ
                                             
                                             
                                                a
                                             
                                          
                                       
                                       ^
                                    
                                    (
                                    
                                       
                                          n
                                       
                                       ^
                                    
                                    )
                                    +
                                    sin
                                    ⁡
                                    
                                       
                                          
                                             
                                                π
                                                i
                                             
                                             
                                                4
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                θ
                                             
                                             
                                                b
                                             
                                          
                                       
                                       ^
                                    
                                    (
                                    
                                       
                                          n
                                       
                                       ^
                                    
                                    )
                                 
                              
                           where 0 ≤ i ≤ 3, 
                              
                                 
                                    
                                       
                                          θ
                                       
                                       
                                          a
                                       
                                    
                                 
                                 ^
                              
                              (
                              
                                 
                                    n
                                 
                                 ^
                              
                              )
                              =
                              
                                 
                                    n
                                 
                                 ^
                              
                              ×
                              
                                 
                                    
                                       
                                          e
                                       
                                       
                                          p
                                       
                                    
                                 
                                 ^
                              
                              /
                              ∥
                              
                                 
                                    n
                                 
                                 ^
                              
                              ×
                              
                                 
                                    
                                       
                                          e
                                       
                                       
                                          p
                                       
                                    
                                 
                                 ^
                              
                              ∥
                           , 
                              
                                 
                                    
                                       
                                          θ
                                       
                                       
                                          b
                                       
                                    
                                 
                                 ^
                              
                              (
                              
                                 
                                    n
                                 
                                 ^
                              
                              )
                              =
                              
                                 
                                    n
                                 
                                 ^
                              
                              ×
                              
                                 
                                    
                                       
                                          θ
                                       
                                       
                                          a
                                       
                                    
                                 
                                 ^
                              
                              (
                              
                                 
                                    n
                                 
                                 ^
                              
                              )
                           , and 
                              ê
                            is the unit vector along the spatial x axis in the Fourier domain.

Finally, standard Bhattacharyya coefficient [56] is used to compute the correlation between energy responses of the test video and template videos. The correlations are then max-pooled, and concatenated to form the final action bank descriptor. Given N
                           
                              a
                            templates, N
                           
                              s
                            scales and an octree of three levels [57], the total length of action bank feature vector is N
                           
                              a
                           
                           ×
                           N
                           
                              s
                           
                           ×73. In this work, we use N
                           
                              a
                           
                           =205 templates and N
                           
                              s
                           
                           =1 scale, which result in feature vectors of length 14,965.

Note that the authors in [51] provided the source code and the action bank features for several benchmark datasets. We used the provided features directly in our experiments, and generated action bank features for datasets that do not already have the features available. The features and source code can be found at the authors' website.
                              1
                           
                           
                              1
                              
                                 http://www.cse.buffalo.edu/jcorso/r/actionbank/.
                           
                        

Action bank features have high dimensionality, which makes it inefficient for neural network training. To obtain low-dimensional features in the subspace, we use principle component analysis (PCA) [58]. PCA aims at reducing the dimensionality of a set of features by finding their eigenvectors and then project the original features onto the top p eigenvectors. By doing so, the original features can be represented by coordinates of the p eigenvectors in the subspace. In our method, we retain the top p eigenvectors such that the cumulative corresponding eigenvalues cover 99% of the total eigenvalues. In our experiments, this reduces feature dimension down to the range of 200 to 300, varying between datasets.

In this section, we present the architecture and details on the training of the neural networks.

We use the centroid of each class as target output (Fig. 2
                           ). The class centroid is computed by averaging over instances' raw features in each class. Let 
                              
                                 
                                    X
                                 
                                 
                                    s
                                    ,
                                    c
                                 
                                 
                                    i
                                 
                              
                            and 
                              
                                 
                                    X
                                 
                                 
                                    t
                                    ,
                                    c
                                 
                                 
                                    j
                                 
                              
                            denote the i-th and j-th training instances (i.e., raw feature vectors) from class c in the source and target dataset, respectively, the target output for instances from class c is: 
                              
                                 (3)
                                 
                                    
                                       
                                          T
                                       
                                       
                                          c
                                       
                                    
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          
                                             
                                                N
                                             
                                             
                                                s
                                                ,
                                                c
                                             
                                          
                                          +
                                          
                                             
                                                N
                                             
                                             
                                                t
                                                ,
                                                c
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      s
                                                      ,
                                                      c
                                                   
                                                
                                             
                                          
                                          
                                             
                                                X
                                             
                                             
                                                s
                                                ,
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          +
                                          
                                             
                                                ∑
                                             
                                             
                                                j
                                                =
                                                1
                                             
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      t
                                                      ,
                                                      c
                                                   
                                                
                                             
                                          
                                          
                                             
                                                X
                                             
                                             
                                                t
                                                ,
                                                c
                                             
                                             
                                                j
                                             
                                          
                                       
                                    
                                 
                              
                           where N
                           
                              s,c
                            and N
                           
                              t,c
                            are the total number of training instances of class c from the source and target datasets.

At this stage, our method trains a pair of many-to-one encoders on the source and target datasets in parallel. Recall that for instances of the same action class, the target outputs of the two many-to-one encoders are identical. This setting forces the two many-to-one encoders to generalize to varying inputs and guide the outputs of same class instances to be similar. The advantages are twofold: 1) intra-class variance is minimized, and 2) instances of the same action class from the two datasets are mapped to the same feature space, thereby allowing knowledge transfer between the two datasets. While the first advantage has been demonstrated in recent publications [59,36] using single many-to-one encoder, in this work, we demonstrate the benefits of using a pair of many-to-one encoders in the context of transfer learning.

The architecture of the dual many-to-one encoders we used is illustrated in Fig. 2. They are fully-connected feedforward neural networks with an input layer, a hidden layer and an output layer. The two many-to-one encoders trained on the source and target datasets have identical architecture; i.e., same number of layers and same number of neurons in each layer. In our method, we use only one hidden layer. Although conceptually, a deep network architecture with more than one hidden layer is beneficial for learning powerful representations, it has been shown that carefully configured and trained single-hidden-layer networks can achieve good performance in many tasks [60]. This observation is validated in our experiments as well.

After the preprocessing stage, the training videos from both datasets are represented by action bank features, which are then reduced to a smaller subspace via PCA. For simplicity, we denote one training instance as x
                           
                              i
                           , with |x
                           
                              i
                           |=
                           L. Thus, both many-to-one encoders have an input layer of size L, a hidden layer of size H and an output layer of size L, where L is the output feature vector length, and H is a user defined parameter. In this work, we experimented with hidden layer sizes that range from 50 to 200, and input and output layer sizes that range from 200 to 300.

The goal of training the two networks in parallel is to find a mapping between training instances and the target outputs. Both networks have the same architecture but different parameters. More specifically, for the source network, the mapping is done via f
                           1:ℝ
                              L
                            →ℝ
                              H
                            and f
                           2:ℝ
                              H
                            →ℝ
                              L
                           . f
                           1 and f
                           2 are defined as follows: 
                              
                                 (4)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         f
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   )
                                                   =
                                                   σ
                                                   (
                                                   
                                                      
                                                         W
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   +
                                                   
                                                      
                                                         b
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   )
                                                   
                                                      
                                                         f
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         f
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   )
                                                   )
                                                   =
                                                   σ
                                                   (
                                                   
                                                      
                                                         W
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                   
                                                      
                                                         f
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   )
                                                   +
                                                   
                                                      
                                                         b
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where σ(⋅) is the activation function, W
                           1, b
                           1, W
                           2 and b
                           2 are the parameters for f
                           1 and f
                           2, respectively. The network parameters are indicated in Fig. 2, where W
                           1, b
                           1, W
                           2 and b
                           2 are parameters to the network trained on the source dataset and 
                              
                                 
                                    W
                                 
                                 
                                    1
                                 
                                 
                                    ′
                                 
                              
                           , 
                              
                                 
                                    b
                                 
                                 
                                    1
                                 
                                 
                                    ′
                                 
                              
                           , 
                              
                                 
                                    W
                                 
                                 
                                    2
                                 
                                 
                                    ′
                                 
                              
                            and 
                              
                                 
                                    b
                                 
                                 
                                    2
                                 
                                 
                                    ′
                                 
                              
                            are parameters of the network trained on the target dataset.

We experimented with various hidden layer sizes and report the results in Section 4.5. Given a hidden layer size H, we initialized the weights and biases in both networks with random numbers drawn from a uniform distribution and with range between 0 and 1. We used the Sigmoid function as the activation function, i.e., 
                              
                                 (5)
                                 
                                    σ
                                    (
                                    x
                                    )
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          1
                                          +
                                          
                                             
                                                e
                                             
                                             
                                                −
                                                x
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        

We define the objective function as follows: 
                              
                                 (6)
                                 
                                    E
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          2
                                          N
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          N
                                       
                                    
                                    ∥
                                    
                                       
                                          T
                                       
                                       
                                          i
                                       
                                    
                                    −
                                    h
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          W
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          b
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          W
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                    
                                       
                                          b
                                       
                                       
                                          2
                                       
                                    
                                    )
                                    
                                       
                                          ∥
                                       
                                       
                                          2
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           where h(⋅)=
                           f
                           2(f
                           1(⋅)), W
                           1, W
                           2, b
                           1 and b
                           2 are the weights and biases of the network, and N is the number of training instances.

Training is done via stochastic gradient descent, where the objective function is minimized by iteratively updating the weights and biases [61]. For example, W
                           1 is updated as follows: 
                              
                                 (7)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   Δ
                                                   
                                                      
                                                         W
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   (
                                                   t
                                                   +
                                                   1
                                                   )
                                                   =
                                                   η
                                                   
                                                      
                                                         δ
                                                         
                                                            
                                                               E
                                                            
                                                            
                                                               t
                                                               +
                                                               1
                                                            
                                                         
                                                      
                                                      
                                                         δ
                                                         
                                                            
                                                               W
                                                            
                                                            
                                                               1
                                                            
                                                         
                                                      
                                                   
                                                   +
                                                   μ
                                                   Δ
                                                   
                                                      
                                                         W
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   (
                                                   t
                                                   )
                                                   
                                                      
                                                         W
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   (
                                                   t
                                                   +
                                                   1
                                                   )
                                                   =
                                                   
                                                      
                                                         W
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   (
                                                   t
                                                   )
                                                   −
                                                   Δ
                                                   
                                                      
                                                         W
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                   (
                                                   t
                                                   +
                                                   1
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where ΔW
                           1(t
                           +1) is the update to W
                           1 at the (t
                           +1)-th iteration, E
                           
                              t
                              +1 is the value of the objective function at the (t
                           +1)-th iteration, η denotes the learning rate, and μ denotes the momentum. W
                           2, b
                           1, and b
                           2 are updated in a similar manner.

After training, we use the values at the output layer as the final features. Note that given the complexity of the problem, when E is minimized, the values at the output layers of the network are approximate solutions instead of being identical to the pre-defined target outputs. The approximate solutions lie in the vicinity of the target outputs. Thus, the final features extracted from instances of the same action class, from both the source and target datasets, would lie in the same cluster. We will illustrate this phenomenon in Section 4.4. As will be shown in the following section, these features are linearly separable and therefore a linear classifier can be used for classification, which is very efficient compared to using kernel-based non-linear classifiers [43].

We use multi-class linear support vector machine (SVM) as classifier. Unlike a naïve binary classifier which does not apply constraints on the separation plane, SVM aims at choosing the separation plane where the distances between support vectors and the plane are maximized. This constraint introduces robustness to the classifier [62].

To perform cross-dataset classification, a linear SVM classifier is trained on features extracted from the source dataset and tested on features extracted from unseen instances from target dataset as shown in the bottom of Fig. 1. In Section 4, we show that such classification scheme can effectively classify unseen data across datasets. This is due to the successful knowledge transfer from the source dataset to the target dataset in stage 2 (Fig. 1).

@&#EXPERIMENTS AND RESULTS@&#

In this section, we present our experimental results on several benchmark datasets. We applied our method to the following pairs of datasets: UCF 50 - UCF Sports, UCF 50 - HMDB 51, UCF 50 - Olympic Sports and HMDB 51 - UCF Sports. Fig. 3
                      illustrates sample actions from these datasets. As can be seen in the figure, these datasets were taken from different sources and they exhibit significant variance for the same action class. We will start with describing the individual datasets, followed by details of the experimental settings.


                        
                           
                              A.
                              UCF Sports

The UCF Sports dataset [1] consists of 150 video sequences and contain 10 actions: diving, golf swinging, kicking, lifting, riding horse, running, skate boarding, swing bench, swing side, and walking. The videos have resolution of 720 x 480 pixels and most were collected from broadcast television channels like BBC and ESPN. The videos feature a wide range of scenes, viewpoints, and scales.

UCF 50

The UCF 50 dataset [63] features 50 real-life activities. They contain consumer videos taken from YouTube and have significant intra-class variance in background, viewpoints, and lighting conditions.

HMDB51

HMDB 51 [2] is a challenging large-scale action dataset with 51 classes and 6,849 video clips. This dataset consists of movie clips and consumer video clips taken from Prelinger archive, YouTube, and Google videos. The 51 action classes span from facial actions, general body movements, to human interactions, all accompanied by varying viewpoints, scale, background, and lighting conditions.

Olympic Sports

The Olympic Sports dataset [64] contains 16 actions. The videos were taken from YouTube and each video shows an athlete practicing one sport. Videos are subject to varying viewpoints, scale, background, and lighting.

All experiments were conducted with Matlab R2012a on a 64-bit Windows 7 PC with two 4-core 2.93GHz Intel i7 CPUs and 8GB of memory. We used the action bank features provided by the authors of [51] for datasets UCF 50, UCF Sports, and HMDB 51, and the code by the same authors to extract action bank features for the Olympic Sports dataset. We wrote the code for the many-to-one encoder and the stochastic gradient descent algorithm. For classification, we used the publicly available LIBSVM package [65].

In all experiments, each dataset was randomly split into training and testing sets. For videos from the same action class, roughly 70% of the videos were used for training and 30% were used for testing. Since the neural networks for the encoders were initialized randomly and the datasets were randomly split into training and testing sets, we repeat the experiments 3 times and report the average accuracy. When performing stochastic gradient descent for encoder training, we set learning rate η
                        =0.1 and momentum μ
                        =0.9 (Eq. 7). Each encoder is trained for about 1,000 iterations.

In this section, we report the experimental results and compare them to prior work. The numbers in the tables below represent average class accuracy and H represents the number of neurons in the hidden layer of the many-to-one encoders. Following [43], we hand picked visually similar or common actions between each pair of datasets. For each pair, we experimented with classification in both directions; e.g., for pair UCF 50 and UCF Sports, we trained classifier on UCF 50 and tested on UCF Sports, and then trained classifier on UCF Sports and tested on UCF 50. We describe the common actions and the experimental settings for each pair as follows. Note that the action names below are identical to the names from the original datasets.


                        
                           
                              A.
                              UCF 50–UCF Sports.

The common actions between UCF 50 and UCF Sports are: Diving/dive, GolfSwing/golf, HorseRiding/riding, SkateBoarding/skate and CleanAndJerk/lift. The total number of videos are 723 and 58, respectively. Experimental results are shown in the following table (Table 1
                                 ). It is interesting to see that although UCF Sports dataset has far less video clips than UCF 50 (58 vs 723), the classifier trained on UCF Sports dataset can still label UCF 50 fairly well.

HMDB 51–UCF 50.

The common actions between HMDB 51 and UCF 50 are: ride_bike/Biking, golf/GolfSwing, pullup/PullUps, ride_horse/HorseRiding and shoot_ball/Basketball. The total number of videos are 559 and 740, respectively. Experimental results are shown in the table below (Table 2
                                 ).

UCF 50–Olympic Sports.

The common actions between UCF 50 and Olympic Sports are: Basketball/basketball_layup, CleanAndJerk/clean_and_jerk, ThrowDiscus/discus_throw, Diving/diving_springboard_3m, PoleVault/pole_vault and TennisSwing/tennis_serve. The total number of videos are 860 and 304, respectively. The results are shown in Table 3
                                 . We compare our results with the best accuracy reported in [43] in Section 4.7. Note that unlike [43], we did not augment the Olympic Sports dataset with horizontally flipped version of the videos.

HMDB 51–UCF Sports.

The common actions between HMDB 51 and UCF Sports are: dive/dive, golf/golf, kick_ball/kick, ride_horse/riding and run/run. The total number of videos are 708 and 71, respectively. Experimental results are reported in the following table (Table 4
                                 ).

To verify that by using the dual many-to-one encoder architecture, varying intra-class distributions from different datasets are indeed transformed to the same cluster, we plotted the distribution of the raw features of instances from the action class ‘Golf’ in datasets HMDB 51 and UCF 50 (Fig. 4
                        ). We compared the distributions of the raw action bank features and the learnt generalized feature. For illustration purposes, all features were reduced to a 2-dimensional subspace via PCA, and scaled to lie within the range [−1, 1]. As shown in Fig. 4, raw features from instances of the Golf class for HMDB 51 and UCF 50 form 2 distinct clusters (Fig. 4a), but they are merged into a single cluster after mapping to the generalized feature space (Fig. 4b).

We are interested in knowing how the hidden-layer size influences classification accuracy. As can be seen from Fig. 5
                        , although different tasks tend to peak at certain hidden-layer size, in general, classification accuracy tends to increase as the number of neurons in the hidden layer increases. This can be explained by the increased flexibility of the network as hidden layer size increases, thus the model can generalize to more complex mapping.

We evaluate the computation time of the proposed method and report the results in Table 5
                        . All experiments were conducted on our lab PC and the language used was Matlab. The reported times were average running time over both directions of dataset pair, and the computation time for action bank features was not included. The reported times include model training, classifier training, and classification. As can be seen from the following table, our method can perform model training, classifier training, and test set classification very efficiently. The longest run time is just a little over 10minutes for the pair UCF 50 and Olympic Sports and for H
                        =200. This pair of datasets contain over 1,100 training and test sequences. We attributed this efficient execution to the PCA dimension reduction of raw features, and the shallow single-hidden-layer network architecture.

We compare our experimental results with the best reported result in [43] (Table 6
                        ). For all four experiments, we used the same action classes and train/test ratio as [43]. It is worth mentioning that we did not augment the Olympic Sports dataset. Our method outperforms all four cross-dataset classification tasks. The key difference between our method and [43] is that our method uses transfer learning to learn a generalized feature across datasets while the latter aims at hand-crafting features on one dataset, and then apply them to the other dataset without explicit knowledge transfer. The strong performance of our method demonstrates the advantage of transferring knowledge between datasets, and more importantly, the efficacy of the proposed dual many-to-one encoder transfer learning method.

We introduced a new transfer learning method based on a novel dual many-to-one encoder architecture. We experimented with cross-dataset action recognition in several benchmark action datasets and demonstrated both the effectiveness and the efficiency of the proposed method on all tasks. We credited the impressive performance of the proposed method to the success of the domain-invariant feature extractions technique, which maps features from different datasets to a unified feature space. In the future, we will develop weakly-supervised or unsupervised algorithms for learning domain-invariant features. Another interesting direction is to investigate the transfer of knowledge between different action datasets using low/mid-level action features. Additionally, the proposed dual many-to-one encoder architecture can be easily generalized to other cross-dataset or cross-domain categorization problems.

@&#REFERENCES@&#

