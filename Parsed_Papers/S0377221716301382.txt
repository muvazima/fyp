@&#MAIN-TITLE@&#Preference stability over time with multiple elicitation methods to support wastewater infrastructure decision-making

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Weights elicited in stakeholder interviews and public online survey were similar.


                        
                        
                           
                           Elicitation method was best explanatory variable for weight stability over time.


                        
                        
                           
                           Weights were more stable after one month with SWING method than with SMART/SWING.


                        
                        
                           
                           SWING method was perceived as more difficult than SMART/SWING method.


                        
                        
                           
                           Despite weight differences, stable ranking of alternatives across methods and time.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Behavioral OR

Weight elicitation

Multiple criteria analysis

Online survey

OR in environment and climate change

@&#ABSTRACT@&#


               
               
                  We used a multi-method and repeated elicitation approach across different stakeholder groups to explore possible differences in the outcome of an environmental decision. We compared different preference elicitation procedures based on Multi Criteria Decision Analysis (MCDA) over time for a water infrastructure decision in Switzerland. We implemented the SWING and SMART/SWING weight elicitation methods and also compared results with earlier stakeholder interviews. In all procedures, the weights for environmental protection and well-functioning (waste-)water systems were higher than for cost reduction. The SMART/SWING variant produced statistically significantly different weights than SWING. Weights changed over time with both elicitation methods. Weights were more stable with the SWING method, which was also perceived as slightly more difficult than the SMART/SWING variant. We checked whether the difference in weights produced by the two elicitation methods and the difference in their stability affects the ranking of six alternatives. Overall an unconventional decentralized alternative ranked first or second in 92 percent of all elicitation procedures, which were the online surveys or interviews. For practical decision-making, using multiple methods across different stakeholder groups and repeating elicitation can increase our confidence that the results reflect the true opinions of the decision makers and stakeholders.
               
            

@&#INTRODUCTION@&#

It is often crucial to choose decision-making processes that reflect values well. Environmental policy decisions are especially challenging and are increasingly addressed with Multi Criteria Decision Analysis (MCDA); for recent reviews see e.g. Gregory et al. (2012), Hajkowicz and Collins (2007), Huang, Keisler, and Linkov (2011) and Mendoza and Martins (2006). Here, we focus on water infrastructures for which we devoted considerable efforts to include stakeholder preferences (Lienert, Schnetzer, & Ingold, 2013, 2015; Scholten, Schuwirth, Reichert, & Lienert, 2015; Zheng, Egger, & Lienert, 2016). Water and sewer networks or wastewater treatment plants provide many services, including public health and protection from urban flooding or environmental pollution. These infrastructures are long-lived and aging; engineering planners face “daunting” future uncertainties (Milly et al., 2008). In many OECD countries, annual investment needs for water infrastructures are estimated at 0.75 percent of GDP (Cashman & Ashley, 2008), i.e. US dollar 300,000 million annually (OECD, 2012). A quarter of Swiss sewers need renovation, and the depreciation of public water infrastructures in the next 40 years amounts to CHF 81 billion (Maurer & Lienert, 2014). So it is almost inevitable that Swiss society will have to make major investments.

MCDA relies on eliciting preferences from stakeholders
                        1
                     
                     
                        1
                        For the sake of conciseness, we use “stakeholder” synonymously to “decision maker”.
                      (e.g. Belton & Stewart, 2003; Eisenführ, Weber, & Langer, 2010; Keeney & Raiffa, 1976). Environmental decisions affect many people, are usually financed by public money, are often long-term and may thus include the interests of future generations, and cannot easily be reversed. There are often very different views about the decision, resulting in conflicts of interest (Gregory et al., 2012). It thus seems especially important for environmental decisions that the elicited parameters reflect the preferences of different stakeholders and the public and that these are stable over time and persist beyond a single situation (Gregory et al., 2012, p. 210), especially if they affect long time ranges. However, empirical evidence suggests that human beings easily become biased during preference elicitation (for reviews see, for example, Hämäläinen, Luoma, & Saarinen, 2013; Montibeller & von Winterfeldt, 2015; Morton & Fasolo, 2009; Payne, Bettman, & Johnson, 1992; Slovic, 1995). The behavioral aspects of environmental decision-making have hitherto received comparatively less attention (Hajkowicz, 2012; Hämäläinen, 2015). For example, strategic bias (for references see Hämäläinen, 2015) is a major concern in public policy and environmental choices where real or perceived conflicts of interest often exist, but this has only recently been tested in an environmental context (Hajkowicz, 2012). To overcome strategic bias, it would seem important to include the views of different kinds of stakeholders, including the broader public (Gregory et al., 2012). Another specific example is groupthink (Janis, 1972), which can be an important bias in environmental modeling and decision-making, but the literature on this is very limited (Hämäläinen, 2015). A recent textbook about environmental management choices mentions important biases such as representativeness, availability, sunk costs, anchoring, overconfidence and motivation (Gregory et al., 2012, pp. 29–31). The main message about how to deal with biases is to be aware of them, to use multiple framings to guard against affective responses and to use multiple elicitation methods (Gregory et al., 2012, p. 212). Additionally, we suggest that further research is required. The MCDA behavioral literature also lacks work on preference stability over time. For instance, we found no reference to preference stability in recent reviews of behavioral operations research (BOR; Hämäläinen et al., 2013; Montibeller & von Winterfeldt, 2015).

Against this background, the main aim of this paper is to explore the extent to which a multi-method and repeated elicitation approach that includes different stakeholders can contribute to increasing the confidence in the results of a decision-making process. Using the example of Swiss water infrastructures, we investigate whether the results of an earlier MCDA are supported by a larger public prepared to finance these investments, independently of the elicitation method and a time gap of one month. In an earlier project, namely SWIP (“Sustainable Water Infrastructure Planning”
                        2
                     
                     
                        2
                        
                           http://www.eawag.ch/en/department/sww/projects/sustainable-water-infrastructure-planning-swip/.
                     ), we interviewed twenty stakeholders using the popular SWING method (e.g. Eisenführ et al., 2010; Scholten et al., 2015), or a variant of SMART/SWING (Mustajoki, Hämäläinen, & Salo, 2005; Zheng et al., 2016; the latter was perceived as less difficult, supporting the claim of Mustajoki et al. (2005). In the two interview series, we found patterns for the weights of objectives and we could identify viable wastewater and water supply alternatives, despite a very large uncertainty of predictions, stakeholder preferences and four future socioeconomic scenarios (Scholten et al., 2015; Zheng et al., 2016). In this paper, we build on this earlier experience and address the following research questions:
                        
                           •
                           Are preferences stable over time?

Do SWING and the SMART/SWING variant differ in the elicited weights and in preference stability?

What are possible explanations for the observed weight profiles or preference instability when elicitation is repeated? As examples, are demographic variables or environmentally friendly behavior possible explanatory factors? Do experts who have more knowledge or experience in the domain of wastewater have different and/or more stable preferences than lay people? Do preferences change if new knowledge is obtained between the first and second survey? Is preference stability related to the perceived difficulty of the questionnaire?

If preferences do differ, does this affect the ranking of alternatives in an MCDA?

Finally, if substantial public decisions are made, agreement between stakeholders and the public is important. In our water infrastructure case, do preferences about the importance of objectives from earlier stakeholder interviews agree with the preferences of the general public and of experts in the water field?

To tackle these questions, we conducted a public online survey in an experimental set-up with a simplified objectives hierarchy. Online tools overcome the problems of personal interviews by allowing access to the larger sample needed for the experimental research proposed here. They also speed up elicitation and allow involving the public. On the other hand, their validity and biases have been criticized (e.g. Marttunen & Hämäläinen, 2008). The use of online tools thus requires careful design and cross-validation with results from other elicitation methods.

The remaining paper is organized as follows: In Section 2 we review the literature on preference stability. Our research approach and statistical tests are presented in Section 3. The results are presented in Section 4 and discussed in Section 5. We summarize our main conclusions in Section 6.

@&#LITERATURE REVIEW@&#

Preference elicitation research has focused strongly on weights, e.g. the splitting bias and range insensitivity (Hämäläinen & Alaja, 2008; Jacobi & Hobbs, 2007; Montibeller & von Winterfeldt, 2015; Morton & Fasolo, 2009; Weber & Borcherding, 1993). For instance, there is compelling evidence that different elicitation methods can lead to divergent results (Borcherding, Eppel, & Von Winterfeldt, 1991; Bottomley & Doyle, 2001; Bottomley, Doyle, & Green, 2000; Mustajoki et al., 2005; Pöyhönen & Hämäläinen, 2001; Van Ittersum, Pennings, Wansink, & Van Trijp, 2007). To illustrate this, AHP is known to cause a larger spread of weights compared to other methods (Pöyhönen & Hämäläinen, 2001; and references therein), and AHP weights can be unevenly dispersed over the weight range (Belton, 1986). Whilst the impact of elicitation methods on weights is undisputed, there does not yet seem to be any consensus about the most valid method.

On the basis of a review, Van Ittersum et al. (2007) explain the apparent lack of validity of elicitation methods by setting up the proposition that different methods measure different dimensions of the importance of an attribute or objective, namely its (a) salience, (b) relevance, and (c) determinance (see Table 1 in Van Ittersum et al. 2007). (a) Methods such as free elicitation, which are not common in MCDA, do not include information about objectives and thus measure only their salience. (b) Methods such as direct rating, direct ranking, point allocation, and AHP contain specific information about objectives, excluding their level. They measure the relevance of the objectives on the basis of the values and desires of the decision-makers. (c) Additional information about the level and ranges of each objective helps decision-makers to determine the importance of an objective in addition to personal values. Trade-off, SWING, and conjoint methods are forms of elicitation that measure determinance. The proposition of Van Ittersum et al. (2007) is strongly supported by their meta-analysis (see Table 2 in Van Ittersum et al. 2007) as well as by an Internet survey that compared direct point allocation, SWING, trade-off, AHP, and SMART weight elicitation (Pöyhönen & Hämäläinen, 2001). This study showed that the weights elicited with the first three methods belonging to group (c) were similar, while SMART and AHP from group (b) tended to generate divergent weights. Despite compelling evidence for the multi-dimensionality of attribute importance, this concept does not explain all the inconsistencies between these methods (see Table 3 in Van Ittersum et al., 2007). The problem of possible inconsistencies between methods measuring the same dimension therefore remains unresolved.

As for any method, it is desirable that MCDA methods are reliable. One characterization of “good elicitation methods” could be that they produce stable weights – across methods and over time. On the other hand, it seems that preferences can easily change due to various influencing factors, and a large body of psychological studies indicates that preferences are affected by the elicitation method and its framing. Thus, some decision analysts postulate that preferences are constructed during elicitation (e.g. Belton & Stewart, 2003; Gregory, Lichtenstein, & Slovic, 1993; Hoeffler & Ariely, 1999; Payne et al., 1992; Slovic, 1995). In contrast, some economists presume that existing preferences can be captured by means of willingness to pay (WTP) surveys, for example.

Economic approaches have a lot in common with MCDA: in environmental decisions, both aim to derive valuations from respondents for an environmental good such as clean air or water quality. Economists model preferences by means of a random utility model (McFadden, 1974). As in MCDA, attribute-based economic methods such as choice experiments describe a decision situation as a function of its attributes (Brouwer, Dekker, Rolfe, & Windle, 2010). For a critical comparison of MCDA with economic valuation, see Gregory et al. (1993). For both schools, it is important to disentangle the various factors that may influence preference stability. Our own review suggests a striking lack of MCDA behavioral research on preference stability over time; this is also indicated by its complete absence in recent BOR reviews (Hämäläinen et al., 2013; Montibeller & von Winterfeldt, 2015). Therefore, we also draw on the economics literature on preference stability. A summary of our literature review is given in the Supplementary material (Section SI-3.1).

Economists have carried out various experiments that address reliability: “Reliability deals with whether a survey item measures something other than random noise” (Jorgensen, Syme, Smith, & Bishop, 2004; also see Liebe, Meyerhoff, & Hartje, 2012; and references therein). To investigate the reliability of contingent valuation (CV) surveys and choice experiments (CE), the test retest method is often used. It measures correlations between, for instance, responses to CV questions at two or more different points in time, or correlations between the mean WTP of the surveyed population. In general, CV and CE surveys applied to environmental valuations, consumer research or healthcare indicate a fair to substantial test retest reliability (see review sections by Jorgensen et al., 2004 and Liebe et al., 2012). For instance, preferences about river restoration in Austria (Bliem, Getzner, & Rodiga-Lassnig, 2012) or water scarcity in Australia (Brouwer et al., 2010) did not change after one year. However, preferences can also be unstable: thus the WTP for natural areas in Flanders changed between two surveys one year apart (Schaafsma, Brouwer, Liekens, & De Nocker, 2014).

There is some evidence that the parameter estimates of Expected Utility Theory (EUT; i.e. risk attitudes) are stable over time (reviewed in Zeisberger, Vrecko, & Langer, 2012), but a comparison of methods seems to produce large biases and noise (Hey, Morone, & Schmidt, 2009). However, our work in this paper does not focus on uncertainty and EUT. Rather, we are interested in determining possible reasons for preference changes under conditions of certainty.

Preferences can be influenced by personal and demographic factors such as gender, age, income or environmental attitude. Gender, as an example, was a good predictor to explain preference stability between two CE surveys of out-of-hours healthcare for children (San Miguel, Ryan, & Scott, 2002). However, preference stability over time does not usually seem to be correlated with socio-demographic factors. For instance, in two CE surveys of environmental amenities, preference changes after one year were not significantly correlated with the financial situation of the respondents and other socio-demographic variables (Liebe et al., 2012; Schaafsma et al., 2014). Similarly, although some studies did find a significant correlation between the WTP and explanatory variables, namely gender, income or environmental interest (e.g. Bliem et al., 2012; Brouwer, 2006), these differences were not related to preference stability over time.

The existing knowledge or past experience of respondents could affect preference stability. Weight elicitation experiments with students indicate that knowing more about buying a car, in this example, increases preference stability over time (Bottomley & Doyle, 2001; Bottomley et al., 2000). A possible explanation is that experts are liable to hold stronger opinions, which are more stable, whereas novices have to construct their preferences during the experiment. Although this construction process may lead to the observed response variation in repeated experiments (Hoeffler & Ariely, 1999; San Miguel et al., 2002), this hypothesis was not confirmed in real world experiments. For instance, previous experience with out-of-hours healthcare before the experiment did not affect preference stability about healthcare management plans when the survey was repeated after two months (San Miguel et al., 2002). Likewise, in a CE about health risks related to poor bathing-water quality in Dutch rivers, preference stability did not differ if respondents had more knowledge about existing quality standards for bathing water (European blue flag system), or if they had more experience than others, such as by swimming more often in open water or becoming ill after swimming (Brouwer, 2006).

There is fairly good evidence to suggest that learning may affect preference formation. For instance, an in-depth experiment over several months concluded that the subjects' preferences are influenced by learning processes (Hanley, Iwata, & Roscoe, 2006). Repeated choice experiments (CE) about purchasing decisions indicate that consumers construct their choices when entering a new domain, while preferences stabilize with growing experience (Hoeffler & Ariely, 1999). A population survey in Australia also concluded that learning occurred: respondents were increasingly more certain about their WTP to reduce water scarcity problems in five repeated CE (Brouwer et al., 2010). However, the initial hypothesis that preferences would be refined due to this learning effect had to be rejected, since preferences remained stable across the experiment. A small learning effect was also found in a CE about onshore wind power in Germany (Liebe et al., 2012). In this case, preferences were only moderately stable, with 59 percent of the choices being identical in the second survey after 11 months.

Over a time span of several months, one might expect external events to affect preferences, but this hypothesis finds scarce support in the literature. The financial crisis did not lead to any changes in the respondents' WTP for onshore wind-power generation (Liebe et al., 2012). Similarly, preferences concerning the WTP to reduce health risks associated with poor bathing water quality were stable despite a very unusual external event (Brouwer, 2006): Here, an earlier CE was repeated after six months, and by chance this coincided with the hot summer of 2003, when extremely high water temperatures had caused the closure of bathing sites in Dutch rivers.

People who have strong preferences might be expected to have more stable preferences than those whose weights are more evenly distributed. We found only one experiment in the literature concerning out-of-hours healthcare which confirmed this idea (San Miguel et al., 2002). One explanation could be that people with a more extreme weight distribution may see the respective objectives as more salient, thus increasing the stability of their weight profiles (based on Hoeffler & Ariely, 1999). This hypothesis could be important for MCDA because there seems to be a tendency for people to distribute equal weights; this phenomenon is also known as an “equalizing bias” (Montibeller & von Winterfeldt, 2015).

Another reason for preference stability to differ between methods could be related to their difficulty. An experiment on consumer decisions revealed that preferences were more stable if choices were hard than if they were easy (Hoeffler & Ariely, 1999). The authors postulate that difficult and repeated trade-offs might help people to consolidate their preferences during their decision-making process, thus making them more stable. We found only two other studies that mention the difficulty of the elicitation process, but they did not discuss it: preference stability concerning out-of hours healthcare after two months was significantly correlated with the perceived difficulty of a CE questionnaire (San Miguel et al., 2002). Similarly, a CV experiment concerning the bathing-water quality of rivers showed a significant negative correlation between increasing perceived difficulty of the questionnaire and preference stability (Brouwer, 2006).

Our review reveals a lack of MCDA behavioral research about preference stability, although some lessons can be learnt from the economics literature. Clearly, different elicitation methods can produce different weights, possibly because they measure different dimensions of the importance of an objective (Van Ittersum et al., 2007). Correlations between preferences from the first and second surveys were often ‘moderate’ (linguistic classification of coefficients in Liebe et al., 2012): the strength of agreement between the two tests was usually >0.6 (Bottomley & Doyle, 2001; San Miguel et al., 2002), and >0.5 in 14 of 17 reviewed papers (Jorgensen et al., 2004, Section SI-3.1). The observed preference changes could be related to socio-demographic factors, past experience, knowledge and expertise about a topic, learning, the occurrence of externals events, the strength of preferences and the difficulty of the elicitation method. However, the literature on these topics is scant and the evidence even contradictory.

Finally, we wish to point out that even if individuals do change their preferences over time, this might not affect aggregated samples and the results of a population survey. This factor is discussed in the literature, which usually showed no significant differences between aggregated sample averages, despite large individual variability (Bottomley & Doyle, 2013; Jorgensen et al., 2004; Liebe et al., 2012; Zeisberger et al., 2012).

A main aim of our research in this paper is to determine the preference stability for two typical MCDA weight elicitation procedures and find possible explanations for the observed weight profiles or preference instability when elicitation is repeated (see research questions in Section 1). In this way, we hope to contribute to closing some knowledge gaps in Behavioral Operational Research and to point out directions of future research.

@&#MATERIAL AND METHODS@&#

We used two questionnaires, each in English and German, one for SWING and one for the SMART/SWING variant (Section 3.2 (B)). A market research company (www.respondi.com.de) recruited respondents from the German-speaking part of Switzerland. They belong to a pool of people that have an agreement with the company to answer surveys on a regular basis. An e-mail invitation was sent to 708 people in July 2013 and 4–5 reminders followed within one week. All 807 Eawag
                           3
                        
                        
                           3
                           Swiss Federal Institute of Aquatic Science and Technology.
                         employees were also invited by e-mail with a reminder after one week. Respondents were randomly assigned to one of the two methods. One month later, those who had fully completed the first questionnaire received a second one with the same elicitation method.

The first questionnaire consisted of four sections.
                           
                              (A)
                              Knowledge and experience about wastewater designed to determine whether experts have different and/or more stable preferences than lay people: to determine their knowledge, we asked ten multiple choice questions, one for each sub-objective (Fig. 1
                                 ), based on a survey (Veronesi, Chawla, Maurer, & Lienert, 2014). For instance: “after how many years do sewer pipes need to be replaced?” or “what type of disease is typically related to direct contact with wastewater?” (full list see Supplementary material, Table SI-1). Each fully correct answer received one point. We asked respondents to specify their experience with seven wastewater topics, either as their own direct experience (two points), or information from mass media/ other sources (e.g. flooding of cellars with wastewater; Table SI-2). The highest score was 3×7=21.

Main section to elicit preferences: we used a simplified objectives hierarchy from the SWIP project mentioned earlier (Lienert, Scholten, Egger, & Maurer, 2015). Elicitation started from the bottom-up by comparing five pairs of sub-objectives followed by five main objectives (Fig. 1). The participants thus understood these after having considered the sub-objectives. To avoid a splitting bias, we used two sub-objectives for each main objective and kept the same design and text length (Schuwirth, Reichert, & Lienert, 2012, Box SI-1–8). We described the best and worst possible case of each sub-objective and the status quo (Fig. 1, Table SI-3). We used data from a real case near Zürich (Lienert et al., 2015).

SWING weights were elicited with the standard procedure (e.g. Eisenführ et al., 2010). Our visualization with emoticons and questionnaires are given in the Supplementary material (Section SI-1.1.3; Box SI-1, SI-2, SI-5), and one example in Appendix A. For the SMART/SWING variant, each respondent defines a most familiar reference objective (Box SI-6; for two sub-objectives no reference was needed), to which all other objectives are compared pairwise (Mustajoki et al., 2005). Respondents then improve the more important (sub-)objective from the worst to the best state, given the ranges of the case study (or choose both as equally important; Box SI-3, SI-7). They then define the strength of importance with a slider (Box SI-4, SI-8). In our variant, respondents used a slider and assigned 10 points to the least important objective, the reference or the other; in SMART/SWING, the reference received 10 points. We also gave verbal cues (Huizingh & Vrolijk, 1997) from 10 to 90 (Box SI-4, SI-8). The scores were normalized (Appendix B).

Feedback questions designed to assess the perceived difficulty of each method: four of five questions were identical for both methods: e.g. “How difficult was it overall to answer the questions?”, “How strongly did you consider the worst possible and best possible state of the two objectives (…)?”, and reasons for the difficulties (Table SI-9). Some questions were method-specific; e.g. “How difficult was it to”: “(…) rank alternatives with objectives you want to improve (SWING)?”

Explanatory variables as possible additional explanations for different preferences among respondents and for preference stability: the questions concerned demographic variables (gender, having children, nationality, age, education), environmentally friendly behavior (public transport, financial contribution to environmental organizations, the respondent's relation to water (living nearby, frequency of visits), and mood (Table SI-4).

The second survey questionnaires were much shorter: we did not repeat the knowledge and feedback questions, but asked about new knowledge or experience between surveys (Table SI-10; “new experience”). The order of sub-objectives was randomized.

Although we randomly assigned the respondents to one of the two methods, a systematic pattern may emerge purely by chance, such as significantly more men being assigned to SWING. We therefore tested for differences in descriptive variables between different groups, e.g. respondents assigned to SWING or the SMART/SWING variant, from the public or Eawag, or those who completed only the first or both surveys. We compared categorical variables with Chi-square tests and ordinal/continuous variables with t-tests (or used the Mann Whitney and Wilcoxon signed rank test if our assumptions were not met; Field, 2009). We used ANOVA for >2 categories and regression for continuous variables (age, knowledge, experience). Additionally, we tested the effect of explanatory variables with hierarchical regression. Hereby, to control for method, knowledge, experience, and new experience were entered first, while the method was entered in the second block. Statistical analyses were conducted with SPSS 21.0. The MCDA was implemented in R (www.R-project.org) using the R package utility (Reichert, Schuwirth, & Langhans, 2013).

Using weights from the first survey, we searched for patterns in the importance of the (sub-)objectives within given range intervals. We then analyzed if there was a difference between SWING and the SMART/SWING variant, if preferences changed over one month, and if so, why. Specifically, we explored:
                           
                              1.
                              Differences in weights elicited with SWING and the SMART/SWING variant.

Differences in stability of weights over time, i.e. between the first and second surveys.

The effect of explanatory variables on weight stability in a regression model: method, knowledge, experience, age, education, and new experience gained between the first and second surveys.

The influence of strong preferences on weight stability, i.e. difference between respondents who gave extreme weights and those with a more even weight distribution.

The perceived difficulty of the SWING questionnaire and the SMART/SWING variant.

We tested for differences between the two methods (point 1) and the two surveys over time (point 2) with t-tests, and analyzed the effect of explanatory variables with hierarchical regression (point 3; see Section 3.3). We needed indicators to test the weight stability (point 2) and used the following four: (i) proportion of rank reversals of sub- and (ii) main objectives; (iii) sum of absolute differences (SAD) in weights of sub- and (iv) main objectives. (i) A rank reversal occurred if the first ranked sub-objective (of two/ main objective) in the first survey ranked second or equal in the second survey, or if an indifference statement was changed to a preference. The proportion of rank reversals for (i) and (ii) ranged from 0 (no rank reversals) to 1 (Appendix C). For the SAD (iii, iv), we summed up all changes in the normalized weights between the first and second surveys (Bottomley et al., 2000). SAD ranged from 0 (no change in weights of (iii) sub-objectives) to 10; and 0 (no change for (iv) main objectives) to 5, respectively.

We addressed strong preferences (point 4) on the basis of a line of reasoning (Supplementary material, Section SI-2.6.4). We tested for correlations between the linear coefficients of the weight vectors in the first survey and the stability indicators (rank reversals, SAD). We then clustered the respondents into “extreme weights” vs. more “even weight distribution” (using thresholds
                           4
                        
                        
                           4
                           For example, we compared respondents who had a weight of 0.4 or higher for one main objective in the first or second survey vs. those with weights <0.4 for all main objectives in both surveys. We chose 0.4, because very few people had weights >0.4 for SWING; for the SMART/SWING variant, very few had <0.4.
                        ) and tested the preference stability for SWING or the SMART/SWING variant. The questions used to assess the perceived difficulty (point 5) are given in Section 3.2.

We analyzed whether differences in weights affect the resulting MCDA values and ranks of alternatives. For this, we used the MCDA model and data from a real wastewater infrastructure case near Zürich that was part of the SWIP project. The case study was analyzed in much more detail than is possible in this paper, and included a combination of MCDA with scenario analysis as well as careful uncertainty modeling and sensitivity analyses. The application to the wastewater infrastructure system and more details concerning the MCDA method applied here are given in Zheng et al. (2016). A separate application to the water supply system is published in Scholten et al. (2015). Here, we chose a subset of six wastewater alternatives that differ in their wastewater, drainage and stormwater systems, rehabilitation strategies and organizational structures (Appendix D; same alternative labels as Zheng et al., 2016. These alternatives cover the extremes and provided the most discriminating results in the earlier MCDA. They include variants of the current system with large central wastewater treatment plants: the status quo with improved rain infiltration implemented over time (A8a; Appendix D); a technically advanced variant to remove micropollutants with good pipe maintenance (A2); a central cheaper system with poor maintenance (A9). The fully decentralized systems cover an inexpensive low-tech system (A5), a high-tech system with good environmental performance (A7) and a semi-decentralized system between the extremes (A8b).

For each of the selected six alternatives, we predicted how well they achieve the ten sub-objectives (the average attribute levels are given in Table SI-3; we also give the best and worst possible states). We used the additive value model with linear value functions for every attribute (Keeney & Raiffa, 1976). The MCDA was performed for the group of respondents using SWING in the first (N
                        =158) or second survey (N
                        =94); or the SMART/SWING variant (N
                        =156 first; N
                        =106 second survey) respectively. For each group, we implemented the average weight over all individuals. We additionally tested whether the best alternative was consistent between the first and second survey for each individual respondent. For this purpose, we introduced their personal weights into the MCDA and used Kendall's tau (1938) correlation coefficient to analyze the ranking of the alternatives.
                        
                     

@&#RESULTS@&#

We sent 708 invitations to the public and 807 to Eawag (for details, see Section SI-2.1). Our sample (Table 1) corresponds well with the average Swiss population (see Section SI-2.2). To ensure unbiased sampling, we compared the demographic and descriptive variables of the groups; we found very few statistically significant differences (see Section SI-2.4).

In
                         the overall sample, the single objectives ‘protection of water and other resources’ and ‘safe wastewater disposal’ received higher weights than ‘intergenerational equity’, ‘high social acceptance’, and ‘low costs’, alone (Table 2; also see Discussion Fig. 3), on the basis of the case study ranges. The sub-objectives followed a similar pattern.

The public (N
                           =249) and Eawag respondents (N
                           =65) had similar weights, with two exceptions in the first survey. Eawag gave statistically significantly higher weights to ‘protection of water and other resources' (0.33, t
                           =−4.2, p
                           <.001) and lower ones to ‘low costs’ (0.11, t
                           =4.4, p
                           <.001) than the public (0.25 and 0.17 respectively; Fig. 2, Table SI-17). The corresponding sub-objectives ‘good chemical state’ and ‘no infections’ received higher weights from Eawag, whereas ‘drainage capacity’, ‘annual costs’, and ‘cost increase’ were weighted more highly by the public. Follow-up tests indicated that the higher weights for Eawag were not related to their higher education level (Section SI-2.5.1).

Gender and age were not statistically significantly correlated with the weights (Tables SI-21 and SI-23). Parents gave slightly higher weights to ‘intergenerational equity’ than those without children (Table SI-21; Fig. SI-1). University graduates gave statistically significantly higher weights to ‘protecting water and other resources’ than people from vocational schools (Tables SI-24–25; Fig. SI-2). However, university graduates gave lower weights to ‘costs’ (Table SI-26). The frequency of visiting or living near water, using public transport for ecological reasons and knowledge were not related to the weights. People who often give financial support to environmental organizations gave statistically significantly higher weights to ‘protecting water and other resources’ (Tables SI-27–28; Fig. SI-3).

‘Protection of water and other resources’ had higher weights if respondents used the SMART/SWING variant (weight 0.30, SWING: 0.24; t
                           =−3.1, p
                           =.002; Table SI-34; Fig. SI-5); SWING respondents gave higher weights to ‘social acceptance’ (0.15vs. 0.12; t
                           =2.54, p
                           =.012). Users of the SMART/SWING variant had higher rank reversals and SAD for the sub- and main objectives (Table 3).


                           
                              
                                 (i)
                                 Difference between two elicitation methods

The hierarchical regression analysis showed a statistically significant to highly significant difference between the two elicitation methods for all four measures of preference stability (Table 3). In three of four cases, the correlation between method and the rank reversals or SAD's of the weights was considerably greater than for the other explanatory variables (Beta=0.39, 0.48, .49; exception: rank reversal of sub-objectives, Beta=.14; Table 4
                           ).
                              
                                 (ii)
                                 Knowledge and experience

Knowledge was statistically significant to highly significant for three of four predictors of preference stability (not for SAD main objectives; Table 4; see similar results for only the public, who had less knowledge than Eawag; Table SI-35). As knowledge increased, so did preference stability. Experience was not correlated with preference stability.
                              
                                 (iii)
                                 Age and education

Age was statistically significantly positively correlated with three of four measures of preference stability (not for RR of sub-objectives): older respondents had less stable preferences (Table 4; SI-35). There was a statistically significant interaction between method and age (Fig. SI-6) for the SAD of sub-objectives: SAD increased with increasing age for the SMART/SWING variant, but remained almost stable for SWING. There was no relationship between preference stability and education.
                              
                                 (iv)
                                 New experience

The hierarchical regression analysis gives no clear indication as to whether new experience between the first and second survey is an important explanatory variable: there was a statistically significant effect for both preference stability measures for the main objectives (Table 4); respondents with more new experience had less stable preferences. However, the Beta value was relatively low, and the effect was only marginally significant for the sub-objectives. There was a marginally significant interaction between method and new experience: as new experience increased, SAD of sub-objectives also increased for the SMART/SWING variant, but not for SWING (Fig. SI-7).
                              
                                 (v)
                                 Influence of strong preferences

In the regression analysis of the main objectives, there was no difference in rank reversals for respondents with more extreme or balanced weights. For SAD, more extreme weights were correlated with less stable preferences (i.e. statistically significant higher SAD; Table 5
                           ).

The cluster analysis produced the same result: there was no difference in the proportion of rank reversals for respondents with more extreme preferences compared to those with a more even weight distribution. However, those with more extreme weights had statistically significantly less stable SAD compared with those with a more even weight distribution. For reasons of space, we present the details in the Supplementary Material (Tables SI-37–41).

A quarter of all respondents ticked that the questionnaire was “somewhat easy”, “easy”, or “very easy”, almost 70 percent of whom used the SMART/SWING variant (Tables 6; SI-9–10; Fig. SI-4). Although 34 percent of those using the SMART/SWING variant perceived it as easy (sum of above classes), the Fig. was only 15 percent for SWING. Consequently, SWING differed highly significantly from the SMART/SWING variant in statistical terms: SWING had an average rating of 4.1, but the SMART/SWING variant only 3.7 (Table SI-31; same both surveys: Table SI-32), on a scale of 1 to 6, where 3= “somewhat easy”, and 4= “somewhat difficult” (Table 6
                           ). We point out that the differences between 4.1 for SWING and 3.7 for the SMART/SWING variant may seem small, but this is a result of averaging the six classes. Differences within classes were substantial, for instance, only 5 percent of people using SWING ticked “somewhat easy”, but the Fig. was 14 percent for those using the SMART/SWING variant (Table 6). SWING users felt statistically significantly less certain about their answers, but again with small average differences between groups (e.g. 3.3vs. 3.5 on a scale of 1 to 5; where 3= “fairly unsure” and 4= “highly sure”; Table 6; SI-9; SI-31). There were no differences between methods for considering ranges of objectives (Tables SI-31–32).

As shown above, we found clear differences between weights of SWING and the SMART/SWING variant (Section 4.3). We then analyzed whether the different weights affect the total value and ranking of the six wastewater alternatives. For this purpose, we first introduced the average weights of groups into an MCDA model. Despite significant differences in weights, there were no statistically significant differences between the resulting MCDA values of the alternatives for the following groups: first survey SWING vs. SMART/SWING variant; first vs. second survey with SWING; and first vs. second survey with SMART/SWING variant (Table 7
                        ). One exception was a minor difference for alternative A5 in the second survey of SWING vs. SMART/SWING variant. Consequently, the ranking of alternatives was identical for all four groups using average group weights. The best-ranked alternative was A7 in all cases. We compared these result with weights obtained from ten stakeholders in face-to-face interviews (Zheng et al., 2016; calculation of predictions of alternatives, see there). To calculate the average values reported in Table 7 (last column), we introduced the weights of the ten stakeholders into the same MCDA model as the online surveys (i.e. linear marginal value functions, additive aggregation, no uncertainty). Again, alternative A7 ranked best.

We then analyzed for each individual whether changes in weights between the first and second survey would lead to different rankings of alternatives. For this purpose, we correlated the rankings of the alternatives in the first and second surveys. The average Kendall's tau correlation coefficient of rankings between the two surveys was 0.625 (SD=0.325). We found a difference between the two weight elicitation methods. Using SWING, the best ranked alternative was always A7 in the first and the second surveys after one month, with one exception, i.e. 93 of 94 people had A7 on the first rank (=99 percent; see Discussion Fig. 4; Table SI-42). The rankings of alternatives were less stable for respondents using the SMART/SWING variant: only for 78 percent (83 of 106) of them did the same alternative rank best in the second survey, as in the first. This was reflected in the average correlations between the two surveys using the weights for individuals, where SWING yielded statistically significantly more consistent rankings (Kendall's tau: 0.730) than the SMART/SWING variant (Kendall's tau: 0.532; N
                        =200, t
                        =4.612, p
                        <.001; Fig. SI-10). But alternative A7 also performed best for the majority of individuals using the SMART/SWING variant: in the first survey, A7 was best in 87 percent of cases, and for 88 percent of the respondents in the second survey. Second best was alternative A2, namely for 10 percent of respondents in the first survey, and 8 percent in the second (Fig. 4; Table SI-42).

@&#DISCUSSION@&#

The use of different elicitation procedures and inclusion of different stakeholder groups can increase our confidence in the resulting weight profiles. We carried out an online survey among the Swiss public and water experts using two different weight elicitation methods and repeated the survey with both methods after one month. The weight distributions of the five main objectives (Table 2) important for good wastewater infrastructures were very similar to earlier face to face interviews with 20 Swiss stakeholders (Scholten et al., 2015; Zheng et al., 2016): in all studies, ‘protection of water and other resources’ and ‘safe wastewater disposal’ (resp. ‘good water supply’) received higher weights than ‘intergenerational equity’, ‘high social acceptance’, and ‘low costs’ (Fig. 3
                           ). The same attribute ranges were used in all studies. Current wastewater costs in the study region near Zürich are CHF 289 per person and year (Table SI-3). Allowing pipes to deteriorate (alternative A9; Appendix D), for example, would reduce costs to CHF 157 per year, but on average people did not find this substantial enough to trade off against the negative effects. The importance of water protection is in line with Swiss Willingness to Pay (WTP) studies (Logar, Brouwer, Maurer, & Ort, 2014; Veronesi et al., 2014).

We found very few statistically significant relationships between the weights for the objectives and demographic or other explanatory variables (Section 4.2.2). We generally conclude that the average weights of the five main objectives are similar, both when they were elicited in personal interviews and online. However, we found a difference in weights with respect to the elicitation method (Sections 5.1.2 and 5.2) and therefore also verified whether this would change the outcome of a real decision (Section 5.3).

AHP is criticized due to inconsistencies (Belton, 1986; Salo & Hämäläinen, 1997) and a larger spread of weights (Pöyhönen & Hämäläinen, 2001; and references therein). Our result of a larger spread of weights for respondents using the SMART/SWING variant agrees with this, because we used an AHP-type verbal scale (Huizingh & Vrolijk, 1997) only in the SMART/SWING variant questionnaire, but not in SWING. Firstly, we found that respondents using the SMART/SWING variant gave statistically significantly higher weights of 0.30 to the most important objective of ‘protection of water and other resources’, while SWING respondents gave only 0.24 (Section 4.3.1). At the same time, users of the SMART/SWING variant gave statistically significantly lower weights to the least important objective of ‘high social acceptance’ (0.12), while SWING respondents gave this a slightly higher weight (0.15). Secondly, we tested whether the SMART/SWING variant leads to a higher spread of weights than SWING by sorting each respondent's weights from highest to lowest and regressing the weights on the ranking to capture the linear component (as in Bottomley et al., 2000; Doyle, Green, & Bottomley, 1997; details see Section SI-2.6.4). Again, we found some statistically significant support that the SMART/SWING variant leads to a larger spread of weights. However, these results are not quite evident because weight differences between the two methods seem small. Moreover, one would expect an effect for the second important objective, which was not the case. It thus remains unclear whether the SMART/SWING variant would also lead to a larger spread of weights in other applications.

The literature provides ample evidence that different elicitation methods produce different weights or risk attitudes (see review Section 2). We found that the method can also affect preference stability over time, which follows the scarce literature (Bottomley & Doyle, 2001, 2013; Bottomley et al., 2000): respondents using the SMART/SWING variant had statistically significantly more changes in weights in the second survey for all four preference stability indicators (Section 4.3.1). The average rank reversal of weights was 51 percent (sub-objectives) and 50 percent (main objectives) for the SMART/SWING variant, but only 43 percent (sub-objectives) and 32 percent (main objectives) with SWING (Table 3). Thus there was a substantial difference between the two methods for the proportion of rank reversals. The weight difference, which we measured as Sum of Absolute Differences (SAD) were smaller, e.g. for main objectives 0.38 (SWING) vs. 0.77 (SMART/SWING variant) on a scale of 0 (no difference first/ second survey) to 5. In combination, our results for the SAD and rank reversals suggest a systematic difference between methods that cannot be explained by any other variable such as demography, since the two groups did not differ in this respect. The questionnaire was also very similar for both methods apart from the fact that an AHP rating scale was used in the SMART/SWING variant. We discussed above that the AHP rating scale might cause a larger spread of weights: this can affect the weighting, which we measured as SAD. The reason is that the important objectives receive a higher weight if there is a larger spread of weights. Therefore, any differences in weights between the first and second survey will also be larger, i.e. the SAD will be larger for the SMART/SWING variant. This in turn leads to the conclusion that the SMART/SWING variant has a lower preference stability compared to SWING if we rely only on the SAD as an indicator. However, the AHP scale cannot account for more rank reversals. Even if there is a larger spread of weights for the SMART/SWING variant, the ranking of objectives remains the same. There should be no difference for the indicator rank reversals when comparing SWING with the SMART/SWING variant if the AHP rating scale and associated larger spread of weights is the only difference between the methods. We must therefore look for additional reasons that could explain the difference in preference stability between SWING and the SMART/SWING variant.

According to the literature, it is difficult to foresee which variables best explain the stability of preferences. Some variables were only rarely correlated with preference stability, including socio-demographic variables and past experience, knowledge and expertise (see literature review in Section 2). Although learning processes are known to influence preferences, there is so far no evidence in the scant literature that new experience or the occurrence of external events affect preference stability. However, learning and new experience are important for environmental decision-making because they can be influenced by information from political actors or environmental agencies. Learning about environmental problems may help respondents to form more stable preferences that endure beyond a single situation, as requested by Gregory et al. (2012).

In our surveys, none of the above explanatory factors were most important to explain preference stability: we used hierarchical regression to test the influence of explanatory variables on the observed weight profiles over time. Quite clearly, the weight elicitation method was the most important predictor of preference stability (Section 4.3.2; Table 4). Additionally, preferences were more stable if respondents had more wastewater knowledge for three of four preference stability indicators and seemed to be less stable for older respondents. Experience and education were completely unrelated to preference stability. Our results are inconclusive for new experience gained between surveys. Although it makes intuitive sense that people update their judgment if they receive new information, we found a statistically significant effect only for the main objectives.

Moreover, there could be a difference in preference stability over time if people have strong preferences compared with those with a more even weight distribution (Hoeffler & Ariely, 1999; San Miguel et al., 2002). Our current results are inconclusive when we tested respondents with more extreme weight profiles versus those with a more balanced weight distribution. We found no difference in the preference stability for our indicator rank reversals. However, for the indicator SAD, respondents with stronger preferences had statistically significantly less stable weights compared to those with a more equal
                              5
                              Reading example: The central high-tech alternative A2 (far left) ranked first (dark grey bars) for 0 percent, 0.6 percent, and 10 percent of the respondents in the interviews, SWING and SMART/SWING variant respectively. It ranked second (light grey bars) for 30 percent, 68 percent, and 46 percent respectively (same order), and third (diagonal bars) for 10 percent in all three methods (for details, see Table SI-42).
                            weight distribution (Section 4.3.2; Table 5; contrasting San Miguel et al., 2002). Currently, it is unclear whether this is a real effect or is again caused by an AHP spread of weights effect, as discussed above (Section 5.1.2): when weights are extreme, small relative deviations produce larger absolute differences, thus larger SAD.

In summary, the method per se was clearly the most important explanatory variable for preference stability over time. Additionally, we have strong indications that having more knowledge and expertise about wastewater-related issues is correlated with higher preference stability. The influence of other explanatory variables remains inconclusive, namely the age of respondents, new experience gained between surveys and the effect of strong preferences. But how can we explain the observed method effect?


                           Hoeffler and Ariely (1999) postulate that preferences could be more stable if choices are hard than if they are easy. Additionally, they found that people who faced an easy choice were surer about their preferences than those facing a hard choice. The effect of the difficulty of elicitation processes on preference stability requires further research, since we found only two other studies that mention this important topic (Brouwer, 2006; San Miguel et al., 2002).

Our results are in line with the scant literature. Both questionnaires were perceived as difficult (see Section 4.3.3). However, SWING was perceived to be more difficult (85 percent found it difficult) than the SMART/SWING variant (66 percent found it difficult; Table 6). This statistically significant difference can be explained by noting that the SMART/SWING variant was designed to ease elicitation: one suggestion to make elicitation easier is to allow respondents to choose any sub-objective as a reference (Mustajoki et al., 2005). Additionally, we gave verbal cues in our SMART/SWING variant to express the strength of preference. In contrast, SWING respondents had to rank hypothetical, extreme alternatives, which can be difficult (Box SI-1, SI-5). Moreover, these respondents felt a bit less certain about their answers: 54 percent using the SMART/SWING variant felt highly or extremely sure about the answers, but only 46 percent of subjects using SWING felt that (Table 6). In summary, we found the interesting effect that although the SWING questionnaire was perceived as more difficult, it produced more stable preferences over time compared with the easier SMART/SWING variant. We suggest that these findings be tested in other applications and with other elicitation methods.

Our results showed that preferences (weights) can change, for instance by using different elicitation methods, and over time. Despite this, our MCDA application to a real Swiss wastewater case consistently produced the same first-ranked alternative: alternative A7 received the highest values (Table 7) and best ranks in the first and second survey, irrespective of whether SWING or the SMART/SWING variant was used. Likewise, alternative A7 performed well in the ten earlier stakeholder interviews (Zheng et al., 2016; Fig. 4). But the alternatives A2, A8a, and A8b also performed well in the interviews (Table 7). We discuss the implications for wastewater management below.

In important decisions that affect a long time period, the ranking of alternatives would ideally stay the same if elicitation is repeated. However, the literature showed that correlations between the first and second survey were often only ‘moderate’ (e.g. Jorgensen et al., 2004; Liebe et al., 2012; see Section 2.8). Our own results are fully in line with the correlation coefficients reported in the literature (average Kendall's tau=0.63, SD=0.33). This relatively low correlation may lead to the premature conclusion that these preferences do not remain stable after one month, which might in turn challenge the decision itself. But if a decision-maker really has to select one of several possible alternatives, the first-ranked alternative is more important than the ranking of all alternatives. In our case, the recommendation about the best alternative would change for about 12 percent of individuals due to instable weights over one month if the weights were elicited with the SMART/SWING variant (Kendall's tau=0.53). For SWING, the best alternative remained the same for all 94 respondents (Kendall's tau=0.73; one exception). This method effect cancelled out if we used average weights of groups: rankings of alternatives were highly stable regardless of the elicitation method and time gap. Alternative A7 always had the highest values (Table 7). Nevertheless, we caution against using values that were averaged over a population sample in important environmental and policy decisions. We recommend that the results of individuals be carefully analyzed before a decision affecting the entire population is made (as do Schaafsma et al., 2014, who recommend using confidence intervals and sensitivity analyses).

The choice of the best-ranked alternative A7 would strongly affect the way we manage our wastewater. A7 is highly unconventional and assumes fully decentralized wastewater systems in houses by 2050. Urine separating toilets allow phosphate to be recovered as fertilizer for agriculture (Larsen, Alder, Eggen, Maurer, & Lienert, 2009). The good performance on this sub-objective ‘phosph’ seems to be the most important parameter for its high value of 0.81 (Table 7), since alternative A8b is similar, but does not recycle phosphorus (Appendix D). Alternative A7 performed better in the survey than in the interviews (Fig. 4). This is probably due to the use of a simplified hierarchy in the survey, with only ten sub-objectives instead of 19, whereby ‘phosph’ was weighted much more highly. In the interviews, the current central system (A8a) performed nearly as well as alternative A7.

Can we now recommend alternative A7? It is reassuring that multiple elicitation procedures at different steps in time produced similar results, and A7 on average ranked first or second in 92 percent (SD=12 percent; Fig. 4; Table SI-42) of cases for all five procedures, which were online surveys or interviews. But the central alternatives A8a (rank 1, 2, or 3 for 73 percent, SD=19 percent) or A2 (65 percent, SD=15 percent) should also be discussed with decision-makers. Decentralized wastewater systems are proposed for sustainability reasons (e.g. Larsen et al., 2009) and are explored in some large engineering projects (e.g. in the USA http://www.renuwit.org/; in Switzerland at Eawag
                           6
                        
                        
                           6
                           
                              http://www.eawag.ch/en/research/humanwelfare/wastewater/projekte/wings/.
                        ). This implies breaking the paradigm of the superiority of the central system, which is deeply rooted in engineering practice. Implementing a transition from central to decentralized systems requires strong evidence of their viability. We propose using complimentary methods, since a decision-maker would probably feel much more confident in choosing a highly unconventional decentralized wastewater system if MCDA interviews are backed up by public feedback.

@&#CONCLUSIONS@&#

To justify environmental and policy decisions, it is important to design elicitation procedures that produce stable results which reflect the stakeholders' preferences. Various factors influence preference statements, including the selected elicitation method. In our case, SWING produced statistically significantly different weights than the SMART/SWING variant. Moreover, in our study, the weights elicited with the SMART/SWING variant changed statistically significantly more often in the follow-up survey after one month compared to SWING. Despite its rigorous design, not all the limitations of such a study can be avoided (see Section SI-3.2). There is also ample scope for future research. To better understand preference formation and stability, we suggest using procedural and experimental analyses that address the following research questions:
                        
                           1.
                           Do popular elicitation methods such as SWING, SMART/SWING or trade-off (see e.g. Eisenführ et al., 2010) differ systematically in their elicited weight profiles, spread of weights and preference stability over time when applied to a variety of decision situations? The validity of online preference elicitation has been questioned (Marttunen & Hämäläinen, 2008). So virtual preference elicitation should also be compared with face-to-face procedures.

What are the reasons and underlying psychological mechanisms for the systematic differences between methods? For example, which explanatory variables influence preference formation and why? We found that preferences were more stable if respondents had more knowledge. To generalize this finding, it should be replicated in other settings. As another example, does the proposition of Hoeffler and Ariely (1999) hold that preferences are more stable if the decision task is more difficult? This question can be approached experimentally by constructing different types of questionnaires which allow easy choices or force people to think harder about a decision. Learning seems important, and observational studies could shed light on how learning processes evolve over time. Experiments allow us to study whether a systematic increase in information promotes more stable preferences. Is there a saturation point after which more information reduces preference stability, possibly because respondents resort to decision heuristics? Moreover, the influence of different information modes such as visual and verbal cues and interactive elicitation should be explored. As an example, does preference stability over time differ if the same information content is presented in different ways?

Environmental and policy decisions are especially sensitive to preference stability over a longer time range. Given that we can identify methods and processes that stabilize preferences, does this stabilizing effect wear off over time? Can this effect be avoided, for instance with targeted interventions at certain points in time?

Many of these research questions can be approached experimentally in decision labs and with the aid of students. However, to assure their generalization, laboratory results must be transferred to different types of real-world decision situations and stakeholders. Finally, for every concrete decision, it is highly relevant to identify whether possible differences in weight profiles and preference stability would result in a different ranking of alternatives in an MCDA. This need not necessarily be the case, as our study demonstrates: the first-ranked alternative was very stable, irrespective of preference differences relating to individual traits such as having more knowledge, the elicitation method (interviews, online elicitation with SWING or SMART/SWING variant), or a one-month time gap.

@&#ACKNOWLEDGMENTS@&#

We thank Christoph Egger, Jonas Maria Eppler, Max Maurer, Peter Reichert, Lisa Scholten, Nele Schuwirth, and Anja Zorn for their valuable contributions, and the Swiss National Science Foundation for funding via the National Research Program "Sustainable Water Management" (NRP 61; www.nfp61.ch), Project number: 406140_125901/1. We especially thank Eawag employees and public respondents for their participation. We are grateful for the patience and valuable comments of four anonymous reviewers and the editors Raimo Hämäläinen and Alberto Franco, all of whom greatly helped to improve the manuscript.

Elicitation of weights (scoring) to compare two sub-objectives with SWING as one example. Full details of both methods see Supplementary Information (Box SI-1–8). First, the ranges of the two attributes were described in words (best and worst case) along with the Status quo, and visualized with emoticons. People were then asked to rank the alternatives (Box SI-1). Based on rankings, people then scored the alternatives:
                        
                           
                              
                              
                                 
                                    
                                       Scoring of alternatives
                                       You ranked the Alternative A as, 1st. This alternative gets the highest score, 100. The Null Alternative with all objectives at their worst level gets the lowest score of 0.Now we will ask you to score Alternative B from 0 to 99 according to its level of importance. Please use the slider to score Alternative B below. You do not need to score Alternative A and Null alternative.
                                       
                                          
                                          Image, table
                                       
                                    
                                 
                              
                           
                        
                     
                  

For the SMART/SWING variant, the elicited scores from 10 to 90 were normalized: the ratios between the score and 10 were regarded as the ratio between the weights of the two objectives. The weights were hierarchically calculated from the ratios with an additional constraint that they sum up to unity. For instance, if the objective ‘low rehabilitation burden’ (‘rehab’) was judged “somewhat more important” than ‘flexible system adaptation’ (‘flex’), the score 30 (10, resp.) was assigned to ‘rehab’ (‘flex’, resp.). The weights of ‘rehab’ and ‘flex’ were calculated as 0.75 and 0.25 using two constraints: 
                        
                           
                              w
                              rehab
                           
                           +
                           
                              w
                              flex
                           
                           
                              =
                              1
                           
                        
                      and 
                        
                           
                              w
                              rehab
                           
                           /
                           
                              w
                              flex
                           
                           
                              =
                              30
                              /
                              10
                           
                           .
                        
                     
                  

The highest possible number of rank reversals is five (five pairwise comparisons for ten sub-objectives; proportion=1). The lowest proportion is 0 (no rank reversals).

Respondents using SWING ranked all five objectives, but for the SMART/SWING variant, respondents chose a reference objective with which the other four objectives were compared. This did not produce a ranking. Therefore, for both methods, the weights given to main objectives were used to derive the rankings.

We computed the proportion of rank reversals with a matrix of five pairwise comparisons for each main objective (ten different comparisons; Table C2). For each comparison, we determined whether objective x was ranked higher than objective y (>), the same (=), or lower (<). We calculated the proportion of changes. For the example in Table C1
                           , eight of ten comparisons were ranked differently in the second survey (Table C2
                           , right), which yields a proportion 0.8 rank reversals. The change in rankings can also be analyzed by correlation statistics, but we preferred this approach because it allowed comparing the rank reversals of sub-objectives with those of the main objectives.


                              Table C3
                              .


                     Table D1.
                  

Supplementary material associated with this article can be found, in the online version, at doi:10.1016/j.ejor.2016.03.010.


                     
                        
                           application 1
                           Image, application 1
                           
                        
                     
                  

@&#REFERENCES@&#

