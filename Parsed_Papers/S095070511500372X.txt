@&#MAIN-TITLE@&#Semi-supervised cluster-and-label with feature based re-clustering to reduce noise in Thai document images

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We proposed a novel noise reduction method for document images.


                        
                        
                           
                           Semi-supervised learning is applied to classify noise from character components.


                        
                        
                           
                           The proposed method is suitable for Non-Latin based scripts i.e. Thai document image.


                        
                        
                           
                           We proposed an enhance labeling method of semi-supervised cluster-and-label approach.


                        
                        
                           
                           The performance of proposed methods are significantly better than comparison methods.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Noise reduction

Document enhancement

Semi-supervised classification

Cluster-and-label

Thai document

@&#ABSTRACT@&#


               
               
                  Noise components are a major cause of poor performance in document analysis. To reduce undesired components, most recent research works have applied an image processing technique. However, the effectiveness of these techniques is suitable only for a Latin script document but not a non-Latin script document. The characteristics of the non-Latin script document, such as Thai, are considerably more complicated than the Latin script document and include many levels of character alignment, no word or sentence separator, and variability in a character’s size. When applying an image processing technique to a Thai document, we usually remove the characters that are relatively close to noise. Hence, in this paper, we propose a novel noise reduction method by applying a machine learning technique to classify and reduce noise in document images. The proposed method uses a semi-supervised cluster-and-label approach with an improved labeling method, namely, feature selected sub-cluster labeling. Feature selected sub-cluster labeling focuses on the clusters that are incorrectly labeled by conventional labeling methods. These clusters are re-clustered into small groups with a new feature set that is selected according to class labels. The experimental results show that this method can significantly improve the accuracy of labeling examples and the performance of classification. We compared the performance of noise reduction and character preservation between the proposed method and two related noise reduction approaches, i.e., a two-phased stroke-like pattern noise (SPN) removal and a commercial noise reduction software called ScanFix Xpress 6.0. The results show that semi-supervised noise reduction is significantly better than the compared methods of which an F-measure of character and noise is 86.01 and 97.82, respectively.
               
            

@&#INTRODUCTION@&#

Normally, the document analysis process works effectively on clean document images. The clean images, however, are rarely found in real-world situations. A real-world document image usually contains noise components that can dramatically decrease a performance of document analysis e.g., accuracy of optical character recognition (OCR). Noise in a document image stems from many sources (e.g., paper background, document aging, water drop and notation) with various properties. Recent research works on noise reduction usually employ one or more image processing techniques with specific noise properties [1]. These aim to reduce a specific type of noise, e.g., salt-and-pepper noise [2], line of writing [3], double-side writing interference [4], preprinted form [5], blob noise [6], and background [7,8].

Although the image processing technique is commonly applied to document image noise reduction, the effectiveness of this method is limited. First, it is only appropriated for a document image with a Latin based script (e.g., English, German and French) but not for a document with a non-Latin based script (e.g., Persian, Arabic and Thai) because the non-Latin script is ordinarily composed of many small characters whose size is similar to noise. The image processing technique typically specifies a component size threshold to separate character and noise components. If the size of character varies as in non-Latin script, some small characters may be removed when its size is less than the threshold, and a noise component will not be removed when its size is larger than the threshold. Second, many historical documents were digitized into black and white or binary images due to the limited storage space and technology during a digitization time. This black and white image contains restricted information (e.g., color depth, contrast) for image processing noise reduction methods. Re-digitizing these documents is impracticable because most of them were considerably more degraded over time, and many of them were destroyed. With the limitations above, an effective method is required to reduce noise in non-Latin script.

In this paper, we propose a novel noise reduction method for reducing noise in a black and white Thai document image. A Thai document is a fascinating example of a non-Latin document because of its intricate properties, i.e., it consists of many small characters, many levels of character alignment and no word and sentence separator. In addition to using an image processing technique, we utilize a machine learning technique for reducing noise in the Thai document image. The benefit of applying a machine learning technique is that it need not specify noise criteria for distinction, but instead, a classifier is trained to distinguish noise from a character. An efficient classifier can discriminate even noise that its size is quite similar to a small character, as frequently presented in Thai script. However, to build an efficient classifier, we need a sufficient number of labeled examples. Although a large number of document images is available, the labeled components are scarce because the labeling process is effort intensive and is a time consuming task. To minimize the cost of human effort, we utilize the available unlabeled examples by applying a semi-supervised classification. The semi-supervised classification is a widely used method for the problem of limited labeled data, for example, a cross-lingual sentiment classification. Hajmohammadi [9] proposed a combination of semi-supervised and active learning methods to classify a sentiment document in the target language, which is not English and rarely finds the labeled data, by using the source language document, which is in English and the labeled sentiment documents are available.

In this work, we apply the semi-supervised cluster-and-label approach to create the classifier for noise reduction. First, all examples, with or without the class labeled, are clustered by their properties. Then, a majority class of labeled examples in each cluster is specified. The unlabeled examples in each cluster are then labeled as the majority class in theirs cluster. These recently labeled examples and the prior labeled examples are used to train the final classifier. Finally, the classifier classifies all components and removes the noise classified components from the document images.

When we applied the cluster-and-label procedure, another problem arose. The mislabeled examples were apparently found in a mixed-class cluster, or a cluster might contain various classes of labeled examples. Hence, we proposed a radical labeling method to improve accuracy of example labeling in the mixed-class cluster, namely, feature selected sub-cluster labeling. The idea is that if different-class examples are unintentionally grouped into the same clusters, sub-clustering will re-organize examples into a proper sub-group. Because the current feature set is ineffectual to separate examples, a new feature set for sub-clustering is then needed. A particular feature set is selected by a feature selection with information gain. This particular feature is then used to cluster examples in each mixed-class cluster into a satisfying sub-cluster. The unlabeled examples are then labeled with a class of labeled examples of their affiliated sub-cluster.

The performance of feature selected sub-cluster labeling is compared with a conventional majority vote labeling. The results show that the proposed labeling method improves the accuracy of labeling in the mixed-class cluster and provides an efficient classifier. The performance of semi-supervised noise reduction with feature selected sub-cluster labeling is compared with two related noise reduction methods, i.e., a two-phased stroke-like pattern noise (SPN) removal [10] and the commercial noise reduction software, namely, ScanFix Xpress 6.0 [11]. The results show that semi-supervised noise reduction is significantly better in noise reduction and character preservation than the compared methods. We further analyzed the reason for improvement by using our method and found that for the small characters that are used frequently in Thai documents, they were clustered in the mixed-class cluster and labeled incorrectly by the standard cluster-and-label method. However, our proposed method can improve the accuracy of the small character portion of the test set.

The structure of this paper is as follows. A review and discussion regarding state-of-the-art noise reduction methods in document images are presented in Section 2. The property of Thai script as opposed to Latin script is described in Section 3. An algorithm for semi-supervised cluster-and-label is described in Section 4. In Section 5, we present a methodology of the proposed semi-supervised noise reduction with feature selected sub-cluster labeling. The results and discussion are presented in Section 6. The last section provides a conclusion and future work.

This work differs from our previous work [12,13] in two aspects. First, in previous work, we considered a line of a connected component as an example with an aim to reduce noise that might attach to a character. However, we found that the line-level example might contain an insignificant amount of information to distinguish a character from noise. As a result, this work uses a connected component as an example and focuses on removing particular noise, whose size is similar to a small character. Second, in previous work, we applied a traditional majority vote labeling in semi-supervised cluster-and-label classification to reduce noise but, in this work, we proposed a novel feature selected sub-cluster labeling method for semi-supervised cluster-and-label.

Noise in a document image, in this work, is defined as any foreground component in the document image except a printed character. Noises can stem from many sources either intentionally (such as a water mark, rubber stamp and a signature) or accidentally (such as a paper wrinkle, a water drop and a worm hole). To the best of our knowledge, these various noises have not been categorized into a distinct category. There was one study that proposed a noise category by a source of noise, e.g., physical noise caused by damage in a document paper and digitization noise caused by an error in a conversion process of a document paper to a digital image [14]. In this work, we categorize noise by its characteristics, i.e., a specific pattern noise that explicitly diverges from a character so-called “fixed-form noise” and fuzzy pattern noise that is possibly akin to some characters so-called “free-form noise”.

Several studies tried to eliminate the fixed-form noise. Examples of fixed-form noise removal methods are a salt-and-pepper noise removed by applying the k-Filled algorithm [2]; a line of writing removed by considering a threshold of the character’s length and width [3]; a pre-printed form removed by comparing it to a blank form [5]; a blob removed by considering the size and position of a large component [6]; and an interfering stroke in a double-sided handwriting document removed by considering contrast and the prior knowledge of alignment of handwritten English characters [4]. To apply any of these methods, a user must define criteria to identify noise from a character component and then apply some image processing techniques to reduce the selected component. However, these approaches seem dubious when applying them to free-form noise or noise whose attribute is quite similar to the character. For example, if line thickness of pre-printed forms is quite similar to that of the character, this noise will not be removed [5]. Although extensive studies have been performed on fixed-form noise removal, very few studies have given attention to free-from noise removal [6].

Another challenge is noise reduction on a binary document image. A binary document image is a document that is digitized into a black and white image due to the limited storage space and technology during the digitization time. Re-digitizing these documents could be impracticable because most of these documents were considerably more degraded over time, and many of them were destroyed. This binary image is typically historical documents that usually consist of considerable noise. So, performing noise reduction on this historical document image is essential. However, because the binary image contains restricted information (e.g., color depth, contrast) for the image processing technique, using a machine learning technique instead could more properly reduce the noise in this binary document image.

An interesting machine learning noise reduction method is the two-phased SPN removal [10] that is a script-independent noise removal for a binary document image using a supervised classification. The two-phased SPN removal divides a component in a document into a prominent text component (PTC) and a non-prominent text component (non-PTC). PTCs are composed of a character component whose characteristic obviously differs from noise components. Non-PTCs are composed of a noise and a character component whose characteristic slightly differs from noise. In the first phase of this method, the classifier for PTC and non-PTC is established. The classifier uses a connected component’s properties as an attribute set, i.e., area, perimeter, convex-area, orientation of fitted ellipse, major and minor axis lengths, eccentricity, filled area, extent, solidity, and equivalent diameter. Classified PTC components are used to calculate an average stroke-width and cohesiveness. These values are used in the second phase to divide non-PTCs into a noise or a character group by unsupervised k-means clustering. The two-phased SPN removal requires a sufficient number of labeled examples for constructing the PTC classifier; however, these labeled examples are scarce.

In this work, we propose a noise reduction method using semi-supervised classification. We apply semi-supervised cluster-and-label with a novel feature selected sub-cluster labeling to reduce noise. The first step is a cluster-and-label process that increases a number of labeled examples. The second step uses the increased labeled examples to create a final classifier of noise and character. The complete algorithm of the proposed method is presented in Section 5.

The additional analysis on the performance of noise reduction and character preservation by comparing the proposed method to the other two related approaches, i.e., the two-phased SPN removal and ScanFix Xpress 6.0, are presented in Section 6.

The Thai language was used in Thailand by more than 65 million people in 2014 (National Statistical Office). The Thai character set [15–17] consists of forty-four consonants, twenty-one vowel letters, four tonal symbols, ten numbers, and fourteen punctuation marks. The appearance of a Thai character is shown in Table 1
                     . Properties of Thai script are similar to that of neighboring countries, i.e., Lao [18] and Khmer [19]. The similarity of these languages is represented by an example word, which means “Thank you” in Fig. 4.

Thai script differs markedly from Latin script, such as English. The first difference is the size of the characters. The size of Thai characters varies from a very small component (i.e., a tonal) to high and wide characters (i.e., consonant). Moreover, the small characters, for example the tonal or small vowels, play an important role in word meaning. If the small component is modified, the word meaning will be totally changed, as shown in Fig. 1
                     . This small component is a troublesome component in the noise reduction process because its properties are similar to a speckle noise component in a document image. The second difference is character alignment. The position of English script is on the baseline, whereas the Thai script position is on four levels, e.g., uppermost, upper, middle and lower baseline. The position of the Thai character is illustrated in Fig. 2
                     . The last difference is separator symbols. The English script has a space, comma, colon or other punctuation as a word separator and a full stop as a sentence separator. Thai script, on the other hand, does not have any word or sentence separator. The different properties of English and Thai scripts are indicated by an example of a Thai sentence and its translation in Fig 3
                     
                     .

These differences are the cause of difficulty in Thai document analysis, specifically in the noise reduction process. A noise reduction approach that works properly in an English document could provide a pitiful result when applying it to a Thai document. Although many topics in Thai document analysis such as Thai OCR have been widely investigated [20–23], a study on a noise reduction method for Thai or non-Latin script has received considerably less attention [24].

One problem when applying machine learning methods to the noise reduction domain is how to label a large number of training examples. Hence, the semi-supervised techniques can be useful for this problem.

Semi-supervised learning is a machine learning technique between supervised and unsupervised learning. The supervised learning algorithm, such as k-nearest neighbor, neural network or decision tree, requires a sufficient number of labeled training examples to create a classifier. The unsupervised learning algorithm, such as k-means clustering, uses only unlabeled training examples to divide all examples into various groups. Semi-supervised learning utilizes both the labeled and unlabeled examples to create the classifier. This approach takes advantage of the unlabeled example to increase number of labeled examples and uses them to create a classifier [25].

There are many approaches for semi-supervised classification. One efficient approach used in this work is cluster-and-label [26,27]. The cluster-and-label semi-supervised classification is appropriated to the problem in which (1) the example can be clustered; and (2) a proper clustering algorithm for current class distribution is available [25]. The cluster-and-label approach uses a clustering technique to group all examples, both labeled and unlabeled. Then, unlabeled examples in each cluster are labeled as a majority class of the labeled example in that cluster [26,28]. The increased labeled examples are combined with a prior labeled example and used to train a final classifier. This cluster-and-label approach has been successfully applied to the text classification domain [29].

An overview process of our semi-supervised noise reduction process is presented in Fig. 5
                     . The proposed semi-supervised noise reduction process consists of four steps, namely, preprocessing, feature extraction, cluster-and-label and classifier creation. A description of each step is shown in this section.

The main goal of preprocessing is to extract valid information from a document paper. Initially, the document paper is scanned as a binary image. The scanned image orientation is corrected by a skew correction.

The substantial information in a document image is then extracted by OpenCV [30], a well-known image processing library. First, a connected component in an image is detected by selecting a group of black pixels or adjacent pixels in eight directions. Then, the extracted component is justified by an edge smoothing technique [31]. This edge smoothing is a crucial task because an uneven edge could distort a component’s structure, as shown in Fig. 6
                        . Finally, a component structure or a skeleton is extracted by the thinning algorithm. We use a one-pass parallel thinning algorithm [32] that is suggested for thinning a component in the Thai-OCR process [33].

The feature or attribute that is broadly used in previous noise reduction studies is a noise property. A defined noise property is used to reduce a specific type of noise whose study intends to reduce. On the contrary, in our study, we aim to reduce an overall noise whose characteristic is indefinite. As a result, we use character property for distinguishing a character from a noise component. The used character property consists of a Thai character structure, i.e., width, height, ratio, density, thickness, upper leg, lower leg, junction point and loop. The feature of the Thai character structure is derived from the ones used in Thai-OCR methods [34–36]. The descriptions of each character structure feature are described in Table 2. The used feature is based on the same set used in our previous works [12,13], except the “position”. The position feature is a component position with respect to a line of writing. Because the line of writing is hardly detected in a highly degraded document image, the position feature was excluded in this research.

The semi-supervised classification, cluster-and-label approach is used to create a classifier for noise and character. The cluster-and-label approach increases the number of labeled examples by clustering examples into groups and labeling each group. There is a two-step process, i.e., clustering process and labeling process.

In the clustering process, we use self-organizing maps (SOM), which were invented by Teuvo Kohonen [37]. SOM is a type of unsupervised neural network with one input layer and one output layer. The number of input nodes equals to the number of attributes, which is nine in this work, and the number of output nodes is set to five by five nodes. The output nodes are connected to each other as a lattice and are fully connected to all input nodes. An example of the SOM structure is presented in Fig. 7
                           .


                           I is a set of input nodes, O is a set of output nodes, wij
                            is a weight of a link between input node i and output node j, x(t) is a vector of training samples with t dimensions, N is the number of training samples, K is the number of neighborhoods, and η is a neighborhood function. The training process of SOM is presented in Algorithm 1
                           .

First, the weight of all nodes is initialized with random values. The training process considers each training sample value and calculates the similarity score of each output node with a comparison metric such as Euclidean. Then, the best match node or the winner node and its neighborhood are identified. Finally, the weight of the winner node and neighborhood are updated to close the training example with the neighborhood function such as Gaussian. A training process continues until it is met a stopping condition such as the number of rounds. The result of the training process is a meaningful SOM lattice.

To cluster with SOM, the training examples, both labeled and unlabeled examples, are mapped to the output nodes. The examples that are mapped into the same node are grouped to the same cluster [26]. The cluster may be a pure-class cluster or a mixed-class cluster depending on the classes of labeled examples in that cluster. If all labeled examples in the cluster have the same class label, the cluster is the pure-class cluster. On the contrary, if labeled examples have different class labels, the cluster is the mixed-class cluster.

The next step is the labeling process. The labeling process will assign a class label for an unlabeled example. In the pure-class cluster, the example labeling is straightforward. The unlabeled examples in each pure-class cluster are labeled as the class of labeled examples in this cluster. On the other hand, the example labeling in the mixed-class cluster is obscure. The two methods used to label the example in the mixed-class cluster are the conventional majority vote and the proposed method, namely, feature selected sub-cluster labeling.

Majority vote labeling is a traditional method to label the examples in the mixed-class cluster [26,28]. The unlabeled example is labeled for the majority class of labeled examples in the cluster. The majority class is the most frequent class whose ratio of examples with this class is greater than a defined threshold. To determine the majority class, the number of labeled examples in each class is first counted as in Eq. 1.

                              
                                 (1)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   N
                                                   
                                                      c
                                                      j
                                                   
                                                
                                                =
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                                
                                                   
                                                      f
                                                      
                                                         c
                                                         j
                                                      
                                                   
                                                   
                                                      (
                                                   
                                                   
                                                      x
                                                      i
                                                   
                                                
                                                
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (2)
                                 
                                    
                                       
                                          f
                                          
                                             c
                                             j
                                          
                                       
                                       
                                          (
                                          
                                             x
                                             i
                                          
                                          )
                                       
                                       =
                                       
                                          {
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   
                                                      if
                                                      
                                                      
                                                         y
                                                         i
                                                      
                                                      =
                                                      
                                                         c
                                                         j
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   
                                                      if
                                                      
                                                      
                                                         y
                                                         i
                                                      
                                                      ≠
                                                      
                                                         c
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where cj
                            denotes the class label, 
                              
                                 N
                                 
                                    c
                                    j
                                 
                              
                            is the number of labeled examples of class cj, n is the number of examples in the cluster and yi
                            is the class label of example xi
                           . Then, a mixed-class ratio in each cluster is calculated by Eq. 3.

                              
                                 (3)
                                 
                                    
                                       
                                          
                                             
                                                mixed-class
                                                
                                                ratio
                                                =
                                                
                                                   
                                                      
                                                         max
                                                         
                                                            j
                                                            =
                                                            1
                                                         
                                                         C
                                                      
                                                      
                                                         (
                                                         
                                                            N
                                                            
                                                               c
                                                               j
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                   
                                                      N
                                                      l
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 N
                                 
                                    c
                                    j
                                 
                              
                            is the number of labeled examples of class cj, C is the number of class labels, and Nl
                            is the number of labeled examples in the cluster.

The mixed-class ratio is the number of the most frequent class examples divided by the number of all labeled examples. If the mixed-class ratio is greater than the predefined threshold, the examples will be labeled as the most frequent class in the cluster. If not, the examples are left as undefined class examples. The threshold value, which is between 0.5 and 1, determines a label assignment in the mixed-class cluster. When the threshold equals to one, no examples in the mixed-class cluster are labeled, and all examples are left as undefined class examples [26]. When the threshold equals to 0.5, all examples in the mixed-class cluster were labeled by the most frequent class [28]. Although the majority vote with threshold correctly labeled most of the unlabeled examples, many unlabeled examples were incorrectly labeled. Most of the incorrect labeling examples belonged to the mixed-class cluster because the actual class of these examples is probably different, but the majority vote with threshold labels all of these examples with the same class, i.e., a majority class. The incorrect labeling examples will be used to train a classifier and may lead to poor classifier performance.

This feature selected sub-cluster labeling is a proposed labeling algorithm to improve the accuracy of labeling examples in the mixed-class cluster. The proposed method uses a feature subset that correlates to example classes to divide examples in the mixed-class cluster into sub-clusters. The overview process is presented in Fig. 8
                           .

The feature selected sub-cluster labeling algorithm is described in Algorithm 2
                           , where x is an example, y is class label of example, C is a cluster and f is a feature set.

In each cluster, the majority class or the most frequent class of the labeled example is determined. Then, it is used to label unlabeled examples in the pure-class cluster. Otherwise, the SplitCluster is performed in the mixed-class cluster. First, the new feature set is selected by a feature selection algorithm. In this work, information gain [38] is used. The information gain feature selection is performed on labeled examples in each cluster to select the best feature with respect to the classes of labeled examples. The recently selected features are then used to cluster examples of each mixed-class cluster into fine clusters. This sub-clustering process is performed by an agglomerative clustering algorithm [39]. Finally, unlabeled examples in each sub-cluster are labeled with a majority class of labeled examples in the sub-cluster.

A major benefit of this labeling method is that it establishes a specific decision boundary of the classes in the mixed-class cluster. This decision boundary more accurately aids labeling for the examples in the mixed-class cluster.

The increased labeled examples by cluster-and-label are combined with prior labeled training examples. These examples are used to train a final classifier for noise and character. In this experiment, we use a simple k-nearest neighbor as our classification algorithm.

@&#RESULTS AND DISCUSSION@&#

In this section, we first describe a dataset used in experiments and also a performance comparison of each noise reduction method. Then, the result of clustering is represented by an image of components in each cluster. We report the performance of the proposed method in two aspects, i.e., the performance of the feature selected sub-cluster and the performance of the proposed semi-supervised noise reduction. First, the classifier results and labeling accuracy of feature selected sub-cluster labeling were compared with a traditional majority vote labeling. Then, we compared and analyzed the performance of semi-supervised noise reduction to the two-phased SPN removal, which is a closely related noise reduction method. A performance comparison of semi-supervised noise reduction and commercial noise reduction software, ScanFix Xpress 6.0, was also measured.

The dataset used in the experiment is real-world Thai historical document images. Each document image was mixed up with printed letters and many noise components. Noise came from many sources including a wrinkle, a rubber stamp, a water drop and a table line. These documents were previously digitized into a binary color images. The number of used document images was 182 images in which there are 1,007,753 connected components in total.

Training images contained seven document images in which there were 40,842 connected components in total. Approximately 4 % of the training examples were a labeled example as a character or noise example, whereas the other examples did not have a class label.

The testing images were 175 document images in which there were 966,911 connected components in total. These testing image components were a component with class labeled. The character class was labeled as the printed text component, and class noise was for the other components. An example of this ground-truth image was illustrated in Fig. 9
                        .

@&#PERFORMANCE EVALUATION@&#

A noise reduction performance was measured on both the capability of the noise reduction and character preservation. These measurements were performed by using the standard accuracy, precision, recall and F-measure of each class from Eq. 4 through 7.

The accuracy was the overall correctness of classification on all examples. The precision of each class was the correctness of the classified component in each class. High precision meant that most of the classified examples were correct. However, the precision did not ensure that the classified examples cover all examples in the class. To measure the coverage of classified examples, a recall was typically used together with the precision. In the binary classification, the harmonic mean of precision and recall was an F-measure. The rest of this section reports experiment results for accuracy and F-measure on class character and noise.

                           
                              (4)
                              
                                 
                                    
                                       
                                          Accuracy
                                       
                                       
                                          
                                             =
                                             
                                                
                                                   True
                                                   
                                                   positive
                                                   +
                                                   True
                                                   
                                                   negative
                                                
                                                
                                                   Number
                                                   
                                                   of
                                                   
                                                   all
                                                   
                                                   examples
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    
                                       
                                          Precision
                                       
                                       
                                          
                                             =
                                             
                                                
                                                   True
                                                   
                                                   positive
                                                
                                                
                                                   True
                                                   
                                                   positive
                                                   +
                                                   False
                                                   
                                                   positive
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 
                                    
                                       
                                          Recall
                                       
                                       
                                          
                                             =
                                             
                                                
                                                   True
                                                   
                                                   positive
                                                
                                                
                                                   True
                                                   
                                                   positive
                                                   +
                                                   False
                                                   
                                                   negative
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    
                                       
                                          F-measure
                                       
                                       
                                          
                                             =
                                             2
                                             *
                                             
                                                
                                                   Precision
                                                   *
                                                   Recall
                                                
                                                
                                                   Precision
                                                   +
                                                   Recall
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

A performance comparison of character recognition between a degraded document image and a cleaned document image is very difficult to measure by the state of the art Thai-OCR method because the degraded document image of various noise components interfere with the process of character recognition, i.e., line of writing detection. Moreover, the documents are composed of typewriter font, which are currently not supported by the current Thai OCR software.

The cluster results are illustrated in Fig. 10
                        . This figure presents examples of clustered components in which the components in the same cluster are colored with the same color. As we can see, the clusters group together similar characters such as a small character, a thin character and a connected character. In the same way, similar noise components are also grouped such as a vertical line, a horizontal line, a blob and a speckle.

In this section, we analyze the performance of a classifier created from a two labeling method, i.e., the majority vote labeling and the feature selected sub-cluster labeling. The majority vote labeling is a traditional labeling method for semi-supervised cluster-and-label, which requires the specific threshold of mixed-class ratio for labeling in the mixed-class cluster. In this experiment, the threshold value varies from 0.5 to 1.

The feature selected sub-cluster labeling applied the information gain feature selection by Weka [40]. The feature selection was to select the most informative attributes for classifying different class examples in each mixed-class cluster. Then, the selected features are used to re-cluster examples in each mixed-class cluster by agglomerative clustering.

The proposed method attempted to label examples in a hard to label cluster or the mixed-class cluster. We found that most of the examples belonged to the mixed-class cluster. The number of training examples in the mixed-class cluster was 24,539 examples from a total of 40,842 examples or approximately 60.08 % of all training examples. Likewise, the number of testing examples in the mixed-class cluster was 550,243 from a total of 966,911 tested examples or approximately 56.91 % of all testing examples. Because the feature selected sub-cluster labeling utilized the examples from these mixed-class clusters, the proposed method produced a more efficient classifier than majority vote labeling. The classification performance on examples in mixed-class clusters is presented in Table 3
                        . The results show that the proposed method provided a greater classification performance for examples in the mixed-class cluster than majority vote labeling for all threshold settings.

This successful labeling in the mixed-class cluster increased the performance of the overall testing examples. The performance of the classifier on all testing examples is shown in Table 4
                        . As we can see, the classifier trained by feature selected sub-cluster labeling is substantially better than majority vote labeling.

Moreover, we examined the performance of the labeling method by measuring the accuracy of example labeling methods. We manually labeled all training examples for evaluating the correctness of labeled examples by both labeling methods. We compared the accuracy of the proposed labeling method to majority vote labeling with threshold value ‘1’, which is the best setting among all threshold settings. A labeling accuracy comparison is presented in Table 5
                        . The results show that accuracy of labeled examples by feature selected sub-cluster labeling is significantly better than majority vote labeling because majority vote labeling only labels examples in the pure-class cluster. On the contrary, examples in the mixed-class cluster are left unlabeled. The efficiency of example labeling accuracy is one cause of success in classifier creation in feature selected sub-cluster labeling.

A performance of semi-supervised noise reduction with feature selected sub-cluster labeling was compared to the related methods, i.e., the two-phased SPN removal and ScanFix Xpress 6.0. For the sake of fairness, a parameter of ScanFix Xpress 6.0 was adjusted to the best for cleaning the training document images. ScanFix cleanup settings were smooth objects, despeckle, line removal, comb removal, border removal and blob removal. More details on these ScanFix configurations are presented in appendix I.

A performance of each noise reduction method is presented in Table 6
                        . The results show that accuracy and F-measure on both classes of semi-supervised noise reduction are considerably better than the two-phased SPN removal and ScanFix Xpress 6.0. This means that semi-supervised noise reduction is more efficient in reducing noise and preserving characters than the two compared methods.

Examples of a cleaned up document image by two-phased SPN removal, ScanFix Xpress 6.0 and semi-supervised noise reduction are illustrated in Figs. 11
                         and 12
                        . In comparison with the ground truth image, the semi-supervised noise reduction method produced the cleanest document among the comparative methods, while the characters within the image are mostly preserved.

The idea of the two-phased SPN removal is similar to our proposed method that tries to reduce character-like noise by using a machine learning technique. However, they are different in many aspects.

The first difference is that the attributes are used to represent examples in these methods. The attribute of semi-supervised noise reduction is a Thai character structure, whereas the attribute of two-phased SPN removal is a connected component property (i.e., area, eccentricity, major axis and minor axis). Because this experiment was performed on a Thai document image dataset, the Thai character structure seemed more appropriate to represent the example for noise reduction than the connected component properties. For the sake of fairness, we performed an additional experiment on the two-phased SPN removal with the same attribute set used in our method. The results in Table 7
                         show that using a Thai character structure can improve the performance of the two-phased SPN removal. However, comparing this result to semi-supervised noise reduction, the proposed method still provided a better outcome compared to the two-phased SPN removal.

Another difference is that a sequence of algorithms is used in these two methods. The earlier step of the semi-supervised noise reduction uses unsupervised learning to help labeling examples, and the next step applies supervised learning to create a final classifier. In contrast, the two-phased SPN removal uses supervised learning to approximately classify components into a PTC or a non-PTC in the earlier step. Then, the properties of these PTC classified examples are used to discover a character within the non-PTC examples by using the unsupervised learning algorithm. The drawback of using supervised learning in the first step is that the misclassified PTCs from the first phase could lead to a failure of the final character and noise classification. Moreover, using supervised learning at the beginning required a substantial number of labeled examples for training. This requires more effort to label examples, which will be presented in the next subsection.

The training document images set was composed of 40,842 connected components that were originally unlabeled. However, the labeled components are required for the proposed semi-supervised noise reduction and the two-phased SPN removal. The semi-supervised noise reduction required labeled examples to create a final classifier for noise and character, and the two-phased SPN removal required labeled examples to create a classifier for PTC and non-PTC. For the two-phased SPN removal, all training examples are required to be manually labeled, while the proposed method uses a cluster-and-label process to label training examples. For this reason, our proposed method requires less user effort to label examples. Table 8
                         presents the amount of user effort that is used to label examples for the semi-supervised noise reduction and the two-phased SPN removal, that is, 1485 and 31,482 clicks, respectively. Please note that the number of clicks was counted for all user clicks, including a number of clicks for re-labeling mistaken labeled components.

The number of examples labeled by each method is also presented in Table 8. In fact, the two-phased SPN removal requires all training examples to be labeled; however, this labeling process is a manual process in which a very small component may be missed. The semi-supervised noise reduction labeled 39,216 examples or 96.02 % of all training examples, which is greater than the manually labeled examples for the two-phased SPN removal in which 31,482 examples or 77.08 % of all training examples were labeled. The semi-supervised noise reduction markedly increases labeled examples approximately 26 times from the user labeled examples. The comparison between the user originated labeled examples and the increased labeled examples by the cluster-and-label process is illustrated in Fig. 13
                        .

The proposed method can apply to the other languages documents with an adjustment of a feature set. The feature set used in this paper is a Thai character structure, which suits Thai document images. However, the Thai character feature may not be applicable for the other documents.

We performed an additional experiment on semi-supervised noise reduction with a connected component structure feature. This connected component feature is used in the two-phased SPN removal method for reducing noise in an Arabic document. The results in Table 9
                         show that using the connected component feature decreases the performance of semi-supervised noise reduction compared to using the Thai character feature. The accuracy dropped from 96.33 to 92.72, and the F-measure dropped from 86.58 to 75.79 because the connected component feature may not properly suit a Thai document as the Thai character structure. Correspondingly, Table 7 shows that the two-phased SPN removal performance on a Thai document was better after changing to a Thai character structure feature.

Therefore, the feature that suits the scripts should be specified to apply the proposed method to the other languages. Due to the limitation of accessibility to the datasets in other languages, the experiments in this paper were only performed on a Thai document image dataset.

@&#CONCLUSION@&#

We have proposed a novel noise reduction method for a Thai document image, which employed a semi-supervised cluster-and-label approach with feature selected sub-cluster labeling. In contrast to the traditional noise reduction method which used an image processing technique, the proposed method uses a machine learning technique to classify noise and character components. The used machine learning technique is a semi-supervised cluster-and-label approach. The traditional cluster-and-label approach groups all examples and then labels unlabeled examples by a majority class in the cluster. The increased labeled examples are then used to train a classifier to classify noise and character components.

Although majority vote labeling provides efficiently labeled examples, many examples are incorrectly labeled or left unlabeled. The failure is presented in a cluster that consists of labeled examples with different class labels or the mixed-class cluster. Unfortunately, most of the examples belong to this mixed-class cluster. In this paper, we proposed an improved labeling method, namely, feature selected sub-cluster labeling. The proposed labeling method tries to re-cluster the mixed-class cluster by a class associated feature set selected by information gain feature selection. The results show that the proposed labeling method improves the accuracy of labeling and establishes a more significantly efficient classifier than majority vote labeling. The feature selected labeling method successfully re-clustered examples in mixed-class clusters into separate groups according to class labels. The proposed labeling method achieved an accuracy of 94.65 %.

Our proposed labeling method was compared with two related approaches, i.e., the two-phased SPN removal, which is a related noise reduction method, and ScanFix Xpress 6.0, which is a commercial noise reduction software. The results show that semi-supervised noise reduction performs significantly better in both noise reduction and character preservation. Semi-supervised noise reduction can effectively classify both noise and character classes, which can be seen from the F-measure of 86.58 for character class and 97.87 for noise class.

Although an example of a cleaned-up image shows that the proposed method can reduce most of the noise and preserve most of the character, there were still misclassified components. For example, Fig. 14 shows an image result in which there is noise, whereas some characters are mistakenly removed. The components within dash components are our limitations. The components in dash circles are rotated characters, whose characteristics are similar to a character component. The component in a dash rectangle is a broken character in which some parts were removed because they are similar to noise. Moreover, in the cluster-and-label process, even though the accuracy of cluster-and-label is remarkable, approximately 3.79 % of training examples were not labeled because some clusters did not have any labeled examples; thus, it is an undefined class for labeling examples in these clusters. In our future work, an interesting approach will be applied to active learning for labeling examples in the undefined classes.

The details of the ScanFix configuration used in the experiments are presented in Table A.1
                     .

@&#REFERENCES@&#

