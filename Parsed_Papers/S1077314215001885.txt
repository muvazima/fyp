@&#MAIN-TITLE@&#Statistical 3D face shape estimation from occluding contours

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel 3D face estimation method based on a regression matrix and occluding contours.


                        
                        
                           
                           3D vertices around occluding boundaries and their corresponding 2D pixel projections are highly correlated.


                        
                        
                           
                           The 3D face estimation method resembles dense surface shape recovery from missing data.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Statistical face models

Linear regression

3D shape recovery

@&#ABSTRACT@&#


               
               
                  This paper addresses the problem of 3D face shape approximation from occluding contours, i.e., the boundaries between the facial region and the background. To this end, a linear regression process that models the relationship between a set of 2D occluding contours and a set of 3D vertices is applied onto the corresponding training sets using Partial Least Squares. The result of this step is a regression matrix which is capable of estimating new 3D face point clouds from the out-of-training 2D Cartesian pixel positions of the selected contours. Our approach benefits from the highly correlated spaces spanned by the 3D vertices around the occluding boundaries of a face and their corresponding 2D pixel projections. As a result, the proposed method resembles dense surface shape recovery from missing data. Our technique is evaluated over four scenarios designed to investigate both the influence of the contours included in the training set and the considered number of contours. Qualitative and quantitative experiments demonstrate that using contours outperform the state of the art on the database used in this article. Even using a limited number of contours provides a useful approximation to the 3D face surface.
               
            

@&#INTRODUCTION@&#

Developing techniques for 3D face shape estimation is a subject of special interest in computer vision due to its potential applications in computer graphics and face identification. While the problem is difficult to model through methods based on inverting the image formation process such as geometric [1] and photometric stereo [2], a face consistency constraint may be included in order to enhance results. Usually, this facial consistency is achieved by redefining the problem in terms of a 3D face model prior, which guarantees that the recovered surface lies within the span of the class of human faces. In particular, the idea of constraining 3D face shape recovery using a training set of facial surfaces has attracted the attention of the research community and established itself as a popular approach.

Among the different statistical methods for 3D face shape recovery, Principal Component Analysis (PCA) is commonly used as a tool to explain the variations of 3D shape and grayscale values independently [3,4]. However, when separate models are constructed, a fitting strategy has to be performed over the 3D shape and texture parameters of the model in order to deform an initial surface in accordance with the features extracted from one or several input images. The results obtained from these techniques usually deliver accurate approximations, but their success depends on factors such as the distance measure used in the fitting process and the number of features estimated by the method, i.e., 3D structure, texture, pose and illuminations variations. If many factors are included or a complex distance measure is used, the elevated computational time consumed in the 3D face estimation prevents these approaches for being used in realtime applications.

Multiple Linear Regression (MLR) methods have been proposed as an alternative to PCA since they are useful to predict 3D face shape from 2D features such as image intensities [5,6]. A review of different MLR techniques was performed in [7] where Partial Least Squares (PLS) was confirmed as the most suitable method for learning shared variations between 3D shape and grayscale images due to its suitability for handling a large number of variables. In the context of the present article, a PLS-based technique is attractive due the possibility of directly estimating a dense 3D mesh from sparse 2D geometrical features obtained from a set of input images. The geometrical information contained in an image of a face can be extracted as separate feature points or as connected components such as edges and contours. Usually, contours refer to the closed boundaries of the face while edges refer to open segments. Edges fall into two categories: inner edges, i.e., the lines contained inside the facial region; and outer edges, which are concerned with the occluding boundaries between the facial area and the background. Binary images represent an alternative representation for contours, as these include not only the boundaries but the whole region of interest of the face. In this work, we will refer to occluding contour as the boundary line depicted between the facial region of the head and the background. Note that the complete shape of the head, i.e., the rear, hair, ears, neck and upper region of the head are not considered in this research in order to focus on the relationship between occluding contours generated only within the facial region.

The present work is motivated by the fact that occluding contours represent strong geometrical information attached to both 2D and 3D shape. Being visible as strong edges, occluding contours may be easily estimated from images, representing a meaningful feature for recovering the 3D surface of a face from the video sequence of a rotating camera. Unlike previous MLR approaches exploring the effect of coupling grayscale images and 3D shape, we now focus on studying the effect of 2D features that explain facial shape rather than facial appearance. In this sense, focusing on contours strengthens the relationship between the data used to train our system, leaving aside factors inherently contained in pixel intensities such as texture and illumination variations. While our method does not recover facial appearance as it does not deal with the modeling of pose and illumination parameters, it does provide a simple and straightforward methodology to approximate the shape of a face from a set of occluding edges, having potential applications in computer graphics and identification.

This article introduces an MLR-based methodology for estimating 3D face shape information when a set of 2D occluding contours is available from multiple views of a face. Our approach aims to directly estimate the 3D face structure by using a regression matrix built through PLS, where common information between 2D contours and 3D data is retained in the regression process. Modelling this matrix can be achieved using the kernel PLS algorithm, which is computationally efficient and only requires the number of observations (training samples) be the same for the dependent and independent variables. To the best of our knowledge, this article is also the first to investigate the coupling of contours and surfaces, in a linear regression fashion, for modelling shared variations between 2D and 3D geometric features within a statistical shape recovery framework.

The paper is organized as follows: Section 2 describes relevant work related to our approach, Section 3 describes the PLS regression, in Section 4 we introduce the methodology proposed in this article and Section 5 presents an analysis of the experimental results. Finally, conclusions and future work are discussed in Section 6.

@&#RELATED WORK@&#

The problem of 3D face shape estimation has been commonly tackled using statistical models, where 3D face shape and grayscale or color information are related. The main objective of statistical methodologies is to find a set of 3D face shape parameters that best fit to an image or a set of images. Atick et al. pioneered this area with their work in [3], where PCA was used to decompose the 3D structure of the subjects in the database. The main idea here was to perform an optimization process carried out over the parameters of the statistical model in order to fit it to the input image. This approach eventually became the foundation of the morphable model of Blanz and Vetter [4], who formulated an optimization problem to estimate 3D face shape and texture from one or more input images.

In [8] PCA was used to construct separate texture and 3D face shape models only considering information contained in a set of landmarks. Texture information was obtained through a strategy based on triangular subdivision, while facial geometry was gathered by projecting texture onto the 3D shape model. Unfortunately, the data calculated from PCA does not suffice to reach an accurate 3D face shape being necessary to apply a Laplacian filter for noise reduction. A similar work was presented in [9] where a triangular mesh was built by interpolating feature points in the input image using barycentric coordinates. In their work, separate PCA models were constructed, one of them containing points over the triangular mesh and the other one containing all the points in the image. The main idea of this work was to calculate, through a sparse model, the eigenvalues associated to the triangular mesh and then use them to calculate a dense output 3D shape. However, since two separate models were constructed the shared information between 3D shape and grayscale image was still ignored.

Other approaches have also been proposed to simplify the fitting process of the statistical framework. In [10] a coupled model was constructed by combining 3D face shape and grayscale models into a single model. In this work, a set of coupled parameters were found by fitting the coupled model to an input image. Coupled parameters were later used in order to recover 3D face shape parameters and consequently the 3D shape of the face. Although this method shared information between 3D face shape and grayscale subspaces, an optimization process was still required to fit the input image with the 3D shape estimation.

Recently, in [11] a framework for the inverse rendering of faces with a 3D morphable model was developed. The authors proposed to decompose the image formation process into geometric and photometric parts. For the geometric part they estimated 3D face shape parameters given the position of a set of 2D sparse feature points. As an initial step, the camera projection matrix was estimated using known 3D–2D correspondences and the mean 3D face shape, then 3D shape parameters were recovered using the estimated camera projection matrix. The recovered 3D face shape was used to reestimate the camera projection matrix and the process was repeated until convergence. For the photometrical part, two different approaches were proposed to recover texture parameters and specular reflectance properties. The first approach assumed arbitrarily distributed but monochromatic illumination, while the second approach allowed unknown illumination and specular reflectance. Although this method showed accurate results, regularization weights were found empirically and prior terms for texture and illumination were used.

A common feature between the 3D face shape estimation methods mentioned above is that they use PCA for model construction. However, when PCA is performed separately for the 3D face shape and texture, shared information between spaces is ignored. As an alternative to exploit the shared information between spaces for 3D face estimation MLR methods have been recently explored. In [12] a method was proposed to estimate 3D face shape from a color frontal image using canonical correlation analysis (CCA). CCA attempts to find two sets of canonical variables such that correlation between projections is maximal. However, the correlation criterion tends to explain the main axis of the model based on information with small variance, i.e., the regression gives importance to features that are more relevant for the grayscale variation than for the 3D shape estimation. Likewise, in [13] and approach based on CCA and tensor models was presented.

Another approach based on MLR techniques was introduced in [14] where PLS was applied in the regression process. PLS seeks for a set of components that perform a simultaneous decomposition of the 3D face shape and grayscale image spaces with the constraint that these components explain the maximum covariance between both sets. Although results were encouraging, using Cartesian coordinates for 3D shape representation complicates the task of novel view synthesis. To overcome this issue, a method was presented in [6] where 3D face structure estimation was addressed through cylindrical coordinates and PLS. The proposed method first predicts a cylindrical grayscale map from its Cartesian grayscale counterpart through a regression matrix. Then, a second regression matrix predicts cylindrical 3D face shape from the predicted cylindrical coordinate map of grayscale values in order to estimate texture. Finally, grayscale and 3D face shape cylindrical maps are used together to generate novel views of the subject. The outcome of this work was an enhanced 3D face estimation compared with those based on Cartesian coordinates, since cylindrical coordinates overcomes the problems of synthesis errors around the occluding areas of the face. The main drawback of this method was its sensibility to appearance factors in the pixel intensities, such as illuminations and texture variations.

Occluding edges represent strong geometrical information attached to both 2D and 3D shape and have been investigated to enhance the performance of 3D face shape estimation from statistical models. The most similar problem setup to what we consider in this paper was presented by [15], the authors proposed two methods to recover 3D face shape from binary images. The first method used a calibrated multiple camera rig to generate eleven images of a subject. A rigid 3D transformation between the cameras and the axis of the human head was performed in order to align a generic 3D model for pose estimation. To this end, manually assigned reference face points were annotated in at least three images of the subject while the corresponding 3D points of the face model were calculated using a Levenberg–Marquardt optimization. Once the alignment was solved, binary images were extracted for both input and corresponding rendered views obtained from an initial approximation of the shape parameters of the statistical model. These parameters were iteratively refined using a probabilistic non-linear optimization and a boundary weighted X-OR cost function. For the second method, a similar algorithm was applied to a video sequence of a fixed camera and a rotating face. Here, four feature points were needed to locate a 3D mask used to track the facial area along the video frames. After finding six-dimensional motion parameters for each frame, a small subset of significant poses was selected to perform shape recovery as in the first method. This work proved to be effective for the purpose of 3D face shape estimation. Nonetheless, while the approach was sensitive to alignment errors, the time consumed in the optimization process is affected by the continuous generation and alignment of binary images. Moreover, the face silhouette segmentation from input images is assumed to be easy, but in practice this is a non-trivial problem.

Other efforts have been focused on incorporating inner and outer contours into the framework of Blanz and Vetter [4]. For instance, the work of [16] has shown how the combination of multiple image features such as pixel intensities, edges, specular highlights and texture constraints were useful for modeling 3D face shape and texture from a single image. The main contribution of this work was a strong fitting algorithm built from multiple weak classifiers that was more robust to local minima problems. Later, in [17], Keller et. al. explored the isolated contribution of inner and outer edges in the process of fitting a 3D morphable model. To this end, a system that matches silhouettes and inner contours of a 3D morphable model with an input contour image was presented. The authors divided the problem into two main parts: feature extraction and fitting. For the fitting process, efforts were focused on finding an appropriate image distance measure for estimating a robust solution with a small contour error. The idea here was using a robust distance function which ignored unmatched edges and unmatched contours. The feature extraction was performed by a general purpose edge detection algorithm. The main conclusion of this work was that a single contour can ensure contour consistency without imposing tight constraints on the full 3D face shape. In other words, the reconstructed 3D face can be very different from the ground truth while displaying the same 2D contour of the input image.

As an alternative from statistical 3D shape models, other approaches have explored the effect of contours when a reference generic model is available. In [18] a contour-based 3D face model was introduced. At first stage, head pose was estimated comparing the edge maps extracted from video frames against the edge maps extracted from a generic 3D face model. Then, the generic 3D face model adapts itself to the outer contours and internal features by global and local deformations. For global deformation an affine model was used, while for local deformation a stochastic search was computed. As in [15], this algorithm proved to be successful for 3D face estimation and more robust to illumination changes than many structure from motion algorithms. However, computational speed is also an issue since the reconstruction time using 9 video frames is about 1 h.

A different solution that mixes statistical models with multiple-view stereo was developed by [19]. As an initial step an iterative bundle adjustment was performed to estimate the 5 camera poses, then, a dense 3D face shape was reconstructed using contour lines for strong matching. Unfortunately, even when contours added constraints into the fitting process, the accuracy of the dense 3D face reconstruction was sacrificed, i.e. the recovered surface was stable mostly around the areas close to the input image poses. This is somewhat related with the work developed by [20] and [21], where flexibility in statistical 3D models was investigated in order to determine the modes of deformation generated in a surface when certain vertices were constrained to remain as fixed as possible. A main conclusion from this study was that the surface of a face greatly changed if the vertices of the profile line were set to minimally vary while the rest of the vertices were allowed to deform freely. In other words, statistical models of faces may generate subjects whose profile contours are practically the same while their overall face appearance differs considerably.

The state of the art in 3D face shape estimation seeks to obtain either very high quality static models or dynamic models from video. The focus is primarily on applications in graphics and animation where high rendering quality or convincing performance capture are necessary. They can be divided into statistical approaches and those that rely on additional modalities of sensor. Suwajanakorn et al. [22] use large image collections of a single subject to build a person-specific morphable model from the image data alone. Subsequently, this model is fitted to unconstrained video and shape detail added by refining the model using a shading cue. While the quality of the recovered shape is very high, the method can only be applied if a model has been built specifically for the person being analyzed.

Recent advances in facial landmarking [23] has led to the role of landmarks in 3D morphable model fitting to be revisited. Amberg and Vetter [24] use Branch and Bound to fit a morphable model to a set of landmarks containing many false detections. Schonborn et al. [25] integrate automatic landmark detection into a fitting algorithm based on Markov Chain Monte Carlo. Huber et al. [26] recently proposed the idea of using local (SIFT) features for morphable model fitting. They compute SIFT features around the projection of key points from a current model estimate into the input image. They learn a cascaded regressor for computing gradient direction from these feature observations.

An alternative to using a statistical face model is to rely on additional sources of data such as depth cameras or multi-view images or video. Although not applicable to existing 2D images or video, these methods offer the potential for very high accuracy. In particular, there has been intense focus recently on reconstructing dynamic face shape from realtime depth sensors [27,28]. These methods fit blend shape models to noisy depth data allowing real-time tracking and expression transfer. Using conventional cameras, multi-view methods (where a face is observed from multiple directions simultaneously) have proven highly effective in obtaining detailed 3D face models. Bradley et al. [29] and Beeler et al. [30,31] take this approach which is dependent on a very accurate camera calibration. Valgaerts et al. [32] reduced the requirement for controlled conditions, using only a pair of cameras and uncontrolled illumination conditions. It is worth noting that these methods are completely generic: they need not only reconstruct face shape. This means they provide no robustness to occlusion and may not cope well with noise. A face-specific take on multiview reconstruction was proposed by Amberg et al. [33]. Their interesting idea was to use a morphable model as a statistical constraint on correspondence between a pair of stereo images. They seek the model that, when projected into the stereo pair, minimize the difference in intensity implied by the correspondence between views.

It is worth mentioning the role that face alignment and dense registration play in the success of statistical models of faces. Bolkart et al. [34] introduced a method that involves feature detection, rigid alignment and dense, nonrigid registration between scans of 3D faces, which allows performing the statistical analysis of 3D face shapes in motion. This finds applications in the automatic generation of facial animations and face recognition for dynamical expressions. For the removal of noise of 3D faces in motion, Brunton et al. [35] have shown how a multilinear wavelet based representation allows a noisy and occluded sequence of 3D face scans to be fitted onto multilinear models, with applications in telepresence and gaming.

The tracking of 3D feature points along 2D image sequences presents different challenges. In [36], the authors present a real-time, person-independent 3D registration from 2D video, based on a cascade-regression approach. The problem of recovering fine facial details is addressed in [37] using personalized blend shape models, while Shi et al. present an approach based on multilinear facial models and the detection of large-to-fine scale facial deformations in [38].

A step further has been taken by Kemelmacher [39] where the problem of automatic generation of a statistical model from internet categories is addressed. Here, the author applies ideas from uncalibrated photometric stereo in order to generate a 3D model from face images obtained from the internet, exhibiting natural lighting variations [40]. These 3D faces are used to train morphable models that can be further applied for face analysis and facial expression transference. The problem of correspondence and alignment is solved by averaging hundreds of images of faces belonging to similar categories, allowing a 1-to-1 correspondence between pixels and height data. It is worth commenting on the face molding method also introduced by Kemelmacher and Basri [41], which by seeking photometric consistency between a reference 3D model and a single image, attempts to mould the particular face features of the subject to reconstruct. This approach was the starting point of a series of articles related to the recovery of 3D face shape from images taken under uncontrolled conditions.

Finally, other issues in statistical models have been researched, for example, the effect of background modeling has been pointed out as an important source of instabilities such as shrinking and pose misalignments [42].

Multiple Linear Regression is a prediction strategy that aims at modelling the relationship between a set of l independent variables (predictors) and m dependent variables (responses) whose values are known for k observations during a training phase. The training sets are usually organized into matrices X
                     
                        k×l
                      and Y
                     
                        k×m
                     , respectively for the predictors and responses. As a result of the regression process, a matrix B that explains the responses as a linear combination of the predictors is estimated as:

                        
                           (1)
                           
                              
                                 Y
                                 =
                                 a
                                 XB
                                 .
                              
                           
                        
                     
                  

Unlike MLR, which usually builds the regression matrix using a generalized pseudo inverse, other methods focus on dimensionality reduction and subspace projection. In this sense, Partial Least Squares is a special kind of regression method that is based on the criterion of maximum covariance among latent variables, i.e., the projections of every observation on the space spanned by their corresponding sets of predictors and responses.

The method introduced in this article uses a set of occluding contours (the boundaries between the face and the background, as shown in Fig. 1
                     ) in order to predict its corresponding 3D dense map. As the variables with which we are dealing lie in both the space of pixels and the space of 3D vertices (see Fig. 2
                      (a) and (b)), X and Y become matrices of long column vectors where the number of observations k is far less than the number of variables l and m for X and Y, respectively. This problem, referred to as the low observation-to-variable ratio, benefits from the maximum covariance criterion because it naturally deals with the collinearity produced by the long structure of matrices X and Y 
                     [43]. Additionally, maximizing covariance provides PLS with the flexibility lacking in other regression methods. For instance, Canonical Correlation Analysis (CCA) [13] applies the maximum correlation criterion between latent variables in order to find strictly linear relationships among them. Other approaches aimed at dimensionality reduction such as Principal Component Regression (PCR), maximize covariance only within the set of the predictor variables. It is worth noticing that, when all the dimensions are kept, either approach delivers results comparable to those delivered by MLR.

The PLS algorithm iteratively builds an r-dimensional regression matrix, where r is the number of retained latent variables considered for regression. In the first step, two association matrices XX
                     
                        T
                      and YY
                     
                        T
                      are used to create the kernel matrix XX
                     
                        T
                     
                     YY
                     
                        T
                     . The first eigenvector 
                        
                           
                              t
                              i
                           
                           ,
                           i
                           =
                           1
                        
                      of the kernel matrix, becomes the first latent vector of the predictor set, but an additional condition of 
                        
                           
                              t
                              i
                              T
                           
                           
                              t
                              i
                           
                           =
                           1
                        
                      is required in order to ensure the maximum covariance criterion. Later, t
                     
                        i
                      is used to calculate 
                        
                           
                              u
                              i
                           
                           =
                           a
                           
                              YY
                              T
                           
                           
                              t
                              i
                           
                           ,
                        
                      the first latent vector corresponding to the response set.

After the initial calculation of t
                     
                        i
                      and u
                     
                        i
                      the kernel matrix must be updated. In the next equations it is shown how XX
                     
                        T
                      and YY
                     
                        T
                      can be updated with right and left multiplications (as shown in [43]) using the matrix 
                        
                           
                              G
                              i
                           
                           =
                           I
                           −
                           
                              t
                              i
                           
                           
                              t
                              
                                 i
                              
                              T
                           
                        
                      for each iteration 
                        
                           i
                           =
                           {
                           1
                           ,
                           2
                           ,
                           …
                           ,
                           r
                           }
                        
                      , where I is the identity matrix and i is the index of the current latent variable being calculated. Once G
                     
                        i
                      is at hand, the ith latent variable is subtracted from XX
                     
                        T
                      and YY
                     
                        T
                      as:

                        
                           (2)
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                X
                                                ^
                                             
                                             i
                                          
                                          
                                             
                                                X
                                                ^
                                             
                                             
                                                i
                                             
                                             T
                                          
                                          =
                                          
                                             G
                                             i
                                          
                                          
                                             
                                                X
                                                ^
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                          
                                          
                                             
                                                X
                                                ^
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                             T
                                          
                                          
                                             G
                                             i
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (3)
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                Y
                                                ^
                                             
                                             i
                                          
                                          
                                             
                                                Y
                                                ^
                                             
                                             
                                                i
                                             
                                             T
                                          
                                          =
                                          
                                             G
                                             i
                                          
                                          
                                             
                                                Y
                                                ^
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                          
                                          
                                             
                                                Y
                                                ^
                                             
                                             
                                                i
                                                −
                                                1
                                             
                                             T
                                          
                                          
                                             G
                                             i
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     
                  

Thus, the XX
                     
                        T
                      and YY
                     
                        T
                      matrices are updated by multiplication with matrix G
                     
                        i
                      on the right and left. It is worth pointing out that the size of the relevant matrices remains k × k. The goal of updated matrices 
                        
                           
                              X
                              ^
                           
                           i
                        
                      and 
                        
                           
                              Y
                              ^
                           
                           i
                        
                      is to store the residues of X and Y, respectively, i.e., 
                        
                           
                              
                                 X
                                 ^
                              
                              i
                           
                           
                              
                                 X
                                 ^
                              
                              
                                 i
                              
                              T
                           
                        
                      is the deflated version of XX
                     
                        T
                      at the kth dimension, the same holding for 
                        
                           
                              
                                 Y
                                 ^
                              
                              i
                           
                           
                              
                                 Y
                                 ^
                              
                              
                                 i
                              
                              T
                           
                           ,
                        
                      while matrix G
                     
                        i
                      is an updating operator.

Once the calculated dimension has been subtracted from the kernel matrix, the following eigenvectors are calculated to obtain the new vectors t
                     
                        i
                      and u
                     
                        i
                     . When the number of dimensions r has been reached, all vectors t and u are stored as the column vectors of matrices T and U, respectively. Finally, the regression matrix B is calculated as:

                        
                           (4)
                           
                              
                                 
                                    
                                       B
                                    
                                    
                                       =
                                    
                                    
                                       
                                          W
                                          
                                             
                                                (
                                                
                                                   P
                                                   T
                                                
                                                W
                                                )
                                             
                                             
                                                −
                                                1
                                             
                                          
                                          
                                             C
                                             T
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (5)
                           
                              
                                 
                                    
                                       W
                                    
                                    
                                       =
                                    
                                    
                                       
                                          
                                             X
                                             T
                                          
                                          U
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (6)
                           
                              
                                 
                                    
                                       P
                                    
                                    
                                       =
                                    
                                    
                                       
                                          
                                             (
                                             
                                                T
                                                T
                                             
                                             X
                                             )
                                          
                                          
                                             
                                                (
                                                
                                                   T
                                                   T
                                                
                                                T
                                                )
                                             
                                             
                                                −
                                                1
                                             
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (7)
                           
                              
                                 
                                    
                                       C
                                    
                                    
                                       =
                                    
                                    
                                       
                                          
                                             (
                                             
                                                T
                                                T
                                             
                                             Y
                                             )
                                          
                                          
                                             
                                                (
                                                
                                                   T
                                                   T
                                                
                                                T
                                                )
                                             
                                             
                                                −
                                                1
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     
                  

The matrices W and C contain the variations of the information extracted from the predictors and the responses, while P is the matrix of the loadings for the predictors. Note that a new estimation response vector y′ can be achieved by directly using matrix B and a new predictor vector x′, i.e.,

                        
                           (8)
                           
                              
                                 
                                    y
                                    ′
                                 
                                 =
                                 B
                                 
                                    x
                                    ′
                                 
                                 .
                              
                           
                        
                     A summary of the relevant technical terms and symbols used in in this paper is presented in Table 1
                     .

This section provides a detailed explanation about the proposed regression method and its application to the problem of face shape estimation. To this end, a first subsection describes how the 2D contour and 3D shape databases were generated using a statistical face model. Four regression cases are presented in a second subsection, where different subsets of 2D contours are presented in order to investigate their influence in the regression process.

The database used for training and experimental evaluation of our system was built using the Basel Face Model (BFM) [44]. The BFM parameterizes faces as triangular meshes. We use a subset of the complete model comprising 
                           
                              m
                              =
                              16512
                           
                         vertices corresponding to the interior region of the face. For each vertex 
                           
                              
                                 v
                                 j
                              
                              =
                              
                                 
                                    (
                                    
                                       x
                                       j
                                    
                                    ,
                                    
                                       y
                                       j
                                    
                                    ,
                                    
                                       z
                                       j
                                    
                                    )
                                 
                                 T
                              
                              ∈
                              
                                 R
                                 3
                              
                              ,
                              j
                              =
                              
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 m
                                 }
                              
                           
                         there exists an associated normalized RGB color vector 
                           
                              
                                 c
                                 j
                              
                              =
                              
                                 
                                    (
                                    
                                       r
                                       j
                                    
                                    ,
                                    
                                       g
                                       j
                                    
                                    ,
                                    
                                       b
                                       j
                                    
                                    )
                                 
                                 T
                              
                              ∈
                              
                                 
                                    [
                                    0
                                    ,
                                    1
                                    ]
                                 
                                 3
                              
                           
                        . The 3D meshes corresponding to the different subjects of the database underwent a dense correspondence estimation process in order to avoid instabilities generated by the spatial variability between the 3D facial shapes. As a result, a unique label is assigned to each vertex and this label is shared among all the subjects in the database. The BFM consists of two independently built statistical models for 3D face surface and texture using PCA. This parametric face model can be represented as:

                           
                              (9)
                              
                                 
                                    
                                       
                                          
                                             
                                                M
                                                s
                                             
                                             =
                                             
                                                (
                                                
                                                   μ
                                                   s
                                                
                                                ,
                                                
                                                   σ
                                                   s
                                                
                                                ,
                                                
                                                   D
                                                   s
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (10)
                              
                                 
                                    
                                       
                                          
                                             
                                                M
                                                t
                                             
                                             =
                                             
                                                (
                                                
                                                   μ
                                                   t
                                                
                                                ,
                                                
                                                   σ
                                                   t
                                                
                                                ,
                                                
                                                   D
                                                   t
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where subindexes s and t, respectively stand for shape and texture, μ
                        {s,t} and σ
                        {s,t} are the mean and the standard deviation vectors, and D
                        {s,t} are the orthonormal basis or principal components of the 3D shape and texture values along the subjects of the database. Thus, new faces can be generated as linear combinations of the principal Captions are correct:

                           
                              (11)
                              
                                 
                                    
                                       
                                          
                                             s
                                             
                                                (
                                                α
                                                )
                                             
                                             =
                                             
                                                μ
                                                s
                                             
                                             +
                                             
                                                D
                                                s
                                             
                                             
                                                σ
                                                s
                                             
                                             α
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (12)
                              
                                 
                                    
                                       
                                          
                                             t
                                             
                                                (
                                                β
                                                )
                                             
                                             =
                                             
                                                μ
                                                t
                                             
                                             +
                                             
                                                D
                                                t
                                             
                                             
                                                σ
                                                t
                                             
                                             β
                                          
                                       
                                    
                                 
                              
                           
                        where the parameter vectors {α, β} represent the linear combinations, i.e., the shape and texture coefficients applied onto the model in order to generate new faces.

Since our work focuses on the relationship between 2D and 3D shape variations, the texture model of the BFM was not considered for the database generation, instead, 120 face surfaces s(α) were obtained from normally distributed random vectors α whose limits were bounded by ±σs
                        . Additionally, a set of eleven subjects is available with the BFM. These examples correspond to faces of real persons and were also used for testing the performance of our methodology. One of the subjects belonging to this subset is depicted in Fig. 1. In this way, 120 synthetically generated faces plus 10 real world faces were available for the purposes of experimental evaluation.

As far as the 2D contour generation is concerned, a set of 
                           
                              n
                              =
                              62
                           
                         poses was obtained from each surface in order to create their corresponding head pose images. The synthesized images were generated with a matlab pointcloud renderer and observed an orthographic projection (examples of these images are shown in Fig. 2). Pose variations separated by 5° were applied along the Pitch and Roll angles of the face within the intervals of 
                           
                              [
                              −
                              
                                 90
                                 ∘
                              
                              ,
                              −
                              
                                 15
                                 ∘
                              
                              ]
                           
                         and [15°, 90°]. Any pose outside this range is not important as it is nearly frontal and therefore does not provide meaningful occluding contour information. Since contour extraction is performed over a set of n pose head images, it is worth mentioning that these images may be obtained by different means, i.e. a rotating camera, a set of multiple fixed cameras and a rotating subject, or even a set of video frames. In this paper, for training and experimental evaluation, a rotating camera was used to obtain the required set of n pose images. Fig. 1 depicts an example of the set of poses used to extract contours for each subjects in the database.

For contour extraction, a generic threshold based foreground extractor was used. The output of this algorithm is a binary image, i.e., the vector of [(x
                        1, y
                        1), (x
                        2, y
                        2), ⋅⋅⋅, (xp, yp
                        )] coordinates corresponding to the columns and rows of the image where the pixel foreground value is equal to 1. For each binary image a final contour corresponding to the boundary between the face and background was finally recorded. From this set of contours, a linear interpolation was performed between neighboring points in order to obtain a new contour whose inter-neighbor distance is constant for a total of 
                           
                              p
                              =
                              80
                           
                         points. This step is performed to guarantee that all contours are sampled with the same number of points. Additionally, in order to isolate facial features, a mask was used to constrain the surface coverage strictly within the facial area, avoiding shape transitions such as chin-to-neck, cheek-to-ears and forehead-to-hair. Fig. 1 shows a diagram where head pose variation and contour extraction is depicted for one of the real subjects of the BFM.

Once the n different contours are acquired, they are stacked together into a long column vector x in order to build the predictor matrix X, whose columns are later aligned through a Procrustes step. This procedure is depicted in Fig. 2(a). Likewise, to construct the response matrix Y, the vertices that shape the 3D face surface vector 
                           
                              [
                              
                                 (
                                 
                                    x
                                    1
                                 
                                 ,
                                 
                                    y
                                    1
                                 
                                 ,
                                 
                                    z
                                    1
                                 
                                 )
                              
                              ,
                              
                                 (
                                 
                                    x
                                    2
                                 
                                 ,
                                 
                                    y
                                    2
                                 
                                 ,
                                 
                                    z
                                    2
                                 
                                 )
                              
                              ,
                              …
                              ,
                              
                                 (
                                 
                                    x
                                    m
                                 
                                 ,
                                 
                                    y
                                    m
                                 
                                 ,
                                 
                                    z
                                    m
                                 
                                 )
                              
                              ]
                           
                         are reshaped into a long column vector y as shown in Fig. 2(b). The training phase and regression process are roughly depicted in Fig. 2(c), where it is shown how an out-of-training input contour vector is used to directly predict 3D face surface vertices, as in Eq. 8. Likewise, the training and regression processes are described in Algorithm 1.
                     

In order to investigate the success of the proposed methodology in terms of the 2D contours used during the training phase, four different regression strategies were explored. The number of contours as well as the importance of the main profiles were considered for designing these strategies, which are outlined in Table 2
                        . The first column of the table shows the identifiers for the four strategies as R1, R2, R3 and R4, the second column lists the angular directions considered for each case, the number of contours used is shown in the third column of the table and finally the separation angle applied to select the different contours is presented in the fourth column.

The first strategy (R1) attempts to estimate 3D face shape by only using the two contours obtained from those images were the value of the Pitch and Roll angles is 90°, i.e., both contours represent the main profiles of the face. Here, the aim was to explore the influence of the two main profiles in the 3D shape prediction. For the second case (R2), all the variations generated for the Pitch direction were included. i.e., the subset of 31 contours generated every 5° was selected out of the complete 62 contour set. The main idea of R2 was to determine the feasibility of only using information related to the Pitch direction. The aim of the third strategy (R3) was to use a smaller amount of information in Pitch (only 12 contours separate 15° each) but including the main profile in the Roll direction, as this contour may be easier to segment from background than the rest of the contours along the Roll direction. Finally, the fourth strategy (R4) considers the complete set of 31 × 2 contours in both Pitch and Roll directions. With R4, the interest is focused on the outcome of using the most variations along both directions. Note that the different strategies listed in Table 2 do not affect the kind of information spanned by the response set, in other words, the number of 3D vertices remains the same regardless of the number of contours.

@&#EXPERIMENTS@&#

In order to calculate the regression matrix it is required to construct both the 2D and 3D information training matrices X and Y. In this work, 120 synthetical 3D face surfaces were generated by BFM for experimental purposes. The 3D vertices of each subject were reshaped into a long column vector y for building the response matrix Y. On the other hand, for 2D contour generation, a set of 
                        
                           n
                           =
                           62
                        
                      poses was obtained from each face surface to create their corresponding head pose images. Later, a generic threshold based foreground extractor was used. Likewise, to construct the predictor matrix X the 2D contours were stacked together into a long column vector x. Additionally, a set of 10 real world faces was available for the purposes of experimental evaluation.

This section presents the result of applying the 3D face estimation method proposed in this paper. The first part of the section provides a visual comparison among the four different strategies performed on both synthetically generated and real subjects. In the second part, a quantitative verification is presented through error distribution diagrams along with an identification performance for each regression strategy. It is worth mentioning that all results shown in this section correspond to out-of-training subjects.


                        Fig. 3
                         provides Lambertian renderings of the recovered surfaces for a visual comparison against ground truth. The figure presents, row-wise, 8 out of the 120 examples of the synthetically generated subjects of the database. The recovered subjects are depicted for each regression strategy through the first four columns of the figure while the right-most column shows the ground truth surface. 3D face shape approximations were performed following the leave-one-out evaluation method, i.e., a subset of 119 subjects was used for training in order to predict the 3D shape of the remaining test subject from its 2D contour data and the process was repeated for all the subjects in the dataset. Before commencing the analysis of the figure, it is important to recall that strategies R1, R2, R3 and R4 involve different number of contours obtained from varying the Roll and Pitch pose angles. As a general observation from Fig. 3, the similarity between ground truth and the obtained surface approximations becomes more evident as the number of 2D contours increases (R2 and R4) or when important information is added (R3).

The surface estimations obtained using R1 reveal that only the data contained in the two main contours, where the Roll and Pitch angles are set to 90°, do not contribute with enough variability for achieving an accurate approximation of the whole 3D vertex map. For strategy R2, where 31 contours are acquired through variations along the Pitch angle, the surface approximations appear visually improved in comparison with those of R1, i.e., the similarity with ground truth corresponding to the eyes, nose, mouth and shape of the face is clearly enhanced. The R3 strategy requires a considerably smaller number of contours along the Pitch direction but includes the main profile for the Roll direction. The results for this strategy are indeed visually comparable to those obtained with R2, which suggests that the number of contours diminished in the Pitch direction are compensated with the main Roll profile. Finally, for R4, when the whole set of 62 contours is used, the outcome of using all the 2D information available also leads to approximations qualitatively similar to those obtained using R2 and R3. However, for some subjects (i.e. 2, 3, 5 and 8), the overall shape of the face tends to better fit the ground truth; for example, after transitioning from R3 to R4, the facial shape of subject 2 appears reduced and the eyes of subjects 3 and 8 undergo a slight change in shape.

As far as the real subjects are concerned, Fig. 4
                         presents a visual comparison among 8 subjects from the set. It is important to mention that none of these subjects was considered for training, therefore the leave-one-out method does not apply here, as the aim of this experiment was to isolate the contribution of the real subjects when building the regression matrix. In other words, the training phase was carried out using the 120 subjects of the synthetic database, while the real world subjects were only used for tests. The figure is organized in a similar manner as Fig. 3, this time including a near-profile pose. The ground truth is labeled as “GT” in columns 4 and 8 of the figure. Results corresponding to R1 are not considered here as it was shown that they not provide approximations comparable to R2, R3 and R4. The texture of the subjects is overlapped on each 3D face surface estimation in order to show how the appearance of a person changes in accordance with the 3D shape variations captured in the regression strategies.

Changes in appearance are more noticeable for some of the frontal pose examples, such as subjects 4 and 6, where the transition from R2 to R4 is more evident than for the rest of the examples. However, in the semi-profile views, changes in the eyes, nose and mouth areas are easier to notice when the different strategies are applied. Through the visual inspection of the surrounding face contours, this figure also highlights the variabilities in shape corresponding to each subject and the extent with which the regression strategies resemble the ground truth. Note how by warping texture onto shape the difference between each regression strategy becomes more noticeable than the difference exhibited in Fig. 3. This is due to the fact that none of the subjects in this set was built from the same parameter distribution as the synthetic training faces, i.e., real subjects exhibit particularities that are more difficult to recover from a normal distribution.

In Fig. 5
                        , the subjects shown in Fig. 4 are used for a profile view comparison among strategies. An interesting observation from this figure is that the main profile seems to be nicely approximated regardless of the strategy used. This fact may be explained as a consequence of the main profile (in the Pitch direction) being always considered in the PLS training process. However, on its own, the information contained in this profile is insufficient to provide the required variability for an accurate 3D face prediction. The difference between each 3D face estimation strategy (red line) and the ground truth (black line) is depicted in the profile line plots. Note how, for some of the subjects, i.e. subject 1, requiring all contours was not necessary (as in R4) in order to achieve an accurate 3D face approximation of the main profile. On the contrary, for subject 6 a more accurate reconstruction is achieved when using all available contours. This is caused by the PLS learning stage, where the retained characteristics describe better the 3D shape of subject 1 than the shape of subject 6.

In order to provide additional visual details on the shape recovery results for these two particular examples, Fig. 6
                         provides a comparison of three different texture and untextured (Lambertian rendering) views for each regression strategy. The aim of the figure is to visually exhibit the variations in both shape and texture caused by the 3D shape approximations. Note how using R4 clearly outperforms R3 in predicting facial shape of subject 6, showing that this subject requires a greater number of contours in both Pitch and Roll directions in order to achieve a reasonable approximation. This is opposed to the results related to subject 1, where the transition from R2 to R4 are not as noticeable as in the case of subject 6. As a concluding observation from this analysis, while real subjects represent challenging examples, our method delivers qualitatively good results when all contours are available (R4), with a similar outcome when the number of contours along the Pitch direction is reduced and the main profile in the Roll direction is retained (R3). Therefore, it is feasible to consider a reduced number of contours, as long as these exhibit a good variability around the facial shape, in order to build a regression matrix that is capable to predict reasonable surface approximations. This observation is supported by the results described in the next subsection.

This subsection presents quantitative results for the different regression strategies depicted in this article. Two measures of error are introduced to evaluate the success of surface approximation.

Let S
                        
                           gt
                         and S′ be the v × 3 matrices whose rows contain the v vertices (x, y, z) for the ground truth and approximated surfaces of a test subject. Let 
                           
                              
                                 s
                              
                              i
                              
                                 g
                                 t
                              
                           
                         and 
                           
                              
                                 
                                    s
                                 
                                 i
                                 ′
                              
                              ,
                           
                        
                        
                           
                              i
                              ∈
                              {
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              v
                              }
                           
                         represent the ith row of matrices S
                        
                           gt
                         and S′, respectively. The Euclidian distance error dist and angular error ang are scalars that represent the difference between the vertices of the ground truth and the approximated surfaces in terms of magnitude (micrometers) and direction (degrees), respectively. These are shown in the following equations:

                           
                              (13)
                              
                                 
                                    
                                       
                                          
                                             d
                                             i
                                             s
                                             t
                                             =
                                             
                                                1
                                                v
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                v
                                             
                                             
                                                ∥
                                                
                                                   (
                                                   
                                                      s
                                                      i
                                                      
                                                         g
                                                         t
                                                      
                                                   
                                                   −
                                                   
                                                      s
                                                      i
                                                      ′
                                                   
                                                   )
                                                
                                                ∥
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (14)
                              
                                 
                                    
                                       
                                          
                                             a
                                             n
                                             g
                                             =
                                             
                                                1
                                                v
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                v
                                             
                                             arccos
                                             
                                                
                                                   
                                                      s
                                                      i
                                                      
                                                         g
                                                         t
                                                      
                                                   
                                                   ·
                                                   
                                                      s
                                                      i
                                                      ′
                                                   
                                                
                                                
                                                   
                                                      ∥
                                                   
                                                   
                                                      s
                                                      i
                                                      
                                                         g
                                                         t
                                                      
                                                   
                                                   
                                                      ∥
                                                      ∥
                                                   
                                                   
                                                      s
                                                      i
                                                      ′
                                                   
                                                   
                                                      ∥
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The error measures were registered for all the 120 synthetically generated examples as well as for the 10 real subjects. The results are shown in Fig. 7
                        , which introduces box plots to depict the distribution of errors for the different regression approaches. For each box, the central mark is the median, the edges of the box are the 25th and 75th percentiles, the whiskers extend to the most extreme data points not considered outliers, and outliers are plotted individually as red marks. Results corresponding to synthetical subjects are shown in (a) and (b). In accordance with the qualitative analysis in the previous subsection, the figure shows that the error measures tends to diminish while more 2D contours are added. i.e., the transition from R1 to R2. However, the number of contours seems to be as important as the quality of information they provide, for example, the error distribution of strategy R3 is smaller than that of R2 even when more contours are available for the latter. This behavior indicates that including the main profile in Roll increases the correlation between a set of 2D contours and the 3D vertex map of a face. It is worth to mention that each 3D face estimation and its corresponding ground truth were aligned through a Procrustes step before computing the error measures.

To complement the conclusions from the qualitative analysis of Fig. 3, the eight subjects appearing in this figure are marked as black asterisks in the boxplots of Fig. 7 (a) and (b). Within these subsets, the subjects with the lowest and greatest errors are indicated with the subject number. This information was included in order to provide a graphical match between qualitative and quantitative results. Note how for the synthetically generated subjects (a) and (b), examples are sorted from subject 1 to number 8 in the regression strategy R1. While the error tends to decrease from R1 to R4, the order of the subjects is not necessarily kept. Also, the smallest transition seems to happen between R3 and R4, indicating the importance of the contours obtained from variations along the Roll direction.

In order to provide a comparison between our regression-based 3D face estimation technique and the existing state-of-the-art in 3D morphable model fitting, we provide two sets of results. First, we apply the recent landmark-based fitting algorithm of Aldrian and Smith [11] to the same set of data used by our algorithm in the four different regression strategies. Specifically, for each view we compute the 2D position of the subset of Farkas landmarks [45] that are visible in each view. We solve for the single set of shape parameters that best fits the landmark observations in all views. To do so, we extend Aldrian and Smith’s formulation to multiple views by stacking the linear system of equations that arises from each view into a single large linear system. For simplicity, we assume constant variance for each landmark point and enforce plausibility of the solution using a hard hyperbox constraint as suggested by Brunton et al. [46]. This avoids having to choose a regularization weight and reduces mean shape bias. This method provides an interesting comparison since it also relies purely on geometric information (landmarks versus contours) but is based on fitting a morphable model rather than regression between observations and shape directly.

Second, we compare to the state-of-the-art analysis-by-synthesis approach: the multi-features fitting algorithm of Romdhani and Vetter [16]. This multi-feature algorithm estimates the 3D face shape and texture from a single image by recovering the parameters of a 3D morphable model using several cues such as pixel intensities, landmark points, specular highlights, contour edges and texture edges. The motivation behind this comparison is to determine in which extent purely geometrical information such as contours, when combined with MLR solutions, achieve results comparable with sophisticated heuristics relying on model fitting and both geometric and photometric features. For each of the 10 real subjects, face surface estimations were performed using a single input image along nine different poses varying between ± 70° in Pitch. The illumination direction taken into account for this experiment was the frontal direction.

It is important to mention that before computing any error measure, for the proposed approach and the comparison techniques, we performed a Procrustes alignment between each approximated surface and its corresponding ground truth. This was applied in order to eliminate additional estimation noise due to small pose variations between the recovered and real surfaces. In Fig. 7(c) and (d), the error boxes labeled as Rom_max and Rom_min show the maximum and minimum Euclidian distance and angular error of the 3D face estimations obtained by the approach of Romdhani and Vetter against ground truth, respectively, i.e., for the 10 × 9 input images we selected the maximum and minimum error per subject in order to provide an idea of the best and worst performance of the method. Note how the errors obtained by our 3D face estimation technique (labeled as R1, R2, R3 and R4) are similar to those obtained by the comparison approach based on multi-feature model fitting. Likewise, the error boxes labeled as A1, A2, A3 and A4 show the error measures obtained by Aldrian and Smithś landmark-based fitting algorithm. It is important to remember that the same set of data used by our algorithm in the four different regression strategies was also used by this approach. Note how the errors of our estimation technique and the comparison approach are similar and tend to decrease when more input information is available. It is also important to note that both approaches approximate the subjects in a similar manner, i.e. both techniques estimate subject 10 with a low error value and subject 3 with a high error value. Interestingly, adding more input images does not seem to alter the results delivered by A2, A3 and A4, which suggests that only varying the Pitch angle provides enough constraints.

The error measures show that our estimation technique exhibits a performance that is comparable with the state of the art, having only slight variations from the best results. It is worth mentioning that face surface estimation using our 3D face recovery methodology represents a direct regression step based on a multiplication between a regression matrix and an input long vector that contains a set of 2D contours. Therefore, our method is the first to model the problem of face shape recovery on the pure basis of statistical regression between 2D and 3D data, which can be thought of a direct mapping of image features onto a 3D shape prediction. Approaches based on multi-features fitting techniques estimate 3D face surface using only a single image as input information, at the cost of iteratively seeking parameters of a 3D morphable model while simultaneously modelling several features, thus sacrificing computational time and model simplicity. On the other hand, techniques based on landmark fitting algorithms depend on the geometrical information contained in a set of 2D landmarks, but they are also subject to the optimization process of fitting a 3D morphable model. In this sense, this paper shows how regression methods also represent a simple yet feasible alternative for 3D surface estimation when more information other than a single image is available, especially if this information contains features that naturally correlate 2D and 3D facial shape, such as outer contours.

To finish this subsection, in Fig. 8
                        , error maps are shown in (a) and (b), respectively, for subjects 1 and 6 of the real database (the two subjects appearing in Fig. 6). From left to right, error maps for R1, R2, R3 and R4 are shown, where brighter values indicate greater errors and darker values represent errors approaching to zero. The main feature to note here is the benefit of adding new contours in terms of the error decreasing around certain surface regions, being the areas near the eyes, mouth and nose the most difficult to recover. Also, unlike subject 6, subject 1 does not seem to noticeably benefit from the transition between R1 and R4, which indicates that subject 1 is a more typical example and can be better approximated by the regression process.

In order to explore the scope for the approximated surfaces for the purposes of facial identification a third error measure is introduced. The escalar id is a measure of error representing the percentage of difference between the vertices of the surface estimated through regression and the vertices of the ground truth surface. The purpose of these new error measure is to provide a sense of divergence from ground truth that can be related to a percentage value, the error is calculated as the reason of the Euclidian distance among every ground truth and estimated vertices with respect to the total length of the corresponding ground truth vertex. This is shown in the following equation:

                           
                              (15)
                              
                                 
                                    i
                                    d
                                    =
                                    
                                       100
                                       v
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       v
                                    
                                    
                                       
                                          
                                             
                                                
                                                   (
                                                   
                                                      s
                                                      i
                                                      
                                                         g
                                                         t
                                                      
                                                   
                                                   −
                                                   
                                                      s
                                                      i
                                                      ′
                                                   
                                                   )
                                                
                                                T
                                             
                                             
                                                (
                                                
                                                   s
                                                   i
                                                   
                                                      g
                                                      t
                                                   
                                                
                                                −
                                                
                                                   s
                                                   i
                                                   ′
                                                
                                                )
                                             
                                          
                                       
                                       
                                          
                                             ∥
                                          
                                          
                                             s
                                             i
                                             
                                                g
                                                t
                                             
                                          
                                          
                                             ∥
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Once all the errors are recorded for the 
                           
                              120
                              +
                              10
                           
                         subjects, the minimum error is located and its correspondence with subjects identity is checked. If the first match is not successful, the second minimum error is evaluated. The procedure is repeated along the next smallest errors, until a successful identification is achieved. An alternative measure of surface similarity is therefore the percentage of subjects successfully identified within a small number of attempts. In the ideal case, all of the examples would have been ranked first place. The main idea of this evaluation procedure is to obtain a measure of accuracy for 3D shape estimation, for the set of 130 subjects of both databases, as well as exploring the potential of such estimations for the purposes of model based identification. Fig. 9
                         (a) presents the final ranking results. It can be noticed from the figure that roughly a 70%, 80% and 90% of the subjects were identified in first place, for R2, R3 and R4, respectively. Around a 10% identification increase occurs when subjects are ranked from first to third place, as percentages raise to near 75, 90 and 100%, respectively for R2, R3 and R4. On the contrary, results delivered by R1 perform poorly, corroborating the fact that only considering the two main contour profiles of the face does not suffice for accurately approximating 3D surface.

Finally, Fig. 9(b) presents, for visual evaluation, four of the cases where the identification system failed. In the first row, the 3D estimation (false negative) obtained following strategy R1 is depicted in the first column, while the second column presents the 3D estimation that minimized the error against ground truth (false positive). Similarly, for the remaining rows, examples are shown for R2, R3 and R4. Due to the low accuracy of the 3D estimation in R1, it is expected that the identification fails. However, for the subjects analyzed from R2, R3 and R4, the false negative estimations seem to keep facial features that resemble better those of the ground truth, as opposed to their false positive counterparts, where the overall shape of the face seems to be responsible for the high ranking. It is worth noticing how the false negative estimations seem to be affected by a scaling which either elongates or shrinks the recovered surface. Because the first eigenvector of the BFM is related with the size of the face, this result suggests that, for some subjects, PLS struggles to predict the dimension corresponding to the main variation of the training set. Since the error measurement is performed between vertices, changes in scale lead to greater distance values, therefore causing a false positive identification.

@&#CONCLUSIONS@&#

This article has introduced a novel methodology for estimating 3D face shape information from a group of contours through a PLS regression matrix. Our approach is based on the idea that occluding contours obtained from multiple image views contain information that is meaningful for a successful prediction of a 3D face mesh. Using contours as 2D training information allows to focus only on geometrical shape variations, leaving aside the modelling of texture and illumination variations. Although this sacrifices modelling appearance, it has potential applications in 3D face modelling, recognition and head pose estimation from video sequences. The major contribution of our MLR based technique is a simple and efficient way for estimating 3D face shape from imagery. For a 1.90 GHz processor with 6 Gb RAM, building the regression matrix takes around 0.7 s while predicting a new example from a set of contours roughly takes 0.01 ms. A limitation of our approach is that either the head or camera poses corresponding to the input images should be estimated in order generate the training samples to build the regression matrix in accordance with the expected inputs. Also, we assume that 2D contours are already available. For this reason, coupling our method with state of the art contour extraction [47] and head pose estimation techniques for video frames [48] is considered as future work. In addition, it is worth to consider using other projections for 2D contours estimation besides orthographic and to combine our direct estimation methodology from contours with texture or shading based data as used in other works.

@&#REFERENCES@&#

