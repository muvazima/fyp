@&#MAIN-TITLE@&#A texton-based kernel density estimation approach for background modeling under extreme conditions

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Background and foreground modeling method able to run seamlessly under extreme conditions.


                        
                        
                           
                           Modeling structural variations of pixels’ neighbors via joint domain-range approach integrating textons into the model.


                        
                        
                           
                           Exhaustive testing of state-of-the-art approaches on real-life scenarios.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Background and foreground modeling

Non-parametric kernel density estimation

Texture analysis

Video analysis

@&#ABSTRACT@&#


               
               
                  Background modeling is a well-know approach to detect moving objects in video sequences. In recent years, background modeling methods that adopt spatial and texture information have been developed for dealing with complex scenarios. However, none of the investigated approaches have been tested under extreme conditions, such as the underwater domain, on which effects compromising the video quality affect negatively the performance of the background modeling process. In order to overcome such difficulties, more significant features and more robust methods must be found. In this paper, we present a kernel density estimation method which models background and foreground by exploiting textons to describe textures within small and low contrasted regions. Comparison with other texture descriptors, namely, local binary pattern (LBP) and scale invariant local ternary pattern (SILTP) shown improved performance. Besides, quantitative and qualitative performance evaluation carried out on three standard datasets showing very complex conditions revealed that our method outperformed state-of-the-art methods that use different features and modeling techniques and, most importantly, it is able to generalize over different scenarios and targets.
               
            

@&#INTRODUCTION@&#

Detecting moving objects in videos is a fundamental task for many machine vision systems such as object tracking, behavior understanding, and event detection. One of the most common approach to identify moving object is resorting to background modeling approaches, which aim at building an estimated image of the scene without objects of interest; this model is then compared to each new video frame for identifying foreground objects. The most popular approaches to achieve this goal are the density-based ones, where the distribution of each background pixel is modeled by either a probability density function (e.g. Gaussian) [1] or non-parametric kernel density estimation [2]. In early years, these approaches exploited only historical variations of pixel colors; recently, we have witnessed a trend towards background modeling methods that, instead, employ spatial and texture information, i.e. methods that build the background scene by taking into account textures computed on neighboring pixels. In addition, approaches that estimate the background and the foreground separately have been favored to the ones relying only on the background model, since the latter do not account for the spatial and temporal changes that may happen in the foreground. For example, let us consider the case of a dark red object moving from a blue background to a light red one; if only the background is modeled, when the object enters the red zone it could be missed, depending on the chosen difference threshold. Instead, this could be avoided if the foreground is also modeled, due to the closer resemblance of the object to the foreground model than to the background one. This example also proves the necessity of resorting to textures, because color similarities may not suffice to discern accurately background from foreground.

While excluding foreground modeling and spatial/texture information might work and make the algorithms run effectively in simple scenarios, the tides turn against them when more complex scenes need to be modeled, as in the case of underwater video surveillance [3] (see the Fish4Knowledge project.
                        1
                        
                           http://www.fish4knowledge.eu
                        
                     
                     
                        1
                     ) The underwater case, in particular, is a rather complex scenario, as it may show a combination of features at the same time that affects the performance of the majority of the existing background modeling approaches. For instance, dynamic or multi-modal backgrounds, abrupt lighting changes, and radical and instant water turbidity changes can all be found in the same scene. To complicate even more the situation, the underwater environment shows two almost exclusive characteristics with respect to other domains: three degrees of freedom and erratic movements of objects (i.e. fish). This makes fish less predictable than people or vehicles, as fish may move in all three directions changing their size and their shape in the video. Moreover, because of light propagation in water, fish may also change rapidly appearance. Background modeling approaches should be able to operate effectively also in these cases without the need to re-design them according to the features of the domain at hand.

The main scientific contributions of this paper are (1) to provide a robust and general background and foreground modeling method able to run seamlessly under extreme conditions (with different targets), (2) to demonstrate how textons outperform other texture descriptors in the background modeling process, and (3) to carry out, for the first time, an exhaustive testing of state-of-the-art approaches on different real-life scenarios.

The remainder of the paper is organized as follows: Sections 2 and 3, discuss, respectively, the current state of the art in background modeling, highlighting current trends and limitations, and how we address the current challenges in the research field. Section 4 shows a performance evaluation of the proposed method, compared to state-of-the-art methods, in three different real-life scenarios dealing, respectively, with people, fish and vehicles. Finally, in the last section, some conclusions are drawn.

@&#RELATED WORK@&#

The objective of background modeling and subtraction algorithms is to create a model of the scene without objects and then to detect the foreground by comparing the current frame with the estimated background. This model should be able to cope with noise and illumination variations, and at the same time, it should also identify and remove object shadows, which technically are part of the foreground but can affect the performance of higher level applications (from object tracking to behavior understanding). Over the years several background modeling approaches of increasing complexity have been proposed. Among these, so far, the most popular are the density-based ones, which model each background pixel through a probability density function pdf based on visual cues extracted during a training period. Such pdf is then updated adaptively according to the input frames. The most used pdf is the Gaussian one and it is considered as the “de facto” standard baseline method for background modeling [4]. For example, Wren et al. [5] modeled each pixel using a single Gaussian distribution in the YUV color space. The main downside of these approaches is that they do not perform well with dynamic natural environments which, instead, involve the use of multimodal density functions. Gaussian mixture models have proved to work reliably in such cases [1]; however, the major concern in these models is the lack of effective strategies to update adaptively the components in the given mixtures. Recently, more sophisticated approaches for identifying the number of Gaussians “on-the-fly” have been proposed [6,7], although these approaches seem to be “ad hoc”. To avoid the difficulty of identifying the appropriate shape of the pdf, non-parametric methods, e.g. the kernel density estimation approaches, have been adopted for background modeling [8] with relative success. More specifically, these approaches build the model by accumulating the values from the pixel’s history. However, they are too dependent on the pixel values, require many samples for accurate model estimation and are often computationally expensive, making them inappropriate for real-time applications.

Recently, in [9] the authors classified background and foreground pixels by comparing, for each, its current value to a history of recent values. They state that the main differences/advantages of their method with respect to the state-of-the-art approaches lie in the randomness introduced to the background update policy: at each frame, a pixel’s model is updated only with probability p (e.g. 1/16), and the choice of which history value to remove is random rather than according to a FIFO policy. However, due to the exploitation of pixel colors only, this method shows several limitations, namely, (1) the impossibility to handle luminosity changes and shadows, (2) correlation between features of a multidimensional feature space and (3) the lack of effective mechanisms for including structural variations of pixels’ neighbors.

Recent research has, instead, proved that methods employing pixel spatial information to build the background model, by joint domain-range density estimation [2], achieve higher accuracy. More in detail, it has been demonstrated that a proper combination of visual features (color, texture and/or motion) modeling temporal and spatial pixel variations improves performance, if the employed features are uncorrelated [10].

Most of these methods employ complex texture features (more robust to luminosity changes than color features) to model the background; for example, Heikkla and Pietikainen [11] have successfully applied the local binary pattern (LBP) texture operator, because of its tolerance to illumination variations. Similarly, scale-invariant local ternary patterns (SILTP) [12] have been also used with success thanks to their insensitivity to light changes and shadows in the scene. Zhang et al. [4] argued that both LBP- and SILTP-based approaches are not able to consider the pattern temporal variations, and to deal with this aspect they maintained a background model using a spatial–temporal feature, the texture pattern flow, to compute inherent motion information and which is able to reflect the fact that the foreground moves always towards a certain direction. While this assertion is generally true for humans, it does not hold for fish because they tend to move erratically with frequent direction changes.

Motion cues have been also used [8,13], though they are rather computationally expensive and fail with low resolution images because of the difficulty to estimate motion. Furthermore, many approaches adopt multidimensional kernel density estimators and they do not perform effectively in case of dependency between features. To address this problem, Han and Davis [10] do not use a multi-feature background modeling, but they adopt multiple single dimensional models and then combine them through Support Vector Machines. However, this approach in practice is not viable because it requires a training phase for each different scenario it has to deal with.

Furthermore, although in many cases it has proved successful, modeling only the background has a main shortcoming: if object features are similar to the background, the object will unavoidably be detected as part of it. This consideration leads to model explicitly the foreground and methods which keep both models (i.e. one for the background and one for the foreground) seem to improve the performance considerably [14].

All the above approaches are pixel-based and often result in misclassification errors because of noise, which often affects isolated pixels [9]. For this reason, block-based methods [15,16], which assume an inter-dependency between background models of neighboring pixels, have been also proposed. For example, Seki et al. [15] model the background by resorting to principal component analysis (PCA) for each image block: the foreground image is created by comparing the current image and the back-projection of its PCA components into the image space. The main shortcoming of block-based approaches is the inaccurate object segmentation which needs a consistent post-processing phase to improve the algorithm’s output.

The key points that summarize the recent literature on background modeling are:
                        
                           •
                           Pixel-wise background model with increasingly sophisticated probabilistic models.

Inclusion of complex texture features, e.g. [12,11], as they are more robust against illumination changes and shadow.

Explicit foreground model [2].

Besides, all of the above approaches have been tested and devised for human-centered contexts, but they fail when it comes to deal with more complex scenarios, such as the underwater one, where the images suffer from low contrast mainly because of light attenuation (caused by absorption and scattering), which strongly limits the object visibility [17]. Another major difficulty is represented by the targets to be detected, i.e. fish, which, unlike humans, move erratically and fast, complicating the background and/or foreground modeling process. Porikli et al. [18] compared different algorithms (both for detection and tracking) under extreme conditions (that somehow resemble the ones in underwater scenes) such as erratic motion, sudden and global light changes, presence of periodic and multimodal backgrounds, arbitrary changes in the observed scene, low contrast and noise. However, the methods performing the best on the considered scenarios show evident limitations in the underwater domain, as shown in [19] and in Section 4.

The joint domain-range model proposed in this paper extends the work in [2] by integrating texture features (computed using textons) into the model. In detail, for each pixel p at location 
                        
                           (
                           x
                           ,
                           y
                           )
                        
                      we build a feature vector 
                        
                           
                              
                                 v
                              
                              
                                 p
                              
                           
                        
                      consisting of the 
                        
                           (
                           x
                           ,
                           y
                           )
                        
                      coordinates, the 
                        
                           (
                           
                              
                                 s
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 s
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 s
                              
                              
                                 3
                              
                           
                           )
                        
                      color channels in a given color space (we tested RGB, HSV and Lab) and the energy of the texton image in a region 
                        
                           R
                           =
                           w
                           ×
                           w
                        
                      centered on p.

More specifically, our approach first extract the magnitude of the maximum gradient image G within the region 
                        
                           R
                           =
                           w
                           ×
                           w
                        
                      by computing the eigenvector corresponding to the greatest eigenvalue of the matrix 
                        
                           J
                           ×
                           
                              
                                 J
                              
                              
                                 T
                              
                           
                        
                      with J being the Jacobian of the color space vector [20]. Afterwards the texton image is obtained from image G. Textons have been extensively adopted in texture analysis especially for image retrieval [21] and can be defined as sets of patterns shared over the image [21]. Several texton images have been proposed in the literature [22], and in this work we adopt the textons shown in Fig. 1
                     . Generally, a smaller number of textons is used (usually 4); we, instead, make use of 7 textons because of the need to capture even the slightest texture variations within the considered region R and a smaller number of textons would prevent us from achieving this goal.

The texton image is obtained by detecting textons on the image G and then setting to zeros all the pixels that do not belong to the detected textons. Fig. 2
                      shows an example of how a texton image is derived using only four textons (for simplicity) on a 
                        
                           7
                           ×
                           7
                        
                      image.

As a global texture feature describing a pixel 
                        
                           (
                           x
                           ,
                           y
                           )
                        
                      and its neighbors in the region 
                        
                           R
                           =
                           w
                           ×
                           w
                        
                     , we compute the energy 
                        
                           
                              
                                 e
                              
                              
                                 T
                              
                           
                        
                      of the texton image T of the region R defined as:
                        
                           (1)
                           
                              
                                 
                                    e
                                 
                                 
                                    T
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          (
                                          i
                                          ,
                                          j
                                          )
                                          ∈
                                          R
                                       
                                    
                                    T
                                    
                                       
                                          (
                                          i
                                          ,
                                          j
                                          )
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           
                        
                     
                  

Therefore, each pixel 
                        
                           p
                        
                      is then represented through the feature vector 
                        
                           
                              
                                 v
                              
                              
                                 p
                              
                           
                           =
                           (
                           x
                           ,
                           y
                           ,
                           
                              
                                 s
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 s
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 s
                              
                              
                                 3
                              
                           
                           ,
                           
                              
                                 e
                              
                              
                                 T
                              
                           
                           )
                        
                      (with 
                        
                           
                              
                                 s
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 s
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 s
                              
                              
                                 3
                              
                           
                        
                      being the color values), and the joint domain-range model consists in the corresponding 6-dimensional space, on which the pdfs of the background and foreground models are built. This is performed by means of kernel density estimation [23]: given the sets 
                        
                           
                              
                                 ψ
                              
                              
                                 b
                              
                           
                           =
                           {
                           
                              
                                 b
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 b
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 b
                              
                              
                                 n
                              
                           
                           }
                           
                           and
                           
                           
                              
                                 ψ
                              
                              
                                 f
                              
                           
                           =
                           {
                           f
                           
                              
                                 
                              
                              
                                 1
                              
                           
                           ,
                           f
                           
                              
                                 
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           f
                           
                              
                                 
                              
                              
                                 m
                              
                           
                           }
                        
                     , containing all background and foreground samples, respectively, the corresponding pdfs can be approximated as:
                        
                           (2)
                           
                              P
                              (
                              p
                              |
                              
                                 
                                    ψ
                                 
                                 
                                    b
                                 
                              
                              )
                              =
                              
                                 
                                    1
                                 
                                 
                                    n
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       n
                                    
                                 
                              
                              φ
                              
                                 
                                    
                                       
                                          
                                             p
                                             -
                                             
                                                
                                                   b
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          
                                             H
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (3)
                           
                              P
                              
                                 
                                    
                                       p
                                       |
                                       
                                          
                                             ψ
                                          
                                          
                                             f
                                          
                                       
                                    
                                 
                              
                              =
                              
                                 
                                    1
                                 
                                 
                                    m
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       m
                                    
                                 
                              
                              φ
                              
                                 
                                    
                                       
                                          
                                             p
                                             -
                                             f
                                             
                                                
                                                   
                                                
                                                
                                                   i
                                                
                                             
                                          
                                          
                                             H
                                          
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           φ
                           (
                           x
                           )
                        
                      is a KDE kernel function with the usual properties of unitary integral, symmetry, zero-mean and with identity covariance and H is the kernel bandwidth. In our case we used the common multivariate Gaussian (as the novelty of the approach relies on integrating textons in the joint-domain range model not in the adopted kernel function), defined as:
                        
                           (4)
                           
                              φ
                              (
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    x
                                 
                                 
                                    N
                                 
                              
                              )
                              =
                              
                                 
                                    1
                                 
                                 
                                    
                                       
                                          2
                                          
                                             
                                                π
                                             
                                             
                                                N
                                             
                                          
                                          |
                                          H
                                          |
                                       
                                    
                                 
                              
                              
                                 
                                    e
                                 
                                 
                                    (
                                    -
                                    
                                       
                                          1
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          (
                                          x
                                          -
                                          μ
                                          )
                                       
                                       
                                          T
                                       
                                    
                                    
                                       
                                          H
                                       
                                       
                                          -
                                          1
                                       
                                    
                                    (
                                    x
                                    -
                                    μ
                                    )
                                    )
                                 
                              
                           
                        
                     with 
                        
                           {
                           
                              
                                 x
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 x
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 x
                              
                              
                                 N
                              
                           
                           }
                        
                      being the variables, H the bandwidth matrix (6×6 diagonal matrix because our model is six-dimensional) and 
                        
                           μ
                        
                      the means’ vector. This representation allows to achieve two objectives: first of all, a spatial dependency relationship between pixels is introduced due to the joint domain-range model; secondly, the foreground model is managed separately from the background one.

In order to reduce the dimensionality of the model matrices and the frame processing time, the Binned KDE [24] is used, i.e. the models are quantized into a more compact 
                        
                           X
                           ×
                           Y
                           ×
                           
                              
                                 S
                              
                              
                                 1
                              
                           
                           ×
                           
                              
                                 S
                              
                              
                                 2
                              
                           
                           ×
                           
                              
                                 S
                              
                              
                                 3
                              
                           
                           ×
                           E
                        
                      space. Practically, the models are two matrices: 
                        
                           
                              
                                 P
                              
                              
                                 b
                              
                           
                           
                           and
                           
                           
                              
                                 P
                              
                              
                                 f
                              
                           
                        
                     , representing at all times the current values of 
                        
                           P
                           (
                           p
                           |
                           
                              
                                 ψ
                              
                              
                                 b
                              
                           
                           )
                           =
                           
                              
                                 P
                              
                              
                                 b
                              
                           
                           (
                           
                              
                                 v
                              
                              
                                 p
                              
                           
                           )
                           
                           and
                           
                           P
                           (
                           p
                           |
                           
                              
                                 ψ
                              
                              
                                 f
                              
                           
                           )
                           =
                           
                              
                                 P
                              
                              
                                 f
                              
                           
                           (
                           
                              
                                 v
                              
                              
                                 p
                              
                           
                           )
                        
                     , respectively. When the algorithm is started, the first N frames are used to initialize the background model. For each pixel p in each frame, the 
                        
                           
                              
                                 v
                              
                              
                                 p
                              
                           
                           =
                           (
                           x
                           ,
                           y
                           ,
                           
                              
                                 s
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 s
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 s
                              
                              
                                 3
                              
                           
                           ,
                           
                              
                                 e
                              
                              
                                 T
                              
                           
                           )
                        
                      feature vector is computed – appropriately quantized for the Binned KDE – and the discrete KDE kernel is applied at its location, thus increasing the 
                        
                           
                              
                                 P
                              
                              
                                 b
                              
                           
                           (
                           
                              
                                 v
                              
                              
                                 p
                              
                           
                           )
                        
                      model cells by one. The neighboring cells are also increased by a quantity given by Eq. (4). After this procedure has been completed for all pixels, the 
                        
                           
                              
                                 P
                              
                              
                                 b
                              
                           
                        
                      matrix is normalized by the total number of pixels used for the initialization. The foreground model 
                        
                           
                              
                                 P
                              
                              
                                 f
                              
                           
                        
                      is initialized (although no foreground pixels have been detected yet) as follows: it is set to 
                        
                           
                              
                                 P
                              
                              
                                 f
                              
                           
                           (
                           v
                           )
                           =
                           γ
                        
                     , for each v cell in the model, where 
                        
                           γ
                        
                      is a low value (0.1 in this work), accounting for the possibility of observing any uniformly distributed pixel value at any locations. The background update procedure, which will take into account the properties of the objects appearing in the following frames, is later described.

As new video frames are available, the current appearance of the observed scene is analyzed to identify areas which present (non-background) motion. In particular, the probabilities that each pixel belongs to either the background or the foreground are computed. Thanks to the discrete KDE representation of the models, such computation is straightforward, since the probability that pixel 
                           
                              p
                           
                         with 
                           
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                              =
                              {
                              x
                              ,
                              y
                              ,
                              
                                 
                                    s
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    s
                                 
                                 
                                    2
                                 
                              
                              ,
                              
                                 
                                    s
                                 
                                 
                                    3
                                 
                              
                              ,
                              
                                 
                                    e
                                 
                                 
                                    T
                                 
                              
                              }
                           
                         belongs to the background or the foreground models are simply 
                           
                              
                                 
                                    P
                                 
                                 
                                    b
                                 
                              
                              (
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                              )
                              
                              and
                              
                              
                                 
                                    P
                                 
                                 
                                    f
                                 
                              
                              (
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                              )
                           
                        , respectively. A motion binary map 
                           
                              M
                              (
                              x
                              ,
                              y
                              )
                           
                         is then built where each pixel is classified according to the log-likelihood ratio:
                           
                              (5)
                              
                                 M
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   0
                                                
                                                
                                                   if
                                                   -
                                                   ln
                                                   
                                                      
                                                         
                                                            
                                                               P
                                                            
                                                            
                                                               b
                                                            
                                                         
                                                         (
                                                         
                                                            
                                                               v
                                                            
                                                            
                                                               p
                                                            
                                                         
                                                         )
                                                      
                                                      
                                                         
                                                            
                                                               P
                                                            
                                                            
                                                               f
                                                            
                                                         
                                                         (
                                                         
                                                            
                                                               v
                                                            
                                                            
                                                               p
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                   >
                                                   T
                                                
                                             
                                             
                                                
                                                   1
                                                
                                                
                                                   otherwise
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                           
                         is the pixel feature vector at location 
                           
                              (
                              x
                              ,
                              y
                              )
                           
                        ; 0’s and 1’s in the output motion map represent background and foreground pixels, respectively, in the current video frame. The threshold T is set to balance sensitivity to foreground and background changes and robustness to noise.

After pixel classification has been completed, it is necessary to update the background and foreground models in view of the current image data.

The background update procedure consists in integrating the current frame’s classification results into the KDE estimation, namely the 
                           
                              P
                              (
                              p
                              |
                              
                                 
                                    ψ
                                 
                                 
                                    b
                                 
                              
                              )
                              =
                              
                                 
                                    P
                                 
                                 
                                    b
                                 
                              
                              (
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         function with 
                           
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                              =
                              (
                              x
                              ,
                              y
                              ,
                              
                                 
                                    s
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    s
                                 
                                 
                                    2
                                 
                              
                              ,
                              
                                 
                                    s
                                 
                                 
                                    3
                                 
                              
                              ,
                              
                                 
                                    e
                                 
                                 
                                    T
                                 
                              
                           
                        ) and 
                           
                              
                                 
                                    ψ
                                 
                                 
                                    b
                                 
                              
                           
                         representing the current background KDE support points. In order to take into account the possibility that new objects appear in the scene (or that background pixels are misclassified), we update the background model with all the background pixels (referred as 
                           
                              
                                 
                                    ψ
                                 
                                 
                                    b
                                    ,
                                    curr
                                 
                              
                           
                        ) in the current image. The 
                           
                              P
                              (
                              p
                              |
                              
                                 
                                    ψ
                                 
                                 
                                    b
                                    ,
                                    curr
                                 
                              
                              )
                              =
                              
                                 
                                    P
                                 
                                 
                                    b
                                    ,
                                    curr
                                 
                              
                              (
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         function is computed from the 
                           
                              
                                 
                                    ψ
                                 
                                 
                                    b
                                    ,
                                    curr
                                 
                              
                           
                         and the new background model 
                           
                              
                                 
                                    P
                                 
                                 
                                    b
                                    ,
                                    new
                                 
                              
                              (
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         is computed as a weighted mean between the current background model 
                           
                              
                                 
                                    P
                                 
                                 
                                    b
                                 
                              
                              (
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         and the 
                           
                              
                                 
                                    P
                                 
                                 
                                    b
                                    ,
                                    curr
                                 
                              
                              (
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                              )
                           
                        , i.e.:
                           
                              (6)
                              
                                 
                                    
                                       P
                                    
                                    
                                       b
                                       ,
                                       new
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       p
                                    
                                 
                                 )
                                 =
                                 α
                                 
                                    
                                       P
                                    
                                    
                                       b
                                       ,
                                       curr
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       p
                                    
                                 
                                 )
                                 +
                                 (
                                 1
                                 -
                                 α
                                 )
                                 
                                    
                                       P
                                    
                                    
                                       b
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       p
                                    
                                 
                                 )
                              
                           
                        
                     

The foreground model is recomputed every time from the 
                           
                              
                                 
                                    ψ
                                 
                                 
                                    f
                                 
                              
                           
                         set of pixels detected as foreground in the current frame. As for the background, KDE is applied to estimate the 
                           
                              
                                 
                                    P
                                 
                                 
                                    f
                                 
                              
                              (
                              p
                              |
                              
                                 
                                    ψ
                                 
                                 
                                    f
                                 
                              
                              )
                              =
                              
                                 
                                    P
                                 
                                 
                                    f
                                 
                              
                              (
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         
                        pdf. Similarly to what was done for the model initialization, a 
                           
                              γ
                           
                         value is added to 
                           
                              
                                 
                                    P
                                 
                                 
                                    f
                                 
                              
                              (
                              
                                 
                                    v
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         to account for the appearance of new objects in the frame [2].

@&#EXPERIMENTAL RESULTS@&#

The proposed method was tested on three different datasets showing complex scenes with different targets: people in the I2R Dataset [25], fish in the Fish4Knowledge video dataset, vehicles and humans in the Background Models Challenge (BMC) dataset [26]. The reason of testing it with different targets was to demonstrate its ability to generalize over multiple domains.

The first evaluation was carried on the I2R Dataset and aimed at identifying which texture feature and which color space performs better (i.e. the background and foreground models were still 6-dimensional vectors, what changed was only 
                        
                           
                              
                                 S
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 S
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 S
                              
                              
                                 3
                              
                           
                        
                      and the value of 
                        
                           
                              
                                 e
                              
                              
                                 T
                              
                           
                        
                     ). The I2R Dataset contains nine videos (frames 120×160) showing people in real-life scenarios; the ground truth was hand labeled, consists of 20 labeled image per video and is available at http://perception.i2r.a-star.edu.sg/bk_model/bk_index.html. Examples of these videos are shown in Fig. 3
                     .

In addition to the energy of the texton image (in the following TEXTON), we employed three other texture descriptors (extensively used in the literature for both background modeling and texture analysis) to compute 
                        
                           
                              
                                 e
                              
                              
                                 T
                              
                           
                        
                     : (1) energy of the local binary pattern texture operator [27] (LBP), (2) energy of the grey level co-occurrence matrix (CO-OCCUR) and, (3) energy of the scale-invariant local ternary pattern operator [12] (SILTP). The LPB and SILTP were used differently than their original application to background modeling, respectively in [11,28], because at this stage we wanted to test only the effectiveness of these methods in describing local textures.

The parameters were set empirically in order to balance the trade-off between efficiency and accuracy: the kernel model was quantized dividing by 10 each dimension (for the I2R dataset, we had 120×160 images, we used 8-bit color depth, hence the possible color values were 255×255×255, and the texture values were normalized as to have maximum 255 and 0 minimum, so the original model dimension was [120,160,255,255,255,255] which was then quantized in a [12, 16, 26, 26, 26, 26] model), w
                     =5 as window for texture computation, 
                        
                           α
                           =
                           0.01
                           ,
                           γ
                           =
                           0.1
                        
                     , and the 50 initial frames were used for the training phase. Moreover, we used two different bandwidth matrices 
                        
                           
                              
                                 H
                              
                              
                                 b
                              
                           
                           
                           and
                           
                           
                              
                                 H
                              
                              
                                 F
                              
                           
                        
                      (defined as diagonal matrices): one for the background with all the values in the diagonal equal to 9 and one for the foreground with all values equal to 25. This was done to account for the fact that background pixels are more frequent than foreground ones. We did not use any post-processing to improve detection results but removed the connected components having area less than 15 pixels. The algorithms’ performance was measured in terms of precision/recall curves and F-measure 
                        
                           
                              
                                 F
                              
                              
                                 1
                              
                           
                        
                     , defined as:
                        
                           (7)
                           
                              Precision
                              =
                              
                                 
                                    TP
                                 
                                 
                                    TP
                                    +
                                    FP
                                 
                              
                              ,Recall
                              =
                              
                                 
                                    TP
                                 
                                 
                                    TP
                                    +
                                    FN
                                 
                              
                              ,
                              
                                 
                                    F
                                 
                                 
                                    1
                                 
                              
                              =
                              
                                 
                                    2
                                    ×
                                    Precision
                                    ×
                                    Recall
                                 
                                 
                                    Precision
                                    +
                                    Recall
                                 
                              
                           
                        
                     where 
                        
                           TP
                           ,
                           FP
                        
                      and FN are, respectively, the true positives, the false positives and the false negatives measured when comparing, on a pixel basis, the ground truth binary masks and the output masks of the background modeling approach.

The results in terms of Precision/Recall curves, respectively, for the Lobby (worst case) and the Curtain (best case) videos, when the RGB color space was adopted, are reported in Fig. 4
                     .

In particular, Fig. 4 shows that textons performed the best, followed by the local binary pattern operator. This is because both techniques are able to describe effectively texture both in small windows (in our case, 
                        
                           5
                           ×
                           5
                        
                      windows) and in low resolution images (
                        
                           160
                           ×
                           120
                        
                     ). In addition, it has to be noticed that both methods achieved always high precision showing robustness against illumination changes and background movements.

To test whether and how different color spaces influenced the performance in background modeling, we also compared the performance of the texton-based approach when combined with three different color spaces, namely, RGB, HSV and Lab and the results are shown in Fig. 5
                     . We also noticed that for window values w larger than 9 the performance worsened while it remained almost constant for lower window sizes (5 and 7) and we used 5 for efficiency purposes.


                     Table 1
                      compares the F-measures of our method to the ones achieved by the most recent KDE state-of-the-art approaches, namely, the one adopting only colors [2] (referred as KDE-RGB), the one using texture features computed through scale-invariant local ternary patterns (SILTP) [12] and the ones combining textures with colors: VKS rgb and VKS Lab plus SILTP [28]. We did not compare our method to the one using the local binary pattern texture operator [11] as the authors in their paper tested the method with their own video sequences.

Although in some cases (e.g. Trees) our approach performed slightly worse than the other methods, on average it outperformed them, keeping also the standard deviation low. The motivation behind this good performance may probably be found in the capabilities and robustness of the textons used to describe image textures.

The successive step of our experimental evaluation aimed at testing the performance of the proposed approach (using the features and parameters previous identified in order to demonstrate the approach to work on a different scenarios it was trained for), in the underwater domain. In particular, we used a set of 14 “real-life”underwater videos (spatial resolution ranging from 320×240 to 640×480) collected within the Fish4Knowledge project to monitor Taiwan coral reef. These videos were categorized into seven different classes according to the typical features of underwater videos, i.e. “Blurred” (smoothed and low contrasted images), “Complex Background Texture” (background featuring complex textures), “Crowded” (lof of fish), “Dynamic Background” (background movements, e.g. plants movements etc.), “Hybrid” (more than one features: e.g. plant movements together with luminosity changes), “Luminosity Change” (videos affected by transient and abrupt luminosity changes), “Camouflage Foreground Object” (e.g. fish with colors similar to the background) and examples of these videos are shown in Fig. 6
                     .

The ground truth (consisting of 20 labeled images per video) on this dataset was hand-labeled using the tool in [29] and it is available together with the videos at http://f4k-db.ing.unict.it/datasets.php. The first evaluation aimed to understand how our approach performed on the different scenarios that may happen in underwater monitoring. Fig. 7
                      shows the Precision/Recall curves achieved for each video class. It is possible to notice that our method performs the best on blurred videos (because of the simplicity of the background), whereas its performance dropped with background object movements (“Dynamic Background” class), though the performance in that case showed an average F-measure of about 0.75.

Then, we compared the results of our method with some state-of-the-art approaches (mainly kernel density estimators) on the same underwater dataset. In order to avoid any implementation bias in the performance evaluation, we used only methods for which the original code was available
                        2
                        Most of the methods are available at https://code.google.com/p/bgslibrary/. The code of the remaining methods were made available by the authors and reference to the code can be found in the corresponding papers.
                     
                     
                        2
                     . For this reason, we selected the following methods for comparison:
                        
                           •
                           P-Finder [5] which models the background with only one single Gaussian pdf (Gaussian).

Two methods that exploit mixture of Gaussians, namely, the original Gaussian Mixture Model by Stauffer and Grimson [30] (GMM) and its improvement by Zivkovic (ZGMM) [6].

The Eigenbackground (EIGEN) subtraction method [31].

Two non-parametric kernel density estimation approaches: Sheikh’s method [2] (KDE-RGB), which uses only color features for modeling the background, and the MultiLayer background model (ML-BKG) by Yao [32], which, instead, employs also texture features computed via Local Binary Patterns.


                              VIBE 
                              [9] (background modeling through actual pixel values instead of using a predefined pdf shape) which has been previously applied to underwater domain [19].

The performance of the different methods are reported as F-measure values and illustrated in Table 2
                     , which shows that combining color and texture features (as in our method and in ML-BKG) enhanced the background modeling’s performance also in complex scenarios where targets and background had similar texture features. Unlike ML-BKG, which performed well with complex background due to the high texture variability, our approach shows good accuracy also in the cases of smooth regions (e.g. Blurred class) because of the ability of textons to describe also tiny texture variations.

The other information that can be derived from these results is that methods relying on a pdf with a predefined form (e.g. Gaussian) are not suitable to deal with complex scenes, where, instead, non-parametric methods perform much better. A qualitative comparison of our approach, VIBE and ML-BKG is presented in Fig. 8
                     , which shows that our approach had good qualitative performance when compared to the other two.

Of course, the increment in accuracy increment was achieved at the expense of efficiency; in fact, the average number of frames (size 320×240) processed per second for VIBE, ML-BKG and our approach were, respectively, 200 frames/s, 20 frames/s and 18.5 frames/s with a C++ implementation running on a PC powered by an Intel i7 3.4 Ghz CPU and 16GB RAM.

To test further more the ability of our method to generalize over different domains and targets, we tested our approach on the dataset provided with the Background Models Challenge (BMC) [26], which deals with vehicles and humans. In particular, the BMC dataset comprises two testing sets: a synthetic one (10 videos) and a real one (9 videos). Tables 3 and 4
                     
                      report the results in terms of precision, recall and F-measure, Peak Signal–Noise Ratio (PSNR), D-score [33] and the Structural SIMilarity (SSIM) [34] for each video of, respectively, the synthetic dataset and real dataset. It is possible to notice how our approach performs pretty well with synthetic videos (even with high level of noise, e.g. in video 512) while the performance dropped of about 15% with realistic video sequences. However, the BMC real video sequences show cases (see Fig. 9
                     ) that make the background modeling task extremely complex (and that are beyond the scope of the considered methods, as for instance, background modeling with moving cameras or at night): snowing (005), indoor monitoring with very scarce illumination (007), high camera movements (008) and dirt in camera lens. If we do not consider the performance achieved in those cases, the average performance in terms of F-measure was of about 78%, which is inline with the one obtained in the other two considered scenarios.

@&#CONCLUSIONS@&#

In this paper we have introduced a joint domain-range kernel estimation approach which builds the background and foreground models by integrating color and texture features. We have, in particular, demonstrated the utility of textures in the background modeling process and how the methods relying on textons and/or local binary patterns perform the best since they are able to extract significant texture also from small regions (e.g. in 
                        
                           5
                           ×
                           5
                        
                      pixel areas). Moreover, we have pointed out that, although dependent on the lighting conditions, the RGB color space is more suitable to model background color variations than the HSV or the Lab spaces, as opposed to what was stated in [28].

The application of our approach on different domains and the comparison with state-of-the-art methods confirmed that background modeling methods which allow spatial influence from neighboring pixels and employ textures robust to illumination changes perform the best over different domains.

As future work, we are focusing mainly to reduce the processing times (as at the method cannot be used for real-time purposes) by exploiting hashing techniques and GPU programming for speeding up the feature extraction and the model update processes.

@&#REFERENCES@&#

