@&#MAIN-TITLE@&#Recognizing activities in multiple views with fusion of frame judgments

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Data fusion based method for activity recognition using multiple views


                        
                        
                           
                           Straightforward architecture to incorporate new cameras or new features


                        
                        
                           
                           Performance generally increases when there are more cameras and features.


                        
                        
                           
                           Comparable performance with that produced by reconstruction


                        
                        
                           
                           Detailed experiments to answer different system considerations


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Video analysis

Human activity recognition

Multiple views

Multiple camera

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

There is a broad range of applications for systems that can recognize human activity in video. Medical applications include methods to monitor patient activity for keeping track of progress in stroke patients; or for keeping demented patients secure. Safety applications include detecting unusual or suspicious behavior, or detecting pedestrians to avoid accidents. The problem remains difficult due to important reasons. There is no canonical taxonomy of human activities. Changes in illumination direction and viewing direction cause massive changes in what people look like. Individuals can look very different from one another, and the same activity performed by different people can vary widely in appearance.

Generally, we expect that having multiple views makes recognizing human activity easier. There is support for this viewpoint in the literature (e.g., see Section 2). However, these results tend not to take into account various desirable engineering features for distributed multi-camera systems. In such systems, we may not be able to get accurate geometric calibrations of the cameras with respect to one another (e.g., if the cameras are dropped into a terrain). Cameras might drop in or out at any time, and we need a simple architecture that can opportunistically exploit whatever data is available. We will not be able to set cameras at fixed locations with respect to the moving people, meaning that training data might be obtained from different view directions than test data.

In this paper, we describe an architecture to label activities using multiple views. Fig. 1
                      shows the main structure of our architecture. We assume that there are one or more cameras, and that each camera can compute one or more blocks of features representing each frame. Breaking features into blocks allows us to insert new sets of features without disrupting the overall architecture. In the first step, each block of features for each frame of each camera is used for a nearest neighbor query, independent of all other cameras, frames or blocks (Section 4).

In the second step, the resulting matches are combined with a weighting scheme. Because the viewing direction of any camera with respect to the body is unknown, some frames (or feature blocks) might be ambiguous. We expect that having a second view should disambiguate some frames, so it makes sense to combine matches over cameras. However, close matches are very likely to be right. This suggests using a scheme that allows (a) several weakly confident matches that share a label to support one another and (b) strongly confident matches to dominate (see Fig. 2
                     ). This stage reports a distribution of similarity weights over labels, but conceals the number of cameras or of features used to obtain it, so that later decision stages can abstract away these details (Section 4.1). Finally, we use temporal smoothing, to estimate the action in a short sequence (Section 4.2).

Our architecture requires no volume reconstruction and makes engineering easy in new sets of features. When a set of features in a camera is confident, it dominates the labeling process for that frame. Similarly, the frames in a sequence that are confident dominate the decision for a sequence. Our experiments (Section 5) demonstrate that our method performs at the state of the art. We show results for several types of features. It is straightforward to incorporate new cameras or new features into our method. Performance generally improves when there are more cameras and more features. Our method is robust to differences in view direction; training and test cameras do not need to overlap. Discriminative views can be exploited opportunistically. Performance degrades when the test and training data do not share viewing directions. Camera drop in or drop out is handled easily with little penalty. There is no need to synchronize and calibrate cameras.

The main point of our paper is to show that, when one has multiple views of a person, straightforward data fusion methods give comparable recognition performance with that produced by 3D reconstruction in the context of a radically simpler system architecture with significant advantages.

@&#BACKGROUND@&#

The activity recognition literature is rich, broad reviews of the topic appear in [6–11]. We confine our review to covering the main trends in features types, and in methods that recognize activities from viewpoints that are not in the training set.

Features can be purely spatial, or spatio-temporal. Because there are some strongly diagnostic curves on the outline, it is possible to construct spatial features without segmenting the body (e.g., this is usual in pedestrian detection [30]). An alternative is to extract interest points that may lie on the body (e.g. [45]). In activity recognition, it is quite usual to extract silhouettes by background subtraction (e.g. [4,31,40]). Pure spatial features can be constructed from silhouettes by the usual process of breaking the domain into blocks, and aggregating within those blocks (e.g. [31,40]). Doing so makes the feature robust to small changes in segmentation, shifts in the location of the bounding window, and so on.

Because many activities involve quite large body motions on particular limbs, the location of motions in an image can provide revealing features. Efros et al. [2] show that averaged rectified Optical Flow features yield good matches. Laptev and Pérez [19] show that local patterns of flow and gray level are distinctive for some actions. Bobick and Davis [1] show that a spatial record of motion (a motion history image) is discriminative. Blank et al. [4] show that joining consecutive silhouettes into a volume yields discriminative features. Laptev and Lindeberg [3] introduce spatio-temporal interest points; descriptors can be computed at these points, vector quantized then pooled to produce a histogram feature. Scovanner et al. [24] propose the spatio-temporal extension of 2-D sift features for action videos.

Changing the view direction can result in large changes in the silhouette and motion of the person in the image. This means that training with one view direction and testing with another can result in significant loss of performance. Rao et al. [13] build viewpoint invariant features from spatio-temporal curvature points of hand action trajectories. Yilmaz and Shah [17] compute a temporal fundamental matrix to account for camera motion while the action is occurring so they can match sets of point trajectories from distinct viewpoints. Parameswaran and Chellappa [15] establish correspondences between points on the model and points in the image; then compute a form of invariant time curve, then match to a particular action. The method can learn in one uncalibrated view and match in another. However, methods to build viewpoint invariant features currently require correspondence between points.

Instead, Junejo et al. [26] evaluate pairwise distances among all frames of an action and construct a self-similarity matrix that follows discriminative and stable pattern for the corresponding action. In contrast to our method, there is no evidence that multiple cameras improve recognition. In the literature, some studies introduce robust silhouette based features to be intended for view invariance [51]. Wang et al. [44] extracts ℜ transform features from two orthogonal views for training and fuses using HMM based graphical model. Our fusion strategy is different, as it collects votes in the form of weight vectors from all cameras and features and hides camera and feature information from the classification stage. Here, our goal is to present a framework providing dynamic scalability to more cameras, and applicable for any kind of feature.

An alternative is to try and reconstruct the body in 3D. Ikizler and Forsyth [25] lift 2D tracks to 3D, then reason there. While lifting incurs, significant noise problems arise because of tracker errors. However, they show that the strategy can be made to work and the main advantage of the approach is that one can train activity models with motion capture data. Weinland et al. [16] build a volumetric reconstruction from multiple views, then match to such reconstructions. Pehlivan and Duygulu [40] introduce a simple method based on volume matching; they obtain high performance in action recognition with a very simple pose representation when the volumes are available. The main difficulty with this approach is that one needs sufficient views to construct a reasonable reconstruction, and these may not be available in practice. Alternatively, one can use volumes only during the training stage [18,21], then generate training frames from those volumes by projection into synthetic camera planes. Doing so requires that training volumes be available; in our study, we assume that they might not be.

For activity recognition, a smaller number of cameras with broad field of view can be adequate. [36,20,53] show a case that trains using all cameras and tests on a single camera sharing its view with one training camera. However, there is no evaluation of a non shared camera case or no evidence that multiple cameras for testing improve performance.

An alternative is to build discriminative models of the effects of viewpoint change (transfer learning). Farhadi and Tabrizi [34] emphasize how damaging changes of viewpoint can be; for example, a baseline method gives accuracies in the 70% range when the train and test viewpoints are shared, but accuracy falls to 23% when they are not. They give a method for transferring a model from one view to another using sequences obtained from both viewpoints. Alternatively, Liu et al. [30] introduce more discriminative bilingual-words as higher level features to support cross-view knowledge transfer. Unfortunately, the methods require highly structured datasets. A more recent paper [38] gives a more general construction. However, it is not used in this study, because the method cannot exploit multiple cameras.

Numerous datasets are now available for activity recognition. The KTH [14] and the Weizman [4] are the state of the art human action datasets used in the literature. The Hollywood [32] dataset contains realistic human actions with camera motion and dynamic backgrounds. Similarly, the YouTube action dataset [34] have various challenging videos. The IXMAS dataset [16] is a multi-camera human action dataset with different viewpoints. The cameras are fixed but the actors perform freely. We have chosen to use the IXMAS dataset because it has been quite widely used, and also it has good support for research on multiple views (each sequence is obtained from five different viewpoints). In Table 1
                        , we show recent methods applied to the IXMAS dataset, broken out by the type of experiment.

Each camera frame can be represented by many different types of features. In this section, we define our choice of features, but many others can be also used as our architecture is able to work with blocks of features at one time. While selecting features, we want feature construction to be simple yet robust. For practical reasons, it should not require camera calibration, or point correspondence. Selected features should be able to work with minor segmentation errors, and ideally they should not be affected significantly by change in viewpoint. Particularly, we expect them to be discriminative enough for matching.

We use three types of frame level features: a coarse silhouette shape feature, a fine silhouette shape feature; and a motion feature. All three types of features are computed over the same region of interest. This region corresponds to the bounding box around human silhouette, which is extracted by background subtraction. Cropped region is resized as having the maximum of width or height to be 120 while keeping aspect ratio the same, and placed inside a 120×120 image. For all feature types, the final feature vector is normalized by l
                     2 norm. Fig. 3
                      simply shows our features.

The rough shape of the silhouette can be discriminative. For example, in many walking frames the arms are away from the body, but this does not happen in standing frames. For the coarse feature, we compute the contour extracted from a 120×120 silhouette image. The feature is constructed from the distance between the outermost contour points at each scan-line. The silhouette is divided into n horizontal layers, and the maximum, minimum and average distance within each layer are stacked to form the feature.

The second feature is an encoding of pixel occupancy over the 120×120 silhouette image, using two common local gridding structures (radial and square, [31,35] respectively).

Motion features are quite informative for some actions (sit down versus stand up). We apply the Kanade–Lucas–Tomasi Feature Tracker [47] to each pair of frames. Flow vectors inside the region of interest are encoded as two different channels corresponding to x and y axis motions [2]. Each channel is formed into a 120×120 image and divided into square grids. These are aggregated into a single feature vector.

Having a spatial feature per frame, multiple frames are further aggregated in the temporal scale. For a temporal scale (tscl) n, the three frames at i, i−n and i+n are picked and stacked into a single feature vector. For temporal scale 0, feature is a frame feature at i. Features are extracted in multiple spatial scales for each feature type forming a spatial pyramid and temporal aggregation is performed individually for every spatial scale.

Our goal is to design an architecture that works efficient with many cameras and features while being robust to changes occurring in the system such as camera drop out, addition of new features or new cameras without disrupting the overall system. In our work, labels as evidences are pooled over cameras and over frames to compute confident label for sequence. Particularly, we fuse information at three levels to produce three kinds of labels. First, different blocks of features in a single camera must produce the camera-specific label for a frame. Second, the camera-specific labels for a frame must produce the frame label. And finally, the frame labels produce the sequence label.

In our system, each camera can compute more than one block of feature and each feature involves an evidence for the frame label. We first label each block of features from each camera with the k-nearest-neighbors (kNN) [41,44] using the E2LSH implementation of Locality Sensitive Hashing (LSH) [24]. It is aimed to compute a weighted vote vector for each block of features representing a frame recorded in the test camera 
                           
                              j
                              ∈
                              
                                 C
                                 ′
                              
                           
                        . Votes are collected for each action class from the nearest neighbors recorded in training cameras 
                           
                              i
                              ∈
                              
                                 C
                                 ′
                              
                           
                        .

The aforementioned procedure means that we collect the nearest neighbors (see Section 5.1) of the test frame from a training camera pool. Each pool contains the frames of various action sequences and these frames are labeled the same as the sequence labels that they are extracted from. Let the recovered set of training frames 
                           
                              
                                 
                                    X
                                    ^
                                 
                                 f
                                 ij
                              
                           
                         for the test frame x
                        
                           f
                        
                        
                           j
                         be computed in block of feature f
                        ∈
                        F. First, we normalize all absolute distances between query frame and 
                           
                              
                                 
                                    X
                                    ^
                                 
                                 f
                                 ij
                              
                           
                         as
                           
                              (1)
                              
                                 
                                    dist
                                    
                                       
                                          x
                                          f
                                          j
                                       
                                       
                                          
                                             
                                                x
                                                ^
                                             
                                             
                                                f
                                                ,
                                                r
                                             
                                             ij
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                         f
                                                         j
                                                      
                                                      −
                                                      
                                                         
                                                            
                                                               x
                                                               ^
                                                            
                                                            
                                                               f
                                                               ,
                                                               r
                                                            
                                                            ij
                                                         
                                                      
                                                   
                                                
                                                2
                                             
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                                r
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                               f
                                                               j
                                                            
                                                            −
                                                            
                                                               
                                                                  
                                                                     x
                                                                     ^
                                                                  
                                                                  
                                                                     f
                                                                     ,
                                                                     r
                                                                  
                                                                  ij
                                                               
                                                            
                                                         
                                                      
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where r indexes the elements 
                           
                              
                                 
                                    
                                       x
                                       ^
                                    
                                    
                                       f
                                       ,
                                       r
                                    
                                    ij
                                 
                              
                              ∈
                              
                                 
                                    
                                       X
                                       ^
                                    
                                    f
                                    ij
                                 
                              
                           
                        .

kNN set for each test frame includes training frames with different action labels. We now construct a set of weighted votes for that query frame and that block of feature per action class with label k
                        ∈{1,…,n}.
                           
                              (2)
                              
                                 
                                    
                                       w
                                       f
                                       ij
                                    
                                    
                                       k
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             a
                                          
                                          
                                             e
                                          
                                       
                                       
                                          
                                             −
                                             
                                                
                                                   
                                                      dist
                                                      
                                                         
                                                            x
                                                            f
                                                            j
                                                         
                                                         
                                                            
                                                               
                                                                  x
                                                                  ^
                                                               
                                                               
                                                                  f
                                                                  ,
                                                                  a
                                                               
                                                               ij
                                                            
                                                         
                                                      
                                                   
                                                   2
                                                
                                             
                                          
                                          
                                             2
                                             
                                                σ
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where a indexes the elements 
                           
                              
                                 
                                    
                                       x
                                       ^
                                    
                                    
                                       f
                                       ,
                                       a
                                    
                                    ij
                                 
                              
                              ∈
                              
                                 A
                                 k
                              
                           
                         and 
                           
                              
                                 A
                                 k
                              
                              ⊂
                              
                                 
                                    
                                       X
                                       ^
                                    
                                    f
                                    ij
                                 
                              
                           
                         with action label k. The length of the vote vector, w
                        
                           f
                        
                        
                           ij
                        , is equal to the number of action labels, n. We normalize w
                        
                           f
                        
                        
                           ij
                         vector by l
                        1 norm.
                           
                              (3)
                              
                                 
                                    
                                       w
                                       f
                                       ij
                                    
                                    =
                                    
                                       
                                          w
                                          f
                                          ij
                                       
                                       
                                          
                                             
                                                
                                                   w
                                                   f
                                                   ij
                                                
                                             
                                             1
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Frames are represented as low level features in multiple spatial scale, l. Every frame is queried separately in these scales and represented by multiple vote vectors. This means having many 
                           
                              w
                              
                                 f
                                 
                                    s
                                    l
                                 
                              
                              ij
                           
                         vectors each computed in a scale of a spatial pyramid. Max pooling is applied over these vectors to fuse them into a single vote vector. The maximum of the votes of various scales is taken as the vote value for each action category, that is 
                           
                              
                                 w
                                 f
                                 ij
                              
                              
                                 k
                              
                              =
                              max
                              
                                 
                                    
                                       w
                                       
                                          f
                                          
                                             s
                                             1
                                          
                                       
                                       ij
                                    
                                    
                                       k
                                    
                                    ,
                                    
                                       w
                                       
                                          f
                                          
                                             s
                                             2
                                          
                                       
                                       ij
                                    
                                    
                                       k
                                    
                                    ,
                                    …
                                    ,
                                    
                                       w
                                       
                                          f
                                          
                                             s
                                             l
                                          
                                       
                                       ij
                                    
                                    
                                       k
                                    
                                 
                              
                           
                        .

Multiple vote vectors are computed for every training and test camera pairs and blocks of features. Now these votes should be combined from different blocks of features, and over viewing cameras. While a single frame can be discriminative, many others may be ambiguous. Also, descriptors of all cameras and features may agree or disagree on action label. We want the final combination of all possible cameras and block descriptors to tend towards the most confident one. Also, this should be done in a straightforward manner for all features and all camera combinations through forming a weighted, normalized sum. This is the thin combination.
                           
                              (4)
                              
                                 
                                    w
                                    
                                       k
                                    
                                    =
                                    
                                       1
                                       
                                          
                                             F
                                          
                                          ∗
                                          
                                             C
                                          
                                          ∗
                                          
                                             
                                                C
                                                ′
                                             
                                          
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             f
                                             ∈
                                             F
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   ∈
                                                   C
                                                
                                             
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         j
                                                         ∈
                                                         
                                                            C
                                                            ′
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         w
                                                         f
                                                         ij
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The weighted combination of various features over cameras can be computed over features F. We do so by minimizing the least square error of weight vectors learned using training cameras (Section 5.3).

This process ensures that strongly confident blocks of features will have a major effect on the final vector w, as will strongly confident viewing directions. We now have, for each test frame, a multi-view weight vector where the most certain class label should be dominant. Notice that this process of fusing evidence is straightforward, and scales well with new blocks of features or new cameras. Architecture fuses what we have with little fuss, and copes with the dropouts of cameras and features without changing the trained model.

We now have a set of weights associating each frame in a sequence with a label. If the frame is unambiguous, then this weight vector will have one large value; on the other hand if the frame is ambiguous, then several values might be quite large. We cannot treat these weights as probabilities, but we can use them as features for a classifier. Naïve Bayes (NB) is one natural approach, but assumes that frame–frame dynamics are not particularly significant in classifying a sequence. The traditional alternative is to learn a Hidden Markov Model (HMM) for each action, then to choose the action with the highest likelihood on the data.

A Naïve Bayes model assumes that the weight vector associated with each frame is emitted independently, conditioned on action class. This ignores the temporal structure of activities, but is well adapted to sequences where some frames are ambiguous and others are highly distinctive (which seems to be the case for most activity datasets). Assuming per-frame weight vectors w are independent conditioned on action, the class posterior probability is:
                              
                                 (5)
                                 
                                    
                                       P
                                       
                                          
                                             k
                                             |
                                             w
                                          
                                       
                                       ∝
                                       π
                                       
                                          k
                                       
                                       
                                          
                                             ∏
                                             
                                                k
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             P
                                             
                                                
                                                   w
                                                   
                                                      k
                                                   
                                                   |
                                                   k
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where π(k) is the class prior. We assume that the per-frame weight vectors are emitted with a Gaussian conditional distribution. We then have
                              
                                 (6)
                                 
                                    
                                       log
                                       P
                                       
                                          
                                             k
                                             |
                                             seq
                                          
                                       
                                       ∝
                                       −
                                       
                                          1
                                          
                                             2
                                             
                                                σ
                                                2
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                m
                                                ∈
                                                seq
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            w
                                                            m
                                                         
                                                         −
                                                         
                                                            μ
                                                            k
                                                         
                                                      
                                                   
                                                   2
                                                
                                             
                                             +
                                             log
                                             π
                                             
                                                k
                                             
                                             +
                                             L
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        


                           L is the constant term. Now assuming the same variance, the class priors and fix constant; then we obtain the activity label for the sequence as:
                              
                                 (7)
                                 
                                    
                                       
                                          
                                             argmax
                                             k
                                          
                                       
                                       −
                                       
                                          
                                             ∑
                                             
                                                m
                                                ∈
                                                seq
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         w
                                                         m
                                                      
                                                      −
                                                      
                                                         μ
                                                         k
                                                      
                                                   
                                                
                                                2
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Parameter μk
                            is a mean weight vector of size n computed for every action class using training samples. μk
                            and w vectors are normalized vectors having positive values summed to one. Here, normalized vectors, transformed distances and mislabeling of shared poses guarantee robustness. They do not have a major impact on the classification results.

An alternative which encodes temporal structure and has a long history in activity recognition is the continuous HMM [12]. HMM provides temporal smoothing over frame weight vectors assuming the dependence of consecutive frames. The HMM parameters are set according to the best rate combination of the number of the states in [1–4,6,8] and the number of the Gaussian mixtures in [2,4,6,8,10,12] by searching over the best NB thin combination single camera experiment reported in Table 3
                           
                           . We model each action category as one state HMM with eight Gaussian mixtures, Fig. 7
                           
                           
                           
                           . The highest likelihood HMM model for each test sequence defines the action label for that sequence.

@&#EXPERIMENTS@&#

To test our approach, we need a dataset consisting of videos in multiple viewpoints performed by actors in free orientations. We use the publicly available INRIA Xmas Motion Acquisition Sequence (IXMAS) dataset [16] (This has been widely used by many others [18,21,37,20,29]). It contains 13 actions (Nk
                        ) performed 3 times (No
                        ) in different orientations by 12 actors (Np
                        ). The action sequences are recorded in 5 cameras (Nc
                        ). [37] reports 10% average cross camera accuracy if one trains with camera 5, and shows how different its view is when just appearance features are used without any special setup. We also have low performance with the combination of camera 5 in the experiments (see Fig. 5). We follow it and report results for 4 cameras excluding camera 5, as its overhead view strongly differs from the others. Fig. 4 gives summary information for this dataset.

Experiments are carried out by leave-one-actor-out cross validation. For a given training 
                           C
                         and test 
                           
                              C
                              ′
                           
                         camera sets, experiments are done Np
                         times and the results are averaged as being the final accuracy. The number of cameras 
                           
                              N
                              C
                           
                         and 
                           
                              N
                              
                                 C
                                 ′
                              
                           
                         associated with training camera set 
                           C
                         and testing set 
                           
                              C
                              ′
                           
                         may vary; and sets are disjoint if 
                           
                              C
                              ∩
                              
                                 C
                                 ′
                              
                              ≡
                              ϕ
                           
                        .

In each leave one actor out experiment, all video samples of an actor pi
                        , i
                        ={1,…,N
                        
                           p
                        }, recorded in 
                           
                              C
                              ′
                           
                         are taken from the dataset and they are used for testing. The rest of the actors {pj
                        }
                           j
                           ≠
                           i
                         whose videos are recorded in cameras of 
                           C
                         is used as training samples to learn activity models.

The dataset has Np
                        
                        ×
                        No
                        
                        ×
                        Nc
                        
                        =180 videos per action. Assume we choose two cameras for training and testing, 
                           
                              
                                 N
                                 C
                              
                              =
                              2
                           
                         and 
                           
                              
                                 N
                                 
                                    C
                                    ′
                                 
                              
                              =
                              2
                           
                        . Every action model is learned using 
                           
                              
                                 
                                    
                                       N
                                       p
                                    
                                    −
                                    1
                                 
                              
                              ×
                              
                                 N
                                 o
                              
                              ×
                              
                                 N
                                 C
                              
                              =
                              66
                           
                         videos. First, every frame of 
                           
                              
                                 N
                                 o
                              
                              ×
                              
                                 N
                                 C
                              
                              =
                              6
                           
                         training videos associated with pj
                        , where j
                        ≠
                        i, is queried over pools. These pools contain frames of 
                           
                              
                                 
                                    
                                       N
                                       p
                                    
                                    −
                                    2
                                 
                              
                              ×
                              
                                 N
                                 o
                              
                              ×
                              
                                 N
                                 C
                              
                              =
                              60
                           
                         videos of {pk
                        }
                           k
                           ≠{i,j} recorded in training cameras 
                           C
                        . Then, closest matches are retrieved and combined over cameras and features (see Section 4.1). Finally, training videos in the form of aggregated vote vectors are used to train the corresponding action classifier (see Section 4.2).

During testing, we follow the same order as in training. This time, 
                           
                              
                                 N
                                 o
                              
                              ×
                              
                                 N
                                 
                                    C
                                    ′
                                 
                              
                              =
                              6
                           
                         videos of pi
                         are used for testing. Frames are queried over the pools that contain video frames of {pk
                        }
                           k
                           ≠{i} recorded in cameras of 
                           C
                        . Please note that pools and query samples are totally disjoint in terms of actor, and also in terms of camera views if 
                           
                              C
                              ∩
                              
                                 C
                                 ′
                              
                              ≡
                              ϕ
                           
                         during experiments.

We make use of memoization and multiple pools. We construct pools for every camera and feature pair containing all videos of an actor (including left–right reflections of frames, except line feature). This results in 
                           
                              
                                 N
                                 p
                              
                              ×
                              
                                 N
                                 C
                              
                           
                         pools for each feature type. For each query sequence, kNN sets from different pools are stored in sorted order. Then, we combine them according to camera, and feature of interest using merge sort. This avoids redundant computations, and makes the system flexible for addition of new training actors, features and cameras.

In the first experimental setup, we use one camera for training and one camera for testing. There are two distinct conditions here: the test view and the training view are the same (the one-shared camera case), or the test view is different than the training one (the no-shared camera case, where we expect reduced recognition rates). We perform experiments with many different parameter configurations for Horizontal Line, Radial Grid and Optical Flow features in two spatial scales. In Table 2, the best results are achieved with the following parameters: Horizontal Line with 10, 20 horizontal slices and tscl
                        =6; Radial Grid with 2×2 square and 12 radial cells, 3×3 square and 12 radial cells, tscl
                        =6; and Optical Flow with 4×4 and 8×8 griddings and tscl
                        =4. The number of the closest matches, knn, are in range [1,5,10,20,50], and the σ value used to compute transform distances in Eq. (2) is in range [0.01, 0.1, 0.5, 0.8]. Experiments using square gridding that is similar to Radial Grid feature is also performed, but the Radial Gridding appeared to be slightly more accurate. Since they encode the same information, we report only experiments of the Radial Grid feature. If we compare three features, Optical Flow significantly outperforms others around 10% in one-shared camera case. Horizontal Line feature has lower recognition rates than other features in one-shared camera. However, we will see later that it helps to improve recognition when combined with other features. We double the set of training frames by inserting left–right reflections; and observe that this increases the recognition performance.


                        Fig. 5 and Table 3 shows activity classification results of individual features in detail. When the training and the testing views have the frames from the same viewpoint, accuracy is high, comparable with the state of the art systems, type 1 and 2 of Table 1. Combined features have 85.31% accuracy if the training and the testing views are shared. Notice that recognition rates fall off to 68.59% when test and training sets do not share the viewpoint; the fall off is consistent with those reported in type 2 of Table 1. We conclude NB and HMM are reasonable choices of classifiers, and we have reasonable features. Table 3 compares classifiers, for different features, combinations and views. The IXMAS dataset is not highly dynamic with few periodic actions (walk and wave). In this case, HMM with 1 state works well (Fig. 7). A similar discussion holds when NB and HMM results are compared. NB with no temporal information may outperform HMM on the same dataset. This is due to the nature of the dataset.

When there are two cameras in training and testing, three cases are important: the two test views appear in the training set (the two-shared camera case); one view is shared (the one-shared camera case), and no test view appears in the training set (the no-shared case). Table 4
                         summarizes the average recognition rates of NB and HMM classifiers for our features and various view cases. As one would expect from the single camera results, when test and training sets share the viewpoints, performance increases. However, notice that having a second view tends to increase recognition rates quite strongly, even if there is no sharing of views between test and training sets (15% in this case). We conjecture that this is because there is a good chance that one view is unambiguous and correct, even when views are not shared. Fig. 6 shows detailed recognition rates for different sets of test and training cameras.

The two-train, two-test case is best compared to systems of types 4 and 5 in Table 1, because in this case one could come up with training (resp. test) volumes, though we do not use as many views to build the volumes as those systems do. Notice that the most favorable cases for our system are those where test and training views are shared (the diagonals in Fig. 6). The recognition rates here compare well with those of types 4 and 5 in Table 1. We are reporting the worst case for multi-view combination, where there are only two views. In particular, notice that for all sets of views and a reasonable feature combination, this worst case is very well behaved (Fig. 8
                        ). Fig. 9
                         shows example confusion matrices for experiments on camera pairs [2 4] on no-shared, one-shared and two-shared cameras cases. For this pair, our best performance over 13 action categories is 94.02% and it is higher than the volume reconstruction method reporting 93.33% accuracy for 11 action categories [16]. The best average result is 92.02% for two-shared, 88.31% for one-shared, and 82.23 for no-shared cases with thin combined features and NB classifier (Table 4). The recognition rates with some slight fall-off most likely due to the relatively small number of views. The comparable performance with respect to volume reconstruction is justified by the tremendous advantage of not needing to build these volume features, and so being able to cope with drop-out, cameras not calibrated to one another, and so on.

There is some evidence of a link between recognition rates and reconstruction here. Cameras 2 and 4, the best behaving pair (94.02% recognition rate in Fig. 8E), are close to perpendicular, and so would reconstruct quite good volumes. This is most likely because it is hard to have two cameras that would (a) reconstruct a volume well and (b) are both ambiguous; understanding this phenomenon would be valuable.

Simple experiments suggest that different sets of features make errors that are somewhat uncorrelated, suggest that it is worthwhile to combine features.

We have two schemes for feature combination (Section 4.1); summing votes (thin combination), or summing weighted votes (β combination). Summing votes is a simple method that directly gets the average of feature votes as in Eq. (4). The weighted voting procedure learns weight matrix β for features to be combined using training samples. As Tables 3 and 4 indicate, weighting votes increases accuracy in some experiments.

Parameters are learned using weight vectors from some subset of training samples. For a given training camera set C, let W
                        ∈ℝ
                           M
                           ∗
                           t
                         be a training set of vote vectors. Vote matrices Wf
                         are concatenated into a single matrix W; where t equals to 
                           
                              F
                           
                           ×
                           n
                         . 
                           
                              F
                           
                         is the number of features and n is the number of action classes. Y
                        ∈ℝ
                           M
                           ∗
                           n
                         is a matrix consisting of {0,1}; and y
                        
                           lk
                         is 1 when the lth sample belongs to the kth action class. We learn a matrix of weights β
                        ∈ℝ
                           t
                           ∗
                           n
                        . Each column vector β
                        .
                           k
                         is learned using y
                        .
                           k
                         and W by minimizing the least square with the following:
                           
                              (8)
                              
                                 
                                    
                                       
                                          
                                             
                                                arg
                                                min
                                             
                                             β
                                          
                                          
                                             
                                                
                                                   Y
                                                   −
                                                   Wβ
                                                
                                             
                                             2
                                             2
                                          
                                       
                                    
                                    
                                       
                                          s
                                          .
                                          t
                                          
                                             .1
                                             T
                                          
                                          
                                             β
                                             
                                                .
                                                k
                                             
                                          
                                          ≤
                                          t
                                          ,
                                          
                                             β
                                             
                                                .
                                                k
                                             
                                          
                                          ≥
                                          0
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

Combined features outperform the best stand alone feature, Optical Flow. Particularly, they are around 6% better over 5 cameras for no-shared case in single camera experiments as shown in Table 3. They are around 4% higher for two-shared, 9% for one-shared and 12% better for no-shared case than Optical Flow in two-camera experiments as shown in Table 4.

In real time, the number of active cameras in a system can change due to drop out or drop in. Fig. 10A shows the accuracies of thin combination for training with two cameras and testing with one camera. Comparing Fig. 10A with the diagonal of Fig. 8E, we observe the average performance drops when a camera switches to off line mode. For example, for training camera pair [1 2], testing with camera 1 has 83.97%, testing with camera 2 has 87.18%, but testing with cameras [1 2] gives 89.53% accuracy. Performance drops when a camera is lost, but the camera dropouts are not significant. Similarly, we can compare Fig. 10B with Fig. 8A to evaluate the addition of a second camera in real time. When camera 1 is the only camera that is used for training, testing with camera 1 performs 81.62% while testing with camera pair [1 2] gives 85.47%. Table 5
                         summarizes the performance of camera drop in for no-shared camera case. For example, when the camera 1 is used for training, the average recognition rate over testing with camera 2 or 3 or 4 is 72.01% and the average recognition rate over testing with camera pairs [2 3] or [2 4] or [3 4] is 82.69%. This shows that even with disjoint camera sets, addition of a second camera during test time increases performance.

We compare our system with the state-of-the-art methods in terms of experimental setups shown in Table 1. For this purpose, Table 7
                        
                         shows a detailed comparison of our results with the other studies in three different settings.

In the first experimental setup, all five cameras are used during training and testing. The volume matching method, Weinland et al. [16], reports 93.33% accuracy. Recently, Wang et al. [46] reports 93.6% accuracy using five views with dense sampled MBH trajectory features without reconstruction. Both of these studies report the results computed over 11 action categories. On the other hand, Liu and Shah [20] use 13 actions and reports 82.8.

In the second type of setup, a single camera is used for training and for testing. Our methods, β Combined-HMM reports 83.68% for shared and 44.36% for non-shared cases. β Combined-NB reports 83.72% for shared and 44.00% for non-shared cases. For this setting, our detailed results are reported in Section 5.1. For the same experimental setting, Junejo et al. [26] reports 80.6% for shared and 42.4% for no-shared cases.

The third experimental setup, uses multiple views for training and a single view for testing. We obtain 84.28% average accuracy over 4 views under thin Combined-NB. This is higher than [20] that reports 73.73% in average of 4 views over 13 actions. However, our results are slightly lower than [36] with 86.97% reported over 11 actions. We also did the same experiment setup for training with two views and testing with one view (Table 6). Our results are 84.64% for shared and 73.78% for no-shared cases. Training with 4 views experiment gives lower rate than training with two views experiment. In this setup, adding more views during training did not improve our rates. Liu and Shah [20] present another experiment using three views for training and the remaining fourth view for testing as the no-shared camera case, and they report 67.09%. For the same experimental setting including three views for training and a single view for testing, we report 75.21% with thin Combined-HMM and 74.83% with thin Combined-NB in no-shared camera case.

@&#CONCLUSION@&#

Our method combines the confidence estimate of multiple cameras and various appearance features. This helps in solving mislabeled frames with a more accurate activity recognition. In our experiments, we show that having a second view outperforms single view systems with a considerable amount. We have shown that, by not reconstructing in 3D, we have gained significant advantages in system architecture without losing much in performance. Views need not be calibrated to one another; training and test cameras do not need to overlap; cameras can drop in and out with little penalty; new features can be incorporated easily; and discriminative views can be exploited opportunistically. We believe that our system is appropriate for large distributed camera systems with its flexible architecture.

@&#ACKNOWLEDGMENTS@&#

S. Pehlivan was supported in part by the research fellowship of Scientific and Technical Research Council of Turkey while studying as a visiting scholar at University of Illinois at Urbana-Champaign.

@&#REFERENCES@&#

