@&#MAIN-TITLE@&#Chromatic shadow detection and tracking for moving foreground segmentation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We address the detection of chromatic moving shadows.


                        
                        
                           
                           Gradient and colour information is exploited for separating shadow regions.


                        
                        
                           
                           Temporal gradients and spatial angle and brightness distortions are used to detect shadows.


                        
                        
                           
                           Shadows and objects are tracked using motion filters.


                        
                        
                           
                           Mutual information and data association between objects and shadows are used to recover misdetected shadows.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Detecting moving objects

Chromatic shadow detection

Temporal local gradient

Spatial and temporal brightness and angle distortions

Shadow tracking

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

A fundamental problem for all automatic video surveillance systems is detecting objects of interest in a given scene [1]. A commonly used technique for segmentation of moving objects is background subtraction [2,3]. Detection of moving regions (i.e., the foreground) is achieved by comparing the current image from a reference background image in a pixel-by-pixel manner. There have been many segmentation problems already tackled in the literature related to motion segmentation [4], such as bootstrapping [5,6], changing background [7–9] and sudden illumination changes [10], to cite but a few, but one of the critical challenges is still shadow detection. Although this issue has been widely studied in [11–13], shadow segmentation is still far from being solved. The focus of this paper is to cope with the shadow problem.

Shadows can be divided into two categories: static and dynamic (moving) shadows. Static shadows occur due to static background objects (e.g., trees, buildings, and parked cars) blocking the illumination from a light source. Static shadows can be successfully incorporated into the background model thus being properly detected. However, the impact of dynamic shadows is critical for foreground segmentation, since objects can be merged or hidden by other objects, and both their size and shape can be distorted. This results in a reduction of the performance of foreground detection approaches applied in scene monitoring, object recognition, target tracking and people counting.

Dynamic shadows are more problematic, since they are due to the moving objects (e.g., people and vehicles). Dynamic shadows can take any size and shape, and can be penumbra (soft) or umbra (hard) shadows. Penumbra shadows exhibit low values of intensity but similar chromaticity values w.r.t. the background. Umbra shadows can exhibit different chromaticity values than the background, and their intensity values can be similar to those of any new object appearing in a scene. When the chromaticity of umbra shadows differs enough from the chromaticity of the global scene illumination, we define this as chromatic shadow. Consequently, umbra shadows are significantly more difficult to detect, and therefore usually detected as a part of moving objects by current state-of-the-art approaches.

This paper presents an approach which successfully detects umbra and penumbra shadows. First a bottom-up approach for detection and removal of chromatic moving shadows in surveillance scenarios is presented based on our previous work [14]. We apply a multi-stage approach combining multiple cues, namely colour, gradient information, and shadow statistics. Secondly, a top-down architecture based on a tracking system is proposed to enhance the chromatic shadow detection presented in [14]. This step is required since shadows can be lost for a number of frames due to camouflage. In these cases the use of motion filters allows our proposed system to track shadows, thus improving the accuracy and robustness of the final foreground detection performance. Experimental results show that applying the top-down shadow tracking, the shadow detection rate is improved by approximately 13%.

Secondly, a top-down architecture based on a tracking system is proposed to enhance the chromatic shadow detection, where motion filters are used for tracking. This step is required since shadows can be lost for a number of frames due to camouflage, so in these cases the use of basic motion filters allows our proposed system to track shadows thus improving the accuracy and robustness of the final foreground detection performance.

The remainder of the paper is organised as follows. Next section reviews the field in shadow detection and tracking, along with our contributions to this subject. In Section 3, the theoretical concept of our approach is outlined. The algorithm for foreground segmentation, along with the detection and removal of chromatic moving shadows, is described in Section 4. The top-down process used to enhance the shadow detection is described in Section 5. Finally, we present experimental results in Section 6 and concluding remarks in Section 7.

Shadow detection is a major field of research within computer vision. Even though many algorithms have been proposed [11–13], the problem of detection and removal of shadows in complex environments is still far from being completely solved. A common direction in the research is to assume that shadows decrease the luminance of an image, whilst the chrominance stays relatively unchanged [15,16]. However, this is not the case in many scenarios, e.g., in outdoor scenes. Other approaches apply geometrical information. Onoguchi [17] uses two cameras to eliminate the shadows of pedestrians based on object height, where objects and shadows must be visible to both cameras. Ivanov et al. [18] apply a disparity model, which is invariant to arbitrarily rapid changes in illumination, for modelling the background. However, to overcome rapid changes in illumination at least three cameras are required. In [19], Salvador et al. exploit the fact that a shadow darkens the surfaces on which it is cast, to identify an initial set of shadowed pixels. This set is then pruned by using colour invariance and geometric properties of shadows. Hsieh et al. [20] first separate the objects of interest and assume that the objects and their shadow have different orientations. Then, several features like orientation, mean intensity and centre position of a shadow region are used to parametrize a shadow model. It should be noted that most of the approaches which apply geometrical information require shadows to be cast on a flat plane, and give strong assumptions that need to be fulfilled.

Another popular approach is to exploit colour differences between shadow and background in different colour spaces. In [21], Cucchiara et al. consider the hypothesis that shadows reduce surface brightness and saturation whilst maintaining the hue properties in the HSV colour space. Liu et al. [22] propose another approach working in the HSV colour space, which combines local and global features for shadow removal. Schreer et al. [23,24] adopt the YUV colour space, whilst Horprasert et al. [25], Kim et al. [16] and [26] build a model in the RGB colour space to express normalised luminance variation and chromaticity distortions. However, these methods require illumination sources to be white, and assume that shadow and non-shadow have similar chrominance.

Several authors use textures to obtain a segmentation without shadows. The idea is that the structure of the texture/gradients/edges of regions lit by shadow is unchanged. Leone and Distante [27] propose a texture-based approach using a preliminary procedure in order to evaluate the photometric information for all pixels marked as foreground. This process shows how much darker the segmented regions are with respect to the background model. Next, texture analysis is performed by projecting the neighbourhood of pixels onto a set of Gabor functions. Another algorithm for detection of moving cast shadows, based on a local texture descriptor called Scale Invariant Local Ternary Pattern (SILTP), is presented by Qin et al. [28]. Zhang et al. [29] use ratio edges to detect and locate where the shadows are, and in Chen et al. [30] HOG descriptors of shadows are learned using SVMs to locate shadow regions. Sanin et al. [31] propose a method similar to Huerta et al. [14] where gradient magnitude and gradient orientation are used to detect shadows, but based on the gradient direction correlation. However, this paper [31] does not take into account cases where there are no texture/gradient in the bacground model. Whereas Heikkila and Pietikainen [32] apply Local Binary Patterns. Nevertheless, it still fails to detect umbra shadows.

To overcome these shortcomings, a number of approaches apply colour constancy methods, combine different techniques or use multi-stage approaches. In addition to scene brightness properties, Stauder et al. [33] extract edge width information to differentiate penumbra regions from the background. In [34], Finlayson et al. use shadow edges along with illuminant invariant (intrinsic) images to recover full colour shadow-free images, and in [35] the authors propose an entropy-based approach. Even so, a part of the colour information is lost in removing the effect of the scene illumination at each pixel in the image. Weiss [36] computes the reflectance edges of the scene to obtain an intrinsic image without shadows. However, this approach requires significant changes in the scene, and the reflectance image also contains the scene illumination. Huang and Chen [37] adopt a bi-illuminant model and apply a Gaussian Mixtures Model (GMM) and confidence-rated learning, whilst Martel et al. introduce a parametric approach based on Gaussian mixtures (GMSM) [38]. Additionally, they propose a nonparametric framework based on the physical properties of light sources and surfaces, and apply spatial gradient information to reinforce the learning of the model parameters [39]. Nadimi and Bhanu [40] propose a multi-stage approach for outdoor scenes, which is based on a spatio-temporal albedo test and dichromatic reflection model. Finally, a number of authors have introduced methods for shadow removal using only one single image [41–46], however, the focus of this work is shadow detection and removal in video sequences. Comparative studies of shadow detection techniques can be found in [11,47].

Exploiting temporal information for shadow detection has been rarely used. Liu et al. [22] use temporal information in order to avoid misclassifying the object under segmentation as shadow. To solve this, a nearest neighbour match method is used to track the object by checking if it is a foreground object in the previous frames. However, the authors only use tracking to recover misdetected objects and thus lose the shadow information. In our case, we take advantage of this information to track shadows, resulting in both improved object and shadow detection. In the previous papers, when a shadow has successfully been detected it is usually removed instantly, since only the object is of interest for further processing and not the shadow. As a result, the shadow information is lost. Our idea is to use this information to improve other aspects of object and shadow detection and tracking. Specifically, if a detected shadow is tracked over time instead of being discarded, it could be used to improve the shadow detection and possibly the object detection and tracking as well.

Summarising the contributions of this paper compared with the reviewed literature: (i) a shadow detection technique is designed to cope with chromatic moving cast shadows, by grouping potential shadow regions and considering the bluish effect, edge partitioning, temporal similarities between local gradient, and spatial similarities between chrominance angle and brightness distortions; (ii) both objects and shadows are tracked, thereby establishing data association between them. We use the mutual information and data association of both object and its shadow for improving the robustness of the detection by recovering misdetected shadows due to shadow camouflages. Consequently, foreground detection does not take into account neither penumbra and umbra shadows, thus improving current state-of-the-art performance without any assumption about camera location, surface geometries, surface textures, shapes and types of shadows, objects and background properties.

The colour information ρ at a given pixel a obtained from a recording camera supposing Lambertian surfaces depends on four components: the Spectral Power Distribution (SPD) of the illuminant denoted E(λ), the surface reflectance R(λ), the sensor spectral sensitivity Q(λ) evaluated at each pixel a and a shading factor σ.
                        
                           (1)
                           
                              
                                 ρ
                                 a
                              
                              =
                              σ
                              
                                 ∫
                                 
                                    E
                                    
                                       λ
                                    
                                    R
                                    
                                       λ
                                    
                                    
                                       Q
                                       a
                                    
                                    
                                       λ
                                    
                                    d
                                    λ
                                 
                              
                           
                        
                     
                  

The surface reflectance R(λ) depends on the material, i.e., materials have different responses to the same illumination.

In outdoor scenes, the environment is mainly illuminated by two light sources: a point light source (the sun) and a diffuse source (the sky) with different SPD E(λ). Besides a reduction in the intensity, an outdoor cast shadow results in a change of the chrominance. The illumination of the sky has higher power components in the lower wavelengths λ (450–495nm) of the visible spectrum, and it is therefore assumed bluish as argued in [40]. When the direct illumination of the sun is blocked and a region is only illuminated by the diffuse ambient light of the sky, materials appear to be more bluish.

By applying gradient information we can obtain knowledge about object boundaries, and thereby improve the foreground segmentation. Additionally, the gradient provides textural information about both the background and foreground image. Although shadows result in a intensity reduction of the illumination, and the texture of a given object or the background has lower gradient magnitude, the structure remains the same, i.e., the gradient orientation is unchanged.

When performing foreground segmentation with the influence of shadows, and taking the temporal local gradients into account, four main cases can occur as illustrated in Fig. 1
                        . The ellipses represent detection of potential chromatic shadows. They are grouped by considering an intensity reduction, the bluish effect and an edge partition.


                        
                           
                              Case 1:
                              Similar local gradient structures are present in the background model and in the current image. By examining similarities between the local gradients, and the fact that there is no foreground object in the current image, potential shadows can be detected and identified as shadow regions (case 1-1). However, if a foreground object is present, it can be misclassified as shadow if the gradients of the background and the foreground region considered as possible shadow are similar (case 1-2).

There are no available local gradients in the background model nor in the current image. Since, the change in illumination of all the potential shadow regions has to be similar, temporal and spatial similarities between chrominance angle and brightness distortions within the potential regions are analysed to detect chromatic shadows (case 2-1). However, a foreground object can be misclassified as shadow if the foreground object has no gradients, and the chrominance angle distortion is similar among the pixels in the region of the object (case 2-2).

Local gradient structure is present in the background model but not in the current image. If there is no gradient in the current image but in the Bg. model there is, there must be a new foreground object in the current image. So, by examining similarities between temporal gradients, the potential shadow will be considered a foreground object.

Local gradient structure is present in the current image but not in the background model. Therefore, there must be a new foreground object in the current image. In this case, the gradients in the current image are employed for object detection. Hence, there is no need to analyse the potential region further.

The described characteristics are not sufficient to address the anomalies in case 1-2 and case 2-2. Therefore, we make further assumptions and apply some additional steps, which are explained next.

Our approach, depicted in Fig. 2
                     , is a multi-stage approach. The first three stages (4.1, 4.2 and 4.3) remove the pixels which cannot be shadow. The fourth step (4.4) divides the regions of potential shadows. Chromatic shadow detection is performed in stages 5 (4.5) and 6 (4.6) based on gradients and chrominance angles, respectively. In stage 7 (4.7) the edges belonging to the shadows deleted in stage 4 (4.4) are considered shadows again. The last step (4.8) avoids foreground regions being erroneously detected as chromatic shadows. An example is given in Fig. 3
                     .

In this stage, foreground objects, shadows and some erroneous pixels are segmented. In order to achieve moving foreground segmentation, a hybrid approach presented in [48], which fuses invariant colour and gradient models, is used (please refer to [48] for further details). This approach can cope with several motion segmentation challenges, e.g., illumination changes, achromatic shadows, since it is based on a chromatic colour model [25]. Note that any other algorithm can also be used to generate this mask [4–10], however, the method proposed in [48] has been selected, since it is able to avoid achromatic shadows by fusion of different cues (chromatic, colour cone and gradient models). Additionally, the colour and gradient models presented in [48] are used for the next steps of our chromatic shadow detection, which requires the use of the gradient model in order to create the edge masks.

After this initial detection, moving foreground objects, chromatic shadows and some isolated pixels are represented by a binary mask named M1. Furthermore, a mask is created using the gradient model in [48] and divided into three masks (Edneg, Edpos, and Edcom), which are used for the next steps. The Edneg mask contains the edges from the background model which are occluded by the foreground object. The Edpos mask contains the edges of the foreground object for the current image, which are not in the background model. The Edcom mask contains the edges that are shared between the foreground object and the edges occluded in the background model for the current image.

In this step the M1 mask is reduced to avoid pixels which cannot be shadows. A foreground pixel cannot be a shadowed pixel if it has a higher intensity than the background model. Hence, a new mask M2 is created according to Eq. (2).
                           
                              (2)
                              
                                 M
                                 
                                    2
                                    
                                       a
                                       ,
                                       t
                                    
                                 
                                 =
                                 M
                                 
                                    1
                                    
                                       a
                                       ,
                                       t
                                    
                                 
                                 ∧
                                 
                                    
                                       
                                          I
                                          
                                             a
                                             ,
                                             t
                                          
                                          R
                                       
                                       <
                                       
                                          μ
                                          a
                                          R
                                       
                                    
                                 
                                 ∧
                                 
                                    
                                       
                                          I
                                          
                                             a
                                             ,
                                             t
                                          
                                          G
                                       
                                       <
                                       
                                          μ
                                          a
                                          G
                                       
                                    
                                 
                                 ∧
                                 
                                    
                                       
                                          I
                                          
                                             a
                                             ,
                                             t
                                          
                                          B
                                       
                                       <
                                       
                                          μ
                                          a
                                          B
                                       
                                    
                                 
                              
                           
                        
                     

where I corresponds to the image, a to a pixel position, and μ is the median value for the RGB pixel of the background model.

The effect of illuminants, which are different than white lights, provokes chromaticity changes, since the intensity varies differently for each colour channel. In outdoor sequences the main illuminants are the sky and the sun (neither of them white illuminant). The sky is the only source of illumination on shadowed regions, and it is assumed to be bluish, as argued in [40]. Therefore, the intensity changes in the red and green channels are larger than in the blue channel. This knowledge is used to reduce the potential shadow region detected in the previous step:
                           
                              (3)
                              
                                 M
                                 
                                    3
                                    
                                       a
                                       ,
                                       t
                                    
                                 
                                 =
                                 M
                                 
                                    2
                                    
                                       a
                                       ,
                                       t
                                    
                                 
                                 ∧
                                 
                                    
                                       
                                          I
                                          
                                             a
                                             ,
                                             t
                                          
                                          R
                                       
                                       −
                                       
                                          μ
                                          a
                                          R
                                       
                                    
                                 
                                 >
                                 
                                    
                                       
                                          I
                                          
                                             a
                                             ,
                                             t
                                          
                                          B
                                       
                                       −
                                       
                                          μ
                                          a
                                          B
                                       
                                    
                                 
                                 ∧
                                 
                                    
                                       
                                          I
                                          
                                             a
                                             ,
                                             t
                                          
                                          G
                                       
                                       −
                                       
                                          μ
                                          a
                                          G
                                       
                                    
                                 
                                 >
                                 
                                    
                                       
                                          I
                                          
                                             a
                                             ,
                                             t
                                          
                                          B
                                       
                                       −
                                       
                                          μ
                                          a
                                          B
                                       
                                    
                                 
                              
                           
                        where a corresponds to a pixel position. Obviously, the bluish effect cannot be applied for indoor sequences, however the entire approach can be applied without this mask for indoor sequences as it has shown in the experimental results with the Hallway sequence.

It is assumed that shadow regions have similar intensity changes for each channel, since the illuminant is the same. However, different surfaces have different reflectance characteristics, hence, the intensity change depends on the surface material. Therefore, we apply edges to describe region borders. Specifically, we build a new mask M4 using the foreground edges detected in the current image (Edpos) to separate the potential shadow regions from the moving foreground objects:
                           
                              (4)
                              
                                 M
                                 
                                    4
                                    
                                       a
                                       ,
                                       t
                                    
                                 
                                 =
                                 M
                                 
                                    3
                                    
                                       a
                                       ,
                                       t
                                    
                                 
                                 ∧
                                 ¬
                                 Edpo
                                 
                                    s
                                    
                                       a
                                       ,
                                       t
                                    
                                 
                                 .
                              
                           
                        
                     

The ¬ symbol before Edpos in Eq. (4) means negation. M3 contains the foreground blobs after being corrected by the shadow intensity and the bluish effect, and Edpos contains only the edges of foreground objects in the current image, not the edges of the background model. Since shadow regions do not contain any edges which are not in the background model, we use the Edpos mask to divide the foreground blobs into smaller regions, and to evaluate if these regions are shadows or not. Consequently, based on Edpos, the holes in M3 will not correspond to shadows. A minimum area morphology is applied to avoid smaller regions, which do not contain enough information for the subsequent steps of the shadow analysis.

Next, the temporal gradients of the regions in M4 are analysed to identify which case of the theoretical shadow analysis (see Section 3) each of the regions complies with. A region will be considered a shadow if it complies with case 1. The negative foreground edges (Edneg) of the region are compared to the common foreground edges (Edcom) to check if the region is a shadow and avoid the anomaly case 1-2.
                           
                              (5)
                              
                                 T
                                 
                                    x
                                    b
                                 
                                 =
                                 
                                    
                                       
                                          
                                             1
                                          
                                          
                                             if
                                             
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                            
                                                               a
                                                               ∈
                                                               
                                                                  R
                                                                  b
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  R
                                                                  
                                                                     b
                                                                     ,
                                                                     a
                                                                  
                                                               
                                                               ∧
                                                               Edne
                                                               
                                                                  g
                                                                  a
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               R
                                                               b
                                                            
                                                            ∧
                                                            Edtot
                                                         
                                                      
                                                   
                                                   .
                                                   
                                                      k
                                                      n
                                                   
                                                   <
                                                   
                                                      
                                                         
                                                            ∑
                                                            
                                                               a
                                                               ∈
                                                               
                                                                  R
                                                                  b
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  R
                                                                  
                                                                     b
                                                                     ,
                                                                     a
                                                                  
                                                               
                                                               ∧
                                                               Edco
                                                               
                                                                  m
                                                                  a
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               R
                                                               b
                                                            
                                                            ∧
                                                            Edtot
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             0
                                          
                                          
                                             otherwise
                                          
                                       
                                    
                                 
                              
                           
                        
                     

where a is the pixel position; R
                        
                           b
                         is the evaluated region and b is the number of the region; |R
                        
                           b
                        | denotes the number of pixels of region b; |R
                        
                           b
                        
                        ∧
                        Edtot| denotes the number of pixels representing the edges detected in the background model and the current image; k
                        
                           n
                         corresponds to a confidence region, set at 1.5 for all the different sequences in the experimental results, which is equal to the probability that the region belongs to a shadow or a foreground object based on the local gradients of the image. Note that Tx
                        
                           b
                         is not a mask, but a boolean expression that will be 1 if the region R
                        
                           b
                         is a shadow, and 0 otherwise. If the region is not a shadow, the evaluation will be performed by the chromatic shadow angle and brightness detection in the next stage (4.6).

In this step temporal and spatial similarities of the chrominance angle and brightness distortion for all pixels belonging to regions which have so far not been classified as shadow are analysed. A region will be considered a shadow if it complies with case 2. The only regions analysed in this stage are those without gradients, neither in the background model nor in the current image. If the pixels do not have a significant gradient, but have similar chrominance angle distortion and similar brightness distortion, the region is classified as shadow.
                           
                              
                                 AB
                                 
                                    d
                                    b
                                 
                                 =
                                 
                                    
                                       
                                          
                                             1
                                          
                                          
                                             
                                                
                                                   
                                                      if
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           
                                                                              ∑
                                                                              
                                                                                 a
                                                                                 ∈
                                                                                 
                                                                                    R
                                                                                    b
                                                                                 
                                                                              
                                                                           
                                                                           
                                                                              
                                                                                 
                                                                                    
                                                                                       R
                                                                                       
                                                                                          b
                                                                                          ,
                                                                                          a
                                                                                       
                                                                                    
                                                                                    ∧
                                                                                    Edne
                                                                                    
                                                                                       g
                                                                                       a
                                                                                    
                                                                                 
                                                                              
                                                                           
                                                                        
                                                                        
                                                                           
                                                                              
                                                                                 R
                                                                                 b
                                                                              
                                                                              ∧
                                                                              Edtot
                                                                           
                                                                        
                                                                     
                                                                     ∧
                                                                     
                                                                        
                                                                           
                                                                              ∑
                                                                              
                                                                                 a
                                                                                 ∈
                                                                                 
                                                                                    R
                                                                                    b
                                                                                 
                                                                              
                                                                           
                                                                           
                                                                              
                                                                                 
                                                                                    
                                                                                       R
                                                                                       
                                                                                          b
                                                                                          ,
                                                                                          a
                                                                                       
                                                                                    
                                                                                    ∧
                                                                                    Edco
                                                                                    
                                                                                       m
                                                                                       a
                                                                                    
                                                                                 
                                                                              
                                                                           
                                                                        
                                                                        
                                                                           
                                                                              
                                                                                 R
                                                                                 b
                                                                              
                                                                              ∧
                                                                              Edtot
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                               =
                                                               0
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      ∨
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     ∑
                                                                     
                                                                        a
                                                                        ∈
                                                                        
                                                                           R
                                                                           b
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        
                                                                           
                                                                              R
                                                                              
                                                                                 b
                                                                                 ,
                                                                                 a
                                                                              
                                                                           
                                                                           ∧
                                                                           Edto
                                                                           
                                                                              t
                                                                              a
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                               
                                                                  
                                                                     R
                                                                     b
                                                                  
                                                               
                                                            
                                                            <
                                                            
                                                               k
                                                               t
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      ∧
                                                      
                                                         
                                                            σ
                                                            
                                                               
                                                                  
                                                                     R
                                                                     b
                                                                  
                                                                  ∧
                                                                  
                                                                     α
                                                                     ⌣
                                                                  
                                                               
                                                            
                                                            <
                                                            
                                                               k
                                                               a
                                                            
                                                         
                                                      
                                                      ∧
                                                      
                                                         
                                                            σ
                                                            
                                                               
                                                                  
                                                                     R
                                                                     b
                                                                  
                                                                  ∧
                                                                  
                                                                     β
                                                                     ⌣
                                                                  
                                                               
                                                            
                                                            <
                                                            
                                                               k
                                                               b
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             0
                                          
                                          
                                             otherwise
                                          
                                       
                                    
                                 
                              
                           
                        
                     

where σ is the standard deviation of 
                           
                              α
                              ⌣
                           
                         and 
                           
                              β
                              ⌣
                           
                         which are the chrominance angle and brightness normalised distortions calculated for each pixel in the region R
                        
                           b
                        , respectively; k
                        
                           t
                         is a confidence region to avoid noise gradients; k
                        
                           a
                         and k
                        
                           b
                         are minimum thresholds used to determine if the angle and brightness distortion are similar among the pixels of the evaluated region. From the experiments of different sequences, a stable detection has been achieved using the following range of values: k
                        
                           a
                        
                        =[3−7], k
                        
                           b
                        
                        =35, and k
                        
                           t
                        
                        =0.1 so if more than 10% of the pixels are considered edges then it cannot be noise.

Pixels of the potential shadow regions, which were neglected in Section 4.4, since they were part of the Edpos mask, are included again in the new set of shadow regions.

A moving cast shadow is always caused by a moving foreground object. Therefore, in this section it is ascertained if a detected shadow has an associated foreground object, in order to avoid the anomaly in case 2-2. Only shadows detected in the chrominance angle and brightness distortion analysis (Section 4.6) will be tested. During a training period T
                        2, the angles between the detected shadows and the foreground objects are calculated. The training period T
                        2 is used to learn the initial position of shadows. In the first frames of a shadow detection, the angle between the detected shadow and the foreground object is calculated based on the shadows detected in stage (4.5). In order to guarantee that these angles will be consistent with the illumination changes during long periods of time, the angle is updated using a first order recursive filter. Hereafter, the most probable angle obtained in the training period is used to discard detected shadows, which do not have any foreground object in its direction.

When a shadow has successfully been detected it is usually removed, since only the Fg. object is interesting for further processing. As a result, the shadow information is lost. Our idea is to use this information a posteriori, in order to improve the shadow detection when it fails (e.g., due to camouflage problems). Specifically, if a detected shadow is tracked over time instead of being discarded, it can be used to recover misdetected shadows, and hereby improve shadow detection.

In this section a top-down approach is applied to enhance the chromatic shadow detection using a Kalman Filter (KF) based tracking. We choose a basic Kalman filter because we are interested in showing how the temporal consistence in the data association step used in tracking can be exploited to improve shadow detection. However, our proposed method can be applied to other different (non-linear) tracking methods. Hence, the extended Kalman filter or unscented Kalman filter which is non-linear could be perfectly used with almost no modification of the proposed method.


                     Fig. 4
                      shows an overview of the top-down tracking process. First, the tracking module tracks objects and shadows through the scene. As input, the tracking module receives a binary mask from the object and shadow detection described in the previous section. The standard tracker and probabilistic appearance models (PAMs) are explained in Sections 5.1, and 5.2, respectively. The output of the tracker is a list of tracks for each object and shadow, and their mutual association. Secondly, in Section 5.3 the association between foreground objects (FG) and shadows (SH) is described and updated for the Kalman filters (KFs). Thirdly (Section 5.4), temporal consistency is investigated in the association between FG and SH blobs, and their assigned KFs, in order to identify possible lost shadows. Once the shadows are detected and tracked, the information is used as feedback for the chromatic shadow detection to recover misdetected shadows in the original image (Section 5.5). Finally, the KF and the PAM are updated by taking the information from the new data association into account, and used for tracking in the next frames (Section 5.6). An example of the entire process can be seen in Fig. 5
                     .

The detected foreground objects and shadows are tracked using first order Kalman filters. The tracking and data association are based on a number of estimated parameters for the detected objects and shadows:
                           
                              •
                              The centroid of an ellipse fitting.

The major and the minor axis length of the ellipse.

The probabilistic appearance model.

Each track is associated with these parameters, and a standard Kalman filter is used to predict the object's location using a first order motion model. Hence, the target state is defined by x
                        
                           t
                        
                        =(posx
                        
                           t
                        ,
                        posy
                        
                           t
                        ,
                        velx
                        
                           t
                        ,
                        vely
                        
                           t
                        , maj
                        
                           t
                        ,
                        min
                        
                           t
                        ,
                        θ
                        
                           t
                        ), which establishes a state vector for every observation, and adds the target speed and the size deformation rate at time t. Where posx
                        
                           t
                         and posy
                        
                           t
                         define the position (the centroid of the ellipse); velx
                        
                           t
                         and vely
                        
                           t
                         are velocity components; maj
                        
                           t
                         and min
                        
                           t
                         are the major and minor axis of the ellipse, respectively; and θ
                        
                           t
                         is the orientation. The KFs are initialised based on the detected foreground and shadow blobs, and the uncertainties are empirically estimated according to the precision of the detector. The foreground blobs extracted and classified as object or shadow (see Section 4) are associated with a list of possible Kalman filters using a modification of the stable marriage algorithm [49], similar to the tracking system presented in [50], see them for details.

Probabilistic appearance models inspired by [50] are applied for data association and to solve inter-object occlusion. Each track has its own PAM, which consists of an RGB colour model with an associated probability mask. The colour model, which is denoted M
                        
                           RGB
                        (x), shows the appearance of each pixel of an object. P
                        
                           c
                        (x) denotes the probability mask and represents the probability of the object being observed at that pixel. The use of PAMs can be viewed as weighted template matching, where the template is M
                        
                           RGB
                        (x) and the weights are given by P
                        
                           c
                        (x). The coordinates of x are expressed using the coordinate system of the model, which is normalised to the object centroid. For each new track, a new PAM is created. In the object match situation, a track refinement step is applied before updating the model by finding the best fit in a small neighbourhood, e.g. 5×5 pixels. Track refinement increases the accuracy of the model; especially the colour model becomes sharper. Detail on building the model can be found in [50].

After the blobs (belonging to a FG or a SH) have been assigned to the KF, the association between which shadow belongs to which FG and vice versa is saved in the KF info for use in the next frames. This information is used to identify the possible cases in the association between FG and SH. An example showing the data association between the blobs and the KFs, and how the data association between FG and SH is saved in the KF info, can be seen in Fig. 6
                        . The first image of Fig. 6 represents the FG detection provided in Section 4.1. The second image shows the shadow detection presented in previous Section 4, and how the FG segmentation is further analysed and divided into FG and SH blobs. These blobs are associated, since both are part of the same FG object. In the third image the tracking system has assigned one KF to each blob, and the data association between the FG and the SH blobs is saved in the KF info.

The information related to the association between FG and SH saved in the KF is analysed to check the possible data association cases, e.g., if a shadow has been lost.

When testing for temporal consistency in the data association between FG and SH with their respective KF, three main situations can occur:
                           
                              •
                              FG and SH match:

The association created at time t−1 continues at time t, which is the ideal case.

New shadow: FG/SH splitting.

A new association between the FG and SH is created at time t. Temporal association of a splitting shadows is applied to avoid misdetected shadows in posterior frames.

Lost shadow: FG-SH merging.

The association between the FG and SH at time t−1 is lost at time t, since the shadow is misdetected (Fig. 5).

The three cases are illustrated in Fig. 7
                        . It is possible that a new shadow appears or a shadow is lost without merging of the FG object. However, these cases are not of interest, since they do not have any FG–SH association, hence, they will be tracked in the usual manner by the KFs.

The shadow region can be recovered by evaluating the FG blob (which contains the merged FG and SH), the blob prediction for the FG KF, and the blob prediction for the lost SH KF. The next subsections explain the recovery process for the lost shadow case and an example of the entire process is shown in Fig. 5.

The FG blob which belongs to the FG KF, associated in the previous frame with the shadow considered lost, is analysed in order to recover the possible shadow region. To do so, the mask of the positive edges (Edpos mask) plus morphological operators are applied, to divide the FG blob into FGs and possible shadow regions. Multiple regions can be found but theoretically only one is the shadow. This happens because the positive edges are used to divide the image, and these edges come from the current image. As explained in Section 3, one of the characteristics of shadows is that they can only have negative edges, i.e., the edges from the background image. Therefore, theoretically several FG regions can be found but only one SH region. Fig. 5 shows how the original FG blob, detected as described in Section 4, is subdivided into possible chromatic shadow regions using the Edpos mask (image Chr.Sh.Regions in the figure; the regions in the image are shown in different colours). In the following, the new divided blobs from the regions are associated with the predictions of the KFs to recover the chromatic shadows.

The weights of the blob prediction for the FG KF and the SH KF are calculated w.r.t. all possible regions found in the previous step. Therefore, two correspondence matrices are computed, where one contains the euclidean distance between the new blobs and the FG and SH KF predictions, and the other the overlapping (matching) between the new blobs and the FG and SH KF predictions. Next, these weights are applied to associate the SH and FG KF predictions with the blobs.

The best match (shortest distance and best overlap) between the SH KF predictions and the blob will be considered as the shadow region, whilst the other blobs will be considered as FG blobs, since only one region can be shadow. Hence, the other blobs have to be FGs. In this way, by using the tracking information, the original FG blob can be segmented into FG and SH regions, and thereby recover misdetected chromatic shadows. This information is used as a feedback from the tracking to the shadow detection step. Fig. 5 shows how the blobs extracted from the divided regions are associated with the predictions of the KFs to detect the chromatic shadow.

Once the chromatic shadows are detected, the original image (original FG blob) is divided into one blob for the FGs and one blob for the SHs, using the information from the positive edges and the shadow tracking. Hence, the FG blob will be associated with the FG KF and the SH blob will be associated with the lost SH KF. Next, the original image is updated, so the misdetected chromatic shadows are now marked as detected. Fig. 5 shows how the detected chromatic shadow is updated according to the original blob after the feedback from the shadow tracking.

Finally, the information related to the new associations between the FG and SH blobs, and their respective KFs must be updated. Additionally, the KFs must be updated with the new associated blobs, and the PAMs must be updated considering the new blobs. It is possible that a new KF is erroneously created because one object together with its shadow was considered a new object. Therefore, the new KFs created in the data association between the blobs and the KFs must be checked. Any unused KFs, which were assigned to blobs considered as lost shadows, are deleted. In Fig. 5 also can be seen the output of the tracker with and without our top-down approach (the last two images at the left called “Tracker End”). Top-down Tracker End image shows how the system recovers the detected chromatic shadows, hence the FG KF and SH KF are correctly updated. On the other hand, the image of the tracker without top-down approach shows how the FG and SH KF are lost, and a new wrong KF is created.

Consequently, due to the data association between FG and SH we have achieved: (i) an enhancement of the chromatic shadow detection by recovering misdetecting shadows, which were incorrectly detected by the shadow detector (Section 4); (ii) an improvement of the segmentation for high level processes, such as detection and tracking, by avoiding shadows; and (iii) a more robust tracking by exploiting FG and SH association.

@&#EXPERIMENTAL RESULTS@&#

The results presented in this section are from tests conducted on datasets selected from well-known databases. Our approach is tested on sequences of outdoor and indoor scenarios, and compared to other statistical approaches when results are available. The test sequences are relatively long and umbra and penumbra shadows are cast by multiple foreground objects. We have considered two different types of databases. On the one hand, shadow datasets like ATON_Intelligentroom, ATON_Campus, ATON_Laboratory, CAVIAR, and NEMESIS do not exhibit chromatic shadows as it is stated in [14,13,51]. In these scenarios, the moving foreground segmentation presented in Section 4.1 already copes well with such shadows, as already demonstrated in [48]. See Fig. 8
                      for a selected frames. On the other hand, we have considered another datasets like Outdoor_Cam1 (800 frames, 607×387pixels), HigwayIII
                        1
                     
                     
                        1
                        
                           http://vision.gel.ulaval.ca/CastShadows/.
                      (2227 frames, 320×240pixels), HallwayI [2] (1800 frames, 320×240pixels), HERMES_ETSEentrance (6500 frames, 640×480pixels), CDW14 Bungalows
                        2
                     
                     
                        2
                        
                           http://changedetection.net/.
                      (1700 frames, 360×240pixels), and CDW14 PeopleInShade (1199 frames, 380×244pixels) which do exhibit chromatic shadows, and for which the whole proposed framework presented here is fully tested, as detailed next.


                     Figs. 9 and 10
                     
                      show qualitative results when comparing our shadow detector with state-of-the-art approaches [16,15,39,48]. As shown in these figures our approach outperforms the other analysed methods.

To evaluate our approach in a quantitative way, a comparison to several other shadow detection approaches is conducted, using the most employed quantitative expressions utilised to evaluate the shadow detection performance: the shadow detection rate (η) and the shadow discriminate rate (ξ). Please refer to [11] for the exact equations. Tables 1 and 2
                     
                      show the computed η and ξ values of each test sequence, comparing our bottom-up (chromatic shadow detection) approach with other successful methods from the state of the art. Table 1 shows a comparison with [28,39,38] approaches, η and ξ are from their respective papers and the ground-truth is from [38]. Table 2 shows a comparison with [31,15,37,20,27] approaches, and the code and ground-truth are from [47].

The results in Table 2 show that our bottom-up method outperforms the other approaches achieving a 0.75 shadow average rate over all the sequences tested. Furthermore, it gives the highest shadow detection rate average (
                        
                           η
                           ¯
                        
                     ) 0.60 and the highest shadow discriminate rate average (
                        
                           ξ
                           ¯
                        
                     ) 0.91, in other words it is the most stable and robust approach of all the methods.

For the HallwayI sequence our bottom-up method outperforms the other approaches. Sanin et al.'s method [31], which is also based on gradient information, gives very similar performance, whilst Qin et al.'s method [28] produces a similar shadow detection rate but at the cost of a markable reduction in the shadow discriminate rate, and Hsieh et al.'s [20] method gives a very poor performance for this particular sequence because of his geometrical approach. Notice this sequence is an indoor scenario, therefore the bluish effect mask is not used. Nevertheless, our bottom-up method obtains the highest values outperforming the other approaches.

For the HighwayIII sequence, our method performs the best when it comes to the shadow discriminate rate, however Sanin et al.'s method [31] gives a better shadow detection. This is because our approach needs a minimum area for the shadow regions, or there might not be enough information for a proper shadow detection and classification. Our method produces a lower detection rate in this sequence because the moving cars are very small when they are far away from the camera.

For the HERMES_ETSEentrance sequence, Sanin et al.'s [31] method gives a very poor performance, since there is no or very little gradient information in the background, which is crucial for this solely texture-based approach. Our method performs the best when it comes to the shadow discriminate rate. However, the approach proposed by Hsieh et al. [20] gives a better shadow detection. This geometric approach performs very well for this particular sequence, since the people and shadows have different orientations and are clearly defined. However, this method usually fails when the objects and shadows are divided, or there are multiple shadows, or the objects and the shadows have the same orientation. This can be seen in the lower values obtained for the other test sequences.

Overall, our multi-stage approach, which takes advantage of known shadow properties, texture and invariant colour models, and spatial geometry give the most robust and stable performance for all indoor and outdoor scenarios.

The top-down process assists the chromatic shadow detector when it fails to detect shadows, as shown in Fig. 11
                     .c. The tracking system is able to track the shadows, and use this information as feedback to the chromatic shadow detector. Hence, the misdetected shadows can be recovered and correctly detected. Fig. 11 presents examples of shadow recovery using our top-down approach for the HERMES_ETSEentrance (Fig. 11 first row), CDW14_bungalows (Fig. 11 second row), and CDW14_PeopleInShade (Fig. 11 third row) sequences. Fig. 11.a shows the foreground detection results from Section 4.1 (mask M1). In Fig. 11.b the chromatic shadow detection results from our bottom-up approach are shown. Note that the shadows are not correctly detected. Fig. 11.c shows the output of the tracker without applying our top-down approach. The KFs associated with the shadows are lost and therefore falsely updated. Fig. 11.d shows the output of the tracker, after the chromatic shadows are recovered, using our top-down process. The shadows are accurately detected, therefore the FG KFs and the SH KFs are correctly updated, and none of them are lost. The a posteriori state is depicted with a red ellipse. Finally, Fig. 11.e shows how the chromatic shadows are accurately detected after feedback from the tracker to the chromatic shadow detector.

To evaluate quantitatively the results of our shadow tracking we have hand-segmented 1500 frames from the HERMES_ETSEentrance sequence
                        3
                     
                     
                        3
                        Original sequence, foreground and shadow hand-segmented ground truth are available at http://www.cvc.uab.es/ivanhc/shadows/huertaShaDows.html for comparison purposes.
                     . Table 3
                      shows the results when applying our top-down shadow tracking to the HERMES_ETSEentrance, CDW14 Bungalows and CDW14 PeopleInShade from HERMES and Changedetection datasets, the shadow detection rate is improved by approximately 13%, which effectively means that we detect more shadow regions using shadow tracking. Additionally, the shadow discriminate rate is pretty stable.

First, a discussion of the limitations of the current approach is presented. Later, some remarks on the computational complexity and the execution time for a possible real-time application are discussed. The tracking method employed is a basic Kalman filter, which is limited in some aspects (linear, not multi-hypothesis, not able to track multiple objects in crowded scenes). However, the main goal of the proposed approach is not to cope with advanced tracking problems, but to show how shadow detection can be improved by exploiting the temporal consistence of the data association between foreground objects and shadows. Other more complex trackers, like the ones presented in [52], could easily be used with slight changes of the approach, in order to tackle problems such as multiple occlusions.

In terms of computational complexity, motion segmentation has a cost that is linear to the number of pixels in the image. The specific implementation used in the experiments [48] runs at about 3fps in MATLAB. However, a faster reimplementation or the use of other algorithms [11,5] can lead to realÂ­time performance. The shadow detection (bottom-up) part runs at about 28fps in MATLAB. Since the models are already computed from [48], so there is no need to calculate them again. If another faster motion segmentation approach is used, then the colour and edge models should be calculated, which runs at about 6fps in MATLAB. The shadow tracking (top-down) part runs at around 10fps in MATLAB. The time will increase with the number of elements to track, but tracking a shadow will have the same computational cost as tracking a new object.

@&#CONCLUSION@&#

In this paper, we have presented two main contributions for surveillance analysis: (i) a novel bottom-up approach removes chromatic moving shadows and (ii) a top-down approach based on motion filters keeps track of both objects and shadows to handle chromatic shadow misdetections. Unlike other approaches, our method does not make any a-priori assumptions about camera location, surface geometries, surface textures, shapes and types of shadows, objects, and background properties.

The bottom-up algorithm exploits gradient and colour information for separating chromatic moving shadows from moving objects. Invariant colour cone and invariant gradient models are used to detect potential shadow regions. Subsequently, these regions are grouped by considering the bluish effect together with edge partitioning. Lastly, (i) temporal similarities between local gradient structures and (ii) spatial similarities between chrominance angle and brightness distortions are analysed to finally identify those umbra shadows. Unfortunately, there are still shadows incorrectly classified, mainly due to camouflage.

In order to cope with the camouflage problem, the top-down technique tracks both objects and shadows using basic motion filters. By performing data association and exploiting the knowledge from mutual information between object and shadows, we are able to detect more shadows thus improving the final segmentation performance compared with the state-of-the-art. Experimental results obtained in well-known outdoor and indoor sequences demonstrate the advantages of our proposed approach since its performance is proven to be more robust and accurate compared to current state-of-the-art techniques.

@&#ACKNOWLEDGEMENTS@&#

This work has been partially funded by the European project CargoANTs (FP7-SST-2013-605598), by the Spanish CICYT project 
                  DPI2013-42458-P and by the CICYT 
                  TIN2012-39051.

@&#REFERENCES@&#

