@&#MAIN-TITLE@&#Point of view: Long-Term access to Earth Archives across Multiple Disciplines

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We report standardization activities on multi-disciplinary Earth System Science.


                        
                        
                           
                           We propose the standardization across multiple disciplines in long term.


                        
                        
                           
                           We consider the application of the knowledge base concept to facilitate data interpretation.


                        
                        
                           
                           Both common and domain requirements are considered.


                        
                        
                           
                           Knowledge gap issues between the source and target communities are addressed.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Interoperability

virtualization

coverage

multi-disciplinary

data preservation

@&#ABSTRACT@&#


               
               
                  Without an approach accepted by the communities at large, domain disagreements will continue to thwart current global efforts to harmonize information models. The research presented here reviewed current standardization activities. A number of observations and possible solutions are proposed to address the topic of standardizing long term access to multi-discipline Earth System archives by considering the application of the knowledge base concept to facilitate data interpretation. Finally, we present a case study as an initial entry point for the further discussion about standardization.
               
            

@&#BACKGROUND@&#

Multi-disciplinary Earth System Science research often involves the use of unfamiliar geo archives from different domains. Raw data, derived products or representation data are delivered either by offline ordering systems or online and web service based delivery systems. However, the heterogeneity of archiving models employed in these systems tends to limit the interoperability of the data and hence their usefulness in today’s highly multidisciplinary Earth system science research. Due to continuous technology change and research development, data access technologies come and go. Although the archived bits may remain the same, the information or knowledge encapsulated by these bits and bytes may be lost. Goodchild et al. [1] argue that scientific grounded information about the planet’s future should be fully understood and absorbed. To ensure these archives are both sustainable and sustained for the long term, the Research Libraries Group (RLG) and Commission on Preservation and Access (CPA) formed a Task Force on Archiving of Digital Information to investigate the means of ensuring “continued access indefinitely into the future of records stored in digital electronic form” [2]. The publication of the final report marked an important point for the digital preservation community and has proved to be a fundamental document identifying core challenges of digital preservation [3].

To standardize digital preservation practice and provide a reference model for repositories, the Consultative Committee for Space Data Systems (CCSDS) developed the Open Archival Information System (OAIS) Reference Model to provide a framework for the standardization of long-term preservation which is applicable in any domain or context beyond its initial space science community [4]. Instead of defining concrete metadata standard, the model provides an abstract framework for designing archival systems or repositories, including all technical aspects of a digital object’s life cycle, from ingestion to distribution. In terms of interpreting the information encoded within an object’s bitstream, representation information maps a Data Object into more meaningful concepts [4]. A specific example provided by Mbaye [5] is using the Data Request Broker (DRB) [6] API to handle the ENVISAT product data [60], regardless of physical formats, for the extraction and interpretation of relevant product information.

Building on this example, long term access to Earth Science data needs to involve a multi-disciplinary interoperability approach, as the community evolution will always result in a change of the disciplinary knowledge base. Standardization work by combining domain best practices and making them generally accepted within communities at large has the potential of achieving long-term global sharing of geospatial information in the heterogeneous world of Earth archives. In this paper, possible approaches to achieve long-term interoperability of existing archives are investigated, on both the horizontal and vertical domain, as well as beyond the Earth science disciplines.

In Earth science, GIS data models [7] are designed to capture, manipulate, analyze, manage, and present all types of geographical phenomena, such as roads, land use, elevation, trees, rainfall amount, etc. Traditionally, there are two methods to abstract these geographical features: vector data and raster images. Points, lines, and polygons are the vector data which mathematically describe the location of each vertex in Coordinate Reference System; while images or arrays are the raster data whose cells' geographic location is implied by their position in the array matrix. Large multidimensional data are being collected by sensors and by humans [8]. These can be from any number of sources, largely unknown and unlimited, and stored in diverse formats for optimizing either read-write efficiency, security enhancement, or data interchange. “This richness of alternatives is more a curse than a blessing since it has created the confusing and apparently chaotic variety of Geographic Information System (GIS) data structures and formats now confronting GIS users“[9]. A standardized Geographical Feature [9] is adopted to provide foundation models that order the chaos and bridge real-world phenomena and their representation as a collection of Features with Geometry. The abstraction is built on the basic concepts of geometry, reference system, relations, quality, metadata, etc. [10]. Common open model languages, such as Geography Markup Language (GML) [11], JSON Geometry and Feature Description [12], and Keyhole Markup Language [13], which implement or partially implement this abstraction, are widely adopted as de facto interchangeable formats for geographic features.

Nevertheless, domain experts have different preferences, e.g., all NASA Earth Observing System (EOS) data products [14] use Hierarchical Data Format (HDF) as the standard data format, while the Standard Archive Format for Europe (SAFE) [15] has been designed to act as a common format for archiving and conveying data within ESA Earth Observation archiving facilities [44]. Further commonly used data formats, such as ESRI shape files, Network Common Data Form (NetCDF) [16] and GeoTIFF formats, are also used to deliver data across the various Earth science disciplines. Actually, there are rich formats, eg. JPG2000 [57] and PNG [58], available beyond these fairly restricted set of data formats. Efforts have been invested into binding existing archives with the standardized models to improve interoperability among various Earth Science domains. For example, Nativi [17] has investigated the mapping model from the Common Data Model of the Unidata [18] to the ISO 19123 coverage [19], which is a special case of Geographical Feature. Based on this research, practical experiments [20] have be taken by GALEON IE [21] to provide interoperable and standard-based solutions [22] for datasets up to 5D and bridge the gap between the atmospheric, oceanographic and GIS communities. The approach provides a model level mapping for generic access interfaces which are independent of how the data are stored physically.

OAIS defines data as any type of knowledge that can be exchanged, and representation information, which maps the data into more meaningful concepts. The Archival Information Package is defined as the data plus representation information plus the additional information needed to support claims of authenticity. The CCSDS/ISO XML Formatted Data Units (XFDU) packaging format is consistent with this approach and is provided as a standard to package data and metadata (including software) into a single package (e.g., file or message) to facilitate information transfer and archiving in the space informatics domain [23]. The standard provides a packaging solution to define the information and its behaviors. The solution leaves the freedom of choosing representation information to domain experts.

For example, by considering access Earth observation archives in long term, SAFE [24] implements the CCSDS/ISO XFDU packaging format, wraps or references Earth Observation (EO) data and associate them with information expressed in EO vocabulary. SAFE is designed to act as a common format for archiving and conveying data within ESA Earth Observation archiving facilities. The integrated representation via XFDU opens the door to the access of bit streams without having to consider the heterogeneous data encodings. The approach is particular important in long-term preservation of Earth Observation data and implies an interoperable framework for packaging a large variety of information for multidisciplinary Earth Science communities.

Furthermore, XFDU allows executable behaviors to be associated with content in the content unit of information package. These behaviors may be represented by abstract definitions or references of concrete modules of executable code that implement and run the behaviors defined abstractly by the interface definition. Domain experts make the decision on which behaviors are to be associated to the information content.

Besides XFDU, OAI-ORE [25], is a recommendation that built on the principles of the web architecture. It defines the description of aggregations of web resources and the digital objects of which they are composed. The specification was extended in the SHAMAN project [26], which focuses on OAIS-based packaging of compound digital objects preserved in a distributed storage environment for future use. The Metadata Encoding and Transmission Standard (METS) [27] is primarily a packaging format for metadata and object references without considering the consistency between the encoding and the information. The BagIt File Packaging [28] is yet another simple layout for exchanging generalized digital content, which is used for document encodings and checksum algorithms associated with the contents of a “bag”. However, the semantics behind it is ignored.

OGC members are proposing an open, non-proprietary, platform-independent GeoPackage container for distribution and direct use of all kinds of geospatial data. Obviously, using a packaging standard would help improve access to historical geospatial data be it XFDU’s information and behavior approach, OAI-ORE’s aggregation approach, METS’ referencing approach or BagIt’s checksum algorithms approach.

The three concepts of data, information, and knowledge are often regarded as the basic building blocks of information science field [29]. Nonetheless, the definitions and usage of these terms are not always consistent between leading scholars from different aspects of the information science academic community. As a starting point for building a systematic conception of communication among different fields, it is essential to achieve a standardized conceptions of data, information, and knowledge. OAIS defines these terms as below:
                           
                              •
                              
                                 Data: A re-interpretable representation of information in a formalized manner suitable for communication, interpretation, or processing.


                                 Information: Any type of knowledge that can be exchanged. In an exchange, it is represented by data.


                                 Knowledge Base: OAIS does not define Knowledge but instead defines knowledge base, which is a set of information, incorporated by a person or system, that allows that person or system to understand received information.

In an information system, data can (usually) be instantiated as a sequence of bits before it is processed. Proper interpretation by a human or machine process produces information which can be derived and used for communication. Different communities hold different understanding about their data and curate the knowledge on how to process and make these data usable.

Initially, the knowledge encoded in the data is understandable by its producing community. When the data is transfer to a different community, the data may be unfamiliar due to the Knowledge Base differentiation. There are always knowledge gaps among different communities, although some common knowledge is usually available as well, which implies that a different community may be able to partially interpret the transferred data. Therefore, the outcome of communicating data between communities depends on the common points and gaps in their knowledge if a community is assumed to be identified by its Knowledge Base. The communication via the specific data model is defined as below:
                           
                              
                                 
                                    K
                                    
                                       B
                                       i
                                    
                                    ≠
                                    ∅
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    Interpret
                                    
                                       
                                          K
                                          
                                             B
                                             i
                                          
                                          ,
                                          
                                             D
                                             i
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    hold
                                    
                                       
                                          
                                             C
                                             i
                                          
                                          ,
                                          K
                                          
                                             B
                                             i
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    K
                                    
                                       B
                                       i
                                    
                                    ∩
                                    K
                                    
                                       B
                                       j
                                    
                                    =
                                    ∅
                                    →
                                    Isolate
                                    
                                       
                                          C
                                          i
                                       
                                       
                                          C
                                          j
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    K
                                    
                                       B
                                       i
                                    
                                    ⊃
                                    K
                                    
                                       B
                                       j
                                    
                                    →
                                    Interpretable
                                    
                                       
                                          C
                                          i
                                       
                                       
                                          D
                                          j
                                       
                                    
                                    ∧
                                    PartiallyInterpretable
                                    
                                       
                                          C
                                          j
                                       
                                       
                                          D
                                          i
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    K
                                    
                                       B
                                       i
                                    
                                    =
                                    K
                                    
                                       B
                                       j
                                    
                                    →
                                    Exchangeable
                                    
                                       
                                          C
                                          i
                                       
                                       
                                          C
                                          j
                                       
                                    
                                 
                              
                           
                        
                     

Knowledge Base (KB), is the knowledge held by Community (C) to interpret the specific Data model (D) being used for the communication. The communication fails when both communities share no common Knowledge Base. A community is able to fully interpret the data from the other when it holds the knowledge base of that community, while the other community can only partially interpret the data from the community if it does not hold all the knowledge needed to interpret its Data Model. Let KBi
                        ⊂KBj, community Ci is able to transfer its information to Cj via Data Model Di, and Cj is also be able to transfer its information to Ci via Data Model Di when this information is not beyond its expressiveness. On the other hand, without full knowledge of KBj, Ci can only partially interpret the information transferred using Data Model Dj, and itself is not able to transfer its information via Dj properly unless it learns the missing knowledge from Cj. When both communities share the same Knowledge Base, the information between the two communities is exchangeable without either having to learn new knowledge.

However, in multidisciplinary Earth System Science most of communities do not have the same Knowledge Base. These disciplines adopt different spatial-temporal concepts to model the measured Earth System phenomena which introduce variability to the archived data. Without knowledge exchange, they can only partially digest archives from each other. This limits the usability of their archives in today’s highly multidisciplinary Earth system science research where data interoperability is critical.

In a specific community, members from different backgrounds and with different levels of experience agree to interpret the domain model in the same way without ambiguity. The model can be a structural, a database, or a semantic model. A community adopts a set of models to convey its information among its members. Different communities tend to use different models for their specific missions due to the different aspects of abstraction that can be derived from measured phenomena and respectively limited model expressiveness. The difference becomes an obstacle for information exchange and integration.

A generic model, which expresses all possible concepts, is an ideal solution to all communities. This monolithic model would have to be complex enough to communicate all possible information of different communities. Obviously, not all of them want to study such a solution that would imply extra resources to consume knowledge not related to their missions. Furthermore, new communities will always emerge due to the increasing awareness of Earth phenomena. A generic model today will not be generic enough tomorrow.

A core model, which expresses common concepts, is yet another promising solution. In this case, only the knowledge intersection needs to be expressed. The knowledge outside the intersection will only be investigated when it is needed for the cross-communities communication. In this case, specific models from a source to a designated community need to be adopted to express these cross-communities concepts. We will not address how these differences are discovered, as it is already studied in the research of Tzitzikas and Flouris [30], which uses Gap Managers to mine the knowledge differentiation between different communities.

The remaining question is whether there exists a single model that is common enough to be “the one”. This would be a very difficult answer for now, however, a common concept in Earth Science System is the use of key value (position, attribute) pairs to describe the space/time-varying phenomenon. Discipline-specific models are based on this core concept. A table or an array is a special case of non-spatial-temporal model, which can use row and column indexes to locate its values.

Data transformation can map data elements from a source encoding to a different encoding which is more useable by the designated community according to the specified transformation rules between the two. One disadvantage is that one may tend to be stuck with obsolete models and users would need to deal with specific domain models separately.

An abstraction based on a widely acceptable common access model will hide most of domain-specific details. By comparing all pairwise mappings, mapping rules between the common access model and domain models are more limited but would be easier to maintain.

Identifying the common access model and building this model on top of a set of mapping rules means that if we can indicate how to implement this for common access, we could immediately leverage rich sets of data access and mapping implementations which already exist, and in the future, we would have the common access capabilities that stand a good chance of coping with historically changing domain models. See Fig. 1
                               for the process of abstracting and mapping disparate models through a single access model. This way, users do not have to deal directly with the underling model mapping.

When domain-specific details are transferred to a different community, the receiver will be not able to interpret the bit streams without proper representation information. As the common model only allows widely acceptable common elements, domain-specific details will be thwarted by the case of one or many to none transformation. Without learning the domain specific knowledge, the receiver gets nothing. To access information behind these discipline-specific details, their representation information must be delivered to the designated communities too.

Normally, specific engines are used to handle the bits together with an appropriate representation information. For example, DRB [31] and Enhanced Ada SubseT (EAST) [32] are used to handle structural representation information. There are many ways to capture representation information, eg. using Resource Description Framework (RDF) [33] and Web Ontology Language (OWL) [34] for semantic representation information or binary executables [35] for software representation information. A designated community may have preferences on how the interpreted information are output, either in binary blobs, structural documents, semantic documents, or others. Then, applications can be built to consume the interpreted outputs, see Fig. 2
                           .

The study case is used to illustrate how to facilitate interoperability and allow long-term access to NetCDF data by using a common OGC model. A common and extensible access layer is demonstrated for the retrieval and navigation of NetCDF archives.

The NetCDF format is an open, self-describing, machine-independent, and array-oriented scientific data standard. The format is widely used in climatology, meteorology and oceanography applications and GIS applications. Mathematically, the NetCDF storage model is a function, denoted as f(x,y,z,…)=value, where x,y,z,… are called dimensions, and the value is called variable. Description of dimensions and values are given by their attributes. Domain specific conventions, such as those in Climate and Forecasting (CF), are defined to provide definitive description of dimensions, variables and attributes.

For example, the E-OBS dataset [36], consists of gridded data for four elements (daily mean, minimum and maximum temperature, and daily precipitation sum). It covers the area of Europe (40W-75E x 25N-75N) from 1950 to today. It has been established for the validation of Regional Climate Models (RCMs) for climate change studies.

All E-OBS data is available on a 0.25° and 0.5° latitude-longitude regular grid, as well as on a 0.22° and 0.44°rotated pole grid. Each of the four elements is published as a single file in NetCDF format. The file declares three dimensions, for time, latitude and longitude, by specifying the number of coordinates on each dimension. Time is of size 'UNLIMIT'. However, not all its users understand the format and know how to use the data, see Fig. 3
                     . Therefore, they have to study the NetCDF format from scratch and it is time consuming to find a proper interpreter. One possible solution would be to deliver the data together with its description and interpreter. Obviously, the description should be detailed enough to be able to deliver sufficient information to users.

A coverage, which is a special type of Geographical Feature [19], is formally defined as a function f: D→R for digital representation of a spatial-temporal phenomenon. D, the coverage’s domain, represents the spatial, temporal or spatiotemporal extent represented by a set of one or more geometric primitives, such as points, curves, surfaces or solids. R, the coverage’s range, specifies the values which can be associated with coverage locations, see Fig. 4
                        . The abstract model is partially implemented by structural XML in GML [37] and further amended in GML 3.2.1 Application Schema for Web Coverage Service (WCS) 2.0 [38] by adding range types for the definition of range set values and metadata for further domain specific knowledge description. The scalable solution allows domain specific digital packages to be bundled into the model. For example, the WCS Earth Observation Application Profile (EO-WCS) candidate standard integrates the Earth Observation model [39] into the coverage to describe Earth Observation products. Furthermore, the solution supports the coverage encoding beyond conceptual GML, a series of data formats can be used to encode a coverage provided that proper mapping models are implemented, such as GeoTIFF [40], NetCDF [22], or HDF-EOS [41].

The GMLCOV [38] implements these via multipart MIME encoding, where the first component consists of the coverage description, such as domain extent, range type, metadata, etc., and the second part is the range set "payload" using the original data formats. Mapping rules are proposed to ensure that coherent information can be derived. For example, the mapping rules between CF-netCDF dataset and GMLCOV:Coverage is defined in [22] by considering the mapping between the GMLCOV:Coverage object and property names to the corresponding CF-netCDF dataset object and property names, ISO 19123 [19] class names and their attributes are provided as bridging concepts for both technical-oriented encodings. Obviously, these conceptual mappings are essential to derive geo information from the original bit stream and should be archived to ensure that the same information can be derived in long term.

One of the most frequently used coverage types is surely the grid, which can encompass several kinds of geometries from the single regular raster to a spatiotemporal collection of irregularly gridded cubes, passing through different tiers of (ir)regularity.

Such coverages can thus show irregular layouts, but still they must retain an inherent grid structure, see Fig. 5
                        . As shown in the coverage Entity-Relationship schema shown in Fig. 6
                        , this could translate into separate management of curvilinear and rectilinear axes: there is indeed a certain degree of implicit topology when the dimensions of a grid are coerced to follow a fixed straight direction; this cannot hold for curvilinear patterns instead, which inevitably require an explicit mapping of the geometry on a point-by-point basis [42].

A first relevant consideration is that a grid coverage is generally associated with an external Coordinate Reference System (CRS) that defines the indexes of the inherent grid in terms of spatio(-temporal) coordinates. Depending on whether the transformation between the different domains is affine or not, the coverage is said to be rectified or referenceable, respectively [19]. A CRS can either be purely spatial, defining a certain geodetic projection for example, or it can be augmented with temporal reference to enable time series of data into a unique object. The dimensionality of the CRS can also be greater than the cardinality of the axes in a grid: just think of an oblique plane in a 3D space. Except for grids with a completely unpredictable geometry, the usual way to define their geometry is by means of origin, offset vectors and coefficients 
                        [42].

The Data Request Broker [6] - DRB API® is an Open Source Java application programming interface for reading, writing and processing heterogeneous data via DRB description, which is a modeling language for describing general text and binary data. It helps accessing bit stream independently from the way data are encoded within files. The language is descriptive and provides a way of describing many different kinds of data formats. The advantage is that it allows direct data interchange among multiple programs. The DRB description as stated in [35] is reused to describe the sample NetCDF data. For example, the following text data stream describes the dimension length information of the data:
                           
                              
                           
                        
                     

Here the dim_length element, is binary and restricted as a non-negative integer. The logical model for this data is expressed by dedicated markups in Structured Data File (SDF) embedded inside the structured hierarchy of XML schema model.

Then, the tree-style description can be further processed to produce meaningful information. For example, to derive GridCoverage’s upper corners, which is a list of grid indexes of array high corner one could be tagged by XPath like the following: string-join($root//header//dim//dim_length,' '). In this way, meaningful information can be derived accordingly.

There are several options that can be used to deliver information contained in NetCDF data. Either approach suffices if it is in the user’s knowledge base. Beyond pure NetCDF or NetCDF in Coverage, there are approaches that help to access the geo information. The integration of the structural representation information for its bit stream interpretation is one of the options. GML coverage supports a standard way to integrate such representation information as its metadata. For example, the client may prefer an XML encoding. In this case, NetCDF together with DRB description in this case is enough to provide such a result:
                           
                              
                           
                        
                     

The pure XML encoding may be too abstract and the client prefers to read the data in a table. The XML result is needed to be further processed to derive the tabular-style result. A FLWOR Expression in XQuery is capable to process the XML in to a tabular format, such as a CSV. In this case, a transformation is needed:
                           
                              
                           
                        
                     

However, the client does not know how to process this representation information without proper behavior definitions. A proper actor is needed for the behavior.


                        
                           
                              
                           
                        
                     

The additional semantic annotations provide additional information about the coverage, by attaching definitions, attributes, comments, descriptions, etc. In the context of OGC standards, the recommendation is to link the data to conceptualized domain knowledge [47]. GMLCOV is open to such links, e.g. by using OGC Sensor Web Enablement (SWE) standards [56]. This supports the data to be geospatially enabled in Linked Data [49] Cloud via URIs [50].

A core and extensions mechanism [46] is proposed to enhance interoperability and compatibility between complex spatial information and services by OGC. The core specifies the basic requirements while extensions define additional requirements which are coherent with the core. OGC WCS 2.0 [45] is the standard that follows such a mechanism. The WCS Core establishes basic data/metadata retrieval based on a unified coverage model. A set of extensions are set up to define additional functionality which expand server capabilities to meet specific requirements. Hence, further domain-specific metadata, semantics and ontologies can be integrated to address the designated communities.

However, the additional information to be integrated for the deliverables vary between from user communities and also evolves over time. In the Earth Observation community, EO-WCS [53] services delivers its coverage by combing EO metadata [39] according to ISO EO model [54]. To support long-term access to historical archives, Preserved Data Set Content (PDSC) [51] is proposed to define the data, associated knowledge and information to be preserved during all phases of an Earth Observation mission.

Archives should integrate adequate associated knowledge and information in what it delivers to its users to ensure that they can use and understand the data.

Coherent semantic information is not the only challenge [52] that needs to be addressed, managing provenance, authenticity, persistent identifiers etc. of the archives are also key research topics. Furthermore, how to logically connect these pieces of information is yet another issue to be considered. One standard alone cannot address all the issues. Logical dependencies have been already be used to trace the relationships among standards, e.g. by ISO and OGC, which are the main driving forces of the development and maintenance of the geographic information standards. The logical and/or dependencies on different granularities [48], such as terms, scopes, requirements, documents, versions and etc. are also being addressed in current research.

A similar approach is proposed in the SCIDIP-ES project [55] to address the representation information required for long-term digital preservation. Therefore, the bridging of interoperability concepts, from the perspectives of multi-disciplinary research and of long term knowledge preservation, by proper modularization and logical connection as shown in our case study, are feasible and highly recommended.

Earth science archives are the property of all mankind. It is not possible to derive a single model which is generic enough to support all user communities and dedicated efforts are needed to make the contents of these archives accessible and usable to all communities, not just now, but in the long term. By means of standardization, communities at large can be connected and provided with the most comprehensive agreed solutions.

We can attempt to bind two standardization approaches together, one is the standardization across multiple disciplines and the other is the standardization of long term preservation of Earth Science archives. The approaches naturally overlap each other and many other areas. Considerable efforts will be needed to disentangle these overlaps.

Based on existing and impending standardization approaches and outcomes, our research investigates the basic building blocks of information science field, and advocates that knowledge exchange will improve community communication by enabling a community to understand and use unfamiliar concepts. Generic, core and discipline-specific models which address community needs are reviewed. A one size fits all generic model is an ideal but impractical solution to all communities; while using core model and superimposing specific models targeting specific user communities address both common and domain requirements.

We proposed different behaviors such as abstraction, transformation or interpretation to address these models. These are also demonstrated within the NetCDF study case, which addresses the deliverable through standardized coverage together with its representation information to support long term preservation and access of Earth Science data. Nevertheless, the approach proposed in this research is not a comprehensive solution as the scenario relies on understanding the knowledge differentiation between the source and target communities. Knowledge base definition of different communities is still not mature, not only for tracking its own knowledge, but also for others to measure the gaps between each other. “Data virtualization is the technology that offers data consumers a unified, abstracted, and encapsulated view for querying and manipulating data stored in a heterogeneous set of data stores” [43]. The concept is relatively new but is becoming mainstream as demonstrated by its role in Bossiness Intelligence systems [59]. We believes that the approach can be fine tuned and complemented with the gap reasoning approaches adopted in Digital Libraries and Earth archive systems to address the goal of seamlessly accessing multidisciplinary archives and ensure that the knowledge curated in Earth Science archives can be preserved for the indefinite future to benefit today and future generations of research scientists.

@&#ACKNOWLEDGEMENTS@&#

The authors acknowledge the help and support from the EU-FP6 project ENSEMBLES (http://ensembles-eu.metoffice.com) which provided the E-OBS dataset and data providers in the ECA&D project (http://www.ecad.eu). The case study was developed as part of the EU-FP7 EarthServer (http://www.earthserver.eu) and Scidip-es (http://www.scidip-es.eu) project. We would also like to thank our colleague, Yannis Tzitzikas, from the Scidip-es project for providing advice on the study case and interoperability issues.

@&#REFERENCES@&#

