@&#MAIN-TITLE@&#Adaptive facial point detection and emotion recognition for a humanoid robot

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a robust landmark detector to deal with pose variation and occlusions.


                        
                        
                           
                           SVRs and NNs are respectively used to estimate intensities of 18 selected AUs.


                        
                        
                           
                           Fuzzy c-means clustering is employed to detect seven basic and compound emotions.


                        
                        
                           
                           Our unsupervised facial point detector outperforms other supervised models.


                        
                        
                           
                           The overall development is integrated with a modern humanoid robot platform.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Facial point detection

Action unit

Pose variation

Occlusion

Emotion recognition

Human robot interaction

@&#ABSTRACT@&#


               
               
                  Automatic perception of facial expressions with scaling differences, pose variations and occlusions would greatly enhance natural human robot interaction. This research proposes unsupervised automatic facial point detection integrated with regression-based intensity estimation for facial action units (AUs) and emotion clustering to deal with such challenges. The proposed facial point detector is able to detect 54 facial points in images of faces with occlusions, pose variations and scaling differences using Gabor filtering, BRISK (Binary Robust Invariant Scalable Keypoints), an Iterative Closest Point (ICP) algorithm and fuzzy c-means (FCM) clustering. Especially, in order to effectively deal with images with occlusions, ICP is first applied to generate neutral landmarks for the occluded facial elements. Then FCM is used to further reason the shape of the occluded facial region by taking the prior knowledge of the non-occluded facial elements into account. Post landmark correlation processing is subsequently applied to derive the best fitting geometry for the occluded facial element to further adjust the neutral landmarks generated by ICP and reconstruct the occluded facial region. We then conduct AU intensity estimation respectively using support vector regression and neural networks for 18 selected AUs. FCM is also subsequently employed to recognize seven basic emotions as well as neutral expressions. It also shows great potential to deal with compound and newly arrived novel emotion class detection. The overall system is integrated with a humanoid robot and enables it to deal with challenging real-life facial emotion recognition tasks.
               
            

@&#INTRODUCTION@&#

In order to build robots that interact in a more human-like and intuitive manner, perception of human emotions is essential [1–3]. Automatic face and expression recognition has greatly benefited such multimodal agent-based interface development. However, detecting emotions from spontaneous facial expressions during real-life human robot interaction could still be challenging because of various pose and subject variations, illumination changes, occlusions and background clutter. Especially, for automatic face analysis, the original vision APIs provided by the robot’s SDK employed in this research were not capable of dealing with such challenging facial emotion recognition tasks.

Optimal, robust and accurate automatic face analysis is thus important to deal with such challenging real-life applications since the performance of advanced applications such as facial action and emotion recognition relies heavily on it. Many parametric and mode specific feature extraction approaches in the computer vision field have been proposed to estimate head pose and detect facial landmarks from real-life images to benefit subsequent automatic facial behavior perception to address the above issues. However, many of the above applications found it difficult to balance well between high quality feature extraction and low computational requirement, which is essential in real-time applications.

For example, although feature extraction techniques, Active Appearance Models (AAM) and Constrained Local Models (CLM), have been widely used for medical imaging and computer vision research, they rely heavily on intensive training to adapt to diverse pose variations and head movements [4]. Although various approaches have been used to improve their robustness and efficiency, they usually require intensive computational convergence time. Thus, other computational-wise optimal unsupervised methods for automatic face analysis are required in order to deal with challenging real-life (spontaneous) facial emotion recognition tasks.

This research is thus motivated to develop a facial emotion recognition system for a humanoid robot to deal with emotion detection from images with pose variations, illumination changes, occlusions and background noise. A cost-effective optimal unsupervised learning scheme for facial point detection is proposed as the first step of this research and implemented incorporating a 2D Gabor filter, a novel feature descriptor, BRISK (Binary Robust Invariant Scalable Keypoints), an Iterative Closest Point (ICP) algorithm and fuzzy c-means (FCM) clustering. In order to deal with images with occlusions effectively, the proposed facial point detector applies ICP to first recover neutral landmark points for an occluded facial region. Then FCM is applied to further inference the shape of the occluded element by taking the prior knowledge of the attributes of the non-occluded facial regions into account. The following post processing is subsequently used to further re-construct the occluded facial region. After we have applied FCM to obtain the shape cluster of the occluded facial element, we select the top five image outputs with the highest correlations to the test image in the cluster and average them to re-construct the best fitting geometry for the occluded facial element. The re-constructed set of landmarks with shape information embedded for the occluded facial region is then used to adjust the neutral landmarks generated by ICP. This background robust facial feature point detector is able to detect 54 facial points for images or real subjects from challenging real-time human robot interaction with great efficiency to fulfil the robot’s computational constraints.

In comparison to several typical supervised facial point detection models, e.g. AAM, CLM and a recent state-of-the-art face alignment method, Gauss–Newton Deformable Part Models (GN-DPMs) [5], our proposed unsupervised model is with optimal computational cost and great efficiency for landmark detection to deal with real-life applications with pose variations, scaling differences, and occlusions. It also allows us to go beyond the constraints of individual databases to perform high quality feature extraction. Evaluated with five well-known image databases, our proposed facial point detector outperforms AAM and CLM greatly in terms of landmarks detection accuracy and computational efficiency and also achieves comparable performance in comparison to the state-of-the-art GN-DPM model. It has also laid a solid foundation for subsequent automatic facial behavior perception.

Moreover, Facial Action Coding System (FACS) has been used as an intermediate channel to bridge raw motion-based facial representations with emotional facial behavior recognition in many applications [6]. It provides an objective approach to describe the truth of human behavior and is closely related to physical indicators of emotional facial expressions. It employed 32 action units (AUs), which represent the muscular activities to describe and score facial expressions. It also provides a versatile method to describe a wide range of facial behaviors, e.g. facial punctuators in conversation and emotional facial expressions [7,8]. FACS is also capable of describing emotion intensities and compound emotions, and distinguishing fake from real emotional expressions. Thus many computational facial emotion recognition studies employed FACS and AUs [6].

According to FACS, the intensity of an AU can be scored on a five-point ordinal level from A to E. Level A refers to a trace of an action. Level B indicates slight evidence. Level C describes pronounced or marked evidence. Level D represents severe or extreme actions with Level E indicating maximum evidence. Each intensity level refers to a range of appearance changes. Despite intensive studies of facial AU detection, automatic AU intensity measurement still posed great challenges to automated recognition systems since the differences between some AUs’ intensity levels could be subtle and subjective during emotional or conversational behavior, and the physical cues of one AU might vary greatly when it occurs simultaneously with other AUs. This research also aims to deal with such challenges to measure AU intensities of those AUs closely related to emotional facial expressions.

Furthermore, in cognitive research, perception of facial emotions was regarded to be based on a categorical model [9], which has been intensively employed in the machine learning field. In comparison with the categorical model, neuroscience research suggested that the perception of facial emotions was best to be described as a continuous model [10], where each emotion was described using characteristics common to all emotions in a multidimensional space. Although this model showed advantages in explaining emotion intensities compared to the categorical model, it was still not easy to use it to describe compound emotions. Therefore, Martinez and Du [11] proposed a new theoretical model for the description of multiple compound emotion categories such as happy or angry surprise. Their model aimed to overcome the difficulty that both of the categorical and continuous models encountered. Their proposed method was to define N distinct continuous spaces and linearly combine these several spaces to recognize compound emotion categories for facial expressions. This new theoretical model pointed out directions for building new computational models for the recognition of compound facial behavior.

This research is also motivated by the above theoretical research on compound emotion modeling and anatomical knowledge of FACS. In order to incorporate anatomical truth of emotional facial behaviors into our research, intensities of 18 selected facial AUs closely associated with emotional expressions are respectively estimated using support vector regression (SVR) and neural networks (NNs) based on the above derived 54 facial points. FCM clustering algorithm is also subsequently used to recognize seven basic emotions and neutral expressions based on the derived intensities of the 18 AUs. The seven basic emotions detected include happiness, anger, sadness, disgust, surprise, fear and contempt. This FCM-based emotion clustering technique also shows great potential to deal with compound and newly arrived unseen novel emotion classification. In comparison to basic rigid clustering algorithms (e.g. partitioning, hierarchical and density-based methods), which force an object to belong to only one cluster, FCM clustering offers the possibilities to cluster an object into multiple clusters. That is, it calculates the degree of membership (i.e. the likelihood) for a test data point belonging to each fuzzy emotion cluster. Therefore this probability based clustering is able to allow us to go beyond rigid classic clustering to recognize a compound emotion, which may belong to more than one emotion cluster.

The overall system has been integrated with the C++ SDK of the latest NextGen H25 NAO robot in order to benefit real-life human robot interaction. The employed NAO robot has a powerful CPU processor and cameras sensors to allow for real-time image processing and better low light perception. Computer vision packages have been provided for the robot to perform basic object and face detection. Especially, the vision API of NAO also allows it to perform basic face recognition by extracting 31 two-dimensional geometric points automatically. But experiments indicated these facial landmark points proved to be sometimes not sufficient enough to capture physical cues of subtle facial behaviors [12]. Especially, they suffered from poor performances when dealing with facial emotion recognition tasks from real-life images with pose variations, illumination changes and occlusions.

Therefore, this research has further extended the NAO’s vision APIs and extends landmark generation from 31 to 54 two-dimensional points. It also allows NAO to deal with challenging facial point detection and emotion recognition tasks with large pose variations (at least up to 60 deg), illumination changes, scaling differences and the presence of occlusions and background clutter from dynamic real-life images and interactions. The overall system architecture with facial point detection for non-occluded images is provided in Fig. 1
                      whereas the system architecture with facial point detection for images with occlusions is presented in Fig. 2
                     .

Overall, the paper is organized in the following way. Section 2 reviews state-of-the-art research on automatic facial point detection, AU intensity estimation and emotion recognition. The overall system architecture is presented in Section 3. The proposed unsupervised facial point detection is discussed in Section 4. The regression and NN based AU intensity estimation and fuzzy clustering based emotion recognition are respectively discussed in Section 5. Evaluations conducted using several well-known image databases for diverse challenging real-life cases and related discussions are provided in Section 6. We draw conclusion and identify future work in Section 7.

@&#RELATED WORK@&#

In this section, state-of-the-art research on automatic facial feature extraction, facial emotion recognition and AU intensity estimation is explored.

A growing amount of computer vision research has conducted automatic face analysis since it is an important step in subsequent sophisticated applications such as face recognition, emotion recognition and gaze detection. In this section, we review several key techniques and applications with impressive performances for facial point detection. For instance, Vukadinovic and Pantic [13] used Gabor feature based boosted classifiers for the detection of 20 facial points. Their system first divided a detected facial region into 20 regions of interest and applied GentleBoost templates integrating gray level intensities and Gabor wavelet features for facial point detection. However, their work had not been developed to deal with various head rotations and occlusions. Senechal et al. [14] presented a facial feature point detection system to automatically track 18 facial landmarks (i.e. four per eye, three per brow and four for the mouth). The tracking system was developed based on improved multi-kernel learning. It combined facial feature points matching between consecutive images with a prior knowledge of facial landmarks. This point matching procedure employed both static (i.e. patches) and dynamic features (i.e. the correlations between the current pixel patch of region of interest (ROI) and those extracted from the previous image sequence) as inputs to a multi-kernel support vector machine (SVM). The output of SVM gave a confidence index of being the searched landmark or not for each candidate pixel. Their system proved efficient enough to deal with real-time applications.

Facial shape and appearance modeling techniques have also been intensively studied. These include supervised models such as the Active Appearance Model, Active Shape Model, Constrained Local Model and regression-based algorithms. They have been widely used for geometric and texture facial feature extraction. We discuss these models and their related applications in the following.

AAM was initially proposed by Cootes et al. [15]. It is able to generate both shape and texture representations of deformable objects and proved to be very effective and flexible for objects tracking in many applications [15]. According to Matthews and Baker [16], there are two types of linear AAMs: independent and combined AAMs. Independent AAMs perform linear modeling of shape and appearance of deformable objects separately, while combined AAMs employ a single set of linear parameters to describe shape and appearance. The training of an AAM requires a set of images and coordinates of landmarks associated with these images as inputs.

The task of AAM fitting is to search for the set of shape and appearance parameters which offer the best fitting between the trained model and the given input image. Many model fitting algorithms and strategies have been proposed in the past to reduce the error between the given input image and the calculated model. One comparatively more effective and computational affordable approach is to update the model with an incremental warp using the inverse compositional algorithm. This algorithm has been regarded as the most effective warp compositional algorithm for AAM fitting [16]. However, research showed that the trained texture models of AAM were often not robust enough to reconstruct generic faces. Also, for real-time fitting, such reconstructions errors dominated the alignment errors, which tended to lead to poor performances [4]. There were also significant efforts made to further improve the robustness, efficiency and discriminative abilities of AAM. For example, some research has made explorations to build AAM in a 3-D space in order to model scaling, translations and rotations more effectively [17]. Sung and Kim [18] also proposed a background robust AAM with the assistance of Active Contour Model. In their approach, the active contour technique was used to detect the face boundary (the object foreground) from a cluttered image background first. AAM fitting was then applied to the selected foreground of the object. Since the discrimination of AAM depends heavily on the accuracy of the fitting and effective AAM-based face tracking also has great potential to deal with head movement and pose variations, adaptive fitting of AAM has been explored in the work of Gross et al. [19]. Their proposed new fitting model was robust enough to deal with image occlusions.

CLMs were proposed by Cristinacce and Cootes [20]. This model employs a class of methods to locate sets of points on a target image, which is constrained by a statistical shape model. Although the fitting accuracy still needed improvements, compared to AAM, CLM offers better real-time efficiency and robustness. Various strategies and optimization methods were also employed to improve the computational efficiency and accuracy for CLM. For example, Asthana et al. [21] proposed a robust discriminative response map fitting method for face fitting. It integrated a discriminative regression based approach with CLM. Comparing to AAM, their approach was able to use an efficient small set of parameters to record and reconstruct response maps. Their method also proved to be able to perform real-time facial point detection efficiently. Experiments conducted with several facial image databases proved the robustness and efficiency of the proposed method for generic face fitting tasks. Gauss–Newton Deformable Part Models, i.e. GN-DPMs, for face alignment have been proposed by Tzimiropoulos and Pantic [5]. Their model employed Gauss–Newton optimization to minimize a joint cost function of shape and appearance to generate a joint translational motion model. It jointly optimized a part-based linear generative appearance model and a global shape model to overcome the limitations of the traditional DPMs which were based on independent fixed part templates and tended to show detection ambiguity. Their model achieved computational reduction in the training stage with the proposed cost function further evaluated during fitting. Their experiments indicated that GN-DPMs outperformed other models proposed in prior work.

There are also other regression-based methods for facial feature detection. Martinez et al. [22] developed a novel algorithm of face fitting and facial point detection for frontal and near-frontal images. The algorithm combined a low computation cost regression-based approach with the robustness of exhaustive search based face shape model. It relied on the regression method to conduct a quality measure of each prediction and employed the exhaustive-search based shape model to provide correction to the sampling region.

Head rotation and pose variation estimation is a challenging topic and essential in order to achieve advanced performance for facial feature extraction and emotion recognition. Zhang et al. [23] proposed a sparse representation based latent space model to deal with face recognition tasks with large pose variations. Their method employed the sparsity property of a face image over its corresponding view-dictionary to deal with pose difference between gallery and probe faces. Orozco et al. [24] produced a pose-robust online appearance based facial feature tracker for the simultaneous tracking of head pose, lips, eyebrows, eyelid and irises from video sequences. This tracking model was able to track both face pose and eye gaze simultaneously in comparison to other tracking techniques. It also integrated with a dynamic learning mechanism to deal with appearance changes of the tracked object without the requirement of prior training. A very robust facial analysis framework was proposed in their work by incorporating three of such facial feature trackers and the application of a Levenberg–Marquardt optimization algorithm to deal with pose variations and occlusions.

Furthermore, since facial expressions relate to upper, middle, and lower regions of the face with similar facial muscle contractions in each region, an occluded facial component (e.g. eyes or mouth) of a facial expression could be predicted by using the statistical dependency among features from paired facial components on non-occluded regions. In Lin et al. [25] and Wu et al. [26], an Error Weighted Cross-Correlation Model (EWCCM) was proposed to generate a set of Gaussian Mixture Model (GMM) based Cross-Correlation Model (CCM) predictors to predict AU or AU combination of the occluded facial component based on the facial deformation parameters (FDPs) extracted from the paired facial components of the non-occluded regions. Within the EWCCM model, a Bayesian classifier weighting scheme was employed to integrate all the GMM-based CCM predictors, each for one paired facial component, to give the final decision on the predicted AU or AUs. The EWCCM and Bayesian classifier weighting scheme based prediction was able to achieve at least 80.68% accuracy [25] and to effectively reconstruct the facial geometric features of the occluded region in facial emotion expressions [26].

In this section, we discuss AU and emotion recognition techniques and well-known developments in recent years. Shan et al. [27] focused on deriving facial feature representations using local binary pattern (LBP) and examined several LBP-based methods, including template matching, SVM, linear discriminant analysis etc., for person-independent facial expression recognition. Their study indicated that in comparison to Gabor wavelets, LBP features could be derived very fast in a single scan through raw images but were able to store sufficient discriminative facial information in a compact representation. The work also further learned the most discriminative LBP features with AdaBoost. The recognition performances of different classifiers proved to be improved by using the Boosted-LBP features. The generalization ability of LBP features across different databases was also evaluated. It concluded that the best recognition performance was obtained by using SVM with Boosted-LBP features, although it showed limitations on generalization to other datasets. Their work proved to be effective for facial emotion recognition for those low-resolution images and compressed video sequences captured in real-world environments. Tsalakanidou and Malassiotis [28] also proposed a real-time 2D+3D facial feature tracker based on Active Shape Model (ASM). The model used local appearance and surface geometry information to extract 81 facial points. Special trackers were also produced to further analyze the texture of the mouth and eyebrows. A rule-based approach was developed to detect four facial expressions and 11 AUs using the derived geometric and textural features. Zheng et al. [29] also made attempts to improve the original ASM on several aspects including extending the profile of the original ASM from 1D to 2D in order to improve its fitting accuracy and efficiency in real-time applications. However, although the new model outperformed the original ASM for facial feature extraction, both of the ASMs were not able to deal with pose variations.

Although AU intensity estimation has been a challenging task and not very well explored, there is some recent development that has drawn our attention [30–32]. Kaltwang et al. [30] proposed a method of automatic continuous pain intensity estimation from facial images to benefit health care applications. A set of relevance vector regressions (RVRs) for continuous pain intensity estimation was trained with diverse geometric and texture feature sets. Evaluation using the recently published UNBC-MacMaster Shoulder Pain Expression Archive Database showed that the proposed feature-fusion scheme outperformed those separately trained RVRs with different feature sets. That is, the RVR with the combined texture features obtained via LBP and discrete cosine transform achieved the best performance. Li et al. [32] proposed a dynamic Bayesian network to model dependence among spontaneous facial AUs and other related temporal behavior for AU intensity measurement. Their work outperformed other image-driven AU estimation methods.

Facial emotion recognition from 3D inputs and video sequences has also been explored in the field. Li et al. [33] presented a probabilistic neutral network based 3D facial expression recognition system, which took not only localized facial feature points but also derived geometric features, such as slopes and angles, into account for facial emotion recognition. Majumder et al. [34] developed a facial emotion recognition system based on geometric features using an extended Kohonen self-organizing map for the recognition of six basic emotions. The system automatically generated a geometric feature-based 26-dimension input vector with the consideration of landmarks for eyes, lips and eyebrows. Their proposed approach outperformed other popular classification schemes such as radial basis function (RBF) network, multi-layered perceptron and multi-class SVM. Fang et al. [35] proposed a novel scheme for automatic dynamic salient information (such as peak expressions) extraction and facial expression analysis from video sequences. The proposed facial emotion recognition system outperformed static emotion recognition systems. Instead of depending on AU detection, their system studied a dynamically extracted feature space of over 300 dimensions for emotion recognition. Six state-of-the-art machine learning techniques were also used for system experiments and evaluation to prove the system’s efficiency and robustness. They have also conducted user studies to investigate the correlation between human perception and their system’s outputs.

Various attempts have been made to develop pose and illumination invariant emotion recognition classifiers. For example, Moore and Bowden [36] proposed a multi-class SVM based approach for facial emotion and pose classification. Their approach investigated variations of LBP-based descriptors and the influence of pose on diverse expression recognition tasks. Local Gabor binary patterns was especially evaluated in their work using images with different viewing angles. Chen et al. [37] also proposed a hybrid-boost learning mechanism to deal with face detection and emotion recognition against pose variations. Skin color detection and segmentation algorithms were used to first locate face region. Subsequently both weak and strong hybrid classifiers were used for face detection and emotion classification against scale and pose variations, and partial occlusions. Especially, they employed local Harr-like features with global Gabor attributes for the selection of the weak hybrid classifiers. Yu et al. [38] also employed an SVM with active learning to collect facial images with realistic expressions from web sources. The database generated had more diversity with collected images much closer to real-world conditions in comparison to other well-known (e.g. Cohn–Kanade) databases. A novel feature descriptor was also employed in their research. Their work showed great potential to advance research on robust facial expression recognition. Moreover, as mentioned earlier, in cognitive research, Martinez and Du [11] proposed a new theoretical model for the description of multiple compound emotion categories such as happy surprise in order to overcome the difficulty that traditional emotion models encountered. This new theoretical model pointed out directions for building new computational models for compound emotional facial behavior recognition. It also motivates us to employ FCM clustering to enable a test facial expression instance to belong to more than one emotion cluster and thus recognize compound emotions.

Motivated by the above related research, we aim to propose a robust facial emotion recognition system to deal with challenges encountered during real-life applications. We employ Gabor filtering, a novel feature descriptor, BRISK, the ICP algorithm and FCM with post correlation processing for effective facial point detection from images with pose and scale variations, illumination changes, occlusion and background clutter to benefit real-life human robot interaction. We also use regression-based AU intensity estimation and probability-based fuzzy clustering to respectively measure the strength of 18 AUs and recognize seven basic emotions, neutral expressions and compound emotions. The system has also upgraded the vision APIs of the NAO robot to enable it to deal with challenging real-life applications. We present this intelligent facial emotion recognition system in detail in the following.

This research is dedicated to a humanoid robot, NAO NextGen H25. It provides C++ SDKs to allow researchers to develop advanced intelligent components for robot vision, speech and motion processing. The robot has two built-in cameras with one located on its forehead and the other located at the mouth level. These are 920p cameras and able to run at 30 images/s for (up to) 1280x720 images. NAO is able to move its head by 239 horizontally and by 68 vertically, and its camera can see at 61 horizontally and 47 vertically. Therefore it has a great vision of its environment. The robot platform also provides vision APIs for image processing, movement detection and background darkness checking. In our previous research [12], the robot was developed to detect emotional facial behaviors only from the frontal views of users’ posed facial expressions with the 31 facial points automatically generated by the robot’s vision API. In this research, we aim to further extend NAO’s vision functions to enable it to capture more discriminative physical cues by generating a more sufficient number of landmarks against illumination changes, pose variations, occlusions, background clutter and scaling differences and recognize posed and spontaneous emotional facial expressions.

In this section, we discuss core functions and data flow of our intelligent facial emotion recognition system including face and ROI detection, feature point detection, AU intensity estimation and emotion clustering. In this research, first of all, we employ an improved version of Viola and Jones [39] algorithm for face detection. This algorithm enables us to effectively deal with face detection tasks with background noise. In order to identify regions of interest effectively, three cascade classifiers are used. The detected ROIs in this research include positions of both eyes along with eyebrows, the nose tip and mouth. The following methods and techniques are then employed for the subsequent automatic facial point detection. In order to reduce the noise embedded in the image, a bilateral filter is applied. Then a 2D Gabor filter is used to extract the contour of each of ROIs including eye shapes, nose tip and mouth outlines. We derive a set of 16 facial points after the application of bilateral and Gabor filtering, but without any landmarks generated for eyebrows at that stage. In order to derive more essential facial points for each facial element and further increase the system’s flexibility, we employ a robust fast performance feature descriptor, BRISK. It is a scale and rotation invariant descriptor and detector, and is able to deal with image keypoint detection without sufficient prior knowledge on the scene and camera poses. BRISK enables us to extend keypoint detection for each facial element (including eyebrows). Its outputs integrated with the edge detection results from the Gabor filtering enable the retrieval of 21 facial points. Importantly, the BRISK package also enables us to deal with feature detection from images with head rotations, pose variations and scaling differences.

In order to capture more detailed physical cues for AU intensity estimation and emotion recognition, the above combined 21 facial points from BRISK and Gabor filtering have also been regarded as a set of reference points. A set of 68 landmarks of any neutral image is also borrowed from the extended Cohn–Kanade (CK+) Facial Expression Database [40,41] and used as a source point cloud. Then the ICP algorithm is applied with both sets of landmarks as inputs. It transforms the source neutral 68 landmarks to best match the reference 21 facial points. Thus it allows us to reconstruct 2D surfaces and extend the detected facial points to 54 (with the potential to grow to 68). Since ICP is only able to reconstruct a neutral occluded facial element, in order to further improve the landmark detection accuracy, FCM is then applied with the prior knowledge of the attributes of the non-occluded facial regions as inputs to further reason the shape of the occluded facial element. That is, FCM groups the test instance to a shape cluster. Post landmark correlation processing is subsequently applied to derive the best fitting geometry for the occluded facial element by averaging highly correlated images to the test image in the same shape cluster. Empirical results indicate that FCM and related post processing enable more accurate landmark generation for occluded emotional facial components. The above proposed facial point detector has been tested using images with diverse facial expressions extracted from the databases of CK+, Labeled Faces in the Wild (LFW) [42], PUT [43], Labeled Face Parts in the Wild (LFPW) [64] and Helen [65] to prove its robustness and efficiency. Experiments indicate that it is able to robustly detect facial landmark points from posed and spontaneous facial expressions with illumination changes, pose variations, scaling differences, occlusions and background noise. It also lays a solid foundation for AU intensity estimation and emotion recognition from such challenging diverse facial expressions.

Inspired by anatomical knowledge of FACS, in this research, the above derived 54 facial points are then used to estimate intensities of the 18 selected AUs closely associated with the expression of the seven basic emotions. We employ 18 SVRs and 18 NNs respectively for AU intensity estimation with one SVR or NN dedicated to intensity regression for one facial action. Subsequently, FCM clustering is employed in this research for emotion recognition since it allows clustering one facial expression data object into more than one fuzzy emotion clusters to recognize basic and compound emotions. It produces a degree of membership for each emotion cluster to indicate how close a test facial expression instance is to the center of each emotion vector. Images from CK+ have also been used to evaluate this emotion clustering technique for the recognition of the seven basic emotions and neutral expressions. Initial experiments have also been conducted to indicate its effectiveness for compound emotion recognition. The overall system is integrated with the robot’s latest Naoqi 1.14.5 C++ platform under Ubuntu. The detailed dataflow of the system is also provided in Algorithm 1
                     .

As discussed earlier, supervised facial feature detection based on AAM, ASM or CLM relies heavily on the training data. Such trained models sometimes do not have very good compatibility across different databases [4]. For instance, research indicated that a supervised AAM model trained with frontal-view images from CK+ tends to have very moderate performance for facial feature extraction from non-frontal or multi-view of images in other databases (e.g. PUT and LFW). Especially its performances deteriorate fast when dealing with large pose variations, partial occlusions, scaling differences and background clutter [4]. Moreover, other research also indicated that these mode specific models tend to require comparatively intensive computational power to perform real-time face fitting, although CLM was more designed for real-time applications [4,20]. Therefore, in order to overcome the above difficulties, we implement an unsupervised robust landmark detector.

Comparing to the above supervised facial feature detection models, this unsupervised facial point detector is more flexible to deal with diverse landmark detection tasks against pose variations, illumination changes, occlusions and background clutter with less computational cost. Evaluation also indicates its robustness across the testing of multiple databases (e.g. CK+, LFW, PUT, LFPW and Helen). This proposed facial point detector incorporates several advanced feature extraction algorithms and balances well between high quality feature extraction and low computational requirement. It also offers efficient optimal performance to deal with real-time facial point detection. Algorithm 2
                      demonstrates the details of this proposed unsupervised keypoint detector.

As mentioned earlier, we use an improved version of the Viola and Jones’ algorithm [39] in this research to locate the face region in a test input image. We first conduct some pre-processing of input images before applying the face detection algorithm. That is, we convert an input image into gray scale. A histogram equalization method is then applied to improve the contrast of the converted image. We then detect the face region in the test image using the improved face detection algorithm. Adaboost was initially used by Viola and Jones [39] for original face detection algorithm development. Lienhart and Maydt [44] then further improved it by replacing Adaboost with Gentleboost in order to achieve computational efficiency in real-time applications. This improved face detection algorithm is thus employed in our previous [45] and this research to locate faces in real-time applications.

As an important step in automatic facial feature extraction, ROI detection is subsequently performed. In this research, we employ three cascade classifiers, i.e. the right and left eye cascade classifiers and a lower facial region cascade classifier, to detect ROIs. These three employed cascade classifiers are borrowed from OpenCV [46,47]. According to [46,47], these classifiers were respectively trained with a large amount of positive and negative images in order to detect each ROI efficiently.

The ROIs are detected in the following way. First of all, an equalized histogram is employed in order to increase the contrast of the detected face region by the face detector to gain more visibility of the areas of the eyes, nose and mouth. The detected face region is then divided into three sections in order to retrieve ROIs with high accuracy. These three divided facial sections include areas of the right and left upper faces and a lower face region (see the left diagram in Fig. 3). Then the three cascade classifiers for ROI detection are respectively applied on these three facial parts. The ROIs recovered include the positions of both eyes along with eyebrows and locations of the nose and the mouth (see the right diagram in Fig. 3). This ROI detection is also proved to be efficient which achieves 100% accuracy for a test set of 1000 images selected from CK+. It also proved to be able to detect ROIs successfully from real-time video inputs with up to 60-deg rotations.

We also employ the following strategy for occlusion detection. For example, for a test facial image where the mouth area is occluded, first we detect the face using the Viola and Jones face detector and after detecting the face, we apply Haar cascades to detect each ROI. As discussed above, we employ three cascade classifiers respectively for ROI detection of both eyes along with eyebrows and a lower facial region. For a test image with the mouth area occluded, while detecting the ROI, the occluded mouth area is not detected by the Haar cascade detectors. That is, if an ROI is not detected, then it indicates that the image is occluded for that region. Therefore image occlusion is detected. Further detailed discussion on the reconstruction of the occluded facial regions is provided in Section 4.4. Also the subsequent Gabor filter and BRISK-based analysis will not be applied to the occluded facial region but only applied to the non-occluded facial elements to identify key facial points.
                     

The detected ROIs are then further processed to recover their corresponding borders and contours by using bilateral and Gabor filtering. First, a bilateral filter is employed to reduce the noise of each ROI. In comparison to other filtering algorithms (such as a normalized box filter and a median filter), bilateral filtering is not only a non-linear and noise-reducing smoothing process for images, but also edge-preserving [48,49]. It replaces the intensity value of each pixel with a weighted average of intensity values from nearby pixels to reduce the noise and preserve sharp edges. Bilateral filtering has been applied individually on each ROI. This process helps us to reduce noise in the input image and increase the accuracy of the subsequent Gabor filtering based edge detection for each ROI.

We subsequently employ Gabor filtering in this research to detect contour of each ROI. A Gabor filter is a linear filter used in the computer vision field for edge detection. Gabor filters are rotation sensitive local frequency detectors. They employ optimal localization properties in both spatial and frequency domains, which make them suitable for texture segmentation analysis. Gabor filters with different frequencies and orientations can form a filter bank, which is proved to be effective for feature extraction from images [50,51]. In this research, we employ a 2D Gabor filter to detect the edge of each ROI. Previous research [52] also indicated that Gabor filtering based feature detection performed better than principal component analysis, local feature analysis and Fisher’s linear discriminant. Most importantly, Gabor filters are able to remove the light and contrast variations in images while preserving their robustness. The 2D Gabor filter employed in this research is defined by the following equations. These include simple definitions for the real and imaginary Gabor kernel generation without DC compensation.

Real:

                           
                              (1)
                              
                                 
                                    g
                                    
                                       (
                                       x
                                       ,
                                       y
                                       ;
                                       λ
                                       ,
                                       θ
                                       ,
                                       ψ
                                       ,
                                       σ
                                       ,
                                       γ
                                       )
                                    
                                    =
                                    
                                       e
                                       
                                          (
                                          −
                                          
                                             
                                                
                                                   x
                                                   
                                                      ′
                                                      2
                                                   
                                                
                                                +
                                                
                                                   γ
                                                   2
                                                
                                                
                                                   y
                                                   
                                                      ′
                                                      2
                                                   
                                                
                                             
                                             
                                                2
                                                
                                                   σ
                                                   2
                                                
                                             
                                          
                                          )
                                       
                                    
                                    cos
                                    
                                       (
                                       2
                                       π
                                       
                                          
                                             x
                                             ′
                                          
                                          λ
                                       
                                       +
                                       ψ
                                       )
                                    
                                 
                              
                           
                        Imaginary:

                           
                              (2)
                              
                                 
                                    g
                                    
                                       (
                                       x
                                       ,
                                       y
                                       ;
                                       λ
                                       ,
                                       θ
                                       ,
                                       ψ
                                       ,
                                       σ
                                       ,
                                       γ
                                       )
                                    
                                    =
                                    
                                       e
                                       
                                          (
                                          −
                                          
                                             
                                                
                                                   x
                                                   
                                                      ′
                                                      2
                                                   
                                                
                                                +
                                                
                                                   γ
                                                   2
                                                
                                                
                                                   y
                                                   
                                                      ′
                                                      2
                                                   
                                                
                                             
                                             
                                                2
                                                
                                                   σ
                                                   2
                                                
                                             
                                          
                                          )
                                       
                                    
                                    sin
                                    
                                       (
                                       2
                                       π
                                       
                                          
                                             x
                                             ′
                                          
                                          λ
                                       
                                       +
                                       ψ
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    
                                       x
                                       ′
                                    
                                    =
                                    x
                                    cos
                                    θ
                                    +
                                    y
                                    sin
                                    θ
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    
                                       y
                                       ′
                                    
                                    =
                                    −
                                    x
                                    cos
                                    θ
                                    +
                                    y
                                    sin
                                    θ
                                 
                              
                           
                        where λ is the wave length of the sinusoidal factor with θ as the anti-clock wise rotation of the Gaussian and the plane wave, ψ as the phase offset, σ as the sigma/standard deviation of the Gaussian envelope, and as the spatial aspect ratio [50].

In this research, we apply the Gabor filter on each ROI to detect its edge. Example outputs of the Gabor filter are shown in Fig. 4
                        . These include the detected outlines of the eyes, mouth and nose. These outputs also give the detected important edge regions in white color with the rest in black color pixels. Also, in order to recover four keypoints for each ROI based on these filtering outputs, we connect the biggest and closest blocks of white pixels to form a big block and draw a rectangle around it. We then subsequently place a keypoint respectively on the left and right side of the rectangle and the mid-point of the upper and lower side of the rectangle (see Fig. 4). Thus we recover 16 keypoints for both eyes, nose and mouth with each of these facial elements allocated four facial points. This 2D Gabor filter is also able to deal with head rotations effectively. However, it is not able to recover any keypoints for both eyebrows, which may play important roles in emotional facial expressions (e.g. AU1 and AU2 are present for surprised emotion).

In order to increase the system’s robustness, identify facial features for both eyebrows and further justify the above detected 16 facial points for subsequent facial analysis, we employ a robust novel keypoint descriptor and detector, BRISK. This keypoint detector has further extended the current detected facial landmarks and is also robust enough to deal with scaling differences, pose variations and head rotations. We discuss this keypoint detector in the following.

There are several feature point detector and/or descriptor packages well-known in the computer vision field for automatic feature extraction. We have conducted investigations on some key techniques and packages in order to find the most suitable facial feature detector for this research. The detectors and descriptors we explored include SURF (Speeded Up Robust Features) [53], BRIEF (Binary Robust Independent Elementary Features) [54], FREAK (Fast Retina Keypoint) [55] and BRISK (Binary Robust Invariant Scalable Keypoints) [56]. Introductions of these packages are provided below.

First of all, SURF is a scale and rotation invariant interest point detector and descriptor. It employs a Hessian matrix-based measure for the detector, and a distribution-based descriptor and also optimizes these methods to the essential in order to achieve competitive computational speed and accuracy. Upright SURF (U-SURF) is a scale-invariant only version of this descriptor [53].

The BRIEF feature point detector and descriptor relies on a relatively small number of intensity difference test to represent an image patch as a binary string. It employs the Hamming distance to compare strings. BRIEF easily outperforms other fast descriptors such as SURF and U-SURF in terms of speed and landmark detection accuracy. However, BRIEF lacks rotational invariance [54].

FREAK is a retina-inspired fast, compact and robust keypoint descriptor [55]. Although, as a descriptor, it is more robust and fast to compute in comparison to SURF and BRISK [55], it is not a feature point detector as required by our research.

BRISK is another novel method for key point detection, description and matching. Evaluation with benchmark datasets proved its computational efficiency and high quality performance compared to other state-of-the-art descriptors and detectors. A novel scale-space FAST (Features from Accelerated Segment Test)-based detector has been embedded in BRISK. Moreover, BRISK computes brightness comparisons from an easily configurable circular sampling pattern to generate a binary descriptor string. The combination of FAST with the assembly of this bit-string descriptor from intensity comparisons enables BRISK’s fast performance. It overcomes the inherent difficulty and balances well between high quality feature extraction and low computational requirement. Thus BRISK is suitable for tasks with limited computational power and time constraints [56]. Therefore it is selected for this research to perform real-time facial point detection to deal with challenging real-life human robot interaction.
                     

In detail, BRISK focuses on scale-space keypoint detection. It estimates the true scale of each keypoint in a continuous scale-space in order to achieve scale invariance. A saliency measurement is also used to detect points of interest across both the image and scale dimensions. These points of interest are also detected in both octave layers of the image pyramid and layers in-between to greatly improve computational efficiency. The quadratic function fitting is also employed to retrieve the location and scale of each keypoint. Especially it is able to deal with image keypoint detection tasks without sufficient prior knowledge on the scene and camera poses. Previous research and experiments also indicated its efficiency and robustness in dealing with rotations and scaling differences [56].

We integrate a C++ version of BRISK with the robot’s vision APIs for facial point detection in this research. Similar to Gabor filtering, BRISK is also applied directly on each ROI image to retrieve key facial feature points. The advantage of using BRISK is that it is able to derive numerous facial points with high accuracy. Since it is combined with a high-speed corner detector, FAST, BRISK is able to provide more reliable corner detection results in comparison to the 2D Gabor filter. It also shows high levels of repeatability under diverse changes of head poses, rotations and scales. In this research, a sequence of landmarks ranging from 100 to 200 points is generated by BRISK for all ROIs. An example output of BRISK is also provided in Fig. 5. Because BRISK is scale and rotation invariance, our facial point detector is able to reliably deal with rotations and pose variations at least up to 60 deg in real-time applications.
                     

However, although Gabor filtering is not as accurate as BRISK-based facial point detection, it generates one core corner detection point for each potential candidate corner without any overlapping and redundancy, which also provides good reference to the BRISK-based feature detection. Thus, we combine these two sets of keypoint outputs for each ROI produced respectively by the Gabor filter and BRISK to generate the final output landmarks for each facial element and further increase the accuracy for feature point detection.

The following processing shown in Algorithm 3 is conducted in order to combine the landmarks generated by BRISK and Gabor filter which consists of three steps. First of all, small circles are used to represent the keypoints detected by BRISK and Gabor filtering. Step 1 in Algorithm 3 is to reduce the number of feature points detected by BRISK using a circle–circle intersection algorithm [57]. Thus a set of important features is collected from the output of BRISK by selecting the feature points with the highest number of overlappings. In order to conduct feature reduction, in step 2, the circle–circle intersection technique is also applied to find out overlapping points in the newly generated BRISK output and Gabor filter output. Finally the most accurate 15 features are generated after the combination of the two outputs. However, the intersection algorithm is not able to provide landmarks for eyebrows since features for eyebrows are not present in the output of Gabor filter. Therefore in step 3, an extra set of six points with highest number of overlappings for the eyebrows generated by BRISK is integrated with the above recovered 15 landmarks to produce a set of 21 facial points as outputs. Algorithm 3 shows the details of the output combination of BRISK and Gabor filter using the circle–circle intersection method.

Most importantly, BRISK is able to provide landmarks for both eyebrows in comparison to the outputs generated by Gabor filtering. Eventually, 21 landmarks are generated based on the combination of the BRISK and Gabor filtering based facial point detection. These 21 generated landmarks include three points for each eyebrow, four for each eye, three for the nose and four for the mouth contour.

However, although this facial point detection process is scale and rotation invariance, it also has limitations. For example, when partial facial images are not visible because of occlusions or structural disturbances (e.g. make-up, glasses, facial hair, mugs etc.), the above facial point detection processing may not be able to recover all of the essential facial points for each facial element. Therefore, in order to deal with such challenges and capture sufficient physical cues to allow for subsequent efficient AU intensity estimation and emotion recognition, we employ an ICP algorithm [58] and FCM with post correlation processing to recover more essential geometric facial features and overcome occlusions.

The ICP algorithm is usually employed to reconstruct 2D or 3D surfaces and restore 2D curves in computer vision research. It is an algorithm employed to minimize the difference between two sets of points. It employs one reference and one source point cloud as inputs. The reference point cloud will be kept unchanged while the source point cloud will be transformed to provide the best match to the reference set [58]. In this research, we take the 21 combined landmark outputs generated by BRISK and Gabor filtering as the reference point cloud and employ a set of 68 neutral landmarks of a randomly selected neutral image provided by the CK+ database as the source point cloud. Since we aim to detect 54 facial landmarks for a test image, we only select 54 landmark points from the 68 original neutral points for this experiment by discarding 14 landmarks for the description of the overall facial contour. We then align the neutral landmarks on each ROI of the test image. The reason that the alignment for each ROI is done separately is to make the neutral landmarks best fitted with the test image.

Using ICP, for each ROI, we then align the source neutral landmarks provided by the database to the reference points generated by BRISK and Gabor filtering using iterative transformation to gradually reduce the distance between the source and the reference points. As mentioned earlier, the reference point cloud, i.e. the generated points by BRISK and Gabor filtering, will be kept unchanged while the source point cloud, i.e. the neutral points provided by CK+, will be transformed to provide the best match to the reference point cloud. This ICP algorithm first of all retrieves the closest point in the reference point set for each source point. It then calculates rotations and translations between each pair of source and reference points in order to provide best alignment between them. Subsequently, it conducts the transformation of the source points based on the above estimation. This process continues until the stopping criterion (i.e. the maximum number of iterations) is reached [58]. The pseudo-code of the ICP algorithm with equations in reference to [58] is provided in Algorithm 4
                        
                        .

In this application, the ICP algorithm transforms the source neutral 54 landmarks to best match the reference 21 facial points. It allows us to reconstruct 2D surfaces and thus extend the detected facial points from 21 to 54. The output 54 landmarks recovered by ICP include 5 points for each eyebrows, 6 landmarks for each eye, 9 for the nose, 20 for the mouth, and 3 points for the chin. An example output of the ICP algorithm is shown in Fig. 6. Fig. 7
                         shows the overall detected 54 facial landmarks for an example image taken from CK+.

However, although the ICP algorithm is able to reconstruct the missing facial features, which also achieves reasonable performance especially for images from real-life applications (e.g. with subtle expressions), the facial landmarks generated by ICP always represent a neutral facial element. Therefore in order to restore emotional expression for the occluded facial regions, FCM clustering, an unsupervised learning technique, is applied to further reason the shapes of the occluded facial elements. The detailed discussion of FCM clustering is provided in Section 5.2. We introduce shape estimation using FCM for the occluded facial elements in the following.

First of all, we divide the whole face into three parts such as left eye, right eye, and lower facial regions. For each facial region, we employ one FCM to inference its shape. Overall, three FCM algorithms are developed to respectively recover the contours or shapes of the occluded left eye, right eye and the mouth. Each FCM uses landmarks of non-occluded facial regions as inputs and outputs three clusters to represent an opened, narrowed and neutral facial element. For the reasoning of the shape of the mouth, 22 landmarks representing both eye regions with 10 landmarks denoting eyebrows and 12 landmarks denoting eyes are used as inputs whereas for the inference of the shape of either eye, 20 landmarks which form the geometry of mouth and 11 points representing the other visible eye region are used as inputs. Depending on which facial element is occluded, the three output clusters of FCM represent either mouth wide open/lip corner puller/closed neutral mouth or widened/tightened/neutral contour of the eye. Subsequently, the output shape information of each FCM is then used to advise a subsequent post landmark correlation processing to further adjust the neutral facial landmarks of the occluded element produced by ICP. In this research, we only use FCM and related post processing on top of ICP if the occlusion is occurred in the test image otherwise the landmark generation completes after the application of ICP.

As discussed earlier, after the attribute or shape of the occluded facial element of a test image is predicted by FCM, the test image is grouped into a specific shape cluster (e.g. mouth open if the mouth is occluded). Then within this shape cluster, landmark correlations between the visible facial elements of the test instance and the corresponding facial elements of other samples in the cluster have been calculated. Then five images with the highest correlations to the visible facial elements in the test image are selected. The landmark points from these highly correlated five images for the corresponding occluded facial element in the test image (e.g. the mouth) are retrieved and then averaged to produce the landmarks for the occluded facial element in the test image.

For example let us consider a facial image with surprise emotion has mouth wide open and eye widened but the mouth is occluded and the intensities of how wide the mouth would be are unknown. We have used the above averaging method to normalize the shape obtained from FCM. That is, once we have applied FCM to obtain the shape cluster of the occluded facial element (e.g. cluster for mouth open), we select the top five outputs with the highest correlations to the test image in the same cluster and average them to re-construct the best fitting geometry for the occluded facial element. This re-constructed set of landmarks with shape information embedded is then used to replace neutral landmarks generated by ICP in the test image.

Evaluation results of facial landmark generation for images with occlusions and rotations are discussed in detail in Section 6. Overall the landmark detection using both ICP and FCM with post correlation processing achieves better detection accuracy in comparison to purely using ICP for images with occlusions because of the employment of the clustering technique to further reason the shape of the occluded facial element based on prior knowledge of the non-occluded facial regions and the retrieval of the best fitted geometry for the occluded facial region based on the highly correlated images with similar emotion indication to the test image. Related discussions are provided in Section 6.1.1. Fig. 8
                         shows an example occluded image before and after applying FCM with post correlation processing for the retrieval of landmarks for the occluded facial element (the first two images) with landmark generation for the same image without occlusion as reference (the third image). It indicates that FCM with post correlation processing has further adjusted the set of neutral landmarks reconstructed by ICP to retrieve an opened mouth based on the inference of the non-occluded facial components and the retrieval of best fitted geometry. Overall, ICP and FCM with post correlation processing enable us to reconstruct missing landmark points for a test image to effectively deal with occlusion.

In summary, as mentioned earlier, facial occlusion in the image is detected when we are detecting ROIs. That is, if an ROI (e.g. the mouth) is not detected, then it indicates that the image is occluded for that region. The step-by-step procedures are also summarized in the following for the occlusion detection (steps 1 and 2) and landmark generation for the overall image (steps 3–6).

                           
                              1.
                              We apply cascade classifiers for ROI detection in order to identify the positions of the left eye, right eye, nose tip and mouth.

If any of the ROI is occluded, then the corresponding Haar cascade detector will not detect it. That is, the image occlusion is detected.

Only the detected ROIs are further processed using Gabor filter and BRISK to recover key landmarks for these facial regions. In another word, Gabor filter and BRISK are not applied to the occluded facial regions.

After applying Gabor filter and BRISK, we get a set of landmark points (less than 21) for the non-occluded facial elements (e.g. a total of 17 landmarks for a test image with mouth occlusion).

Then we employ a set of neutral landmarks with ICP to construct 54 landmarks for the overall image with neutral landmarks recovered for the occluded region (i.e. the mouth).

To further re-construct the landmarks for the occluded facial region, we employ FCM and some post correlation processing to identify the best fitting geometry, further adjust the shape of the occluded facial element and output the final set of 54 landmarks.

These detected landmarks for the overall image are subsequently used for SVR-based facial AU intensity estimation and FCM-based emotion recognition.

Moreover, as discussed earlier, the NAO robots original vision ALFaceDetection API is only able to provide 31 2D points for a facial representation including the contour of the mouth (eight points), nose position (three points), shape of each eyebrow (three points) and contour for each eye (seven points). Although these geometric-based features were able to capture some indicators for the expressions of the Ekmans six basic emotions and have been used to perform facial emotion recognition in our previous initial stage of the research [12], they proved to be sometimes not sufficient enough to capture physical cues of subtle facial behaviors. This unsupervised facial landmark detection is able to produce 54 2D points to extend the robots vision API with 2 points extra for the description of each eyebrow, 12 points extra for the mouth contour, 6 landmarks extra for the nose, 3 points extra for the chin and a similar number of landmarks for each eye contour. Five image databases, i.e. CK+, LFW, PUT, LFPW and Helen, are used for evaluation of this unsupervised facial feature point detection to prove its flexibility and computational efficiency. It also allows NAO to deal with challenging facial point detection and emotion recognition tasks with large pose variations, illumination changes, scaling differences and the presence of occlusions and background clutter from dynamic real-life images and interactions. Detailed evaluation results for this facial point detection processing using the above five databases are discussed in Section 6.

Since geometric features are capable of capturing physical cues effectively embedded in emotional facial expressions, they are widely used for facial action and emotion recognition research [12,33]. In this work, we also employ motion-based features, i.e. the derived 54 facial points, to estimate AU intensity and recognize emotions of facial images. We respectively employ SVR and NNs for the AU intensity estimation for the selected 18 AUs. FCM clustering is also used to detect the eight basic emotions including happiness, anger, sadness, disgust, surprise, fear, contempt and neutral. Experiments of compound emotion recognition using FCM are also conducted.

Among the 32 AUs defined by the FACS, this research focuses on the recognition of 18 AUs closely associated with the expression of eight basic emotions including Inner Brow Raiser (AU1), Outer Brow Raiser (AU2), Brow Lowerer (AU4), Upper Lid Raiser (AU5), Cheek Raiser (AU6), Lid Tightener (AU7), Nose Wrinkler (AU9), Upper Lip Raiser (AU10), Lip Corner Puller (AU12), Dimpler (AU14), Lip Corner Depressor (AU15), Lower Lip Depressor (AU16), Chin Raiser (AU17), Lip Stretcher (AU20), Lip Tightner (AU23), Lip Pressor (AU24), Lips Part (AU25), Jaw Drop and Mouth Stretch (AU26/27). We respectively employ 18 SVRs and 18 NNs to measure the intensities of these 18 selected AUs with each SVR/NN dedicated to intensity estimation for each AU. As discussed earlier, in recent research, several techniques have been proposed for AU intensity measurement [30,32]. We employ SVR and NN based AU intensity estimation in this research because of their promising performances and robustness of the modeling of the problem domain. Detailed discussions and justifications are provided in the following.

As well-known, SVR is a non-linear regression technique. It uses a non-linear mapping to transform the original training data into a higher dimension and computes a linear regression function in this transformed high dimensional feature space. SVR aims to identify the most suitable hyper-plane, which is able to accurately predict the distribution of data within an error tolerance value of ε. In comparison to support vector classification (SVC), it estimates the function of data rather than to classify data into distinctive two or more classes. Moreover, the crucial differences between SVR and SVC are: (1) The predicted label value of an instance is a continuous value in SVR but a discrete one in SVC. (2) For SVR, there is a tolerable error, ε, between the predicted value and the actual label value. But for SVC, the predicted and actual class types must be either exactly matched or no matching at all. SVR has been applied in various fields to solve regression problems such as financial time series forecasting [59].

We employ the Libsvm [60] package integrated with OpenCV for SVR-based AU intensity estimation. Libsvm is an efficient software package for SVM classification and regression. Epsilon-SVR from Libsvm is employed in this research. We construct 18 epsilon-SVRs in order to effectively estimate intensities for the 18 selected AUs. Facial elements related to each AU intensity estimation are also identified. We use 31 landmarks associated with eyes, eyebrows and nose as inputs to the SVRs to respectively measure the intensities of AU1, 2, 4, 5, 6, 7 and 9, while 20 facial points related to the mouth contour are used to estimate the intensities of lower facial AUs including AU10, 12, 14, 15, 16, 17, 20, 23, 24, 25, and 26/27.

After the associated facial landmarks are determined for each AU intensity estimation, we perform linear scaling of the input values of each training instance. The scaling pre-processing provided by Libsvm scales each attribute value to the range of [0, 1]. The same scaling method is also used to adjust the test data. This scaling process ensures that attributes in greater numeric ranges will not dominate those in smaller numeric ranges.

In order to select the most suitable kernel for each SVR-based AU intensity estimation, linear and non-linear kernels of SVR are explored. In this research, we select the non-linear RBF kernel. This is mainly because: (1) RBF nonlinearly maps inputs into a higher dimensional space, thus it is able to deal with the case that the relation between facial features and AU intensity levels is non-linear. (2) RBF has fewer number of hyperparameters than other non-linear kernels (e.g. polynomial kernel), which may reduce the complexity of model selection [61]. (3) RBF usually has lower computational complexity, which leads to optimal real-time computational performance. Also, since the number of attributes for each AU intensity estimation is not very large, this makes the RBF kernel more suitable to this application domain in comparison to a linear kernel [61]. Thus an RBF kernel is selected for each of the 18 SVRs in order to achieve promising performances.

The subsequent essential step is to find optimized kernel parameters for each RBF-based SVR. There are three parameters whose values need to be determined for an RBF-based regressor, including a soft-margin constant, C, a kernel parameter, gamma and an epsilon in the loss function. The combination of these three parameters plays very important roles in affecting each SVR’s performance. In order to identify the most optimal set of the three parameters, a grid search on C, gamma and epsilon, and cross validation have been performed [61]. A tenfold cross validation has also been conducted in order to find the best combinations of parameters and in the meantime avoid overfitting. We use exponentially growing sequences and employ values respectively ranging from 
                           
                              
                                 
                                    2
                                 
                                 
                                    −
                                    5
                                 
                              
                              
                              to
                              
                              
                                 
                                    2
                                 
                                 15
                              
                           
                         for C, 
                           
                              
                                 
                                    2
                                 
                                 
                                    −
                                    10
                                 
                              
                              
                              to
                              
                              
                                 
                                    2
                                 
                                 5
                              
                           
                         for gamma and 
                           
                              
                                 
                                    2
                                 
                                 
                                    −
                                    8
                                 
                              
                              
                              to
                              
                              
                                 
                                    2
                                 
                                 
                                    −
                                    1
                                 
                              
                           
                         for epsilon for the grid search. The search experiment is guided by the mean squared error (MSE) of each AU intensity estimation. The MSE value is computed using the following equation [62]:

                           
                              (9)
                              
                                 
                                    MSE
                                    =
                                    
                                       1
                                       n
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       
                                          (
                                          
                                             y
                                             i
                                          
                                          −
                                          
                                             
                                                
                                                   y
                                                   i
                                                
                                             
                                             *
                                          
                                          )
                                       
                                       2
                                    
                                 
                              
                           
                        where yi
                         is the predicted value, and 
                           
                              y
                              
                                 i
                              
                              *
                           
                         is the original annotation.

The best parameter set of C, gamma and epsilon within the search space in our application is identified for each SVR, which yielded the lowest MSE for each AU intensity estimation. These identified most optimal parameters for each AU intensity estimation are then used for the training and testing of the corresponding nonlinear SVR model. As mentioned earlier, 18 SVRs are employed for AU intensity measurement of the 18 selected AUs.

In this research, images from the CK+ database are used to perform the training and the grid search of these 18 SVRs. The extended CK+ database contains 593 sequences of frontal-view posed facial images contributed by 123 subjects. Each image also contains AAM tracked 68 two-dimensional landmark points stored in an individual file. The database also includes 593 FACS coded peak frame emotional images. 327 peak facial images have also been annotated with the selected seven emotion labels including happiness, anger, sadness, disgust, surprise, fear and contempt [40,41]. Therefore, we employ 200 FACS coded peak emotional images with AU intensity annotation and 50 neutral images from the CK+ database for the cross validation processing of these 18 SVRs. Moreover, a different number of images is also used for the training of each SVR based on the availability of the corresponding AU among the extracted 250 training images. It ranges from 15 images used for the training of intensity estimation for AU10 to 125 images used for intensity estimation for AU25. On average, 75 images are used to train each SVR.

Moreover, since the previously selected databases for the evaluation of the facial point detection, i.e. PUT, LFW, LFPW and Helen, do not provide any AU and emotion annotation also with majority of the images indicating neutral expressions, we employ the CK+ database with AU intensity annotation for the evaluation of 18 SVRs. 127 peak emotional images from the CK+ database with fair distribution of each AU and 30 neutral images are used to evaluate the 18 SVRs. In this research, the OpenCV version of the Libsvm package is also recompiled under the latest NAO C++ SDK and integrated with NAO to perform real-time facial AU intensity estimation.

We have also employed 18 feedforward NNs with backpropagation to perform intensity regression for the 18 AUs to further evaluate the performance of SVRs. These 18 NNs employ the same corresponding set of either 31 upper or 20 lower facial landmarks as inputs for the AU intensity measurement for the 18 AUs with each NN dedicated to the intensity estimation for each AU. Moreover, since a single hidden layer can approximate any continuous functions, each NN model has one single hidden layer in this application. Also, each NN is trained with the same training set employed by the corresponding SVR for each AU intensity estimation. The same test set of 157 (127 emotional + 30 neutral) images extracted from CK+ is also used to test the performance of NNs.

The experiments indicated that SVR-based AU intensity estimation outperforms the NN-based measurement. SVRs show better performance for intensity estimation for the following AUs: AU1, 2, 4, 5, 6, 10, 12, 14, 15, 17, 20, 23, 25, and 26/27 in comparison to the NNs based estimation. The performance comparison between SVRs and NNs is discussed in detail in Section 6. Subsequently these estimated intensities for the 18 selected AUs are used to infer emotions embedded in the real-time facial expressions using FCM.

Facial emotion recognition has been carried out using many supervised learning techniques in the past, e.g. NNs and SVMs [6,12,33,36]. In this research, we employ an unsupervised FCM clustering technique to recognize the seven basic emotions and neutral expressions. This clustering technique also shows great potential in detecting compound and newly arrived novel emotions. Clustering algorithms generally organize objects into groups based on similarity criteria. In order to deal with different clustering mining tasks, clustering techniques can be classified into the following categories: partitioning, hierarchical, density-based and grid-based clustering methods. However, these basic clustering algorithms tend to force clustering an object into only one cluster. Sometimes, this rigid clustering may not be desirable for the application of some of the problem domains. For instance, a compound facial emotion may belong to more than one emotion cluster. An online shopping review may contain comments related to several products, which may fall into several product clusters. Therefore, in this research, we employ probabilistic model-based clustering to allow one facial expression image represented by facial actions to be grouped into more than one emotion cluster [63]. It would be even more useful if a weighting is also calculated to reflect the strength of an object belonging to one cluster. Thus, a well-known probabilistic model-based clustering algorithm, FCM clustering, is used in this research to detect emotions.

Given a set of objects, 
                           
                              
                                 o
                                 1
                              
                              ,
                              
                                 o
                                 2
                              
                              ,
                              …
                              ,
                              
                                 o
                                 n
                              
                              ,
                           
                         and k pre-defined fuzzy clusters, 
                           
                              
                                 C
                                 1
                              
                              ,
                              
                                 C
                                 2
                              
                              ,
                              …
                              ,
                              
                                 C
                                 k
                              
                              ,
                           
                         FCM clustering groups each object into more than one cluster and generates a partition matrix, 
                           
                              M
                              =
                              
                                 [
                                 
                                    w
                                    
                                       i
                                       j
                                    
                                 
                                 ]
                              
                              
                                 (
                                 1
                                 ≤
                                 i
                                 ≤
                                 n
                                 ,
                                 1
                                 ≤
                                 j
                                 ≤
                                 k
                                 )
                              
                              ,
                           
                         where wij
                         is the degree of membership that a data point belongs to a cluster. The partition matrix also needs to fulfil the following criteria [63]. (1) The degree of membership wij
                         for each object oi
                         belonging to a cluster Cj
                        , should be in the range of [0, 1] to ensure that a fuzzy cluster is a fuzzy set. (2) For each object, oi
                        , 
                           
                              
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    k
                                 
                                 
                                    (
                                    
                                       w
                                       
                                          i
                                          j
                                       
                                    
                                    )
                                 
                              
                              =
                              1
                           
                        . This indicates that each object participates in the clustering equivalently. (3) For each cluster, Cj
                        , 
                           
                              0
                              <
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 n
                              
                              
                                 (
                                 
                                    w
                                    
                                       i
                                       j
                                    
                                 
                                 )
                              
                              <
                              n
                           
                        . This requirement is used to ensure that for each cluster, there is at least one object in this cluster with a non-zero membership value.
                     

In order to calculate a probability distribution over the clusters to obtain degrees of membership for each data object, an expectation-maximization (EM) algorithm is employed. It includes two procedures: the expectation and maximization steps. The algorithm starts with initialized random parameters and iterates until the clustering cannot be improved (i.e. the clustering converges or the change to the membership values between the most recent two iterations is sufficiently small). In each iteration, it calculates the center of each cluster and updates memberships for fuzzy clustering. Both of expectation (E-step) and maximization (M-step) steps are involved in each iteration. The E-step first of all selects random data instances as the initial centers of clusters. It then calculates degrees of membership for each data in each cluster. We consider the idea that if a data point, oi
                        , is closer to a cluster, Cj
                        , the degree of membership of oi
                         for this cluster Cj
                         should be higher. For one data object, the sum of its membership values for all the clusters will be 1. Thus, the E-step produces fuzzy memberships and partitions objects into each cluster. Eq. (10) is used for the calculation of the degree of membership for a given data object xi
                         belonging to a cluster Cj
                         
                        [63].

                           
                              (10)
                              
                                 
                                    
                                       w
                                       
                                          i
                                          j
                                       
                                    
                                    =
                                    
                                       1
                                       
                                          
                                             ∑
                                             
                                                k
                                                =
                                                1
                                             
                                             c
                                          
                                          
                                             
                                                (
                                                
                                                   
                                                      
                                                         ∥
                                                      
                                                      
                                                         x
                                                         i
                                                      
                                                      −
                                                      
                                                         c
                                                         j
                                                      
                                                      
                                                         ∥
                                                      
                                                   
                                                   
                                                      
                                                         ∥
                                                      
                                                      
                                                         x
                                                         i
                                                      
                                                      −
                                                      
                                                         c
                                                         k
                                                      
                                                      
                                                         ∥
                                                      
                                                   
                                                
                                                )
                                             
                                             
                                                2
                                                
                                                   m
                                                   −
                                                   1
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where, m is the pre-defined fuzziness coefficient and Cj
                         represents the center of each cluster. The fuzziness coefficient m, where 1 ≤ m < ∞, indicates the tolerance of the required clustering. It determines the level of overlapping between clusters. The higher the fuzziness coefficient is, a larger number of data objects will be categorized into a fuzzy band and carry a membership value between 0 and 1. We use 
                           
                              m
                              =
                              2
                           
                         in this application.

The M-step will recalculate the centroids, i.e. the new centers of clusters, in each iteration using Eq. (11) 
                        [63].

                           
                              (11)
                              
                                 
                                    
                                       c
                                       j
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             
                                                x
                                                i
                                             
                                             .
                                             
                                                w
                                                
                                                   i
                                                   j
                                                
                                                2
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             w
                                             
                                                i
                                                j
                                             
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        where j = 1, 2, … , k, and wij
                         representing the value of the degree of membership calculated using Eq. (10) and xi
                         is a data object.

In each iteration of the fuzzy clustering algorithm, we aim to minimize the following sum of the squared error (SSE) defined in Eq. (12).

                           
                              (12)
                              
                                 
                                    SSE
                                    
                                       (
                                       C
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       k
                                    
                                    
                                       w
                                       
                                          i
                                          j
                                       
                                       p
                                    
                                    
                                       
                                          d
                                          i
                                          s
                                          t
                                          (
                                          
                                             x
                                             i
                                          
                                          ,
                                          
                                             c
                                             j
                                          
                                          )
                                       
                                       2
                                    
                                 
                              
                           
                        where wij
                         represents the degree of membership for an object xi
                         belonging to a cluster Cj
                         and dist(xi, Cj) measures the distance between xi
                         and the center of the cluster Cj
                        . Also, p (
                           
                              p
                              ≥
                              1
                           
                        ) determines the effect of the degrees of membership (
                           
                              p
                              =
                              1
                           
                         in this context) [63].

Such an iteration process in FCM clustering completes when the cluster centers converge or the change to the degrees of membership is less than a pre-set epsilon threshold value. In each iteration, we use Eq. (13) to calculate the difference, ε, between each pair of new and old degrees of membership in the most recent two iterations across all the data objects. In this application, we set the maximum difference between membership values in the adjacent two iterations, i.e. the termination criterion, as 0.0005.

                           
                              (13)
                              
                                 
                                    
                                       ɛ
                                    
                                    =
                                    
                                       Δ
                                       
                                          i
                                       
                                       n
                                    
                                    
                                       Δ
                                       
                                          j
                                       
                                       k
                                    
                                    
                                       |
                                       
                                          w
                                          
                                             i
                                             j
                                          
                                          
                                             l
                                             +
                                             1
                                          
                                       
                                       −
                                       
                                          w
                                          
                                             i
                                             j
                                          
                                          l
                                       
                                       |
                                    
                                 
                              
                           
                        where, 
                           
                              w
                              
                                 i
                                 j
                              
                              l
                           
                         and 
                           
                              w
                              
                                 i
                                 j
                              
                              
                                 l
                                 +
                                 1
                              
                           
                         respectively represent the degree of membership at iteration l and 
                           
                              l
                              +
                              1
                           
                         and the operator Δ returns the largest value in a given vector of values. Therefore, in this way, a maximum change to the membership values among all the data objects can be identified. The overall FCM clustering algorithm is listed in the following.

In this application, we use the above derived set of facial actions with 18 dimensions for the representation of one facial expression data as inputs to the fuzzy clustering algorithm. Since we aim to recognize seven basic emotions and neutral expressions, we set the pre-determined number of clusters as ‘8’. The output of the clustering algorithm is the identified eight membership values for the eight emotion clusters. That is, each facial expression input represented by 18 facial AUs will have a membership value for each fuzzy emotion category. In this research, since the CK+ database provides a sole emotion annotation to represent the most intended emotion category for each peak facial expression, for the evaluation purposes, we select the one with the highest membership value from the fuzzy clustering outputs as the potential detected emotion for each test image. Since for each test instance, the sum of its membership values across all the eight emotion clusters will be 1, a threshold value (1/8 = 0.125) based on the equal distribution of the test data among the eight clusters is also defined as the criterion for one emotion clustering result to be counted as a valid final detected emotion output. For example, if the highest membership clustering output of a test instance is less than this threshold value, then this test image will not be regarded as carrying any of the eight existing known emotions but indicating a newly arrived novel unseen emotion category. Thus this clustering approach allows us to recognize the arrival of a new emotion class. Since it provides a degree of membership for each emotion cluster for each test object, which indicates how close this test instance is to the center of each emotion vector, it also enables us to identify a compound emotion and initial experiments have also been conducted to test its efficiency.

In this research, we implement a C++ version of this FCM clustering for emotion recognition. We have conducted evaluations of FCM clustering using the CK+ database for the detection of the seven basic emotions and neutral expressions. 127 FACS coded peak emotional images and 30 neutral images from the CK+ database used for the AU intensity estimation are employed to evaluate this clustering based emotion recognition. A majority vote is used to label each emotion cluster derived by the clustering algorithm with emotion annotations of images provided by CK+. The evaluation results indicate that the FCM clustering technique achieves a promising average accuracy rate of 90.38% for the detection of the eight basic emotions. Detailed results are provided in Section 6.

Moreover, in order to prove its capability in recognizing compound emotions, some initial testing is also conducted. As discussed previously, Martinez and Du [11] proposed a new theoretical cognitive model for the description of multiple compound emotion categories such as happy surprise or angry surprise. We have borrowed some example images with compound emotional expressions shown in Fig. 9
                         from their work to test the emotion clustering technique. First of all, our unsupervised facial point detector is used to generate 54 facial points for each of these images. Then AU intensity estimation is also conducted using SVRs for the selected 18 AUs. The estimated intensity outputs of the 18 AUs are then used as an input vector to the FCM clustering algorithm to detect emotion. The top two emotion clustering outputs above the pre-defined threshold value (0.125) are selected as the final emotion clustering results for each test compound expression image. The example clustering results are also provided in Fig. 9.

As shown in Fig. 9, the FCM clustering technique is able to identify two comparatively valid higher membership values for each of the first three compound facial expression inputs while it is also able to generate a highest sole dominating membership value for the last surprised facial image. This indicates its great potential and flexibility for tackling challenging compound emotion recognition. For example, the clustering technique produces a highest membership for the surprise emotion category among all the eight emotion clusters respectively for both happy and fearful surprised facial expressions. It also generates the second highest valid membership values respectively for happy and fear emotion clusters for these two compound facial images. The clustering algorithm also produces two very close highest membership values for the disgust and surprise emotion clusters for the disgusted surprise compound expression since it seems that this image carries comparatively less surprised indication in comparison to other compound surprised test images but also containing clear physical cues for disgusted expressions. The last example image with strong physical cues of a surprised expression is with a sole dominating membership value for the surprise emotion cluster.

The results shown in Fig. 9 also indicate the efficiency of the facial point detection and AU intensity estimation presented in this research, which lay a solid foundation for the clustering based compound emotion recognition. For instance, besides the deriving of AU1, 2, 5, 25, 26/27 which indicate a dominating surprised facial expression, the SVR-based AU intensity estimation also respectively identifies intensities of AU6 and AU12 for the happy surprise compound expression and AU20 for the fearful surprise facial image. Regression-based AU intensity estimation also produces intensities of AU1, 2, 4, 7, 17, 23 and 24 for the disgusted surprise facial image. In future work, further evaluation will be conducted for this clustering based emotion recognition by employing databases with compound and more basic emotion annotations to further prove its robustness.

@&#EVALUATION@&#

The facial landmark point detection, regression-based AU intensity estimation and emotion clustering have been integrated with the robot’s latest C++ SDK 1.14.3 under Ubuntu. The robot’s cross-toolchain compilation is also used in order to enable the overall system to execute on both the robot and the desktop platforms. A real-time video reflecting the robot’s vision and the real-time facial landmark extraction are displayed by OpenCV windows on the computer. The AU and emotion recognition results are communicated using both text messages shown on the computer terminal and speech synthesized by the robot’s text-to-speech engine. The robot is also able to function very well for facial data collection from real-time interaction under normal lab lighting conditions.

During the testing, the robot is programmed to first greet the user and make a brief introduction about what the testing is mainly about. Then a database image is displayed in front of NAO. Or the robot requires the user to show a specific emotional facial expression if a real testing subject is used. When an emotion is detected for each posed or spontaneous facial expression, NAO conducts speech-based interaction to communicate back about the details of the facial emotion recognition results. Its speech synthesis engine is therefore activated to report the features of the upper and lower facial parts of an emotional facial image and also inform the user the emotion embedded in the real-time input facial expression. We present the testing conducted with NAO in detail in the following.

We have employed 200 images from each of the five well-known databases: CK+, LFW, PUT, LFPW and Helen to evaluate our facial feature point detection. As discussed earlier, CK+ contains 593 sequences of frontal-view posed facial images contributed by 123 subjects. The PUT database [43] is especially designed to provide a benchmark dataset with a significant size and diversity to evaluate the efficiency and robustness of face pose estimation, 2D/3D statistical face shape model development, and pose invariant face recognition algorithms. The database consists of 9971 images from 100 subjects with 30 annotation points provided for each image. Images in PUT show pose variations, partially controlled illumination conditions, occlusions or structural disturbances (e.g. make-up, glasses, facial hair, etc.) with a uniform background. The LFW database [42] has images captured from real-life situations with spontaneous facial expressions. The images in this database usually show different head poses and facial expressions, partial occlusions and background noise, which pose great challenges to automatic facial analysis and emotion recognition systems. LFW contains more than 13,000 images of faces collected from the web with each image annotated with 20 facial points. The LFPW database [64] provides 1432 real-life face images obtained via simple text queries through several search engines. Each image was originally labeled with 29 landmark points whereas a training set of 811 images from LFPW has been re-annotated with 68 landmarks by Sagonas et al. [66]. The Helen database [65] is also composed of real-world high-resolution fully annotated face images. It contains 2000 training and 330 test images. Each image is originally annotated with 194 facial points. The training set of the 2000 images has also been re-annotated with 68 landmarks by Sagonas et al. [66]. We select these five databases as benchmark datasets for system evaluation since they provide diverse posed and spontaneous emotional facial expressions with frontal and multi-views, pose variations, scaling differences, partial occlusions and background clutter. As mentioned earlier, the five databases also provide landmark annotations to allow for the evaluation of the proposed facial point detector.

We also compare our facial point detection with the current existing supervised learning approaches such as AAM, CLM [20] and GN-DPMs [5]. An independent AAM has been implemented in our work [45]. We also employ a CLM produced by Cristinacce and Cootes [20] while GN-DPM produced by Tzimiropoulos and Pantic [5] is also imported in our experiments for comparison. GN-DPM is able to detect 49 landmark points and the code for GN-DPM provided in the website of the authors [5] indicates that it has been trained using 811 images from LFPW and 2000 images from Helen with the publicly available 68-point landmark configurations provided by Sagonas et al. [66]. However, since GN-DPM does not release the training code, we have to retrain AAM and CLM using the same training set of GN-DPM, i.e. 811 images from LFPW and 2000 from Helen with 68-landmark configuration in order to validate the comparison. The three models, i.e. AAM, CLM and GN-DPMs, are used to perform landmark detection using the same 1000 images as those used for the evaluation of this research with 200 selected from each of the above five databases.

We calculate landmark detection accuracy in order to compare the performances of AAM, CLM, GN-DPMs and our model. An average error is calculated in the following way for a whole set of detected landmarks for each model. For example, if we consider f(x1
                           n
                        , y1
                           n
                        ) as an original set of landmarks provided by the database and f(x2
                           n
                        , y2
                           n
                        ) as the set of landmarks generated after applying a feature point detector, where 
                           
                              n
                              =
                              
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 N
                              
                           
                         is the number of landmarks, we calculate the absolute value of the difference between this pair of facial points using the following formula: 
                           
                              
                                 err
                                 n
                              
                              
                                 =
                                 |
                              
                              
                                 (
                                 
                                    x
                                    
                                       1
                                       n
                                    
                                 
                                 −
                                 
                                    x
                                    
                                       2
                                       n
                                    
                                 
                                 )
                              
                              
                                 |
                                 +
                                 |
                              
                              
                                 (
                                 
                                    y
                                    
                                       1
                                       n
                                    
                                 
                                 −
                                 
                                    y
                                    
                                       2
                                       n
                                    
                                 
                                 )
                              
                              
                                 |
                                 ,
                              
                           
                         where ‘||’ represents the mathematical absolute symbol which only returns positive difference values. Subsequently the average error for a whole set of landmarks is calculated below:

                           
                              (14)
                              
                                 
                                    img
                                    
                                       (
                                       err
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             n
                                          
                                          
                                             err
                                             i
                                          
                                       
                                       n
                                    
                                 
                              
                           
                        where err
                           i
                         represents the error calculated for each derived landmark and img(err) is the total image error for a whole set of generated landmarks. We then obtain the landmark detection accuracy rate by the following formula: 
                           
                              1
                              −
                              img
                              (
                              err
                              )
                              ,
                           
                         which has been used to contribute to the results shown in Table 1
                        .

We notice that the four test models, i.e. our model, AAM, CLM and GN-DPMs, generate a different number of landmark outputs with 54 landmarks for our model, 68 landmarks respectively for AAM and CLM and 49 for GN-DPMs. In order to conduct a fair evaluation, we use 49 landmark points for the evaluation of all the models. For example, we discard 14 points for the description of the facial contour respectively produced by AAM and CLM to obtain a set of 54 landmarks for both AAM and CLM. Then we further take off 3 landmark points for the chin and 2 landmark points for the inner corners of the mouth from the remaining landmarks generated by AAM and CLM and the 54 landmarks generated by our model to retrieve a set of 49 landmarks. Thus the average error for a set of detected 49 landmarks is calculated for AAM, CLM, GN-DPM and our model respectively for comparison.

Furthermore, we also noticed that there is a different number of landmark annotations provided by each of the selected databases. For example, the CK+, Helen and LFPW databases provide 68 landmarks for each image while LFW offers 20 landmarks and PUT provides 30. In order to conduct effective evaluation across databases, we also discard the corresponding 17 landmarks for the facial contour description and 2 points for the inner corners of the mouth among the 68 annotation points respectively provided by the CK+, Helen and LFPW databases and employ the remaining 49 points as the ground truth for the evaluation of AAM, CLM, GN-DPM and our facial point detector. Also, since the LFW and PUT databases respectively provide only 20 and 30 landmarks for the annotation of each facial image, we use the direct neighborhood in the range of e.g. 10–13 pixels of each original landmark annotation to search if there is any generated landmark present. If there is no generated landmark within the region, then this corresponding original landmark will not be considered for evaluation. Experiments with the direct neighborhood checking of the generated landmarks in the range of other number of pixels have also been conducted which also draw similar evaluation conclusions. Using this method, we are also able to evaluate the four facial point detectors using landmark annotations provided by LFW and PUT.

The landmark detection results of our model, AAM, CLM and GN-DPM with 200 test images from each of the five selected databases are presented in Table 1. Overall, our proposed unsupervised method has outperformed these existing supervised models and achieved an average accuracy rate of 80% for CK+, 73% for LFW, 78% for PUT, 85% for both LFPW and Helen without any prior training required. The results in Table 1 indicate that our unsupervised model performs robustly when tested with images from the five databases with great diversity. Also, it is noticed that the both LFW and PUT databases posed great challenges to both AAM and CLM since these two models were both originally trained with the other databases (i.e. LFPW and Helen) and did not possess any knowledge of these two new test datasets in their learned models, which also contain diverse challenging cases such as pose variations, occlusions and scaling differences, etc. However both AAM and CLM seem to adapt better to the testing of CK+ which contains less challenging purely posed frontal facial expressions. Therefore, the performances of AAM and CLM have dropped dramatically when tested with PUT and LFW in comparison to those obtained from the testing of LFPW, Helen and CK+. The GN-DPM model achieves competitive performance in comparison to our facial point detector. It shows best performances when tested with Helen and LFPW followed by the results for CK+, PUT and LFW. The competitive performance of GN-DPM is mainly because of the employment of Gauss–Newton optimization to minimize a joint cost function of global shape and local appearance models to generate a joint translational motion model. In comparison to GN-DPM, AAM and CLM, our proposed unsupervised landmark detector does not require any prior knowledge of any of the test databases and shows great flexibility and robustness in dealing with facial point detection tasks against head rotations, pose variations, scaling differences, occlusions and background clutter for images across databases. Fig. 10
                         shows example landmark detection outputs of AAM, CLM, GN-DPM and our model for example images extracted from the three new test databases, CK+, LFW and PUT with images selected from LFW and PUT containing rotations and/or occlusions.
                     

As discussed earlier, when occlusions are occurred, in our approach, FCM and related post processing are applied to further adjust the shape of the reconstructed neutral facial element by ICP. However, in the above experiments shown in Table 1, occlusions mainly occur for a subset of images from LFW, LFPW and Helen with CK+ containing purely frontal-view emotional images and PUT containing test images mainly with rotations. Moreover, those test images with occlusions in LFW, LFPW and Helen also tend to show neutral or very subtle emotional expressions in real-life situations. Therefore the detection accuracy for landmark generation before and after applying FCM and the related post processing for our model does not show much difference for LFW, LFPW and Helen in the above experiments shown in Table 1. In order to further evaluate the efficiency of FCM and related post processing for landmark generation for images with occlusions, the following experiments are conducted. First, 100 peak emotional images from the CK+ database are randomly selected and edited using Adobe Photoshop in order to bear occlusion effects. For example, a black box is used to cover one of the facial elements to generate occlusions. Then another 300 images bearing either upper or lower facial occlusions are also selected from LFW, LFPW and Helen with 100 images extracted from each database for further system evaluation.
                           
                        

We have also compared the landmark detection accuracy of our facial point detector with those obtained from AAM, CLM and GN-DPM for these selected 400 occluded test images. Table 2 shows the detailed evaluation results. On average, our proposed unsupervised approach has outperformed the three existing supervised models of AAM, CLM and GN-DPM and achieved accuracy of 80% for images with occlusions from CK+, 78% for LFW, 80% for LFPW and 82% for Helen. Again in comparison to our model, the GN-DPM model achieves competitive performance with slightly better accuracy for face alignment for LFW and Helen databases, a similar performance of face alignment for LFPW and lower accuracy for landmark detection for CK+. AAM and CLM show limitations to such challenging face alignment tasks and achieve comparatively worse performances especially for LFW and CK+ in comparison to GN-DPM and the proposed approach.

Furthermore, a detailed inspection of our approach also indicates that when only ICP is applied, the detection process of our model tends to suffer from poor fitting accuracy for emotional images from CK+ since ICP is only able to reconstruct a set of neutral landmarks and may not be able to restore the best fitted geometry for occluded emotional facial components. However, after applying FCM and related post correlation processing to further adjust the neutral facial landmarks to recover the emotional indication of the occluded facial element with the knowledge of non-occluded facial regions as inputs, the detection accuracy of our model has been greatly improved, especially for emotional images from CK+.

In order to further identify the efficiency of the proposed facial point detector, we also divide the occluded images into upper and lower facial occlusion categories and provide the detection accuracies for these two categories respectively for the four facial point detection models in Table 3. In order to obtain a fair split between upper and lower facial occlusion cases across databases, we select 40 images from the CK+ database and transform half of them into upper facial occlusions and the other half of them into lower facial occlusions using Adobe Photoshop whereas a set of 40 images is also extracted respectively from LFW, LFPW and Helen with 50% indicating upper facial occlusions and 50% showing lower facial occlusions (i.e. a total of 120 images are extracted from LFW, LFPW and Helen). The results shown in Table 3 indicate that overall landmark detection achieves better accuracy for the upper facial occlusions than for lower facial occlusions irrespective of the model applied. Also, GN-DPM (with an average accuracy of 77.5%) and our model (with an average accuracy of 77.5%) achieve comparably better detection accuracies for both upper and lower facial occlusions than AAM (with an average accuracy of 58.13%) and CLM (with an average accuracy of 63.75%). GN-DPM outperforms our model when LFW is used whereas our model outperforms GN-DPM when the transformed occluded images from the CK+ database are used. Overall, our model achieves a slightly lower average accuracy of 83.75% compared to GN-DPM with an average accuracy of 85% for landmark detection for upper facial occlusions and a slightly higher average accuracy of 71.25% than GN-DPM with an average accuracy of 70% for landmark detection for lower facial occlusions. Fig. 11 shows example landmark detection results for images with occlusions from the CK+ database.

Since Gabor filter and BRISK are rotation invariant facial feature detectors, the proposed feature detection model of this research shows great capacity in dealing with images with rotations and pose variations. After applying Gabor filter + BRISK + ICP, it is able to produce 54 landmarks for each test image. Evaluated with 400 randomly selected images with pose variations from LFW, PUT, LFPW and Helen with 100 images from each database, our model achieves 81% landmark detection accuracy and outperforms the other three supervised methods. Table 4
                            illustrates the detailed evaluation results for images with rotations. Again, the GN-DPM model has outperformed our approach when tested with LFW whereas our model performs better than GN-DPM for PUT and Helen databases for rotated images. Overall, without any prior knowledge of the application domain or training conducted with images with occlusions or rotations, the proposed unsupervised facial point detector shows impressive adaptability and robustness, achieves competitive/highest average accuracies and outperforms the three supervised models for majority cases in the experiments conducted.

Our facial point detector also shows optimal computational costs during the evaluation. Based on the experiments conducted across the five databases, the averaged processing time of one facial image for our facial point detector is 80 ms, respectively 124 and 59 ms faster than the performance of AAM and CLM. Our model also shows slightly better computational efficiency than GN-DPM. The detailed average processing time of one facial image for the four models is listed in Table 5
                           .

For AU intensity estimation, as discussed earlier, we have employed 18 SVRs and 18 NNs to measure the intensities of the 18 selected AUs respectively. 200 FACS coded peak emotional images of the seven emotions and 50 neutral images obtained from the CK+ database are used respectively for the training of both of the 18 SVRs and 18 NNs. Since CK+ does not provide AU annotations for neutral images, we assume all the AUs are not present in the neutral images with ‘0’ assigned to each AU in the training stage. Then 127 images for the seven emotions and 30 images from the neutral category from the CK+ database are used to evaluate the performances of SVR and NN based AU intensity regression. We also automatically extract 54 landmarks for each of these test images using our facial point detector. The landmark points of corresponding upper and lower facial elements are then used as inputs to the SVR and NN based AU intensity estimation. MSEs of the intensity estimation for the 18 AUs are also calculated for SVRs and NNs based on the comparison with the ground truth annotations provided by the CK+ database. The detailed results of MSE for both SVRs and NNs are provided in Table 6
                        . (The
                         last column in Table 6 also shows the MSE differences between each pair of SVR and NN.)

As shown in Table 6, the SVR-based intensity measurement outperformed the NN-based method. In comparison to the NNs, the SVR-based approach performs well for the intensity measurement for the following AUs: AU1, 2, 4, 5, 6, 10, 12, 14, 15, 17, 20, 23, 25, and 26/27, while it needs improvements for the intensity measure for AU7, 9, 16 and 24. For both SVR and NN based regression, MSE for the following AUs is less than 0.05: AU4, 5, 6, 7, 10, 12, 14, 15, 16, and 20 with AU17 posing the maximum MSE for both approaches. We also produce the evaluation results based on the ‘presence’ or ‘absence’ of each AU based on the comparison of the results produced by SVRs and NNs and the original database annotations. The SVRs show great performance (more than 80% detection accuracy) for the detection of the following AUs: AU1, 2, 4, 5, 9, 12, 14, 17, 25, 26/27, while the NNs perform well for the identification of AU1, 4, 5, 12, 17, 25, and 26/27.

The estimated intensities of the 18 AUs for each of the 157 (127 emotional + 30 neutral) test images from CK+ are then used to detect the seven basic emotions and neutral expressions. The evaluation results of the fuzzy clustering based emotion recognition are provided in Table 7. Generally each emotion category is reasonably recognized. Among the eight emotions, surprise and happiness are reliably detected respectively with 98% and 97% accuracies followed by well detected disgust with 95% and neutral with 93% accuracies. Sadness, anger and fear are reasonably recognized respectively with an accuracy of 90%, 85% and 85%, while contempt is with the least recognition accuracy of 80%.

We have also compared our results with other well-known image-based emotion detection applications. For example, Shan et al. [27] employed Boosted-LBP based SVM to recognize six basic emotions and neutral with evaluation conducted using images from CK+. Jain et al. [67] presented a facial emotion recognition system from video sequences using Latent-Dynamic Conditional Random Fields. Their model performed temporal modeling of shapes with evaluation conducted using image sequences. Wu et al. [68] employed spatio-temporal Gabor filters for automatic facial emotion recognition. Because of the above three related developments’ state-of-the-art performance and the employment of the same database (CK+) for evaluation, we select these applications to compare with our research to further evaluate our system’s efficiency. Moreover, our previous research [12] employed the 31 facial points generated by NAO’s vision APIs to detect 17 AUs and the six basic emotions from CK+ database images. We also employ this previous development for system comparison in order to prove the efficiency of the new development. Therefore, Table 8 shows the comparison between the above four related applications and this research.

Generally, our system outperforms all the above related research, i.e. spatio-temporal Gabor filtering based facial emotion recognition [68], the approach of Latent-Dynamic Conditional Random Fields [67] and the Boosted-LBP based SVM [27], especially for the recognition of anger, sadness and neutral. In comparison with the Boosted-LBP based SVM [27], our research focuses on eight-class emotion recognition and shows comparatively more stable performance for the recognition of all emotion categories. It also outperforms our previous work [12] significantly, which was conducted purely based on the original vision APIs of the robot. The above comparisons also further indicate the efficiency of the proposed facial point detector, regression-based AU intensity estimation and the clustering based emotion recognition.

Although we discuss the employment of image databases for the evaluation of each core function of this research to prove its efficiency, NAO is also able to perform real-time facial point detection and emotion recognition from real testing subjects with pose variations (at least up to 60 deg), scaling differences, and partial occlusions, etc., during natural robot human interaction. Especially 30 posed compound facial expression data are gathered from five real subjects aged between 25 and 35 with each subject contributing five images respectively for the following five categories of compound surprised expressions: happy surprise, angry surprise, fearful surprise, sad surprise, disgusted surprise and surprise. Evaluation results show that pure surprise emotional expressions achieve the highest 90% recognition accuracy followed by happy surprise of 88% detection accuracy, fearful surprise with 83%, angry surprise with 81% and 80% accuracy for disgusted surprise. Sad surprise has the lowest result of 75%. We notice that some compound emotions, e.g. happy/angry/fearful surprised expressions, tend to have comparatively stronger physical cues than other compound emotions such as disgusted/sad surprise emotions. Therefore, happy/angry/fearful surprised emotions are better recognized than disgusted/sad surprise indications. In future work, more evaluation with real testing subjects will be provided to further prove the system’s efficiency.

@&#CONCLUSIONS AND FUTURE WORK@&#

In this research, we have developed an unsupervised facial point detector (a rarely explored topic), regression-based AU intensity estimation and emotion clustering for the recognition of the eight basic and compound emotions from posed and spontaneous facial expressions. They are integrated with NAOs C++ SDK under Ubuntu to benefit real-life robot human interaction. The proposed facial point detection model is able to perform robust landmark extraction from images with illumination changes, head rotations, pose variations, scaling differences, partial occlusions and background clutter. Our facial point detector has achieved an averaged accuracy rate of 80%, 73%, 78%, 85% and 85% respectively for the evaluation of 200 diverse images taken from CK+, LFW, PUT, LFPW and Helen databases. On average, it has outperformed AAM and CLM respectively by 13% and 9% and achieved comparable performance in comparison to the state-of-the-art supervised model, GN-DPM, for facial point detection across the five databases with posed and spontaneous facial expressions. It also has optimal computational cost and is significantly fast than AAM and CLM with comparable computational costs to GN-DPM. Moreover, the AU intensity estimation and emotion clustering are also evaluated using images from the CK+ database. The SVR-based AU intensity estimation outperformed the NN based method. The average MSE of the SVR-based intensity estimation for the 18 AUs is 0.0397. In comparison to NNs, the SVR-based method performs well for the intensity measurement for the following AUs: AU1, 2, 4, 5, 6, 10, 12, 14, 15, 17, 20, 23, 25, and 26/27. Moreover, FCM clustering also not only enables the robot to recognize the seven basic emotions and neutral expressions but also shows great potential to detect compound and newly arrived novel emotion classes. Evaluated with CK+, it achieves an average accuracy of 90.38% for the recognition of the eight basic emotions. It also outperforms other state-of-the-art research in the field. Initial experiments also indicate its efficiency in tackling compound emotions with an average detection accuracy of 82.83% for surprised compound emotion recognition.

In future work, we aim to extend our system to deal with emotion recognition for 90-deg side-view images of spontaneous expressions, which pose challenges to many state-of-the-art applications. We also aim to employ other techniques (e.g. Grey-level Co-occurrence Matrices and evolutionary optimization [69]) to extract appearance deformations embedded in textures to inform affect analysis when geometric features are not able to provide a thorough full view of emotional behaviors. Compound and spontaneous emotional expressions from databases and more testing subjects will also be employed to further evaluate the system’s efficiency. Min-margin based active learning techniques will also be explored to deal with emotion annotation ambiguity of the clustering results of FCM for challenging real-world emotion recognition tasks. Moreover, according to Kappas [1], human emotions are psychological constructs with notoriously noisy, murky, and fuzzy boundaries. Therefore, in the long term, we also aim to incorporate affect indicators embedded in body gestures and dialogue contexts with facial emotion perception to draw a more reliable conclusion on affect interpretation to better deal with open-ended challenging robot human interaction.

@&#REFERENCES@&#

