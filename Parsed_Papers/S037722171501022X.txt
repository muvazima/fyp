@&#MAIN-TITLE@&#A data collection and presentation methodology for decision support: A case study of hand-held mine detection devices

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We develop and apply a methodology to characterise hand-held detection systems.


                        
                        
                           
                           Methodology, which incorporates field trials and workshops, allows scrutiny.


                        
                        
                           
                           Robust data presentation technique was developed to facilitate common understanding.


                        
                        
                           
                           Methodology and presentation satisfies the needs of multiple stakeholder groups.


                        
                        
                           
                           Can be applied to future detectors allowing direct comparisons to be made.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

(T) OR in defence

Decision analysis

Problem structuring

Field data collection

@&#ABSTRACT@&#


               
               
                  Faced with a short turn-around request to characterise several hand-held mine detection systems the authors developed and applied an analytical methodology that was sufficiently robust and pragmatic to satisfy the needs of the various military stakeholders involved yet it was appropriately rigorous and transparent to bear external scrutiny. The methodology can be applied in situations where data collection and analysis must be done quickly while preserving scientific veracity. For mine detection systems considerable uncertainties existed that needed to be characterised including: application, location, operational situation and involvement of human operators. Constraints on the time and expertise available implied there would be difficulties ensuring a sufficient number of trials could be conducted to levels of statistical confidence that would assure appropriate credibility across all of the parameters. This problem was effectively rectified through experimental design and by heavily involving the sponsor stakeholders and subject matter experts throughout the study thus boosting the credibility and acceptance of its results. The process followed involved: liaison with the sponsor, identification of critical issues, measurements in field environments, reporting mechanisms and discussion on implementation and further development. The critical focus was operational capability rather than specific equipment characteristics. A robust data presentation technique was developed to deal with the complexities associated with different needs of multiple stakeholders. This technique enabled the results to be reviewed from different stakeholders’ perspectives, the formation of a common understanding and the results to be reusable in future analyses.
               
            

@&#INTRODUCTION@&#

Trials of military equipment play a critical role in the selection (acquisition) of new equipment and its entry into service. Such testing may form part of an overarching methodology incorporating workshops, theoretical models and trial data, thus combining the technological characteristics of the equipment (in this case mine detectors) with the practicalities of their use. Ideally, any methodology would draw upon the principles and practices of Operations Research (OR) and take a holistic view, i.e. structuring the problem, guiding the data collection, providing context to the results and reporting findings (Howick & Ackermann, 2011; Mingers & Rosenhead, 2004; Ormerod, 2006; Ormerod, 2010a; Ormerod, 2014b; Rouwette, 2011; van Antwerpen & Bowley, 2012; von Winterfeldt & Fasolo, 2009; White, 2009). While OR has supported military commanders and forces since its earliest days (Copp, 2000), the typically sensitive nature of such support means there can be limited opportunity to share contemporary work and observations with the broader OR community (Ormerod, 2014a). There may also be limited independent, external peer review before delivery to the client. In this paper we use a recent Australian military OR study of hand-held mine detectors to illustrate the methodology we developed where techniques from OR were suitably combined with technical specialists and military practitioners to understand a complex situation. Such an approach enabled us to resolve operational concerns, and deliver advice for military decision making and planning. The problem space can be summarised as finding a suitable methodology to gather relevant data, including from field trials, to guide future decisions.

The design and conduct of studies incorporating realistic trials of military equipment present the following challenges: uncertainty (e.g. where and why the data would be used?); variability (e.g. operator dependence and specific location effects); and opportunity (e.g. availability of equipment and skilled users). We contend that such studies lie in the centre of Ackoff and Pidd's familiar “puzzle, problems and messes” model (Ackoff, 1979a, 1979b; Pidd, 2004) since it is broadly known what the equipment will be used for but it is not possible to reduce the analysis to purely quantifiable terms (Wijnmalen & Curtis, 2013). In the case of mine detection some of the variables include: type of mine (e.g. level of metallic content); soil type (e.g. level of magnetic susceptibility); conduct of the operation (e.g. render safe, avoidance or removal by remote clearance methods); operational context (e.g. hostile or benign environment); and operator effects (e.g. skill and fatigue levels). This is a “problem” since any study will have to rely upon: identification of vignettes; selection of a limited number of representative soil types (an enormous number could potentially be examined); design and manufacture of a set of inert surrogate mines (to reflect the variety of mine types that can be encountered); comparative terms being defined (e.g. representation of probability of detection); and judgements being made of the relative importance (weightings) of each issue. Such an approach needs a cooperative relationship between the analysts, the study requesters and the equipment and resource providers (Rees & Curtis, 2013; Wijnmalen & Curtis, 2013).

An Army General directed that a number of hand-held mine detection systems be characterised and compared across a range of conditions and circumstances. Such detection systems are currently integral to carrying out military tasks involving the discovery of improvised explosive devices (IEDs) (Winter, Meiliunas, & Bliss, 2008). Typically such devices are either metal detectors and/or ground penetrating radar. Operation is relatively simple: the hand-held detector is swept across the ground and the operator is typically alerted to any anomalies detected, such as ferrous metal, through a cue or set of cues. The operators are often using these hand-held detectors in hostile environments where in addition to the presence of IEDs they may be subject to direct attack by an enemy. In this study we did not focus on the internal technical mechanisms of the detector but rather its operational effectiveness. Consequently, a number of convolving factors arise including: operational appreciation; human-operator interaction; and technical performance. This presents a challenge in determining the scope of the study and delivering findings to stakeholders in the time available.

Another study aspect was how to coordinate the contributions of the three interacting groups: the sponsor group, the study group and the scrutiny group. Each group occupies a space within the system defining the study problem space as shown in Fig. 1. (i) the Australian Army (represented by the military point of contact); (ii) the multi-disciplinary analyst community (including both participators and scrutineers); and (iii) the various technical specialists who are experts in the technologies involved and are responsible for the considerable effort of designing any trials and making the measurements. The military stakeholders (the sponsor group) can have various ranks from private through to a general. The operational user community – consisting of soldiers whose lives depend upon the devices – are mostly focused on current and near-term use of such devices. This results in very different perspectives of the type of findings of such a study and how useful they are. The ‘temporal’ nature of the user community in Fig. 1 is associated with when and where they were deployed, the nature of their missions, what threats they faced and what devices were provided. For the analyst one result of this broad user community is the range of perceptions of the relative abilities of the different devices but limited clarity on the reality of these perceptions, many of which are implicit.

There is considerable literature on Problem Structuring Methods to access tacit knowledge or use existing data, particularly for analyst/stakeholder discussions or workshops (Ackermann, Howick, Quigley, Walls, & Houghton, 2014; Barford, 2012; Bell, 2012; Espinosa & Walker, 2013; Franco, 2013; Schuwirth, Reichert, & Lienert, 2012). There is unfortunately limited material that includes military field practices to gain previously unknown data (Copp, 2000; Curtis, Rees, & Hobbs, 2013; Hobbs & Curtis, 2011; Rees & Curtis, 2013). This is particularly acute for complex and uncertain problems such as preparation for future (unknown) military actions.

There has also been a call for more exposure of procedures and experiences in actual implementation of OR practice (Ormerod, 2014a) and the sharing of case studies. In this paper we report not only on the methodology, but also the experiences and observations from the completed case study. In this respect, we concentrate on the study itself and use simulated results as illustrations to accompany the reader though the journey, rather than presenting a mine detector based study per se which would be of limited value to the broader OR community. Importantly, the methodology reported here is pragmatic and based on sound OR practice (Keys, 1998; O'Keefe, 2014; White, 2009). We believe it is thus highly applicable to other Defence, paramilitary or emergency services studies, especially where there is an expenditure of public money, and thus public scrutiny, for selection of new equipment.

The remainder of the paper is structured as follows: we first present in Section 2 the initial interpretation of the context of the request, we then discuss in Section 3 the framework adopted and the methodology applied. In Section 4 we discuss the conduct of the framework components while in Section 5 we present the indicative data presentation methodology. A concluding discussion is presented in Section 6.

Previous work has described how equipment trials can be considered as comprising two discrete aspects: “collecting the right data” and “collecting the data right” (Rees & Curtis, 2013). Thus the elements derived from an understanding of the system coupled with the requirements of the study sponsors and the analysts’ interpretation, led to identification of the nature of the data that needs to be collected. Naturally constraints are imposed by timelines and availability of the facilities, technical expertise and skilled users, leading to the need to make suitable compromises as to what can be collected (O'Keefe, 2014; Ormerod, 2006), e.g. flexibility with the scheduling of events which may not be able to be conducted in an ideal sequence, training requirements, actual versus planned number of participants available, etc. It is important that thresholds be established and understood as to what level any changes would impact the integrity of the results and if a planned activity will meet its data integrity requirements.

We also comment that for a “problem” such as mine detection that there is no single “right” way of doing things; a study should meet the sponsor's needs for “validity, credibility and acceptance” (NATO RTO SAS-087, 2012a, 2012b; Wijnmalen & Curtis, 2013). In judgement-based analysis such as this, agreement needs to be reached on what is measured and how. Thus for instance, specific vignettes need to be identified as representative of actual operations which cover the likely spread of the analytical landscape. From a practical point of view, the balance of breadth and scope (many discrete single measurements) to depth (many measurements of the same data point) and reporting timeliness should also be agreed. Critically, the issue is predominately one of perception rather than inherent, in the sense of absolute, correctness i.e. what is acceptable to the decision-making stakeholders? Finally, we note that such a study must be compatible with the mutual responsibilities of the participant groups identified and be agreed at the outset. For example: the Australian Army sponsor provides equipment and skilled operators; the technical experts provide instrumented ranges (or other equipment); and the analysts deliver a report incorporating the resulting analysis that is of utility to the sponsors (NATO RTO SAS-087, 2012a, chap. 4). Adherence by the participant groups to an agreed program of work, including “visible” and “invisible” features (NATO RTO SAS-087, 2012a, chap. 8) such as a shared goal to protect soldiers from danger, would lead to an enhanced final outcome for the sponsor (NATO RTO SAS-087, 2012a, 2012b). NATO, along with additional nations, has recognised the importance of judgement-based analysis and have codified aspects of it in
NATO RTO SAS-087 (2012a) and
NATO RTO SAS-087 (2012b) as a guide to what is considered best agreed practice for military matters.

Our approach to this study was based on the steps outlined in Table 1 with an expansion of step two outlining the questions that we posed to ourselves in framing and designing the overall campaign of activities. These considerations are somewhat similar to a number of those listed in Table 4 of
O'Keefe (2014) (broad activities: 1. Problem identification and motivation, 2. Define the objectives for a solution, 3. Design and development, 4. Testing and demonstration, 5. Evaluation, and 6. Generation of Theory) and while not the subject of this paper, can be linked to the concept of OR competences (Ormerod, 2010b, 2010c) (broadly: 1. Conducting analysis, 2. Designing and managing, and 3. Appreciating context). We also note that we have extended this list to include the practical aspects of reporting and delivery of results to the stakeholders. By addressing these considerations there is increased confidence that the study will be both sound and useful to the sponsor as well as providing the analysts with multiple lenses to aid in framing the problem and potential approaches.

By using OR techniques we have been able to support military decision making for hand-held detectors in general, rather than constrained to addressing a specific hand-held detector concern; providing a “data pack” of information to aid future decision making including for a range of unknown futures. Thus we present a study whose results have supplied useful data that was applied to supporting current operations as well as planning future operations as they arise and whose current details are unknown (e.g. location, threat types etc.).

The study being reported occupies a specific position in the field observation spectrum and can be properly described as a trial or experiment (to learn something) rather than an exercise (to practice something) (Curtis et al., 2013; Kass, 2006; Labbé, 2006; Uchida, 2002). Studies such as this lie in the mid-range of the complementary factors of analytic control and realism (Curtis et al., 2013) (rigorous experimental conditions for selected examples obtained in a non-operational environment). Probably the most pertinent feature is that the operating environment for testing is benign rather than hostile. Although benign, it is assumed that the detectors will be used in a similar manner on the trial to how they would be in actual operations, even though the context will be different. This approach is appropriate for a comparative characterisation study such as this, though it must be accepted as such by all involved in the study.

The study ultimately consisted of a number of phases undertaken by a combined workgroup comprising a number of separate multi-disciplinary teams of analysts, technologists and Australian Army personnel who conducted different components. Many of the analysts and technicians involved in the workgroup had experience in trials and knowledge of hand-held mine detectors and so could be considered to be in “expert mode” while the complexity of the interactions within the stakeholder community meant that aspects of a facilitated modelling approach were also adopted to refine the study space (Franco & Montibeller, 2010).

The sponsor direction that forms the basis of this case study is situated within an ongoing set of uncoordinated time-constrained requests for technical performance information on individual mine detectors as technologies and threats evolve. This introduces a potential problem. When the acquisition of military equipment is not driven by short time imperatives, a considered deliberate process involving context refinement, studies, simulations and trials can be applied and many potential factors explored before a final decision and selection is made (van Antwerpen & Bowley, 2012). In the context of supporting deployed personnel on active operations the acquisition of military capability is driven by short timelines to respond to a dynamic and life threatening environment. While technical assessments of actual equipment should routinely be undertaken, it is often under very specific constraints that are focused on the current situation so as to address immediate concerns. As a number of iterations of this process occur, many new capabilities are introduced to meet specific needs resulting in a growing inventory of capabilities and associated test results collected under varying constraints. An additional consequence of the rapid insertion of new equipment and capability is the generation of a new user community for each instantiation. Finally we note that there have been both ad hoc (to solve an immediate problem) and rationalist (to reduce the number of types of devices for logistic and training purposes) drives to the problem. Taken together this implies that there is potential for misuse of the evolving data stream, with four outstanding features:

                           
                              •
                              Danger in comparison of discrete measurements from widely different experiments;

Differing understanding of the issues according to background (e.g. planners, trainers and users);

The changing nature of the technology and threat environment led to different experiences for the military and technical specialists (i.e. when they were involved); and

The importance of perception by some of the user groups gained through experience under stressful conditions (Müller, Groesser, & Ulli-Beer, 2012; White, 2009).

This has also led to groups of end-users with different backgrounds and experiences, represented on the bottom left of Fig. 1.
                     

Thus when this detector assessment arose with the additional need to compare to other detectors within a short timeframe it was also an opportunity to undertake a review of the previous uncoordinated work to understand the issues, essentially a precursor or “step 0” before conducting the steps in Table 1. It was deduced that a framework was needed to: undertake technical testing for direct comparisons now and into the future; elicit many of the implicit factors present within the various detector user communities; present the results in a manner to enhance a common understanding of the data-set amongst the detector user community; and aid the multiple stakeholders in their decision making. The framework detailed in Section 3.4 would also provide a foundation for consistent testing of any subsequent request(s) for detector assessments as technologies and threats evolved.

Analysis of previous work and stakeholder issues led to the formulation of the framework, development of an initial data collection regime and the formation of an initial plan which was taken to the stakeholder point-of-contact (effectively an ad hoc problem structuring exercise). The key features were: an agreed approach for characterising detectors; the development of an agreed framework to guide discovery of discriminating factors; construction and field trials; data collection and reporting; a contextual evaluation activity; and an independent to those who undertook the work – although internal to the organisation – review of all previous work. While not all aspects of the proposed framework were able to be applied in the short time available, nevertheless based on the statistical guidelines set by the various technical specialists, sufficient data was obtained to address the detector comparison requirement. A comprehensive fit-for-purpose report, as judged by independent although internal OR specialists, was produced for the sponsor group (Fig. 1) allowing senior decision makers and users to make immediate informed capability acquisition choices. Feedback from the sponsor group indicated that the report addressed the detector comparison requirement.

The sponsor of the overall follow-on request was the Chief of the Defence Force (CDF, a four star officer) who directed that a comprehensive characterisation activity of current and potential near-future available mine detectors be undertaken using the framework. In keeping with military procedure he gave broad intent pertaining to the kinds (but not numbers) of mine detectors, the potential targets (but not exact types), the likely areas of operation (but not explicitly detailed), some of the factors of interest (but not all) and the deadline for delivery of the report (non-negotiable). The uses of the study were also potentially broad though it was noted that both current operational issues (which equipment to use on operations) and future acquisitions (which detector equipment to buy) were implicitly involved in the request, along with a need for a thorough characterisation of mine detection.

Specific factors of interest listed by the sponsor included probability of mine detection, interoperability with other electronic equipment and a range of human factors, with an emphasis on understanding what the discriminating factors between detectors might be and to make as many implicit factors explicit as possible. Importantly no success criteria or measures were to be applied to any data collected but rather the data collated into a common reference source for the stakeholder community to draw upon. This appreciation formed “step 1” of Table 1.
                     

The sponsor directed various parts of the Australian Defence Force, in particular parts of Australian Army, to refine the requirements and requested that DST Group lead and undertake the analytical program based on the previous work. The commitment from the Australian Army was to provide equipment, subject matter experts (SME), skilled operators and continual feedback on the progress of the work. The analysts interpreted the study sponsor's guidance to prepare a program of work that would allow differentiation of the detectors under likely operational conditions using the previously articulated and agreed characterisation framework, i.e. this interpretation coupled with previous understanding developed during “step 0” formed the foundation of “step 2” of Table 1.
                     

The sponsor direction was broad and required refinement before the workgroup could progress. In addition, the technology experts needed to be consulted to determine what was possible to be measured and what was expected, as well as the duration and access to subject matter experts and skilled operators. In providing broad direction, the sponsor also nominated the principal parts of their organisation that were to provide further information to refine the boundaries of the requirement.

Although the characterisation framework was endorsed and agreed in the overall methodology there remained a number of challenges for the analyst. In particular within the study sponsor's nominated stakeholder community there were legacy issues associated with diverse interpretations of the results of reporting on previous requests as well as a number of different views as to what was considered within the scope of the study sponsor's current intent. Furthermore, a number of the stakeholders had similar levels of authority and accountability. Members of the stakeholder community also made a number of assumptions with regards to the funding and capacity of the analyst and technical communities to undertake the volume of expected work within the specified non-negotiable time frame.

An early priority, given the range of stakeholders and strength of opinion on what was meant by the sponsor's intent, was to clarify who was considered the study results recipient (referred to as the study custodian on behalf of the CDF). While the analyst community had no input into the decision of who was the study custodian, the lead analyst was active in precipitating and guiding meetings as needed as well as identifying and seeking active participation from all the stakeholders to commence the clarification of requirements (Müller et al., 2012). Ultimately the study custodian was acknowledged, confirmed and agreed amongst the stakeholder community as being the capability manager who took advice from the various identified stakeholders and became accountable for the final choices that were made in shaping the study context and direction. An early indication from the study custodian was reinforcing that the analysts were not to provide any opinions or recommendations of which detector might be best but to have a robust, repeatable and defendable methodology to collect and then report results thus leaving the custodian and the stakeholder community to make decisions and informed capability choices now and into the future.

Concurrently it was also necessary to undertake the preliminary scoping of what were the types of detectors, targets, environments and factors of interest that were to be examined. The technical specialists were consulted to gain an understanding of: (i) the timeframes and scale of effort per detector per environment; (ii) how many and what types of environments were possible in the timeframe; and (iii) an estimate of the number of trained detector operators needed.

With basic information obtained a stakeholder activity was conducted that resulted in: an agreed and later endorsed project plan including timeline, constrained parameter space, expected key activities, and resource estimates including personnel requirements (which formed the agreed program of “step 3” in Table 1). The project plan provided the basis for a common understanding and was also used to maintain focus within the dynamic group during the study. This focus was important as there were a number of external factors that sought to expand or adjust the program potentially threatening its consistency or timeliness. Subsequent regular communication throughout the study secured resources, access to expertise and defused misunderstandings early. A level of skill is required to anticipate possible friction points and resolve them before they escalate within or across the various communities.

As mentioned detector evaluation requests were a regular occurrence although at random intervals and with variable constraints. To address the expected future requests as well as the immediacy of another short-term requirement the following framework was developed, illustrated in Fig. 2
                        , consisting of six major components:

                           
                              
                                 
                                 
                                 
                                    
                                       1. Threats:
                                       The expected range of threats that the capability type is expected to perform against. E.g. mines, typically ranked by an agreed set of physical characteristics.
                                    
                                    
                                       2. Environments:
                                       The expected physical regions, conditions and circumstances a capability type is to perform in. E.g. the range of soil types a detector is expected to find mines in and the range of vignettes it is employed.
                                    
                                    
                                       3. Operational evaluation:
                                       Using the range of vignettes and conditions to determine what the generic characteristics the capability type is expected to address with an indicative relative importance weighting. E.g. for a detector the importance of detection performance, interoperability, weight, operation time etc.
                                    
                                    
                                       4. Technical performance:
                                       Using the environment and threat context to obtain physical performance information for the capability type under optimal conditions. E.g. the probability of detection and the electronic interoperability distances for a detector under controlled conditions.
                                    
                                    
                                       5. Human and Design:
                                       The ergonomic parameters of the capability type and how it interacts with a human operator. E.g. for a detector the dimensions and weight, how it provides feedback to the operator etc.
                                    
                                    
                                       6. Logistics and training:
                                       The factors associated with operationally maintaining the capability type and its associated training standards. E.g. for a detector, the number of and type of batteries, stowage requirements, frequency and intensity of training to maintain currency etc.
                                    
                                 
                              
                           
                        
                     

The above components are presented as a numerical listing for convenience; it is not to convey any sense of procedural or priority order as a number of the components can be conducted in parallel. In general a specialist team or combination of teams was formed to take carriage of and undertake each of the components or sub-components, depending upon the complexity of the expected data collection.

The first and second components (Threats and Environment) provide the physical and vignette-defined boundaries of the parameter space to be explored by setting the threat and environment range limits. These two components are foundational to the conduct of the remaining components forming the basis from which data collection activities will be conducted. Thus, these two components need to be agreed by the study sponsor as being suitably representative noting that in general it will not be possible to fully span the entire parameter space as the number of possible permutations is too large. With these two established once, they form the basis for repeatability for the current study and any future characterisation efforts. If the stakeholder community revisits and adjusts either or both of these components, then care will need to be taken for any subsequent comparison of past efforts with future efforts. It is important to note that these aspects are owned by the sponsor and not the analyst.

Once the first and second components are established, the third component (Operational Evaluation) is preferably conducted next to determine and provide a ranked set of generic characteristic descriptors for an ideal instantiation of the capability type within the agreed vignettes. These characteristics describe what the capability type must ideally address. The ranked list of characteristics then form the basis for subsequent data collection and reporting activities conducted in the other components. To some extent this component provides a reality check to establish confidence that the “right” data is being collected. In general there is a strong linkage between the outputs of this component and the inputs for the Human and Design component. As with the first and second components, this component would ideally be conducted once and be considered valid for the life of the vignettes. If the stakeholder community adjust the vignettes then the activity would need to be repeated and care will need to be taken for any subsequent comparison of past efforts with future efforts.

The fourth, fifth and sixth components (Technical Performance, Human and Design, Logistics and Training) are generally able to be conducted independently and potentially in parallel. The focus of these activities is to obtain data for each of the capability types. This data will form the basis for discriminating between the different options. The Technical Performance component will in general be trial intensive in its efforts to gather physics-constrained data on information such as probability of detection (Pd
                        ) and interoperability distances with other electronic equipment. The Human and Design component will also be trial intensive but will involve a level of subjective data collection in its efforts to characterise the interaction of the capability type with its human operators. The Logistics and Training component may involve its own separate activities or analytical study. These components then would be repeated under the same set of testing regimes as required for each new capability type and/or mode that is being considered. All of these features need to be scrutinised to provide confidence that the data is collected “right”.

The overall objective for the analysts within the framework was to gather information on as many factors as possible that can provide potential discrimination between similar capability types i.e. what are the set of characteristics that uniquely differentiate the detectors? A complementary objective was to make as many implicit factors between different user communities explicit to facilitate a common understanding for decision making by the capability manager. The characterisation framework was endorsed by the capability manager and their senior chain of command. In endorsing the characterisation framework, the capability manager also provided endorsed vignettes thus establishing the preliminary boundary for the threat and environment components.

In addition to the articulated framework components, there are three additional constituents of the study, also illustrated in Fig. 2, which were implemented to maintain as well as facilitate the integrity, delivery and reporting. The three constituents are:

                           
                              
                                 
                                 
                                 
                                    
                                       A. Governance:
                                       Setting the overall objectives in consultation with the stakeholders; facilitating the delivery of all of the components; ongoing interaction and communication with the stakeholders and amongst the teams; team formation; and provision of oversight.
                                    
                                    
                                       B. Scrutiny:
                                       Integrity of the overall process and the final reporting as being fit-for-purpose in addressing the objectives; this can involve a level of independent review.
                                    
                                    
                                       C. Reporting:
                                       Synthesis and production of the final reporting products, be they presentations or formal written reports.
                                    
                                 
                              
                           
                        
                     

While these three additional constituents have been separated out in the discussion here, practicalities may see them combined into one Governance–Scrutiny–Reporting team as happened in this case. They describe the roles carried out by the “analyst coordinator” and “external scrutiny” elements of Fig. 1.
                     

In addition to the points made above, the Governance component also provides the means to raise issues and the authority to address them including potential failure points, errors or problems. The Governance component works closely with the Scrutiny personnel to maintain the integrity of the outcomes of the program. For example, should an activity need to be modified due to insufficient personnel of suitable skills attending, then options such as providing additional trained personnel, rescheduling the activity or cancelling etc. would be considered and an assessment made as to its impact upon the quality of the results of the overall program. Thus Governance provides an active feedback loop into the conduct of the program (“step 4” of Table 1). Furthermore, it provides a means to maintain (and if necessary reach) agreement on any changes that maybe necessary as well as what the impacts of changes might be upon the outputs. The Governance team would ideally have the lead analyst, senior representation from the sponsor and key individuals from the different components and would hold regular communication to assess progress, address issues and provide synchronisation. Support to the Governance could be provided by status tools such as Dashboards although in the case considered in this paper regular video-teleconferencing and weekly status emails from the lead analyst was sufficient within the time constraints.

The Scrutiny component provides oversight of the veracity of the process, its components and the fitness-for-purpose of the conduct and results. Ideally the oversight would be provided by personnel independent/separate to those undertaking the design and conduct of the activities themselves. In conjunction with the Governance component, the Scrutiny component provides a feedback loop to the study program to increase stakeholder confidence in the delivery of the program, especially in the case where the ideal post-conduct full academic peer review process would affect delivery timelines.

Using the characterisation framework with the endorsed targets and environments the agreed project plan for the case study consisted of the following components: (i) an operational evaluation workshop to establish a weighted list characteristics; (ii) technical testing activity to establish detection performance; (iii) technical testing activity to establish electronic interoperability; (iv) human factors activity to establish human interface parameters; and (v) consolidation of results and reporting.

Addressing each of these components involved the formation of a series of multi-disciplinary teams of technical specialists, analysts, operators and subject matter experts. Each team had a principal point of contact to coordinate their component and there was regular communication with the analyst coordinating the overall effort. The technical specialists established the minimum requirements for data collection and rigor for their respective components e.g. the minimum number of detector passes over a target for statistical confidence levels. There were tension points to manage as various resources, personnel availability and training challenges occurred which necessitated pragmatic and at times adaptive solutions to maintain the level of scientific rigor. While the form of the challenges and associated solutions are not the subject of this paper, an unsurprising observation for any OR specialist who finds themselves in the coordinating role of a large study is the need to be flexible including potentially cancelling activities and managing the repercussions if data integrity is compromised.

Recalling the framework outlined in Section 3.4, within this case study the Logistics and Training component was not explored by a dedicated multi-disciplinary team. Within the timeframe and resources (personnel) available and the priorities indicated by the stakeholders, it was deemed sufficient to gather information on Logistics and Training concerns via insights collected during the operational evaluation and the human and design components.

The remainder of this section (“step 4” of Table 1) discusses the instantiation of the various components of the project plan in this case study to provide the practitioner with some insights on the challenges of conducting an activity of this scale.

The purpose of the workshop was to conduct the operational evaluation through identification of the key issues (Cronin, Midgley, & Jackson, 2014) by using a structured process to guide facilitated discussion of the initial and potential additional vignettes. Ideally a preliminary characteristics workshop would be conducted before major work was undertaken and this would guide both the overall direction and fine detail of the study. However, availability of participants led to this being a parallel activity designed to verify the assumptions made in the initial request, to flesh out the issues of concern in more detail and identify any aspects that were not considered in the initial appraisal.

The workshop followed the System Instantiation Comparison Method (SICM), based on Soft Systems Methodology, to develop a set of assessment criteria based on operator proposed vignettes (Rees & Bowden, 2007) and followed the inclusive facilitation guidelines and philosophy espoused and described in Table 2 (the Being Engaging Contextualising and Managing matrix) of Bell & Morse (2013). Ten participants with operational experience with ranks up to Australian Army captain were led by two analyst facilitators. The major (visible) outputs of the workshop were:

                           
                              •
                              A review of the vignettes;

Identification of “key characteristics” – items that need to be considered in any comparative evaluation; and

Priorities of the “key characteristics” calculated for each vignette.

An appraisal by the participants suggested that the range of the original sponsor endorsed vignettes was not as broad as it might be, given their operational experiences. As a result a further vignette was proposed to allow greater differentiation of the characteristics. While this additional vignette was not formally endorsed it was used in conjunction with the endorsed set and two sets of weighted characteristics were determined by a modified Analytical Hierarchy Process (Rees & Bowden, 2007), one for the endorsed vignettes only and another incorporating the participant's additional vignette. This was subsequently reported in the analysts’ write-up of the activity for the consideration of the study custodian.

The key characteristics, of which there was more than forty, were largely comparable with the initial assessment prior to the commencement of the workshop and confirmed the data collection regime for the wider study. Some absences were identified when comparing the characteristics list with the study program which related to support aspects such as logistics and training. While these would be part of a larger study they were not considered here due to time constraints furthermore the stakeholders agreed they had enough shared uncontested information against these characteristics to aid their decision making. Similarly, the weightings for the characteristics were noted but as they were found to be consistent with the initial guidance, and varied little across vignettes, no large changes were made to the program. The weightings, while presented to the stakeholders, were not used in distilling the final analysis to a single metric, as the analytic team concentrated on presentation of the raw results in such a way as to allow the stakeholders to perform the comparative analysis. The characteristics and their weightings did form part of the overall library of results for the study recipients.

The major outcomes of the workshop were less tangible than other parts of the study as they were not based on physical measurement. In particular, the workshop affirmed the analysts understanding of the system and both the needs and positions of the user group resulting in a shared picture (Curtis, Rajesh, & Moon, 2014; Hay, Curtis, Moon, & Lewis, 2013). Thus the inclusion of experienced operators, gave independent verification that the right measurements were being made and in the right situations. This triangulation of the basic precepts led to a higher confidence by the study stakeholders that a reasonable and practical course was being followed. Based on this workshop, some modifications of the human factors measurements were made allowing more focused data collection and more efficient experimental program with a condensed number of vignettes.

The initial sponsor direction specified that the detectors were to be characterised in areas of interest that were to be articulated by specialist advice from their organisation. In addition there was the analytical requirement to be able construct and conduct a repeatable testing regime across the potential environment (soil) types over large time scales so that any results would be able to be compared into the future rather than be indicative one-off outcomes. Incorporation of a range of environmental conditions, to some extent, countered the artificiality of the trial (i.e. non-operational) conditions thus pushing the focus towards a higher level of reality. The resulting dynamic discussions between the specialists, analysts and geologist reached the agreement that soil conditions for testing be governed by a regime of a series of fixed magnetic susceptibility ranges and rubble size distributions across the susceptibility range; these being the key controllable parameters for the soils and detectors of interest. While magnetic susceptibility is dominant for metal detectors it is less so for ground penetrating radar where electric conductivity is an influencing factor. For the purpose of detector characterisation the main priority is associated with metal detectors hence soil selection based on a regime of magnetic susceptibilities, however the electric conductivity of each soil type can be measured for understanding influences on ground penetrating radar (Takahashi, Preetz, & Igel, 2011).

The construction of a repeatable testing regime for soil conditions involving salt, moisture or humus is difficult and a topic for further research, although it is expected that such regimes would make it harder to find targets of interest. It is expected that with an appropriate selection of magnetic susceptibility ranges, rubble sizes and measurements of electric conductivity it should be possible to interpolate and predict maximum expected technical detection performance for the targets of interest in a specified soil type (Takahashi et al., 2011). There were four magnetic susceptibility ranges chosen for the testing regime to provide technical detection performance data that could be extrapolated to other regions of interest whose soil magnetic susceptibility is able to be established or estimated. With the susceptibility ranges agreed, specialist mine lanes were then constructed and filled with suitable soil materials.

In addition to the need to refine the range of soil conditions clarification was also required for which detectors were to be considered and what range of target types were of interest so as to achieve a representative and manageable number of permutations. Within the agreed set of detectors, a number have multiple modes, i.e. they can operate as a metal detector, ground penetrating radar or in various combinations of both with differing sensitivities. Each of these modes, for the purposes of technical performance testing, needed to be considered as a separate detector. While a number of the stakeholders desired all modes of all agreed detectors be characterised, consultation with the technical specialists (who had commenced the trial design) indicated which modes were considered essential (encapsulating other modes) and achievable within the timeframe and resources available. The smaller more manageable subset of modes was accepted by the stakeholder community. Similarly, various target types (mine surrogates) were identified as spanning the range and sizes of those likely to be encountered during operations. A manageable agreed set was established and suitable numbers then constructed for use in the mine lanes.

To determine the mine detector's ability to locate a target, the field trial was constructed in the following manner:

                           
                              i.
                              Various mine surrogates were buried in the different soil-type mine lanes for the duration of the trial in a random arrangement unknown to the operators;

The operators would use a specific detector in one of its modes to scan the lane;

When the operator believed the detector indicated the presence of a target a marker (coloured gaming chip) was placed and the operator continued scanning the lane; and

Upon completion of the lane scan the positions of the chips were measured with respect to a fixed point for later analysis and comparison with known target locations.

Several factors influenced the final number of markers positioned on the lanes. To determine what constituted a successful detection, those markers that were within a set distance of the known position of the target were deemed to be a successful detection. An aspect of this “success” distance was also based on the dimensions of the detector head. The trial design is similar to that undertaken in Takahashi et al. (2011) in which they also followed the four steps above although the Pd
                         is defined on a per-unique target basis in the work discussed here.

To reduce operator and random effects each lane was scanned by several combinations of operator and detector, with the lanes being raked over between serials. A standardised steel ball bearing on the surface at the start and end of the lane provided a check that the detector was working. As far as possible the operators moved about the detection lane site and were unable to see previous scanning. Measurements of non-controllable parameters such as moisture and temperature were carried out over the trial period during which there was no rain. Very little variation was recorded providing additional confidence in the integrity of the data collected.

In designing the detection performance trial, the specialists indicated the number of operators and skill sets required for the trial conduct. Operators were then allocated by the Australian Army and had different levels of skills and experience using the equipment and may have taken different times to return to a peak of user capability. The realities of trials such as these is that one has to accept that there will be operator learning effects, differing skill levels and fatigue as the trial progresses, but to do enough measurements to ensure that effects are minimised. Additionally, while requests for required minimum operator skill levels are made, it is not always possible to obtain so it is important to have a means of assessing skill levels prior to data collection and a means of adjusting the program to minimise impact on data collection integrity.

In designing the trial, the operators were formed into teams and allocated a subset of the detector modes that they were to operate along with a schedule for testing in the various lanes. Importantly there was also scheduled time for rest and recovery as this both allowed time for measurements to occur and the lane to be prepared for the next set of operators. There was also a disparity in size, weight and the amount of time required by the different detector modes to traverse the lanes. The operators were given guidance to take as long as needed rather than a time imperative as the purpose of the trial was to establish the optimum detector performance.

For improved confidence in the statistical robustness of the results the testing protocol was designed to have a minimum number of encounters per-target for each detector mode. It was planned that each mode would be operated in each soil type by at least three different operators and that each target would be encountered at least once by each operator in each soil type. In the case of this study each target was encountered a minimum of thirty-six times by each detector mode in each soil type (often more) and a total of more than 1780 target encounters occurred although many more data points were recorded. The lanes themselves were laid out in a field with no nearby buildings or other infrastructure to interfere with the measurements or to guide the operators of likely emplacements. In addition operators were instructed to ignore any physical (non-detector) indications of where a target might be located. Thus the Pd
                         results can be considered as maximum detections possible, since any other constraints such as operational circumstances would be expected to degrade the detector/operator performance.

In summary, the discussion between the sponsors, the technical specialists and the analysts led to agreement that this regime was suitable within the time available to give an overall comparison of the technical detection performance of the detectors in a reasonable range of operational circumstances. Data was then collected using the agreed approach and included sweep time, Pd
                         and false detection rate for each combination of test-lane and detector mode.

While equipment may be measured for its inherent technical characteristics it is always intended to be used in an operational sense. As detectors generally involve electronic equipment and are expected to operate within the modern electronic environment it is important to gain an understanding of any potential points of interference. Although modelling may be used to simulate actual performance some form of confirmatory testing or calibration is often needed. Thus, in the same way that representative vignettes were addressed for Pd
                        , a set of combinations of detectors and other forms of electronic equipment were examined in a technical trial. Given that what is desired is an understanding of any sources of electronic interference between the detector and operational equipment, a suitable electronically quiet trial area of appropriate size is needed to isolate any operational sources of interference.

In discussions with the operators and stakeholders, it was clear that a point of interest is the distance at which interference between the electronic environment and the detector is first noticed. It was not important to assess what impact, if any, the interference may have upon the detector's performance. As a result the trial was designed to determine when first interference was detected by the operator for each mode under various electronic environment configurations with distances determined associated with cardinal directions related to the operator i.e. front, back, left and right noting that front is for completeness rather than of interest to the stakeholder. In order to attain a level of statistical robustness, there were at least twenty approaches from each direction for each operator, detector mode and electronic scenario configuration. To minimise operator dependence each mode was operated by at least three different operators for each configuration. This resulted in the need to plan for the worst case of conducting many thousands of runs (e.g. for each mode and each scenario for that mode: 20 runs, 4 directions, 3 of same devices, 3 different operators using the device equals 720 runs; and there are multiple modes, devices and scenarios being examined). However through careful design the number of runs can be collapsed down once suitable null cases are found.

As with the technical detection performance trial, the discussion between the sponsors, the technical specialists and the analysts led to agreement that this regime was suitable within the time available to give an overall comparison of the distance of first interference of the detector modes. Data was then collected using the agreed approach for each combination of electronic equipment (labelled by scenario number) and detector mode with an indicative presentation style displayed in Fig. 4.
                     

Humans are inseparable from the detection system and as such the complete system study should include both physical and cognitive measurements. Logically, any trial measurements must be qualified, as the individuals involved will not be subject to the rigours of an actual operation, such as being in a heightened state of tension or have levels of fatigue and cognitive load consistent with a hostile environment. Nevertheless trial measurements are still valuable as they, arguably, represent best case examples and moreover are conducted under controlled (or at least similar) environments.

The approach taken was to view detection as a part task activity (Hobbs & Curtis, 2011) whereby the action was deemed independent of the overall mission such as patrol. To this end, human factors based vignettes were developed during the operational evaluation by distilling the sponsor endorsed set thereby setting the conditions of the trial. Physical measurements were made of muscle strain and this was complemented by questionnaires and interviews of the trial group, seeking subjective user opinion of such things as the ease of use, the quality of the cues, ability to interact with the environment while conducting sweeps, likely training requirements etc. Given that the trial subjects were actual users (though with different levels of capability and experience) it was possible to elicit credible results for the final report. Levels of acceptability were defined according to the operator's judgement such as for integration with body armour, the detector could be considered acceptable for operations if no modification was needed and slightly unacceptable if some minor changes to procedures or layout were needed, etc. Measureable terms such as unpack, set up and warm up times were also determined. Finally, the physical characteristics such as weight
                         and power consumption were compiled.

In undertaking any study the nature of how the outcomes will be reported forms a key consideration to reflect the assumptions, aims, methodology, and results against the objectives and expenditure of resources (“step 5” of Table 1). In the case reported here there are multiple stakeholders (Fig. 1), each with a different need from the data from detailed tactical considerations (a specific mission in an identified environment) through to strategic (positioning to address possible future contingencies in a span of regions and across various mission types). Some also have differing degrees of familiarity with the detection devices and operational settings.

For the characterisation framework where teams of specialists were formed to address the various components, each team wrote a comprehensive report on their particular component. The lead analyst (as part of the Reporting component) maintained regular communication with each of the specialist teams and was provided with copies of the collated results as they were produced. While each report was comprehensive for its particular activity and is part of the package delivered to the stakeholder group, the challenge was to then bring together the results into a consolidated whole that accurately summarised the data, addressed multiple stakeholder viewpoints, and importantly anticipated potential future queries e.g. how would the detector perform in environment X, against threat set Y under mission constraints Z. As such, the presentation of any data generated would ideally be in such a way as to provide as many suitable lenses as possible to address the multiple stakeholder needs and not be focussed on speculations drawn from a specific vignette.

At the time of the creation of the framework there were a number of proposals for how the data may be displayed although there was no finalised arrangement between the analysts and the stakeholders prior to the commencement of the work. Coupling the preliminary results from Sections 4.1 and 4.2 as they became available with the anticipated form of the results of Sections 4.3 and 4.4, the authors in conjunction with experienced analytics personnel, examined various permutations of data presentation. Presentation considerations took into account multiple stakeholders and multiple potential queries that might be made of the final data set both now and into the future. Additionally there is the flexibility to add information about more detectors as they became available and are trialled within the framework. As a result the concept of building an enduring library of information was conceived that could be interrogated as needed and in addition allowed for the direct comparison of detectors now and into the future. The concept of “Device on a Page” (Figs. 3 and 4) was formed where all pertinent information for a particular device was able to be summarised on a single sheet of paper with extra detail able to be accessed through the supporting reports, conceptually somewhat similar to a technical specifications sheet for a piece of technology. By adopting this concept the data can be presented objectively without ranking the devices. The multiple stakeholders are then able to make their own assessments at different times based on their individual criteria and it also meets the principal sponsor's direction of not providing any analyst opinions of which devices might be “better” based on some potentially transient criteria or time dependent relative ranking.


                        Fig. 3 provides a template and an indicative example, using simulated information, of how some of the data collected through the course of the study might be displayed for the stakeholders. Fig. 3 summarises physical parameters, a picture of the detector (image shown is not of one actually used in this study) and human factors while Fig. 4 details Pd
                         and interference.

In developing the concept of “Device on a Page” the authors also extended the concept to “Soil on a Page” and “Target on a Page” which presented the same data but grouped in an alternative forms. By presenting the data across these three forms allows the stakeholder and user communities to consider the overarching questions of: “what detector mode to use?” (Target on a Page), “what detector to have in my inventory?” (“Device on a Page” incorporating “Mode on a Page”) and “what can I expect?” (“Soil on a Page”). The grouping of these three forms is then the core of the enduring library of information given that the data was collected under a set of conditions allowing for a level of repeatability over an extended timeframe i.e. examination of additional detectors and their modes can be incorporated and compared to existing data. These various forms of display allows, for example, the device user community to rapidly understand which device to take and which mode to use (using “Target on a Page” and “Soil on a Page”) and the capability manager to make inventory choices (using “Device on a Page”).

With the concept of summarising the data into “X on a Page” there was then the distillation of what data was to be summarised for each component and how it was to be best presented. Considerations included easy visual conveyance and the ability to present the data accurately but not in a manner that invites debate beyond the accuracy of the information. Consultation with DST Group data visualisation specialists provided a number of display options for consideration and these formed the basis for the final arrangement of the data. Figs. 3–6 display the indicative final arrangement using simulated data. Not all data collected was able to be displayed in these figures, in particular data spanning across detectors such as some of the general vignette considerations, qualitative training information and the table of ranked characteristics that came from the operational evaluation. This data was summarised in the body of the overall report that was delivered to the stakeholders.

The preliminary results were presented to the sponsor's delegates to confirm that the concept of “X on a Page” was suitable and to seek their input as to how they may wish the data to be arranged. For example, should the arrangement of the labels on the axes of graphs be sorted by performance in each case or left consistent between the cases, are the graph types suitable to convey the information quickly. Once sponsor concurrence and guidance was confirmed, the lead analyst developed the final package of reports and presentations.

While there are many possible ways to display the data, each with their own strengths and weaknesses, a key consideration for the lead analyst was to minimise opportunities for debates to occur that were beyond the veracity of the data. In the case of the technical detection performance data for each of the device modes two principal means of conveying the data were adopted to display the same information.

In the case of the “Soil on a Page” the method chosen was for each entry of device mode versus target to be displayed in a table format by using a graduated greyscale with no numbers shown (Fig. 5
                        
                        ). While there are known visual challenges associated with using a graduated shading scale (for this and other visual examples see (Changizi, Hsieh, Nijhawan, Kanai, & Shimojo, 2008)), where two equivalent shades can appear different to the viewer under certain conditions, it was felt that using this type of scale would not present as many potential issues as a dis-jointed colour scale. Some of the visual effects are mitigated by placing a visually thick border around each of the grey scale table entries. As noted a key consideration was to display the data in a manner that allowed the viewer to draw their own summations, if a dis-jointed colour or a “traffic-light” type schema is used then judgements are made regarding which points would correspond to the various colours. While the judgements might be suitable at the time of the study, at a subsequent date they may be less so. Further in the case of the “traffic-light” choice of colours there would then be the implied red being “bad” against some criteria which may not be valid under other considerations. It was also agreed at the outset that no threshold values of acceptability would be formulated. Finally, on the “Soil on a Page” graphic the actual values are purposefully not displayed on the table so the user is not focussed on exact percentages but rather a sense of relative performance i.e. the absolute results are treated as comparative. The actual values are available via a second means of displaying the information i.e. the graphs on “Target on a Page” or “Mode on a Page” the latter forming part of the “Device on a Page”. In these cases the graphs show the Pd
                         and their associated error intervals.

For the purposes of this paper the indicative data presentations do not make use of colour. In the final reporting, various colours were assigned to aid the report recipients to quickly distinguish between the soil types, target types and the detectors with their various modes. Each soil type was assigned its own unique shade with increasing magnetic susceptibility corresponding to increasing shading level. For the target types, each sub-set of target types was given a unique set of shades. In the case of the detectors, each was given a colour and in the case of multi-mode detectors, each mode was assigned a unique shade of the detector colour.

Previous research has shown that involvement of the study stakeholders, including the analysts, results in better shared understanding, mutual reinforcement, improved commitment, and improved quality of the final product(s) (Cavana, Boyd, & Taylor, 2007; Midgley, Cavana, Brocklesby, Foote, & Wood, 2013; Müller et al., 2012; von Winterfeldt & Fasolo, 2009). The outcomes of this study reinforce this observation as the study design, drawing upon OR techniques, directly involved the stakeholders in active roles, provided feedback loops to address issues and developed a method of data presentation to address multiple stakeholder viewpoints into the future.

For this study, the final package of products consisted of a presentation, the overall report containing the “X on a Page” plus additional consolidated information and the in-depth reports on: soil and target selection, the characteristics workshop, technical detection performance, technical interoperability and human factors. This package was then delivered in person to the study custodian (Fig. 1) including a detailed verbal presentation focused on the results (“step 5” of Table 1).

At the conclusion of the work and its presentation, the study custodian was very complimentary of the conduct of the program and its results providing feedback that: the data collected and the presentation methodology adopted were accepted without any disagreement amongst the stakeholder community; the study program and data captured the implicit and explicit concerns addressing the stakeholder and user requirements as well as resolving many of the differing viewpoints; provided a common understanding for the stakeholder community; and led to informed decisions being made with increased confidence that the foundational data was consistent and of suitable quality (“step 6”of Table 1).

Further, in as far as repeat business constitutes recognition then this study was also successful as a request has been made to undertake characterisation of additional hand-held detectors using the same methodology.

While the preceding paragraphs may appear as self-congratulatory that is not its intent. Rather it is to demonstrate that by the application of OR, involving the stakeholders (Fig. 1) in the process, constructing the framework, undertaking the program, creating the data presentations and incorporating the governance and scrutiny feedback it was possible to satisfy multiple stakeholder lenses and deliver an enduring set of outcomes (Midgley et al., 2013; Müller et al., 2012; von Winterfeldt & Fasolo, 2009).

@&#DISCUSSION@&#

The articulated characterisation framework is of a general form and we believe can be applied to a range of potential circumstances where there might be a requirement to collect and present information to distinguish between similar capability types. In this paper the case of characterising mine detectors is used to demonstrate the framework. It is worth noting that a full instantiation would involve a large range and volume of resources and this was the case for the study presented here. We estimate that the overall cost was greater than $1 million (Australian) with approximately 100 personnel involved at one time or another. There was a large travel component with specialists flown from across Australia and the construction of dedicated facilities (able to be used into the future). With such a significant investment, there is an expectation that any results would be not only fit-for-purpose but also enduring in nature and this was able to be achieved.

It is of value to note that in the current study it is not possible to separate the technical aspects of the detector performance (signal of detection) from the operator understanding (recognition and interpretation of the signal). In order to examine the operator-technology synergy, it would be of benefit to conduct operator independent technical testing as well as the operator dependent testing and thus gain insights into the coupling of operator with the technology – this is an area for future investigation.

Using the considerations of Section 2 to shape a common (shared) understanding of the problem space across the various communities involved we were able to construct and implement a framework that not only addressed the community's immediate needs but also allows for robust capability comparisons into the future. By noting that we are situated in the “problem” space of the “messes, problems, puzzles” construct (Ackoff, 1979a, 1979b; Pidd, 2004) and so overall there is no single “right” answer or definitive “right way” of doing things, we were able to: (i) create an approach that met the sponsor's need for “validity, credibility and acceptance”; (ii) identify and gather data on many of the implicit and explicit factors present using sound analytical approaches; and (iii) collate the results into an overall defendable data resource. Meanwhile, recognising that the concept of no “right way” is not in conflict with the circumstances where, for certain individual data collection activities that might contribute to an overall approach, there may exist a “right way” which must be followed for credible data to be obtained. By adopting this approach we protect the sponsor from unhelpful scrutiny, allowing for the outcomes to be suitably transparent for constructive debate (NATO RTO SAS-087, 2012b) and support the value of PSM (Ackermann, 2012). In considering the challenges highlighted in
Ackermann, Franco, Rouwette, & White, (2014) this paper meets numbers 1, 3, 4 and 5 in that it is a quality intervention into the military problem (#1), it exploits existing practice (#3), it accommodates several discrete players (#4) and the role of the analyst can be seen as neutral (#5).

By following an interactive approach we were able to: obtain a common understanding of the different needs of the various communities; foster collaboration; draw upon individual expertise; reduce risk through recognised accountabilities (e.g. sponsor provided expertise, sound analytical practice); construct an enduring approach; and implement via sets of multi-disciplinary teams. Importantly, the analysts and technologists were able to design and conduct suitable data collection activities; and to design and populate a data construct that allowed the stakeholder community to elicit pertinent information in an effective, efficient and unbiased manner both now and into the future for making operational and capability decisions. Table 2 shows the approach taken to address the guiding questions of Table 1.
                  

The characterisation framework (Fig. 2) allowed for: establishing the assessment context of the threats and environments; conducting the context of the operational evaluation that refined what were considered the important characteristics; the technical performance to gather the statistically robust performance data; and the human trials to collect statistical and subjective data. The governance and scrutiny components gave a means for dynamic review as the program unfolded increasing stakeholder confidence in the quality of the outcomes allowing a level of mitigation as the timelines did not allow for a full external peer review process to be applied before final reporting and delivery. The structure of Fig. 2 had the added benefit of there being more time available for undertaking the field trials.

Finally, it is pertinent to consider whether a code of best practice would assist in studies such as this. Using the headings provided by Ormerod's three strands of OR competence (Ormerod, 2010b, 2010c), key elements can be identified and classified. Reporting of the process also fulfils the proposed requirements of information transfer (Ormerod, 2014a). Using the three strands of OR competence, and relating to the considerations from Section 2:

                        
                           •
                           Conducting analysis
                                 
                                    ○
                                    Problem structuring: initial discussions, workshop; (what are the manageable components?)
                                    

Rational inference: inductive assessment of the body of data, identification of technological or procedural limitations; (how can the study be constructed?) (what steps can be taken to ensure credibility?)
                                    

Rational choice: presentation of results (“X on a Page”) for the user-selected options, no guidance (recommendations) from the analysts; (in what form are the results required?)
                                    

Policy analysis: linked contribution to the larger goal of personnel protection and mission achievement. (what will the results be used for?)
                                    

Designing and managing process
                                 
                                    ○
                                    Intervention design: agreed framework (updated as needed), defined roles and responsibilities for the participants; (what are the discriminating features?)
                                    

Facilitation: agreed provision of equipment, subject matter experts and users (requester group), analysts and technology specialists (science group); (what equipment etc. is available?)
                                    

Project management: analyst as lead coordinator, single point of contact with requester group. (what is important to the users and decision makers?)
                                    

Appreciating context
                                 
                                    ○
                                    Outer context: mission types and vignettes, environmental settings; (how is the capability likely to be used?) (what level of risk is acceptable?)
                                    

Inner context: military tactics, techniques and procedures. (how is the capability likely to be used?) (what level of risk is acceptable?)
                                    

While it is not suggested that this is a formal process to be followed, it would be good practice if the above check-list was used as a guide for those items that may need to be considered during a study like this. In effect, it represents an extended problem structuring approach and declaration that such a process has been followed would reduce damaging criticism of the study and allow concentration on the results.

@&#ACKNOWLEDGEMENTS@&#

The paper presented here is an analytic overview. The authors are indebted to the many Australian Army personnel and Defence Science and Technology Group staff who participated in this study and associated trials. Some technical elements may be reported elsewhere.

@&#REFERENCES@&#

