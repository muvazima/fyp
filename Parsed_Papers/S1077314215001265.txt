@&#MAIN-TITLE@&#Improving scene attribute recognition using web-scale object detectors

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Humans often describe scenes by their affordances, which are suggested by objects.


                        
                        
                           
                           Object detectors trained at the web scale can improve scene attribute recognition.


                        
                        
                           
                           We experiment on a semi-supervised continuous learner and a supervised deep network.


                        
                        
                           
                           Learned models capture intuitive and useful object-scene attribute relationships.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Affordances

Scene understanding

Semantic attributes

Semantic features

@&#ABSTRACT@&#


               
               
                  Semantic attributes enable a richer description of scenes than basic category labels. While traditionally scenes have been analyzed using global image features such as Gist, recent studies suggest that humans often describe scenes in ways that are naturally characterized by local image evidence. For example, humans often describe scenes by their functions or affordances, which are largely suggested by the objects in the scene. In this paper, we leverage a large collection of modern object detectors trained at the web scale to derive effective high-level features for scene attribute recognition. We conduct experiments using two modern object detection frameworks: a semi-supervised learner that continuously learns object models from web images, and a state-of-the-art deep network. The detector response features improve the state of the art on the standard scene attribute benchmark by 5% average precision, and also capture intuitive object-scene relationships, such as the positive correlation of castles with “vacationing/touring” scenes.
               
            

@&#INTRODUCTION@&#

When searching for an image, a user may want to describe the desired characteristics of the scene, such as “cluttered”, “soothing”, or “open area”, instead of being limited to basic category labels such as “city”, and interactively refine search results according to those characteristics. When a learning system encounters an object or scene that is very different from any of the categories it has encountered before, it might be useful for the system to output at least a high-level semantic description of the object or scene. Similarly, given only a high-level semantic description of an object or scene category, it may be useful for a learning system to be able to generalize and recognize new instances of that category, without seeing any examples. These are only a few of the use cases for semantic attributes – human-nameable, descriptive properties of an object or scene [1–6]. In this paper, we focus on using semantic attributes to describe scenes. Attributes are particularly appropriate for describing scenes because scene categories can exhibit wide intra-class variation, the scene space is continuous (i.e. there are smooth transitions between scene categories), and a single image can contain multiple scene categories [7].

Traditionally, scenes have been analyzed using global image features such as Gist [8] or HOG visual words [9,10]. In their pioneering work [8], Oliva and Torralba showed that scene classification is possible using global, “holistic” scene properties without the need to first perform individual object detection. These global scene properties can also be used to predict perceptual properties of the scene, such as degree of naturalness or openness, which have remained highly influential in modern work on scene attributes. In particular, the taxonomy of scene attributes in the recently developed SUN attribute database [7] includes several “spatial envelope” attributes inspired by the early Gist work of Oliva and Torralba.

While global image features such as Gist can be used to recognize many types of scene attributes, a large number of interesting scene attributes are not well captured by global features. In particular, many scene attributes in the crowd-sourced SUN attribute database are more naturally characterized by localized image evidence. For example, Patterson and Hays [7] found that humans often describe scenes based on their functions or affordances, such as “camping” or “studying/learning”. A strong cue for “camping” (defined as “either an actual camp site, or scene in wilderness suitable enough for humans to make a tent and/or sleep” [7]) would be the presence of a tent. However, a tent is unlikely to be detected by global image features such as Gist. On the other hand, a suitably trained tent object detector would likely fire on the image, making it useful for recognizing the “camping” attribute. We hypothesize that scene attribute recognition can benefit from the integration of localized object detector information.

Though object detectors in the early days of scene understanding may have been limited in scope and performance, recent years have seen the development of high-quality detectors trained at the web scale (e.g. [11–13]). In this paper, we demonstrate that scene attribute recognition can be improved by leveraging a large collection of modern object detectors trained at the web scale. We perform experiments using both the Never Ending Image Learner (NEIL) [13], which continuously learns object models in a semi-supervised manner from web images, as well as a state-of-the-art deep network [14] trained on the ImageNet Large Scale Visual Recognition Challenge 2012 dataset [15]. Both types of detectors have advantages: NEIL learns continuously with limited supervision and its models can be expected to improve over time, while deep learning features have been shown to provide state-of-the-art performance in diverse recognition tasks. We find that detector-based features perform better than any individual global image feature baselined in the SUN attribute database, including Gist, HOG2x2, self-similarity, and geometric context color histograms, as well as their combination.

The core contribution of our paper can be summarized as follows. Motivated by recent findings that humans often describe scenes by their functions or affordances, which are largely suggested by the objects in the scene, we propose leveraging modern, web-scale object detectors to improve scene attribute recognition. We demonstrate the effectiveness of this idea on the standard scene attribute benchmark.

@&#RELATED WORK@&#

Traditionally, the task of high-level scene classification has focused on naming – determining the correct category of the scene, such as ‘city’ or ‘forest’ [16–19]. However, novel developments have been made in recent years on describing objects and scenes by their semantic attributes [1–6].

Farhadi et al. [1] argued that attributes provide a semantically richer and more useful means to describe and categorize objects. Their semantic attributes relate to object parts, shapes, and materials, such as “has wheel”, “is 3D boxy”, and “has metal”. Besides traditional naming (categorization), the authors demonstrated how attributes can be used to describe objects belonging to previously unseen classes, and enable the learning of class models from few or even no training images. Lampert et al. [2] were principally interested in the problem of learning with disjoint training and test classes: that is, classifying objects of unseen classes using semantic attributes only.

Berg et al. [4] proposed a method for learning semantic attributes for several product categories using product images and their associated text descriptions. Wang and Mori [3] suggested learning object classes and semantic attributes jointly since each provides cues for the other. For example, an object of class “bird” is likely to “have eye”, while an object with attribute “furry” is more likely to be a dog than a car. To learn classes and attributes jointly, the authors constructed a model that combines five components: an object class model, a global attribute model, a class-specific attribute model, attribute-attribute interaction, and object-attribute interaction. The model is learned using a latent SVM framework, with attributes as latent variables. Parikh and Grauman [5] proposed relative attributes as a more precise way to describe objects and scenes than binary attributes. Relative attributes compare an object or scene with respect to other examples or classes, and are especially useful when a binary decision would be ambiguous. Kovashka et al. [6] developed an interactive image search method that refines results using relative attribute feedback. Shrivastava et al. [20] used binary and relative attributes to reduce semantic drift in semi-supervised learning for scene categorization. Categories are learned jointly, incorporating constraints from known category-level attributes (e.g. amphitheatres are “more circular” than auditoriums).

Besides semantic attributes, which are human-nameable and useful for object and scene description, some approaches learn “discriminative attributes” that are useful for distinguishing between classes but are not necessarily nameable. For example, Farhadi et al. [1] made use of auxiliary discriminative attributes in conjunction with semantic attributes for object classification. Rastegari et al. [21] proposed a method for learning compact binary codes that are discriminative between object categories. The authors showed how individual bits in the binary codes can also be interpreted as discriminative attributes. Yu et al. [22] proposed a method for learning a discriminative category-attribute matrix from training data. However, in this paper we focus on semantic attributes that are human interpretable and nameable.

Our work is also related to methods that construct high-level features from multiple class detector responses. Torresani et al. [23] proposed the classeme representation of an image, which concatenates the responses of detectors of generic concepts from a multimedia concept ontology. Li et al. [24] proposed the Object Bank representation of an image for scene classification, which spatially pools the responses of multiple object detectors applied at various scales and locations in the image. The resulting descriptor is classified using three forms of regularized logistic regression that induce different sparsity properties. In contrast to classemes, our high-level features have semantic interpretability and correspond to meaningful object categories. In contrast to Object Bank, we leverage detectors trained on web-scale data, individual detectors may be tuned to distinctive sub-clusters within object categories (an object category may be represented by multiple detectors), and an ordinal feature transform is applied on top of the detector responses to increase robustness in the high dimensional feature space.

@&#METHOD@&#

In their crowd-sourcing study, Patterson and Hays [7] observed that humans naturally describe scenes using a wide range of attributes, including attributes based on materials, surfaces, and functions or affordances, as well as the traditional spatial envelope properties associated with early work on scene gist [8]. In this work, we explore the hypothesis that since many of these crowd-sourced scene attributes are characterized by localized image evidence, recognition of scene attributes can be improved via integration of a large collection of object detectors trained at the web scale. Our approach can be thought of as a form of transfer learning, in which we leverage CPU-years of object detector training to improve the state of the art in the description of scenes by semantic attributes.

We perform experiments using two distinct object detection approaches: the Never Ending Image Learner (NEIL) [13], which continuously learns object category models from the web in a semi-supervised manner, and a state-of-the-art convolutional neural network [14] trained on an ImageNet challenge dataset [15].

Inspired by developments in text understanding, Chen et al. [13] recently introduced the Never Ending Image Learner (NEIL), an iterative, semi-supervised algorithm that learns objects and their relationships from downloaded web images. In each iteration, only the most confident detections and relationships are added to the knowledge base, a strategy the authors refer to as “macro-vision”. Detectors are based on color HOG features. Confidence is determined by both classification scores and co-occurrence statistics. In this work, we use the most recent collection of object detectors released by NEIL (as of June 2014, this was the Dec. 2013 version), which consists of 8685 detectors spanning 1190 unique object categories. The NEIL project also includes attributes learning, however it is concerned with the semi-supervised discovery of attributes from web images, while we are interested in the supervised learning of attributes given a labelled database of scenes with crowd-sourced attributes.
                        
                     

Deep networks represent the current state-of-the-art in object detection. We adopt a detection framework similar to R-CNN [25]. R-CNN first generates a set of object proposal windows using selective search [26]. Next, for each proposal window, deep features are extracted using a pre-trained convolutional neural network (CNN). Features are finally classified using domain-specific linear support vector machines (SVMs) that adapt the CNN output to the object categories of interest; for example, the CNN may be pre-trained on the ImageNet categories and linear SVMs may be learned to adapt the CNN response vector to the Pascal VOC categories. Our implementation generates object proposal windows using selective search and applies a state-of-the-art CNN from the recent study of Chatfield et al. [14] (specifically, the study’s CNN-M network based on Zeiler and Fergus [27]). The CNN is trained on the ImageNet Large Scale Visual Recognition Challenge 2012 dataset, consisting of 1000 object categories. We use the 1000-dimensional CNN output vector (the output vector of the last fully connected layer) without a final softmax.

Let fc
                        (I, w) denote the response of an object detector of category c evaluated in window w of an image I. We form a feature descriptor F for image I that concatenates the responses of all object detectors in the collection, max-pooled over the image windows

                           
                              (1)
                              
                                 
                                    F
                                    
                                       [
                                       c
                                       ]
                                    
                                    =
                                    
                                       max
                                       w
                                    
                                    
                                       f
                                       c
                                    
                                    
                                       (
                                       I
                                       ,
                                       w
                                       )
                                    
                                    ,
                                    
                                    c
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    D
                                 
                              
                           
                        where F[c] denotes the cth component of the feature vector F ∈ R
                        
                           D
                        , and D is the number of object detectors in the collection. For example, 
                           
                              D
                              =
                              8685
                           
                         using the NEIL object detectors and 
                           
                              D
                              =
                              1000
                           
                         using the deep network.

The basic feature descriptor (1) can be enhanced by incorporating spatial response information and partial order statistics.

We encode spatial response information using a two-level, max-pooled spatial pyramid [16]. Detector responses are max-pooled across regular spatial partitions of the image, at multiple levels. At the finest level, the image is partitioned into a regular 4  ×  4 grid. The next level partitions the image into a 2  ×  2 grid. The final level pools responses over the entire image. Hence, the final feature descriptor is 21D-dimensional.

In high dimensional feature spaces, the partial order statistics of a feature descriptor are often more robust for classification and retrieval tasks than the descriptor’s precise numeric values [28]. Following [28], we capture partial order statistics by taking random subsets of the feature descriptor’s dimensions. Specifically, we derive an ordinal feature representation of the detector response vector F as follows. Given a subset size K ≪ D, generate m random ordered subsets of size K of the dimensions in F (in general, larger m captures more ordinal relationships but increases memory requirements as the transformed features are larger). That is, each ordered subset θ consists of K unique indices from 1 to D: 
                           
                              θ
                              ∈
                              
                                 
                                    {
                                    1
                                    …
                                    D
                                    }
                                 
                                 K
                              
                           
                        . Denote by Θ the matrix formed by stacking all m ordered subsets: 
                           
                              Θ
                              ∈
                              
                                 
                                    {
                                    1
                                    …
                                    D
                                    }
                                 
                                 
                                    m
                                    ×
                                    K
                                 
                              
                           
                        . Form an intermediate matrix Z ∈ R
                        
                           m × K
                         by looking up the entries in F corresponding to the indices in Θ:

                           
                              (2)
                              
                                 
                                    Z
                                    [
                                    i
                                    ,
                                    j
                                    ]
                                    =
                                    F
                                    [
                                    Θ
                                    [
                                    i
                                    ,
                                    j
                                    ]
                                    ]
                                 
                              
                           
                        where i and j are row and column identifiers. Next, let 
                           
                              
                                 x
                                 ˜
                              
                              ∈
                              
                                 
                                    N
                                 
                                 m
                              
                           
                         collect the indices of the largest elements of each row in Z
                        
                           
                              (3)
                              
                                 
                                    
                                       x
                                       ˜
                                    
                                    
                                       [
                                       i
                                       ]
                                    
                                    =
                                    
                                       arg max
                                       j
                                    
                                    Z
                                    
                                       [
                                       i
                                       ,
                                       j
                                       ]
                                    
                                 
                              
                           
                        
                     

Each entry in 
                           
                              x
                              ˜
                           
                         encodes the maximum rank information for the corresponding random ordered subset θ. The final ordinal feature representation x is a binary output encoding of 
                           
                              x
                              ˜
                           
                         in which each scalar in 
                           
                              x
                              ˜
                           
                         is translated into a binary indicator vector of length K. Hence, x ∈ {0, 1}
                           m × K
                        . An example of the ordinal feature transform is illustrated in Fig. 1.

@&#EXPERIMENTS@&#

We conducted experiments on the SUN attribute benchmark [7], which contains over 14,000 scenes spanning over 700 categories. Attribute annotations are sourced for each image individually instead of assumed the same for all images of the same scene category. The crowd-sourced scene attributes span materials, surface properties, functions or affordances, and spatial envelope [8] properties. The complete list of attributes as well as a discussion of the methodology used in their collection can be found in Patterson and Hays [7].

Given a semantic attribute of interest, we train a linear SVM on detector response features and the ground-truth attribute presence labels. In the SUN attribute benchmark, each database scene is associated with an attribute occurrence vector based on votes from Amazon Mechanical Turk workers. For example, if a scene is given 2 of 3 possible votes as being a “cluttered space”, then the corresponding entry in the vector is 2/3. We follow the standard practice of the SUN attribute database and consider an attribute to be present in an image if the attribute receives at least two votes, and absent if the attribute receives no votes. An attribute that receives a single vote is considered neither absent nor present, and the scene should have no influence when performing recognition of that particular attribute. The regularization parameter C for the linear SVM trained on our detector-based features and the corresponding binary attribute labels (formed by thresholding the attribute occurrence vector at 2/3 as described above) is set by five-fold cross-validation. For a direct comparison with the SUN attribute benchmark baseline that uses global image features, we train each attribute classifier independently and do not take advantage of possible correlation cues.


                     Table 1
                     
                     
                     
                      summarizes experimental results using the standard train and test splits of the benchmark. The top rows show the average precision achieved by traditional global image features baselined in the SUN attribute benchmark: Gist, HOG2x2, self-similarity, and geometric context color histograms. The top performing individual global image feature is HOG2x2, with an average precision of 84.8%. Combining all four global image feature types boosts recognition performance to 87.9%. Perhaps surprisingly, the basic detector-based feature descriptor (1) outperforms all individual global image features. Using the NEIL detectors, an average precision of 86.4% is achieved, which marginally improves upon the top performing individual feature by 1.6%. A further improvement is obtained by adopting the deep neural network detectors, which achieve an average precision of 91.1%. Next, we apply the spatial pyramid and ordinal transform enhancements to boost the basic feature derived from the deep detectors. Adding spatial information using a two-layer, max-pooled spatial pyramid results in an average precision of 92.4%. Adding the ordinal transform to capture the partial order statistics of the spatial pyramid provides a small further improvement in accuracy, obtaining a final average precision of 92.8%.

We find that combining the two types of image evidence – global image features and object detector responses – via averaging does not produce further gains after spatial pyramid and ordinal transform enhancements are applied: only 92.4% average precision is obtained (e+i). Without the enhancements, the combination of global image features and detector responses does improve results slightly from 91.1% (g) to 91.9% (e+g).


                     Fig. 2 shows the improvement in average precision for each individual attribute. The comparison is with respect to the combination of all four global image features (row i vs. row e in Table 1). In addition to the motivating function or affordance based attributes, we found object detector responses to broadly outperform traditional global features in recognizing scene attributes related to materials, surface properties, and spatial envelope properties.

We set the transform parameters 
                        
                           K
                           =
                           4
                        
                      and 
                        
                           m
                           =
                           32
                           k
                        
                      to generate the ordinally transformed features. In general, larger m gives higher performance but also increases memory requirements. Fig. 3 shows how the recognition performance varies for different values of m.


                     Tables 2 and 3 show sample qualitative results. Each row shows an image from the SUN attribute database, the most confident attributes predicted using object detector responses (row i in Table 1: deep detectors, spatial pyramid, ordinal transform), and the ground truth annotation. For each image, the length of the sorted list of most confident attributes is set to the number of ground truth attributes. Predicted attributes that match the ground truth are highlighted in bold. We observe that the predicted attributes are generally semantically valid and in agreement with the ground truth annotations, across a wide range of outdoor and indoor scenes. Occasionally, our detector-based features enable the prediction of function/affordance attributes that do not appear in the ground truth annotation, such as “farming” in the terrace farm image (first image in Table 2), “praying” in the pulpit image (third in Table 2), and “using tools” and “working” in the auto mechanics image (fourth in Table 3).


                     Table 4
                      illustrates some interesting object-scene attribute relationships learned using the proposed approach, and validates the intuition that object categories can provide strong cues for some scene attributes. Each box shows a scene attribute and the five object categories corresponding to the highest weights in the linear SVMs trained on basic response features (deep detectors, no spatial pyramid, no ordinal transform). Each of the 1000 weights corresponds to a class in the ImageNet challenge dataset used to train the deep detectors. For instance, for the scene attribute “vacationing/touring”, we observe high weights associated with the object categories “palace”, “castle”, and “canoe”, reflecting the positive correlation between these objects and vacationing. As another example, the object categories “ballplayer”, “racket”, and “scoreboard” provide strong cues for the scene attribute “competing”.

@&#CONCLUSION@&#

Motivated by recent findings that humans often describe scenes by their functions or affordances, which are largely suggested by the objects in the scene, we explored the use of modern object detectors trained at the web scale to improve scene attribute recognition. We experimented with two modern object detection frameworks: the Never Ending Image Learner (NEIL), which continuously learns object category models in a semi-supervised manner from web images, and a state-of-the-art deep neural network trained on an ImageNet challenge dataset.

We found that detector response features outperform all individual global image features baselined in the SUN Attribute benchmark, as well as their combination, not only for function or affordance attributes but also for attributes related to materials, surface properties, and spatial envelope properties. Further improvements in accuracy can be obtained by applying spatial pyramid max-pooling and an ordinal feature transform to capture partial order statistics. Moreover, we found that the proposed approach captures intuitive and useful object-scene attribute relationships, such as the association of scoreboards and ballplayers with “competing” scenes.

Finally, detector response features offer the advantage of scalability: they can be expected to improve in performance over time, as the collection of detectors grows or as the individual detectors improve. For instance, we anticipate that plugging in future releases of the NEIL object detectors, potentially incorporating state-of-the-art deep features, will enable further gains in scene attribute recognition.

@&#ACKNOWLEDGMENTS@&#

This work was funded in part by the Natural Sciences and Engineering Research Council of Canada.

@&#REFERENCES@&#

