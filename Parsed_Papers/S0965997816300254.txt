@&#MAIN-TITLE@&#Artificial neural networks in the calibration of nonlinear mechanical models

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Paper reviews applications of artificial neural networks in model calibration.


                        
                        
                           
                           Neural network-based calibration strategies are classified into three groups.


                        
                        
                           
                           Identification strategies are compared on calibration of affinity hydration model.


                        
                        
                           
                           The most precise strategy uses an ANN-based surrogate of each response component.


                        
                        
                           
                           Principal component-based inverse mapping is the best for a repeated use on new data.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Artificial neural network

Multi-layer perceptron

Parameter identification

Principal component analysis

Sensitivity analysis

Affinity hydration model

Concrete

@&#ABSTRACT@&#


               
               
                  Rapid development in numerical modelling of materials and the complexity of new models increase quickly together with their computational demands. Despite the growing performance of modern computers and clusters, calibration of such models from noisy experimental data remains a nontrivial and often computationally intensive task. Layered neural networks provide a robust and efficient technique for overcoming the time-consuming simulations of calibrated models. The potential advantages of neural networks include simple implementation and high versatility in approximating nonlinear relationships. Therefore, there were several approaches proposed in literature for accelerating the calibration of nonlinear models by neural networks. This contribution reviews and compares three possible strategies based on approximating (i) the model response, (ii) the inverse relationship between the model response and its parameters and (iii) an error function quantifying how well the model fits the data. The advantages and drawbacks of particular strategies are demonstrated with the calibration of four parameters of an affinity hydration model from simulated data as well as from experimental measurements. The affinity hydration model is highly nonlinear but computationally cheap, thus allowing its calibration without any approximation and better quantification of results obtained by the examined calibration strategies. This paper can be viewed as a guide for engineers to help them develop an appropriate strategy for their particular calibration problems.
               
            

@&#INTRODUCTION@&#

Development in numerical modelling allows for the description of complex phenomena in material or structural behaviour. The resulting models are, however, often highly nonlinear and defined by many parameters which have to be estimated so as to properly describe the investigated system and its behaviour. The aim of model calibration is thus to rediscover unknown parameters, knowing the experimentally obtained response of a system to given excitations. The principal difficulty of model calibration lies in the fact that while the numerical model of an experiment represents a well-defined mapping from input (model, material, structural, or other parameters) to output (structural response), there is no guarantee that an inverse relationship even exists.

The most widely used approach to parameter identification is usually an error minimisation technique, where the distance between parameterised model predictions and observed data is minimised [1]. Since the inverse relationship (mapping of model outputs to inputs) is often ill-posed, the error minimisation technique leads to a difficult optimisation problem that is highly nonlinear and multi-modal. Therefore, the choice of an appropriate identification strategy is not trivial.

Another approach intensively developed during the last decade is based on Bayesian updating of uncertainty in parameters’ description [2,3]. Uncertainty in observations is expressed by a corresponding probability distribution and employed for estimating the so-called posterior probabilistic description of identified parameters together with prior expert knowledge about the parameter values [4,5]. The unknown parameters are thus modelled as random variables originally endowed with prior expert-based probability density functions which are then updated using the observations to the posterior density functions. While the error minimisation techniques lead to a single point estimate of the parameters’ values, the result of Bayesian inference is a probability distribution that summarises all available information about the parameters. Another very important advantage of Bayesian inference consists in treating the inverse problem as a well-posed problem in an expanded stochastic space.

Despite progress in uncertainty quantification methods [6,7], the additional information provided by Bayesian inference is generally related to more time-consuming computations. In many situations, the single point estimate approach remains the only feasible one, and the development of efficient tools suitable for this strategy is still a current topic. Within the last several decades, much attention was devoted to so-called intelligent methods of information processing, particulary soft computing methods such as artificial neural networks (ANNs), evolutionary algorithms or fuzzy systems [8]. A review of soft computing methods for parameter identification can be found e.g. in [9]. In this paper, we focus on applications of ANNs in the single point approach to parameter identification. In particular, we elaborate upon our previous work presented in [10,11] with the goal of presenting a detailed and comprehensive comparison of three different strategies for using ANNs in parameter identification problems.

The next section briefly recall the basics of ANNs. The classification of the different applications of ANNs in calibration problems is introduced in Section 3 and a description of an illustrative example — affinity hydration model for concrete — follows in Section 4. In the context of this particular example, the calibration strategies are then described in detail in five sections, beginning with training data preparation and sensitivity analysis in Section 5. Neural network inputs and outputs in particular strategies are discussed in Section 6, and training with topology determination is described in Section 7. Verification and validation on simulated and experimental data are summarised in Sections 8 and 9, respectively. Finally, results are discussed in Section 10.

Artificial neural networks (ANNs) [12,13] are powerful computational systems consisting of many simple processing elements — so-called neurons — connected together to perform tasks analogous to biological brains. Their main feature is the ability to change their behaviour based on the external information that flows through an ANN during the learning (training) phase.

A particular type of ANN is the so-called feed-forward neural network which consists of neurons organised into layers, where outputs from one layer are used as inputs into the following layer, see Fig. 1
                     . There are no cycles or loops in the network, no feed-back connections. The most frequently used example is the multi-layer perceptron (MLP) with a sigmoid transfer function and a gradient descent method of training called the back-propagation learning algorithm. In practical usage, MLPs are known for their ability to approximate nonlinear relationships and therefore, when speaking about an ANN, the MLP is considered in the following text.

The input layer represents a vector of input parameters which are directly the outputs of the input layer. The outputs 
                        
                           o
                           
                              i
                              −
                              1
                              ,
                              k
                           
                        
                      of the 
                        
                           (
                           i
                           −
                           1
                           )
                        
                     th layer are multiplied by a vector of constants w
                     
                        i, j, k
                     , the so-called synaptic weights, summarized and used as inputs u
                     
                        i, j
                      into the jth neuron in the following ith layer. Elements in the hidden and output layers — neurons — are defined by an activation function fa
                     (u
                     
                        i, j
                     ) which is applied to the input and produces the output value of the jth neuron in the ith layer, i.e.

                        
                           (1)
                           
                              
                                 
                                    o
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 =
                                 
                                    f
                                    a
                                 
                                 
                                    (
                                    
                                       u
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    )
                                 
                                 
                                 where
                                 
                                 
                                    u
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       0
                                    
                                    K
                                 
                                 
                                    (
                                    
                                       o
                                       
                                          i
                                          −
                                          1
                                          ,
                                          k
                                       
                                    
                                    
                                       w
                                       
                                          i
                                          ,
                                          j
                                          ,
                                          k
                                       
                                    
                                    )
                                 
                                 .
                              
                           
                        
                     The synaptic weights w
                     
                        i, j, k
                      are parameters of an ANN to be determined during the training process. K is the number of neurons in the 
                        
                           i
                           −
                           1
                        
                      layer. The type of activation function is usually chosen in accordance with the type of function to be approximated. In the case of continuous problems, the sigmoid activation function given as

                        
                           (2)
                           
                              
                                 
                                    o
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 =
                                 
                                    f
                                    a
                                 
                                 
                                    (
                                    
                                       u
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    )
                                 
                                 =
                                 
                                    1
                                    
                                       1
                                       +
                                       
                                          e
                                          
                                             −
                                             
                                                u
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     is the most common choice.

One bias neuron is usually added to the input and hidden layers. It does not contain an activation function and only has a constant value. Its role is to enable the shift in the value of a sum over the outputs of its neighbouring neurons before this sum enters as the input into the neurons in the following layer. The value of biases is determined by the training process together with the synaptic weights.

Despite the popularity of ANNs there are very few published recommendations for selecting a particular ANN architecture. The authors, e.g. in [14,15], show that an ANN with any of a wide variety of continuous nonlinear hidden layer activation functions and one hidden layer having an arbitrarily large number of units suffices for a “universal approximation” property. Therefore, we limit our numerical experiments to such a case. The number of units in the input and the output layers is usually given by the particular problem under consideration, but there is no theory yet which specifies the number of units in the hidden layer. On the one hand, too few hidden units leads to large prediction errors. On the other hand, a large number of hidden units may cause so-called overfitting, where the ANN provides precise outputs for training samples but fails when it encounters previously unseen samples. In such situations, an ANN tries to fit the training data despite increasing oscillations in the intermediate space.

To overcome this problem, a model selection technique [16] has to be applied in order to perform a guided choice of an ANN topology. Recent approaches encompass e.g. growing-pruning methods (see e.g. [17]) or more complex techniques designed for optimising ANN topology, such as meta-learning [18,19]. Here, we employ a simple and general strategy to evaluate a particular ANN topology: cross-validation, which does not involve any probabilistic assumptions or dependencies on an identification problem. The idea of cross-validation is based on repeating evaluation of the prediction error obtained by individual ANNs for a chosen subset of training data and selecting the ANN with the smallest averaged prediction errors. Comparing to a well-known model validation on some independent set of data, the advantage of cross-validation is better behaviour over smaller data sets where an independent data set cannot be afforded [20].

Before applying an ANN to any engineering problem, one must also resolve several questions regarding training data preparation. This involves not only the transformation of input and output data into a range of activation functions. In simulation problems where an ANN is applied to mimic some unknown relationship between observed quantities, the training data coincide with measured data. In inverse problems, we already have some theoretical model relating those quantities and we therefore train the ANN on simulated data, see a recent review of ANN’s application in structural mechanics [21]. Preparation of a suitable training set becomes another nontrivial task in which sensitivity analysis plays an important role. For the sake of clarity, we address these topics in more detail in Section 5 in regard to a particular model for cement hydration.

In model calibration, the goal is to find a set of model parameters minimising the difference between model response and experimental measurements, see Fig. 2
                     . An intuitive way of solving the calibration problem is to formulate an error function quantifying this difference and to minimise the error function using some optimisation algorithm. The most common error functions are given as

                        
                           (3)
                           
                              
                                 
                                    
                                       
                                          F
                                          1
                                       
                                    
                                    
                                       =
                                    
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                N
                                                
                                                   R
                                                
                                             
                                          
                                          
                                             
                                                (
                                                
                                                   r
                                                   i
                                                
                                                −
                                                
                                                   d
                                                   i
                                                
                                                )
                                             
                                             2
                                          
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (4)
                           
                              
                                 
                                    
                                       
                                          F
                                          2
                                       
                                    
                                    
                                       =
                                    
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                N
                                                
                                                   R
                                                
                                             
                                          
                                          
                                             |
                                             
                                                r
                                                i
                                             
                                             −
                                             
                                                d
                                                i
                                             
                                             |
                                          
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     where ri
                      is the ith component of the model response corresponding to the ith measured quantity di
                     , and where N
                     R is a number of measured quantities. The nonlinear relationship between the model response and model parameters often causes complexity in the error function, including multi-modality or non-differentiability. Therefore, computationally efficient methods based on analytically or numerically obtained gradient can be applied only in specific cases.

A more general approach is to apply an optimisation algorithm able of handling the multi-modality once furnished by a sufficient number of function evaluations. However, one evaluation of an error function always involves a simulation of the model. Even for a relatively fast model simulation, optimisation can become easily unfeasible because of the huge number of function evaluations commonly required by evolutionary algorithms, even though they typically require fewer simulations than the uncertainty-based methods mentioned in the introduction to this paper.

One way of reducing the number of model simulations is to construct a forward model approximation based e.g. on an ANN. Error function minimisation then becomes a minimisation of distance between the ANN’s predictions and experimental data. The efficiency of this strategy relies on the evaluation of a trained ANN being significantly much faster than full model simulation. The advantage of this strategy is that the ANN is used to approximate a known mapping which exists and is well-posed. The computational costs of this strategy are separated in two parts, each of a similar size: (i) ANN training: optimisation of synaptic weights and (ii) the minimisation of an error in an ANN's prediction for experimental data: optimisation of ANN inputs (i.e. determination of investigated model parameters). An important shortcoming of this method is that an ill-posed optimisation problem needs to be solved repeatedly for any new experimental measurement. This method for applying an ANN to parameter identification was presented e.g. in [22], where an ANN was used for predicting load-deflection curves and the conjugate directions algorithm is then applied to optimise ductile damage and fracture parameters. The authors in [23] train an ANN to approximate the results of finite element simulations of jet-grouted columns and optimise the radius and cement content of the columns using a genetic algorithm. Similar methods are used for identifying elasto-plastic parameters in [24].

One more difficulty regarding forward model approximation involves the number of parameters and response components. It is very common that experimental observations are represented by discretised curves or surfaces in time or space dimensions being defined as a vectors with a large number of components. A forward model, therefore represents a mapping from usually low-dimensional parameter space to high-dimensional response space. Although such mapping is well-posed, the surrogate model must have a large number of outputs or the time and/or space dimensions must be included among the model inputs.

Another way of avoiding mapping to a large number of outputs is to construct an error function approximation, where model parameters are mapped onto only one scalar value. One important inconvenience of such a strategy is, of course, the complexity of the error function, which can be, as mentioned above, highly nonlinear, multi-modal, and/or non-smooth. The higher complexity of the approximated relationship leads to a higher number of simulations needed for the construction of the approximation. This concerns another problem of estimation and the choice of an appropriate design of experiments, i.e. sets of parameters, to perform the simulations which will enable creation of the surrogate with a relatively small margin of error. This problem can be reduced by adaptive addition of design points, i.e. new simulations, close to the minimum of the error function approximation. The result of the new simulation is then used for improving the surrogate and a new optimisation process is run again. Such an approach is usually well-suited for surrogates based on kriging or radial basis function networks [25,26]. In this paper, we limit our attention to the application of feed-forward layered neural networks and explore their ability to approximate an error function with a limited number of simulations in a non-adaptive fashion.

While the strategy of forward model approximation necessitates a new optimisation process for any new data, the strategy of error function approximation involves not only the optimisation process but also construction of surrogate model. Regarding this aspect, the most convenient strategy is inverse relationship approximation, which requires only one evaluation in order to furnish the parameters corresponding to new observations. Of course, by new observations we mean observations of the system with different properties but performed under the same external conditions (e.g. different material but the same geometry of the specimen and loading conditions). The strategy of inverse relationship approximation assumes the existence of an inverse relationship between the outputs and the inputs of the calibrated model. If such a relationship exists at least on a specified domain of parameter values, it can be approximated by an ANN. Here, the ANN’s training process is responsible for all computational costs arising from the solution of the ill-posed problem. This way of applying an ANN to parameter identification was presented e.g. in [27] or recently in [28] for identification of mechanical material parameters; in [29] for estimation of elastic modulus of the interface tissue on dental implants surfaces; in [30] for identification of interfacial heat transfer coefficient; or in [31] for determination of geometrical parameters of circular arches.

In order to illustrate the advantages and disadvantages of the outlined strategies for applying ANN to model calibration, we have chosen a computationally simple but nonlinear affinity hydration model which we briefly describe in the following section. The model was successfully validated using Portland cements in [32] and thus allows us to also validate the described identification strategies on experimental data as summarised in Section 9.

Affinity hydration models provide a framework for accommodating all stages of cement hydration. We consider hydrating cement at isothermal temperature 25°C. At this temperature, the rate of hydration can be expressed by the chemical affinity
                     
                        
                           
                              
                                 A
                                 ˜
                              
                              25
                           
                           
                              (
                              α
                              )
                           
                        
                      at isothermal 25°C

                        
                           (5)
                           
                              
                                 
                                    
                                       
                                          d
                                       
                                       α
                                    
                                    
                                       
                                          d
                                       
                                       t
                                    
                                 
                                 =
                                 
                                    
                                       A
                                       ˜
                                    
                                    25
                                 
                                 
                                    (
                                    α
                                    )
                                 
                                 ,
                              
                           
                        
                     where the chemical affinity has a dimension of 
                        
                           time
                           
                              −
                              1
                           
                        
                      and α stands for the degree of hydration.

The affinity for isothermal temperature can be obtained experimentally; isothermal calorimetry measures a heat flow q(t) which gives the hydration heat Q(t) after integration. The approximation is given

                        
                           (6)
                           
                              
                                 
                                    
                                       
                                          
                                             Q
                                             (
                                             t
                                             )
                                          
                                          
                                             Q
                                             
                                                p
                                                o
                                                t
                                             
                                          
                                       
                                    
                                    
                                       ≈
                                    
                                    
                                       
                                          α
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (7)
                           
                              
                                 
                                    
                                       
                                          
                                             1
                                             
                                                Q
                                                
                                                   p
                                                   o
                                                   t
                                                
                                             
                                          
                                          
                                             
                                                
                                                   d
                                                
                                                Q
                                                (
                                                t
                                                )
                                             
                                             
                                                
                                                   d
                                                
                                                t
                                             
                                          
                                       
                                    
                                    
                                       =
                                    
                                    
                                       
                                          
                                             
                                                q
                                                (
                                                t
                                                )
                                             
                                             
                                                Q
                                                
                                                   p
                                                   o
                                                   t
                                                
                                             
                                          
                                          ≈
                                          
                                             
                                                
                                                   d
                                                
                                                α
                                             
                                             
                                                
                                                   d
                                                
                                                t
                                             
                                          
                                          =
                                          
                                             
                                                A
                                                ˜
                                             
                                             25
                                          
                                          
                                             (
                                             α
                                             )
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     where Qpot
                      is expressed in J/g of cement paste. Hence the normalised heat flow 
                        
                           
                              q
                              (
                              t
                              )
                           
                           
                              Q
                              
                                 p
                                 o
                                 t
                              
                           
                        
                      at isothermal 25°C equals chemical affinity 
                        
                           
                              
                                 A
                                 ˜
                              
                              25
                           
                           
                              (
                              α
                              )
                           
                        
                     .

Cervera et al. [33] proposed an analytical form for the normalised affinity, refined in [34]. Here we use a slightly modified formulation [35]:

                        
                           (8)
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                A
                                                ˜
                                             
                                             25
                                          
                                          
                                             (
                                             α
                                             )
                                          
                                          =
                                          
                                             B
                                             1
                                          
                                          
                                             (
                                             
                                                
                                                   B
                                                   2
                                                
                                                
                                                   α
                                                   ∞
                                                
                                             
                                             +
                                             α
                                             )
                                          
                                          
                                             (
                                             
                                                α
                                                ∞
                                             
                                             −
                                             α
                                             )
                                          
                                          exp
                                          
                                             (
                                             −
                                             
                                                η
                                                ¯
                                             
                                             
                                                α
                                                
                                                   α
                                                   ∞
                                                
                                             
                                             )
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     where B
                     1, B
                     2 are coefficients related to chemical composition, α
                     ∞ is the ultimate hydration degree, and 
                        
                           η
                           ¯
                        
                      represents microdiffusion of free water through formed hydrates.

When hydration proceeds at varying temperature, the maturity principle expressed via the Arrhenius equation scales the affinity to arbitrary temperature T
                     
                        
                           (9)
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                A
                                                ˜
                                             
                                             T
                                          
                                          =
                                          
                                             
                                                A
                                                ˜
                                             
                                             25
                                          
                                          exp
                                          
                                             [
                                             
                                                
                                                   E
                                                   a
                                                
                                                R
                                             
                                             
                                                (
                                                
                                                   1
                                                   
                                                      273.15
                                                      +
                                                      25
                                                   
                                                
                                                −
                                                
                                                   1
                                                   T
                                                
                                                )
                                             
                                             ]
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     where R is the universal gas constant (8.314 Jmol
                        
                           
                           
                              −
                              1
                           
                        
                     K
                        
                           
                           
                              −
                              1
                           
                        
                     ) and Ea
                      [Jmol
                        
                           
                           
                              −
                              1
                           
                        
                     ] is the activation energy. For example, simulating isothermal hydration at 35°C means scaling 
                        
                           
                              A
                              ˜
                           
                           25
                        
                      with a factor of 1.651 at a given time. This means that hydrating concrete for 10 h at 35°C releases the same amount of heat as concrete hydrating for 16.51 h at 25°C. Note that setting 
                        
                           
                              E
                              a
                           
                           =
                           0
                        
                      ignores the effect of temperature and simulates the hydration at 25°C. Evolution α is obtained through numerical integration since there is no exact analytical solution.

Since an ANN training process requires the preparation of a training data set, it is also worth using these data for a sampling-based sensitivity analysis [36,37] in order to obtain some information about the importance of particular observations or the significance of each parameter for system behaviour. To yield reliable information from sensitivity analysis as well as a good approximation by an ANN, one has to choose the training data carefully according to a suitable design of experiments, see e.g. [38] for a competitive comparison of several experimental designs.

As the model parameters are defined for various intervals, they need to be transformed into standardised parameters (e.g. pi
                      ∈ [0; 1]), defined on the intervals suitable for chosen activation functions. When the bounds for a parameter vary in order, this typically suggests a highly nonlinear relationship with model response. At this moment, any expert knowledge about a parameter's meaning can be employed to decrease the nonlinearity by introducing a nonlinear transformation to the standardised parameter. This is demonstrated by parameter B
                     2 in Table 1
                     , where bounds for the affinity model parameters together with their relationships to the standardised parameters pi
                      are listed.

The affinity hydration model was chosen not only for its nonlinearity, but especially for its ability to be relatively simply interpreted and computationally fast simulation characteristics. Hence, we assume that the model is eligible for illustrating the typical features of particular identification strategies. In order to better understand the influence of the model parameters to response, Fig. 3
                      demonstrates the changes of the response induced by changes in a chosen parameter while other parameters are fixed. To additionally illustrate the spread of model response corresponding to the parameters varying within the given domain, we prepare a design of experiments (DoE) having 
                        
                           
                              N
                              
                                 DoE
                              
                           
                           =
                           100
                        
                      samples in the space of standardised parameters. The DoE is generated as a Latin Hypercube Sampling optimised with respect to the modified L
                     2 discrepancy. Such an experimental design has a good space-filling property and is nearly orthogonal [38]. For each design point we perform a model simulation to obtain a bundle of N
                     DoE curves for the degree of hydration α(t), see Fig. 4
                     a.

Since the model response is represented by the degree of hydration being a function of time, the time domain is discretised into 1161 steps uniformly distributed according to the logarithm of the time. Hence, the model input vector 
                        
                           p
                           =
                           (
                           
                              p
                              1
                           
                           ,
                           
                              p
                              2
                           
                           ,
                           
                              p
                              3
                           
                           ,
                           
                              p
                              4
                           
                           )
                        
                      consists of 4 parameters and the output vector 
                        
                           
                              α
                           
                           =
                           (
                           
                              α
                              1
                           
                           ,
                           …
                           ,
                           
                              α
                              
                                 N
                                 
                                    time
                                 
                              
                           
                           )
                        
                      consists of 
                        
                           
                              N
                              
                                 time
                              
                           
                           =
                           1161
                        
                      components. In order to quantify the influence of the model parameters to particular response components, we evaluate Spearman’s rank correlation coefficient ρ for each (
                        p
                     
                     
                        i
                     , 
                        α
                     
                     
                        i
                     ) pair using all the 
                        
                           i
                           ∈
                           {
                           1
                           ,
                           …
                           ,
                           
                              N
                              
                                 DoE
                              
                           
                           }
                        
                      simulations. The results of such a sampling-based sensitivity analysis [36] are plotted in Fig. 4b.

In the inverse mode of identification, the model output vector 
                        α
                      consisting of 
                        
                           
                              N
                              
                                 time
                              
                           
                           =
                           1161
                        
                      components is too large for usage as an input vector for the ANN. Hence, we performed a principal component analysis (PCA) in order to reduce this number to 
                        
                           
                              N
                              
                                 PCA
                              
                           
                           =
                           100
                        
                      components 
                        
                           
                              
                                 α
                              
                              ¯
                           
                           =
                           
                              (
                              
                                 
                                    α
                                    ¯
                                 
                                 1
                              
                              ,
                              …
                              ,
                              
                                 
                                    α
                                    ¯
                                 
                                 2
                              
                              )
                           
                        
                      with non-zero variance. This number is related to the number of simulations involved in PCA, i.e. 
                        
                           
                              N
                              
                                 PCA
                              
                           
                           =
                           
                              N
                              
                                 DoE
                              
                           
                        
                     . The components are ordered according to their value of variance, see Fig. 5
                     a for the nine most important ones. The resulting principal components are technically new quantities obtained by a linear combination of the original model outputs 
                        
                           
                              
                                 α
                              
                              ¯
                           
                           =
                           
                              A
                              ¯
                           
                           
                              (
                              
                                 α
                              
                              )
                           
                        
                     . This transformation of course has an influence on the sensitivity analysis and thus we computed correlations between the model inputs 
                        p
                     
                     
                        i
                      and principal components 
                        
                           
                              
                                 
                                    α
                                 
                                 ¯
                              
                              i
                           
                           ,
                        
                      see Fig. 5b.

The results of the described simulations were also used as training simulations for the ANNs, i.e. 
                        
                           
                              D
                              
                                 train
                              
                           
                           =
                           
                              {
                              
                                 (
                                 
                                    p
                                    i
                                 
                                 ,
                                 
                                    
                                       α
                                    
                                    i
                                 
                                 )
                              
                              |
                              i
                              ∈
                              
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 
                                    N
                                    
                                       train
                                    
                                 
                                 }
                              
                              ,
                              
                                 N
                                 
                                    train
                                 
                              
                              =
                              
                                 N
                                 
                                    DoE
                                 
                              
                              =
                              100
                              }
                           
                        
                     . Particular approximation strategies, however, process the training simulations in different ways.

The strategy of forward model approximation can be formulated in two ways that handle the high dimensionality of the model output 
                        α
                      differently. In the first formulation, we can consider the time step tk
                      as the fifth model parameter (i.e. the fifth model input) and thus the model output reduces to only one scalar value of the hydration degree αk
                      corresponding to the given time tk
                     . As the objective of the ANN is to span the parameter as well as the time space, we called this strategy Forward Complex (ForwComp). In such a configuration, the results of N
                     train training simulations turn into 
                        
                           
                              N
                              
                                 train
                              
                           
                           ×
                           
                              N
                              
                                 time
                              
                           
                           =
                           116
                           ,
                           100
                        
                      training samples. Evaluation of so many samples at every iteration of the ANN’s training process is, however, very time-consuming. Therefore, only every mth time step is included for ANN training and thus the training set is given as 
                        
                           
                              D
                              
                                 train
                              
                              
                                 ForwComp
                              
                           
                           =
                           
                              {
                              
                                 (
                                 
                                    (
                                    
                                       p
                                       i
                                    
                                    ,
                                    
                                       t
                                       k
                                    
                                    )
                                 
                                 ,
                                 
                                    α
                                    
                                       i
                                       ,
                                       k
                                    
                                 
                                 )
                              
                              
                              |
                              
                              i
                              ∈
                              
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 
                                    N
                                    
                                       train
                                    
                                 
                                 }
                              
                              ,
                              k
                              ∈
                              
                                 {
                                 1
                                 ,
                                 1
                                 +
                                 m
                                 ,
                                 1
                                 +
                                 2
                                 m
                                 ,
                                 …
                                 ,
                                 
                                    N
                                    
                                       time
                                    
                                 
                                 }
                              
                              }
                           
                        
                     . In our particular implementation, we selected 
                        
                           m
                           =
                           10
                        
                      leading to 
                        
                           
                              |
                           
                           
                              D
                              
                                 train
                              
                              
                                 ForwComp
                              
                           
                           
                              |
                              =
                              11
                              ,
                              700
                           
                        
                      samples. Note that in all other strategies, the number of training samples equals the number of training simulations, see Table 2
                     , where the significant parameters of particular approximation strategies are briefly summarised.

The second method of model output approximation is based on training an independent ANN for every time step tk
                     . Here, the particular ANN approximates a simpler relationship and only spans the parameter space. A training data set for ANN approximating the response component αk
                      is thus given as 
                        
                           
                              D
                              
                                 train
                              
                              
                                 
                                    ForwSpli
                                    ,
                                 
                                 
                                    α
                                    k
                                 
                              
                           
                           =
                           
                              {
                              
                                 (
                                 
                                    p
                                    i
                                 
                                 ,
                                 
                                    α
                                    
                                       i
                                       ,
                                       k
                                    
                                 
                                 )
                              
                              
                              |
                              
                              i
                              ∈
                              
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 100
                                 }
                              
                              }
                           
                        
                      having only 
                        
                           
                              |
                           
                           
                              D
                              
                                 train
                              
                              
                                 
                                    ForwSpli
                                    ,
                                 
                                 
                                    α
                                    k
                                 
                              
                           
                           
                              |
                              =
                              100
                           
                        
                      samples. A disadvantage of such an approach consists in training a large number N
                     ANN of smaller ANNs. As training of 
                        
                           
                              N
                              
                                 ANN
                              
                           
                           =
                           
                              N
                              
                                 time
                              
                           
                           =
                           1161
                        
                      different ANNs is almost unfeasible, we select only a few of the time steps where the approximation is constructed and thus the model output approximation is rougher. The choice of the important time steps and their number can be driven by expert knowledge or the results of sensitivity analysis. Hence, we present three different choices so as to illustrate this influence, see Table 2. We further call these strategies Forward Split (ForwSpli), Forward Split II (ForwSpliII), and Forward Split III (ForwSpliIII).

Error function approximation is the only strategy where the high dimensionality of the model output does not impose any complications. The model output is used for evaluating the error function and the ANN is trained to approximate the mapping from the parameter space to a single scalar value of the error function, i.e. 
                        
                           
                              D
                              
                                 train
                              
                              
                                 
                                    Error
                                    ,
                                 
                                 
                                    F
                                    a
                                 
                              
                           
                           =
                           
                              {
                              
                                 (
                                 
                                    p
                                    i
                                 
                                 ,
                                 
                                    F
                                    a
                                 
                                 )
                              
                              
                              |
                              
                              i
                              ∈
                              
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 
                                    N
                                    
                                       train
                                    
                                 
                                 }
                              
                              }
                           
                        
                      and 
                        
                           
                              |
                           
                           
                              D
                              
                                 train
                              
                              
                                 
                                    Error
                                    ,
                                 
                                 
                                    F
                                    a
                                 
                              
                           
                           
                              |
                              =
                              100
                              ,
                           
                        
                      where Fa
                      stands for a chosen error function. As we already mentioned in Section 3, there are two very common error functions given by Eqs. (3) and (4) and thus we investigate both two strategies referred to hereafter as Error F
                        1
                      and Error F
                        2
                     , respectively.

For inverse relationship approximation, the high dimensionality of model output again needs special treatment to limit the number of ANN inputs and thus keep the complexity of the ANN reasonable. An intuitive approach is a simple selection of a limited number of output values 
                        
                           a
                           =
                           A
                           (
                           
                              α
                           
                           )
                        
                     . Here, one ANN is trained to predict one model parameter pj
                      and thus 
                        
                           
                              D
                              
                                 train
                              
                              
                                 
                                    InvExp
                                 
                                 ,
                                 
                                    p
                                    j
                                 
                              
                           
                           =
                           
                              {
                              
                                 (
                                 
                                    a
                                    i
                                 
                                 ,
                                 
                                    p
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 )
                              
                              
                              |
                              
                              i
                              ∈
                              
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 
                                    N
                                    
                                       train
                                    
                                 
                                 }
                              
                              }
                           
                        
                      and 
                        
                           
                              |
                           
                           
                              D
                              
                                 train
                              
                              
                                 
                                    InvExp
                                    ,
                                 
                                 
                                    p
                                    j
                                 
                              
                           
                           
                              |
                              =
                              100
                           
                        
                     . A particular choice of components in the vector 
                        a
                     
                     
                        i
                      defined by the operator A should take into account not only the results of sensitivity analysis but also a possible measurement error in experimental data as well as any other expert knowledge. Hence we present again two different choices in order to illustrate this influence, see Table 2, and we further call these configurations Inverse Expert (InvExp) and Inverse Expert II (InvExpII).

In order to reduce the influence of expert choice, the principal components 
                        
                           
                              α
                           
                           ¯
                        
                      computed as described in the previous section can be used as inputs for the ANN and therefore one only has to choose their number. To compare the information contained in the same number of inputs selected by an expert, we have chosen the same number of principal components as the number of inputs in the Inverse Expert configuration and thus 
                        
                           
                              D
                              
                                 train
                              
                              
                                 
                                    InvPCA
                                 
                                 ,
                                 
                                    p
                                    j
                                 
                              
                           
                           =
                           
                              {
                              
                                 (
                                 
                                    (
                                    
                                       
                                          α
                                          ¯
                                       
                                       
                                          i
                                          ,
                                          1
                                       
                                    
                                    ,
                                    …
                                    ,
                                    
                                       
                                          α
                                          ¯
                                       
                                       
                                          i
                                          ,
                                          9
                                       
                                    
                                    )
                                 
                                 ,
                                 
                                    p
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 )
                              
                              
                              |
                              
                              i
                              ∈
                              
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 
                                    N
                                    
                                       train
                                    
                                 
                                 }
                              
                              }
                           
                        
                      and 
                        
                           
                              |
                           
                           
                              D
                              
                                 train
                              
                              
                                 
                                    InvPCA
                                    ,
                                 
                                 
                                    p
                                    j
                                 
                              
                           
                           
                              |
                              =
                              100
                           
                        
                     . The principal components-based strategy is further called Inverse PCA (InvPCA). In our preliminary study presented in [10], we have also tested the possibility of choosing a smaller number of PCA-based inputs selected separately for each parameter to be identified according to sensitivity analysis. Nevertheless, such sensitivity-driven reduction of PCA-based inputs was shown to reduce the quality of trained ANNs.

The last preparatory step concerns generating testing data for final assessment of the resulting ANNs consisting of 
                        
                           
                              N
                              
                                 test
                              
                           
                           =
                           50
                        
                      simulations for randomly generated sets of input parameters. The obtained data are then processed by approximation strategies in the same way as the training data described above.

The quality of the ANN-based approximation estimated for a given data set 
                        D
                      can be expressed as the mean relative prediction error 
                        
                           
                              
                                 ɛ
                              
                              
                                 MRP
                              
                           
                           
                              (
                              D
                              )
                           
                        
                      given as

                        
                           (10)
                           
                              
                                 
                                    
                                       ɛ
                                    
                                    
                                       MRP
                                    
                                 
                                 
                                    (
                                    D
                                    )
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             |
                                             D
                                             |
                                          
                                       
                                       
                                          |
                                          
                                             O
                                             i
                                          
                                          −
                                          
                                             T
                                             
                                                i
                                                ,
                                                D
                                             
                                          
                                          |
                                       
                                    
                                    
                                       |
                                       D
                                       |
                                       (
                                       
                                          T
                                          
                                             max
                                             ,
                                             
                                                D
                                                train
                                             
                                          
                                       
                                       −
                                       
                                          T
                                          
                                             min
                                             ,
                                             
                                                D
                                                train
                                             
                                          
                                       
                                       )
                                    
                                 
                                 
                                 ,
                              
                           
                        
                     where Oi
                      is the ANN’s output corresponding to the target value 
                        
                           T
                           
                              i
                              ,
                              D
                           
                        
                      contained in the data set 
                        
                           D
                           ,
                        
                      which consists of 
                        
                           |
                           D
                           |
                        
                      samples. 
                        
                           T
                           
                              max
                              ,
                              
                                 D
                                 train
                              
                           
                        
                      and 
                        
                           T
                           
                              min
                              ,
                              
                                 D
                                 train
                              
                           
                        
                      are the maximal and minimal target values in the training data set 
                        
                           
                              D
                              
                                 train
                              
                           
                           ,
                        
                      so the error 
                        
                           
                              
                                 ɛ
                              
                              
                                 MRP
                              
                           
                           
                              (
                              D
                              )
                           
                        
                      is always scaled by the same factor for any chosen data set 
                        D
                     , and this factor corresponds to the range of the training data.

The conjugate gradient-based method [39] was applied as a training algorithm for synaptic weights computation and the cross-validation method was employed to determine the number of hidden neurons. In V-fold cross-validation we break the training data set 
                        
                           D
                           
                              train
                           
                        
                      into V subsets 
                        
                           
                              D
                              
                                 train
                              
                           
                           =
                           
                              D
                              
                                 
                                    train
                                 
                                 ,
                                 1
                              
                           
                           ∪
                           
                              D
                              
                                 
                                    train
                                 
                                 ,
                                 2
                              
                           
                           ∪
                           ⋯
                           ∪
                           
                              D
                              
                                 
                                    train
                                    ,
                                 
                                 V
                              
                           
                        
                      of approximately equal size. Then we perform V training processes, each time leaving out one of the subsets 
                        
                           D
                           
                              
                                 train
                              
                              ,
                              i
                           
                        
                      and using the rest of the training data set 
                        
                           
                              D
                              
                                 train
                              
                           
                           ∖
                           
                              D
                              
                                 train
                                 ,
                                 i
                              
                           
                        
                     .

The criterion for stopping the training process is governed by the prediction errors ratio 
                        
                           r
                           k
                           
                              PE
                           
                        
                      computed at the kth iteration of the training algorithm given as

                        
                           (11)
                           
                              
                                 
                                    r
                                    k
                                    
                                       PE
                                    
                                 
                                 
                                    (
                                    
                                       D
                                       
                                          train
                                       
                                    
                                    ∖
                                    
                                       D
                                       
                                          train
                                          ,
                                          i
                                       
                                    
                                    )
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             k
                                             −
                                             J
                                          
                                          k
                                       
                                       
                                          
                                             ɛ
                                          
                                          j
                                          
                                             MRP
                                          
                                       
                                       
                                          (
                                          
                                             D
                                             
                                                train
                                             
                                          
                                          ∖
                                          
                                             D
                                             
                                                train
                                                ,
                                                i
                                             
                                          
                                          )
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             k
                                             −
                                             2
                                             J
                                          
                                          
                                             k
                                             −
                                             J
                                             −
                                             1
                                          
                                       
                                       
                                          
                                             ɛ
                                          
                                          j
                                          
                                             MRP
                                          
                                       
                                       
                                          (
                                          
                                             D
                                             
                                                train
                                             
                                          
                                          ∖
                                          
                                             D
                                             
                                                train
                                                ,
                                                i
                                             
                                          
                                          )
                                       
                                    
                                 
                                 
                                 ,
                              
                           
                        
                     where 
                        
                           
                              
                                 ɛ
                              
                              j
                              
                                 MRP
                              
                           
                           
                              (
                              
                                 D
                                 
                                    train
                                 
                              
                              ∖
                              
                                 D
                                 
                                    train
                                    ,
                                    i
                                 
                              
                              )
                           
                        
                      is the mean relative prediction error obtained at the jth iteration of the training algorithm obtained on the training data set without its ith partition. J is the chosen number of iterations considered for computing the ratio 
                        
                           r
                           k
                           
                              PE
                           
                        
                      for its smoothing effect on 
                        
                           r
                           k
                           
                              PE
                           
                        
                     . The training process is stopped either when the number of iterations achieves its chosen maximal value K or if the prediction errors ratio 
                        
                           r
                           k
                           
                              PE
                           
                        
                      exceeds a chosen critical value 
                        
                           r
                           
                              max
                           
                           
                              PE
                           
                        
                     .

Once the training process is completed, the ANN is evaluated using the part of training data 
                        
                           
                              D
                              
                                 train
                                 ,
                                 i
                              
                           
                           ,
                        
                      which was not previously used in the training process. The quality of the ANN with a particular number of hidden neurons h is assessed by the cross-validation error 
                        
                           
                              
                                 ɛ
                              
                              h
                              
                                 CV
                              
                           
                           ,
                        
                      which is computed as a mean of the errors obtained for the ANNs trained on subsets 
                        
                           
                              D
                              
                                 train
                              
                           
                           ∖
                           
                              D
                              
                                 train
                                 ,
                                 i
                              
                           
                        
                      and then evaluated on the remaining subset 
                        
                           D
                           
                              train
                              ,
                              i
                           
                        
                      , i.e.

                        
                           (12)
                           
                              
                                 
                                    
                                       ɛ
                                    
                                    h
                                    
                                       CV
                                    
                                 
                                 =
                                 
                                    1
                                    V
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    V
                                 
                                 
                                    
                                       ɛ
                                    
                                    
                                       MRP
                                    
                                 
                                 
                                    (
                                    
                                       D
                                       
                                          train
                                          ,
                                          i
                                       
                                    
                                    )
                                 
                                 
                                 .
                              
                           
                        
                     We start with an ANN having h
                     min hidden neurons and we compute the corresponding cross-validation error. Then one hidden neuron is added. After all the training processes on training data subsets, a new cross-validation error is evaluated. We compute the cross-validation error ratio 
                        
                           r
                           h
                           
                              CVE
                           
                        
                      as

                        
                           (13)
                           
                              
                                 
                                    r
                                    h
                                    
                                       CVE
                                    
                                 
                                 =
                                 
                                    
                                       ɛ
                                    
                                    h
                                    
                                       CV
                                    
                                 
                                 /
                                 
                                    
                                       ɛ
                                    
                                    
                                       h
                                       −
                                       1
                                    
                                    
                                       CV
                                    
                                 
                                 
                                 .
                              
                           
                        
                     We count situations when the ratio 
                        
                           r
                           h
                           
                              CVE
                           
                        
                      exceeds a chosen critical value 
                        
                           r
                           
                              max
                           
                           
                              CVE
                           
                        
                     . If this happens W times, the addition of hidden neurons is stopped. We then choose the architecture having the smallest cross-validation error 
                        
                           
                              ɛ
                           
                           h
                           
                              CV
                           
                        
                      and the particular ANN having the smallest training error εMRP. Specific parameter setting of the training algorithm as well as the cross-validation method for our computations is given in Table 3
                     .

The resulting ANNs are tested using an independent testing data set 
                        
                           D
                           
                              test
                           
                        
                     . Since some of the approximation strategies consist of a high number of ANNs, the resulting number of hidden neurons and errors with training and testing data for all trained ANNs are listed in Appendix A. A brief summary of these results is presented in Table 4
                     
                     
                        1
                     
                     
                        1
                        The error function approximation strategies are intrinsically related to particular experimental curve. The results here are obtained for experimental ”Mokra” data described in Section 9 in more detail.
                     .

Regarding the number of hidden neurons, the results indicate higher complexity of the error function relationships. Nevertheless, the differences in hidden neurons among particular strategies are relatively small.

The quality of the resulting ANNs in approximating the given relationships is measured according to the errors obtained for all the training 
                        
                           
                              
                                 ɛ
                              
                              
                                 MRP
                              
                           
                           
                              (
                              
                                 D
                                 
                                    train
                                 
                              
                              )
                           
                        
                      and testing 
                        
                           
                              
                                 ɛ
                              
                              
                                 MRP
                              
                           
                           
                              (
                              
                                 D
                                 
                                    test
                                 
                              
                              )
                           
                        
                      data. Small differences between the training and testing errors illustrate well-trained ANNs and the good quality of the training method as well as the method for topology estimation. Note that overtrained ANNs usually lead to significantly higher errors with testing data.

Comparing the approximation quality of the particular strategies, we can point out good results for forward model approximation and error function approximation, where errors did not exceed 3%. The good approximation for the forward model is not surprising since the relationship is well-defined, smooth, and relatively simple. Good results for the error function approximation are more unexpected because the relationship here is more nonlinear and complex. One possible explanation for this is a large spread of error function values used to scale the errors (see Eq. (10)) with the training data. While the error functions converge to zero near the optimal parameter values, they quickly rise to extremely high values for parameter values more distant from the optimal ones. Hence, we presume that the small errors obtained with error function approximation do not promise comparably good results for final parameter identification.

The results of inverse relationship approximation are not very good, but this was foreseen due to an unknown and probably ill-posed relationship. Nevertheless, the errors obtained are actually the final errors for the whole identification process with training and testing data, since there is no other following step concerning optimisation as in the case of the other identification strategies. Hence, further comments on these results are presented in the following section with regard to verifying the overall identification strategies with testing data.

Since the errors in Table 4 represent only the quality of constructed ANNs, we must also investigate the quality of the identification procedures. This section is devoted to verification of the model calibration, where the goal is to predict model parameter values corresponding to simulated data which are not perturbed by any noise. The advantage of verification is that we also know the true values of the parameters and thus we can easily evaluate the quality of their estimations according to each strategy. In particular, calibration strategies were applied to estimate the parameters’ values for all the training and testing simulations.

As mentioned above, in the case of inverse relationship approximation, the outputs of ANNs are directly the predicted values of the identified parameters 
                        
                           p
                           ^
                        
                     . In the case of forward model approximation, we have to run a subsequent optimisation process. Here, the evolutionary algorithm GRADE, see [9] for details about this method
                        2
                     
                     
                        2
                        The parameters of GRADE algorithm were set to pool_rate = 4, radioactivity = 0.33 and cross_limit = 0.1. The algorithm was stopped after 10, 000 cost function evaluations.
                     , is applied to find a set of parameter values 
                        
                           p
                           ^
                        
                      minimising the square distance δ between components of the model response αk
                      and their corresponding ANN-based approximated counterparts 
                        
                           
                              
                                 α
                                 ˜
                              
                              k
                           
                           ,
                        
                      i.e.

                        
                           (14)
                           
                              
                                 δ
                                 =
                                 
                                    ∑
                                    k
                                 
                                 
                                    
                                       (
                                       
                                          α
                                          k
                                       
                                       −
                                       
                                          
                                             α
                                             ˜
                                          
                                          k
                                       
                                       )
                                    
                                    2
                                 
                                 
                                 ,
                              
                           
                        
                     where k corresponds to the selected approximated components defined for the specific identification strategies in Table 2. In this way, parameters 
                        
                           p
                           ^
                        
                      are predicted for all the training and testing data. Since the true values of parameters 
                        p
                      are known in the verification process, mean prediction errors 
                        
                           
                              ɛ
                           
                           ^
                        
                      are computed relatively according to the spread of training data, i.e.

                        
                           (15)
                           
                              
                                 
                                    ϵ
                                    ^
                                 
                                 
                                    (
                                    
                                       
                                          p
                                          ^
                                       
                                       j
                                    
                                    )
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             |
                                             D
                                             |
                                          
                                       
                                       
                                          |
                                          
                                             p
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                          −
                                          
                                             
                                                p
                                                ^
                                             
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                          |
                                       
                                    
                                    
                                       |
                                       D
                                       |
                                       (
                                       
                                          p
                                          
                                             
                                                max
                                             
                                             (
                                             
                                                D
                                                
                                                   train
                                                
                                             
                                             )
                                             ,
                                             j
                                          
                                       
                                       −
                                       
                                          p
                                          
                                             
                                                min
                                             
                                             (
                                             
                                                D
                                                
                                                   train
                                                
                                             
                                             )
                                             ,
                                             j
                                          
                                       
                                       )
                                    
                                 
                                 .
                              
                           
                        
                     Obtained errors for particular identification strategies are listed in Table 5
                     .

The parameter values for applying the identification strategy to real experimental data, are not known, but the success of the identification process is quantified by the quality of fitting the data by the model response obtained for the identified parameters. Hence, the model simulations were performed for all the identified parameter sets and prediction errors 
                        
                           
                              ɛ
                           
                           ˜
                        
                      in terms of predicted responses 
                        
                           
                              α
                           
                           ˜
                        
                     , computed analogously to Eq. (15). Their values, averaged over all response components are then listed in Table 5.

Results for strategies based on an approximation of the error function are missing here because they require building a unique ANN for every curve for each hydration degree, requiring performing an additional minimisation procedure. This is overwhelming, and thus these strategies are only validated using experimental data, as described in the following section.

One can see that among forward strategies, the complex variant provides the worst results in the training process as well as in the final identification. The complex relationship covering the time domain apparently causes certain difficulties during the training process. We can conclude that training of a set of neural networks means more work but yields significantly better quality in model approximation. We can also point out large differences in errors for particular parameters which correspond to the influence of particular parameters for model response. As demonstrated in Fig. 3, the largest spread of model response is related namely to a change in parameters p
                     4 and p
                     3, while parameters p
                     1 and p
                     2 have almost negligible spread. The sensitivity analysis illustrated in Fig. 4b shows very high sensitivity of model response to the parameter p
                     2 at an early stage of hydration; nevertheless, at this stage, the spread of model response is almost negligible, and even a very small error in response approximation can be fatal for identification of parameter p
                     2. On the other hand, it is not surprising that identification accuracy is significantly improved with an increased number of approximated response components, i.e. an increased number of trained ANNs.

Despite the inferior results in training of ANNs, the inverse strategies achieved comparably good results with the forward strategies in parameter identification and also in fitted measurements. More precisely, the results of measurements fitting are slightly worse but errors in parameter prediction are smaller than in the case of forward strategies. Especially the Inverse Expert strategies provided surprisingly small errors in p
                     2 prediction and errors in parameters are generally more balanced. This phenomenon can be possibly explained by the fact that each ANN is trained to predict each parameter separately, thus automatically selecting and emphasising the combinations of model response critical for the parameter. With the Inverse Expert II strategy, the usage of one additional input at an early stage of hydration caused no improvement in the resulting prediction, which is likely caused again because the responses at this stage have a negligible spread and almost no predictive value. A final interesting result concerns the application of principal component analysis. The Inverse PCA strategy again provided significantly different errors in predicting particular parameters, similar to the forward strategies. The reason for this is likely the fact that PCA emphasises the most important components, while mixing the effects of less significant parameters. Nevertheless, when compared with Forward Split and Inverse Expert strategies using the same number of response components, Inverse PCA provided the best results in prediction of all the parameters except p
                     2. Its quality of measurement fitting is, however, the worst among all strategies.

From this thorough comparison we may conclude that all inverse strategies provide very good results, which makes them highly promising considering they are very simple to implement and that they do not require any additional optimisation process once the ANNs have been trained. Moreover, Inverse Expert strategies can be recommended for identifying less significant parameters.

The previous section was focused on comparing the presented identification strategies for simulated data. However, a complete comparison must include validation for experimental data. To that end, we used four experimental data sets obtained using isothermal calorimetry: one for “Mokra” CEM I 42.5 R cement (taken directly from Heidelberg cement group’s kiln in Mokrá, Czech Republic [35]) and three others from the following literature: “Boumiz” [40], “Hua” [41] and “Princigallo” [42].

For parameter identification using experimental data, one often faces to difficulties related to (i) experimental errors and (ii) model imperfections. Especially in case of models with parameters having a specific physical meaning such as the affinity hydration model, it can happen that the experimental data seems to lie beyond physically meaningful values for model parameters. This is exactly what we faced in the case of the four experimental curves depicted in Fig. 6. The grey curves represent training samples generated in an optimised fashion so as to maximally cover the parameter space. Nevertheless, it is clear that all the experimental curves lie out of the bundle of the training samples. Applying identification strategies to these data would require the ANNs to extrapolate and would probably lead to unrealistic and wrong predictions for model parameters. Such results were presented for “Mokra” in [11]. Looking in more detail at the experimental curves, one can see that the difference between experimental data and the simulations can be explained by an incorrect estimation regarding the origin of hydration. Correction of the starting time moves the curves into the bundle of response simulations. As a matter of fact, the correction in orders of hours is negligible compared to the duration of the entire hydration process, which often lasts days or weeks. Moreover, the goal of this paper is not to argue against the correctness of the model or data, but to demonstrate the properties of particular identification strategies which can be better illustrated in a situation where observed data are not outliers w.r.t. the sampled parameter domain. We recommend [11] to readers interested in the identification of outliers.

In general, validation does not allow for a comparison in terms of parameter values because these are not known a priori. Nevertheless, the simplicity and rapid simulation of the affinity hydration model allow direct optimisation of the model parameters so as to fit the measured data without any incorporated approximation. The resulting optimal solutions can be then compared with the results obtained using the ANN approximations. To that end, we again employ the error functions given in Eqs. (3) and (4) and the GRADE algorithm with the same settings as in the previous section to minimise both error functions. The results obtained are referred to as Direct1 and Direct2, respectively, and they represent the best results that can be achieved with the current model on the given data.

Subsequently, the identification strategies were applied to the experimental data using prepared ANNs. Since the ANNs were constructed for specific time steps of particular hydration degrees, the experimental curves are interpolated according to the time steps required by particular ANNs. If necessary, the data are extrapolated beyond the last measured time step assuming further progress of the hydration to be constant for the last measured value. The identified parameters together with the parameter values obtained by direct optimisation are written in Tables 6
                      and 7
                     . Note that the parameter values highlighted in bold font refer to situations in which the measured data lie beyond the domain of training data and the ANN is forced to extrapolate. The identified parameters were used as inputs for simulations and results are compared with experimental data in Figs. 7
                      and 8
                     . To quantify the quality of obtained fits, Tables 6 and 7 contain also the mean relative error 
                        
                           
                              
                                 ɛ
                              
                              ^
                           
                           
                              (
                              
                                 α
                                 ^
                              
                              )
                           
                        
                      [%] computed in the same manner as in Table 5 for easy comparison of the verification and validation results.

The strategies based on error function approximation are illustrated for parameter identification from “Mokra” data used to define error functions approximated by the ANNs. The trained ANNs are then optimised by the GRADE algorithm so as to provide optimal set for the identified parameters. As we presumed, the identification results are not satisfactory despite very good results for ANN training processes, see Table 4. The training and testing errors are relatively small when compared to the spread of error function values. These values increase quickly as the distance from the optimal solution grows. This strategy, however, requires high precision of ANN approximation near the optimal solution, which can be hardly achieved due the overall complex shape of the error functions.

The worst results for all the experimental curves were obtained by the inverse strategies based on selected components of model response used as ANN inputs. These results illustrate the high sensitivity of this strategy to measurement noise and specific choice of inputs. Both drawbacks were overcome by employing principal component analysis, which allows a high number of the response components and which filters measurement noise out of the several first principal components. The Inverse PCA strategy thus achieved significantly improved results.

The forward strategies generally provided the best results, consistent with the results of verification on simulated data. These strategies thus proved to be rather immune to the noise from experimental data.

@&#CONCLUSIONS@&#

This paper reviews and compares several possible applications of artificial neural networks to the calibration of numerical models. In particular, a feed-forward layered neural network is employed for three basic schemes to surrogate: (i) response of a model, (ii) inverse relationship of model parameters and model response and (iii) error function quantifying how well model response fits experimental data. Advantages and drawbacks are illustrated with calibration of four parameters for the affinity hydration model. This model was chosen for its nonlinearity as well as its difference in sensitivities to particular parameters on the one hand and its simplicity and very rapid numerical evaluation, on the other. These advantages allow for model calibration based on the stochastic evolutionary algorithm without any involved approximation and thus result in better quantification of calibration results provided according to particular strategies. The investigated calibration strategies are verified for 50 simulated curves of hydration degree and validated on four experimental ones.

A simplified summary of obtained results is provided in Table 8. One of the simplest strategies from the implementation standpoint is based on an approximation of the error function (Error F), where only one neural network needs to be trained for the prediction of error function values. This simplicity, however, does not hold in case of multiple experimental measurements where the whole identification process (including the neural network training as well as its optimisation) needs to be conducted all over again for every new experiment. Moreover, the presented examples reveal that the complexity of error function may cause difficulties for neural network training, resulting in high errors for identified parameters. The potential of the neural network is wasted on approximating the whole domain, while accurate predictions are required only in the vicinity of optimal values for parameters. Hence, this strategy is more suited for surrogate models based on radial basis function networks or kriging, which can be trained along with the optimisation of error function, thus allowing for improving the precision of a promising area, see e.g. [26].

An equally simple strategy is based on the approximation of model response, where time or space variables are included among neural network inputs (Forward Complex). This strategy is better suited for layered neural networks, trained only once and then used repeatedly for any new observations. The effort invested into the approximation of a whole domain is thus not wasted. The application to new data requires only one new optimisation process. The results obtained by this strategy were not excellent but can be considered to be a satisfactory solution at a low price.

The best results were achieved by separate approximations of particular response components where a higher number of neural networks is trained to approximate rather simple relationships defined by the calibrated model (Forward Split). This procedure requires more work for networks preparation, but yields highly accurate results which increase proportionally to the number of approximated response components and can thus be influenced by work invested in constructing the surrogate. Moreover, such constructed approximations can be then used again for any new data, where only the optimisation of model parameters needs to be repeated.

The strategy approximating inverse mapping from the response components to the model parameters (Inverse Expert) yielded the worst results. Such relationships do not have to exist and are difficult to approximate. Moreover, if the inputs for a neural network are not properly selected and are therefore highly sensitive to measurement error, unsatisfactory results are obtained. Nevertheless, using expert knowledge for proper selection of inputs (as presented in [28]) provides good results at a very low price, since neither training nor optimisation process are required. Only a simple evaluation of the trained networks is needed for parameter identification from new data.

The necessity of expert knowledge and sensitivity to measurement error can be easily circumvented by employing principal component analysis on the model response components (Inverse PCA). Then, only the number of components entering as inputs to the neural network needs to be selected. The strategy thus represents a compromise solution, providing satisfactory results at a low price, particularly for repeated application with new observed data.

@&#ACKNOWLEDGEMENT@&#

This work was made possible with financial support from the Czech Science Foundation (project nos. 16-11473Y and 15-07299S, which we gratefully acknowledge. We would like also to thank Vít Šmilauer (Czech Technical University in Prague) for providing us with code for the affinity hydration model, experimental data, and helpful advice.

ANN inputs and outputs are presented in Table A.9, for forward and inverse mode strategies, respectively.

@&#REFERENCES@&#

