@&#MAIN-TITLE@&#High dimensional low sample size activity recognition using geometric classifiers

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This paper addresses classification of High Dimensional, Low Sample Size Data (HDLSS).


                        
                        
                           
                           An efficient QR factorization base Nearest Affine Hull approach NAH-lsq is proposed.


                        
                        
                           
                           Extensive evaluation of 8 approaches is carried out on 5 HDLSS datasets.


                        
                        
                           
                           NAH-lsq outperforms other Geometrical approaches in accuracy and efficiency.


                        
                        
                           
                           In online settings, it achieves faster classification than online SVMs.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

High dimension low sample size classification

Action recognition

Geometric classification

Multimedia analysis

@&#ABSTRACT@&#


               
               
                  Research on high dimension, low sample size (HDLSS) data has revealed their neighborless nature. This paper addresses the classification of HDLSS image or video data for human activity recognition. Existing approaches often use off-the-shelf classifiers such as nearest neighbor techniques or support vector machines and tend to ignore the geometry of underlying feature distributions. Addressing this issue, we investigate different geometric classifiers and affirm the lack of neighborhoods within HDLSS data. As this undermines proximity based methods and may cause over-fitting for discriminant methods, we propose a QR factorization approach to Nearest Affine Hull (NAH) classification which remedies the HDLSS dilemma and noticeably reduces time and memory requirements of existing methods. We show that the resulting non-parametric models provide smooth decision surfaces and yield efficient and accurate solutions in multiclass HDLSS scenarios. On several action recognition benchmarks, the proposed NAH classifier outperforms other instance based methods and shows competitive or superior performance than SVMs. In addition, for online settings, the proposed NAH method is faster than online SVMs.
               
            

high dimension low sample size

linear discriminant analysis


                        k nearest neighbor

nearest affine hull

nearest convex hull

nearest hyperdisk

one-against-all SVM

one-against-one SVM

support vector machine

SVM with stochastic gradient descent

singular value decomposition

@&#INTRODUCTION@&#

Modern computer vision and pattern recognition tasks deal with large amounts of data of arguably moderate dimensionality. High dimension, low sample size (HDLSS) data therefore constitute a special case which, however, is becoming increasingly common in practical settings. Consider, for example, the problem of human activity recognition from unconstrained web videos. While the dimensionality of spatio-temporal features extracted from videos often ranges in the tens of thousands, the number of labeled instances per activity class seldom exceeds a few hundred. For such data, the curse of dimensionality causes distances among feature vectors to become uniform [1,9]. Scarcity and sparsity of labeled training data result in simplicial class regions in feature space where each training instance forms a vertex of the convex hull of the data set. Hence, classifiers based on approximating class regions, discriminating among class regions, or computing low-rank representations suffer from artifacts due to high dimensionality. It is therefore important to understand and analyze the distribution and geometry of data in multiclass HDLSS scenarios. Yet, recent approaches in computer vision and, in particular, in human activity recognition, tend to ignore these issues and use off-the-shelf approaches such as Support Vector Machines, Linear Discriminant Analysis, or k Nearest Neighbors without consideration.

In this paper, we investigate the geometry of subspaces spanned by high dimensional feature vectors related to human activities and affirm the neighborlessness of these data. We show that Nearest Affine Hull (NAH) classifiers which loosely approximate class regions yield competitive or even better classification rates than SVMs for multiclass HDLSS. While existing NAH approaches [6] to HDLSS classification suffer from high time and memory requirements, we propose an efficient NAH approach by adopting a least squares method based on the QR factorization.

We compare the empirical performance of the proposed NAH-lsq classifiers with that of other hull based methods, namely Nearest Hyperdisk, Nearest Convex Hull, and NAH-svd as well as with more traditional approaches such as kNN, LDA, and SVM. Note that the latter represents different paradigms. For instance, kNN classifiers are based on local proximities; LDA focuses on low rank approximations, and SVMs apply the idea of maximum margin separation. Our results show that NAH approaches are competitive to SVMs in terms of accuracy and superior to all other classifiers in our tests. Like other lazy classifiers (e.g. kNN and NCH), NAH requires no training. We also show the efficiency of NAH-lsq to be comparable to one-against-one SVMs and that it is far superior to other instance based approaches including NAH-svd.

Our empirical evaluation also reveals that, for the case of multiclass HDLSS data, optimal classification is achieved when using almost all the training data to support decision surfaces. In addition, we show that NAHs are well suited for online learning scenarios where even fast SVM-based algorithms such as the sequential approximation method LASVM [3] and stochastic gradient descent method SVM-SGD [4] suffer from expensive retraining. Moreover, unlike other methods, NAHs are inherently non-parametric and require no cross validation in online settings. In short, this paper provides empirical insights into the complexity of the multiclass HDLSS classification problem and offers a simple yet effective solution.

Our presentation proceeds as follows: Section 2 reviews related work; Section 3 discusses geometric classifiers and a novel, efficient approach to NAH classification is presented in Section 4. Section 5 provides details as to our benchmark datasets and feature extraction methodologies while Section 6 reports our results. Finally, Section 7 concludes the article.

@&#RELATED WORK@&#

Analyzing properties of HDLSS data has been an active area of research. Asymptotic studies reveal a tendency for high dimensional data to form vertices of regular simplices [9,11]. Hall et al. [11] show that for two sets X and Y in 
                        
                           
                              R
                           
                           
                              d
                           
                        
                      where 
                        d
                        ≫
                        |
                        X
                        |
                        +
                        |
                        Y
                        |
                      and no k points form a 
                        k
                        −
                        2
                      dimensional hyperplane, it is always possible to find a hyperplane that separates X and Y. Donoho and Tanner [9] show that projecting a simplex from a very high dimension d to a 
                        c
                        =
                        ρ
                        d
                      dimensional space does not reduce the number of corresponding l-dimensional faces for 
                        l
                        ≤
                        ⌊
                        ρ
                        c
                        ⌋
                     . Even the property of k-neighborliness is retained (a polytope is called k-neighborly if every subset of k vertices forms a 
                        (
                        k
                        −
                        1
                        )
                     -face [10]), so that dimensionality reduction does not simplify the problem [1].

Murtagh [17] showed that ultrametricity becomes pervasive as dimensionality and spatial sparsity increases and used this property in model based clustering. Klement et al. [15] proved that, for 
                        d
                        →
                        ∞
                     , random and non-random scenarios are not distinguishable by any metric such that distances become approximately equal. They also showed that soft-margin approaches do not improve the generalization capabilities of SVMs on HDLSS data. Zhang and Lin [27] compared the performance of several classifiers on simulated two-class data and report that SVMs and distance weighted discriminant techniques perform better than mean difference techniques or Naive Bayes classifiers.

A common approach to high dimensional classification consists in fitting models that maximally separate class regions; examples are SVMs and LDA. An alternative is to build geometric models that approximate the class regions. Such approximations include affine hulls [23], convex hulls and polytopes [18], bounding hyperspheres [22] and bounding hyperdisks [6]. Unlike margin based classifiers, these techniques are instance based in that they do not require explicit decision boundaries. Rather, they represent a class as a bounded region in a subspace, except for affine hull-based approaches which consider whole affine subspaces spanned by given data points. For high dimensional data, this is preferable over conventional instance-based models such as kNN since the latter implicitly assume dense samples and require training sets that are exponentially large w.r.t. the dimension of the underlying feature spaces. Cevikalp et al. [6] suggest the use of NHD as a compromise between too loose a structure of affine hulls and too tight a structure of convex hulls. Recently, large margin classifiers based on NAH, NCH, and NHD have been studied further in [5,7] and affine hull based modeling was applied in [13] in the context of image classification.

Human activity recognition in unconstrained videos and still images poses significant problems and stresses the need of investigating HDLSS data in real world scenarios. Our experiments in this article consider challenging benchmarks of videos [16,20], still images [14], and depth and skeletal data [19] on which most prior works applied SVMs with linear or Gaussian kernels. An increasingly popular trend in action recognition is to use multiple features such as motion, pose, and scene related information [20,24,26]. These methods either apply early fusion of features or late fusion of the results of a classifier ensemble. Since ensemble classifiers or multiple features are beyond the scope of this article, we restrict our practical experiments to recent single descriptors which are known to show good performance.

Classification aims at discriminating between different classes either by determining appropriate decision functions in the feature space (e.g. SVMs or Decision Trees) or by using local or global instance-based representations of the classes (e.g. kNN or NCH). SVMs are often used de facto without paying attention to geometry and distribution of the class regions. This, however, becomes very important in high dimensional classification problems. In this section, we review geometric classification methods which address these issues in the context of high dimensional data.

The affine hull of a set of data is the smallest affine subspace that contains all the samples. Given training samples 
                           
                              
                                 x
                              
                              
                                 c
                                 i
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                         where 
                           c
                           ∈
                           
                              {
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              C
                              }
                           
                         and 
                           i
                           ∈
                           
                              {
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              
                                 
                                    N
                                 
                                 
                                    c
                                 
                              
                              }
                           
                         are class and instance indices, their affine hull is defined as
                           
                              (1)
                              
                                 
                                    
                                       Φ
                                    
                                    
                                       c
                                    
                                    
                                       aff
                                    
                                 
                                 =
                                 
                                    {
                                    x
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          
                                             N
                                          
                                          
                                             c
                                          
                                       
                                    
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          x
                                       
                                       
                                          c
                                          i
                                       
                                    
                                    
                                    |
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          
                                             N
                                          
                                          
                                             c
                                          
                                       
                                    
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    1
                                    }
                                 
                                 .
                              
                           
                         The affine hull provides a loose approximation of the class region in that it ignores exact locations of the training data but models each class as an affine subspace. Consequently, it is least effected by artifacts due to missing neighborliness in high dimensions. The distance 
                           d
                           (
                           
                              
                                 x
                              
                              
                                 q
                              
                           
                           ,
                           
                              
                                 Φ
                              
                              
                                 c
                              
                              
                                 aff
                              
                           
                           )
                         from a query point 
                           
                              
                                 x
                              
                              
                                 q
                              
                           
                         to an affine hull is the norm of the displacement from to the closest point 
                           
                              
                                 x
                              
                              
                                 c
                              
                              
                                 ⁎
                              
                           
                         on the hull. Equivalently 
                           d
                           (
                           
                              
                                 x
                              
                              
                                 q
                              
                           
                           ,
                           
                              
                                 Φ
                              
                              
                                 c
                              
                              
                                 aff
                              
                           
                           )
                         can be expressed as the orthogonal projection of the normal to the subspace. Fig. 1
                         visualizes the concept of nearest affine hull classification.

The work in [6] proposed an offline training procedure using SVD and projections. Let 
                           
                              
                                 X
                              
                              
                                 c
                              
                              
                                 m
                              
                           
                         denote a centered matrix whose columns are training vectors from class c. A projection 
                           
                              
                                 P
                              
                              
                                 c
                              
                           
                         onto the spanning subspace is determined as 
                           
                              
                                 P
                              
                              
                                 c
                              
                           
                           =
                           
                              
                                 U
                              
                              
                                 c
                              
                           
                           
                              
                                 U
                              
                              
                                 c
                              
                              
                                 T
                              
                           
                         where 
                           
                              
                                 X
                              
                              
                                 c
                              
                              
                                 m
                              
                           
                           =
                           U
                           Σ
                           
                              
                                 V
                              
                              
                                 T
                              
                           
                        . Given a query 
                           
                              
                                 x
                              
                              
                                 q
                              
                           
                        , its distance 
                           d
                           (
                           x
                           ,
                           
                              
                                 Φ
                              
                              
                                 c
                              
                              
                                 aff
                              
                           
                           )
                         to the affine hull is then computed as
                           
                              (2)
                              
                                 d
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       q
                                    
                                 
                                 ,
                                 
                                    
                                       Φ
                                    
                                    
                                       c
                                    
                                    
                                       aff
                                    
                                 
                                 )
                                 =
                                 
                                    ‖
                                    
                                       (
                                       I
                                       −
                                       
                                          
                                             P
                                          
                                          
                                             c
                                          
                                       
                                       )
                                    
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          q
                                       
                                    
                                    −
                                    
                                       
                                          μ
                                       
                                       
                                          c
                                       
                                    
                                    )
                                    ‖
                                 
                              
                           
                         where 
                           
                              
                                 μ
                              
                              
                                 c
                              
                           
                         is the center of class 
                           
                              
                                 X
                              
                              
                                 c
                              
                           
                        .

The convex hull of a set of points is the minimal convex set that encloses them (see Fig. 2
                        ). Given training samples 
                           
                              
                                 x
                              
                              
                                 c
                                 i
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 d
                              
                           
                        , their convex hull is defined as
                           
                              (3)
                              
                                 
                                    
                                       Φ
                                    
                                    
                                       c
                                    
                                    
                                       conv
                                    
                                 
                                 =
                                 
                                    {
                                    x
                                    =
                                    
                                       ∑
                                       i
                                    
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          x
                                       
                                       
                                          c
                                          i
                                       
                                    
                                    
                                    |
                                    
                                    
                                       ∑
                                       i
                                    
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    1
                                    ,
                                    
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    ≥
                                    0
                                    }
                                 
                                 .
                              
                           
                         Compared to an affine hull, the convex hull gives a tighter approximation of the class region. The distance 
                           d
                           (
                           x
                           ,
                           
                              
                                 Φ
                              
                              
                                 c
                              
                              
                                 conv
                              
                           
                           )
                         between a query 
                           
                              
                                 x
                              
                              
                                 q
                              
                           
                         and the convex hull of class c is the norm of the displacement of 
                           
                              
                                 x
                              
                              
                                 q
                              
                           
                         to its closest point 
                           
                              
                                 x
                              
                              
                                 c
                              
                              
                                 ⁎
                              
                           
                         on the hull. Unlike for affine hulls where class regions are subspaces, convex hull classification projects data onto class specific regions within a subspace. To determine the closest point on the convex hull is to solve the following constrained quadratic program
                           
                              (4)
                              
                                 
                                    min
                                    
                                       
                                          α
                                       
                                       
                                          c
                                       
                                    
                                 
                                 ⁡
                                 
                                    1
                                    2
                                 
                                 
                                    
                                       ‖
                                       
                                          
                                             x
                                          
                                          
                                             q
                                          
                                       
                                       −
                                       
                                          
                                             X
                                          
                                          
                                             c
                                          
                                       
                                       
                                          
                                             α
                                          
                                          
                                             c
                                          
                                       
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 
                                 s.t. 
                                 
                                    ∑
                                    i
                                 
                                 
                                    
                                       α
                                    
                                    
                                       c
                                       i
                                    
                                 
                                 =
                                 1
                                 ,
                                 
                                 
                                    
                                       α
                                    
                                    
                                       c
                                       i
                                    
                                 
                                 ≥
                                 0
                                 
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 
                                    
                                       N
                                    
                                    
                                       c
                                    
                                 
                              
                           
                         whose solution 
                           
                              
                                 α
                              
                              
                                 c
                              
                              
                                 ⁎
                              
                           
                         provides mixture coefficients to obtain 
                           
                              
                                 x
                              
                              
                                 c
                              
                              
                                 ⁎
                              
                           
                           =
                           
                              
                                 X
                              
                              
                                 c
                              
                           
                           
                              
                                 α
                              
                              
                                 c
                              
                           
                        . Thus, the distance between the query and a class region becomes
                           
                              (5)
                              
                                 d
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       q
                                    
                                 
                                 ,
                                 
                                    
                                       Φ
                                    
                                    
                                       c
                                    
                                    
                                       conv
                                    
                                 
                                 )
                                 =
                                 
                                    ‖
                                    
                                       
                                          x
                                       
                                       
                                          q
                                       
                                    
                                    −
                                    
                                       
                                          X
                                       
                                       
                                          c
                                       
                                    
                                    
                                       
                                          α
                                       
                                       
                                          c
                                       
                                       
                                          ⁎
                                       
                                    
                                    ‖
                                 
                                 .
                              
                           
                        
                     

Note that each time a new query arrives, several closest points need to be determined. This is costly and may not be feasible. Also note that SVMs solve similar problems but determine separating hyperplanes. Moreover, if each query is considered as a class of its own, finding maximum margins between this class and any other class c is equivalent to finding the closest points on the hulls 
                           
                              
                                 Φ
                              
                              
                                 c
                              
                              
                                 conv
                              
                           
                         
                        [2]. NCH classification is thus equivalent to SVM classification in an instance based fashion. In fact, the piecewise linear decision boundary of NCH contains the query-vs-class boundary of an SVM as one of its facets. This is shown in Fig. 3
                        .

A hyperdisk as proposed in [6] is a compromise between the tight convex hull and the loose affine hull (see Fig. 4
                        ). It is defined as the intersection of the bounding hypersphere and the affine hull of the data. Formally, the hyperdisk of class c is
                           
                              (6)
                              
                                 
                                    
                                       Φ
                                    
                                    
                                       c
                                    
                                    
                                       disk
                                    
                                 
                                 =
                                 
                                    {
                                    x
                                    =
                                    
                                       ∑
                                       i
                                    
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          x
                                       
                                       
                                          c
                                          i
                                       
                                    
                                    
                                    |
                                    
                                    
                                       ∑
                                       i
                                    
                                    
                                       
                                          α
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    1
                                    ,
                                    
                                    
                                       ‖
                                       x
                                       −
                                       
                                          
                                             s
                                          
                                          
                                             c
                                          
                                       
                                       ‖
                                    
                                    ≤
                                    
                                       
                                          r
                                       
                                       
                                          c
                                       
                                    
                                    }
                                 
                              
                           
                         where 
                           
                              
                                 s
                              
                              
                                 c
                              
                           
                           =
                           
                              
                                 ∑
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       c
                                    
                                 
                              
                           
                           
                              
                                 α
                              
                              
                                 i
                              
                           
                           
                              
                                 x
                              
                              
                                 c
                                 i
                              
                           
                         is the center and 
                           
                              
                                 r
                              
                              
                                 c
                              
                           
                         is the radius of the minimal bounding hypersphere. Finding the radius of the bounding hypersphere requires to solve the following quadratic programming problem
                           
                              (7)
                              
                                 
                                    min
                                    α
                                 
                                 ⁡
                                 
                                    ∑
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 
                                    
                                       α
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       α
                                    
                                    
                                       j
                                    
                                 
                                 
                                    〈
                                    
                                       
                                          x
                                       
                                       
                                          c
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          c
                                          j
                                       
                                    
                                    〉
                                 
                                 −
                                 
                                    ∑
                                    i
                                 
                                 
                                    
                                       α
                                    
                                    
                                       i
                                    
                                 
                                 
                                    〈
                                    
                                       
                                          x
                                       
                                       
                                          c
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          c
                                          i
                                       
                                    
                                    〉
                                 
                                 
                                 s.t. 
                                 
                                    ∑
                                    i
                                 
                                 
                                    
                                       α
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 1
                                 ,
                                 
                                 0
                                 ≤
                                 
                                    
                                       α
                                    
                                    
                                       i
                                    
                                 
                                 ≤
                                 γ
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 
                                    
                                       N
                                    
                                    
                                       c
                                    
                                 
                                 ,
                              
                           
                         where 
                           
                              
                                 α
                              
                              
                                 i
                              
                           
                         are Lagrange multipliers and 
                           γ
                           ∈
                           [
                           0
                           ,
                           1
                           ]
                         is used to exclude distant points (outliers). The radius then becomes 
                           
                              
                                 r
                              
                              
                                 c
                              
                           
                           =
                           ‖
                           
                              
                                 x
                              
                              
                                 c
                                 i
                              
                           
                           −
                           
                              
                                 s
                              
                              
                                 c
                              
                           
                           ‖
                         for any 
                           
                              
                                 x
                              
                              
                                 c
                                 i
                              
                           
                         with 
                           0
                           ≤
                           
                              
                                 α
                              
                              
                                 i
                              
                           
                           ≤
                           γ
                        .

The distance between a query 
                           
                              
                                 x
                              
                              
                                 q
                              
                           
                         and a hyperdisk 
                           
                              
                                 Φ
                              
                              
                                 c
                              
                              
                                 disk
                              
                           
                         is determined by two terms: the norm of the displacement from 
                           
                              
                                 x
                              
                              
                                 q
                              
                           
                         to its projection 
                           
                              
                                 x
                              
                              
                                 q
                              
                              
                                 aff
                              
                           
                         on the affine hull (2) and the distance of 
                           
                              
                                 x
                              
                              
                                 q
                              
                              
                                 aff
                              
                           
                         to the boundary of the hypersphere, i.e.
                           
                              (8)
                              
                                 d
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       q
                                    
                                 
                                 ,
                                 
                                    
                                       Φ
                                    
                                    
                                       c
                                    
                                    
                                       disk
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ‖
                                          
                                             
                                                x
                                             
                                             
                                                q
                                             
                                          
                                          −
                                          
                                             
                                                x
                                             
                                             
                                                q
                                             
                                             
                                                aff
                                             
                                          
                                          ‖
                                       
                                       +
                                       max
                                       ⁡
                                       
                                          
                                             (
                                             
                                                ‖
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      q
                                                   
                                                   
                                                      aff
                                                   
                                                
                                                −
                                                
                                                   
                                                      s
                                                   
                                                   
                                                      c
                                                   
                                                
                                                ‖
                                             
                                             −
                                             
                                                
                                                   r
                                                
                                                
                                                   c
                                                
                                             
                                             ,
                                             0
                                             )
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

In terms of efficiency, NHD outperforms NCH since model parameters related to hypersphere and affine hull can be computed beforehand. Obviously, NHD is slower than NAH where only the affine hull is needed. Interestingly, our results below reveal that the involvement of the neighborhood term (hypersphere distance) does not improve classification accuracy. Instead, we observe degrading performances in many cases.

Most instance based representations such as Convex Hull, Bounding Hyperdisk, and Bounding Hypersphere rely on the notion of neighborhoods within the training data. Affine Hulls, on the other hand, give a loose approximation of class regions as they model each class as an affine subspace. Therefore NAH classification may prove a promising instance based classifier in HDLSS settings where the notion of a neighborhood breaks down [9,11,17].

The existing SVD based approach to NAH classification discussed Section 3.1 is time- and space consuming. It requires to store a 
                        d
                        ×
                        d
                      matrix 
                        
                           
                              P
                           
                           
                              c
                           
                        
                      for every class which is unreasonable when d is large. For example, fitting an NAH model for the HMDB dataset discussed below would require more than 80 GB of memory. Alternatively, one may store the 
                        
                           
                              N
                           
                           
                              c
                           
                        
                        ×
                        d
                      matrix 
                        
                           
                              U
                           
                           
                              c
                           
                        
                      during training and compute 
                        
                           
                              P
                           
                           
                              c
                           
                        
                      during classification. Yet, this is still very demanding since the computing 
                        
                           
                              P
                           
                           
                              c
                           
                        
                        =
                        
                           
                              U
                           
                           
                              c
                           
                        
                        
                           
                              U
                           
                           
                              c
                           
                           
                              T
                           
                        
                      requires efforts of 
                        O
                        (
                        
                           
                              d
                           
                           
                              2
                           
                        
                        
                           
                              N
                           
                           
                              c
                           
                        
                        )
                     .

Here, we propose an efficient least square approach to NAH that applies the QR factorization. Our approach builds on the observation that HDLSS training matrices 
                        
                           
                              X
                           
                           
                              c
                           
                        
                      are of full rank since high dimensional data are vertices of a simplex and hence linearly independent [11,9]. We therefore propose to compute NAH classification in an entirely lazy fashion by finding a point 
                        
                           
                              x
                           
                           
                              ⁎
                           
                        
                      in the affine hull that is closest to 
                        
                           
                              x
                           
                           
                              q
                           
                        
                     . This requires solving
                        
                           (9)
                           
                              
                                 min
                                 
                                    
                                       α
                                    
                                    
                                       c
                                    
                                 
                              
                              ⁡
                              
                                 
                                    ‖
                                    
                                       
                                          x
                                       
                                       
                                          q
                                       
                                    
                                    −
                                    
                                       
                                          X
                                       
                                       
                                          c
                                       
                                    
                                    
                                       
                                          α
                                       
                                       
                                          c
                                       
                                    
                                    ‖
                                 
                                 
                                    2
                                 
                              
                              
                              s.t. 
                              
                                 ∑
                                 i
                              
                              
                                 
                                    α
                                 
                                 
                                    c
                                    i
                                 
                              
                              =
                              1
                              ,
                              
                              i
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              
                                 
                                    N
                                 
                                 
                                    c
                                 
                              
                              .
                           
                        
                     
                  

This problem does not involve inequality constraints and can therefore be cast as a simple least square problem
                        
                           (10)
                           
                              
                                 [
                                 
                                    
                                       
                                          
                                             
                                                X
                                             
                                             
                                                c
                                             
                                          
                                       
                                    
                                    
                                       
                                          1
                                       
                                    
                                 
                                 ]
                              
                              
                                 [
                                 α
                                 ]
                              
                              =
                              
                                 [
                                 
                                    
                                       
                                          
                                             
                                                x
                                             
                                             
                                                q
                                             
                                          
                                       
                                    
                                    
                                       
                                          1
                                       
                                    
                                 
                                 ]
                              
                           
                        
                      where 1 is a row vector of all 1 s of dimension 
                        
                           
                              N
                           
                           
                              c
                           
                        
                     . Then, distance between the query point and the affine hull is
                        
                           (11)
                           
                              d
                              (
                              
                                 
                                    x
                                 
                                 
                                    q
                                 
                              
                              ,
                              
                                 
                                    Φ
                                 
                                 
                                    c
                                 
                                 
                                    aff
                                 
                              
                              )
                              =
                              
                                 ‖
                                 
                                    
                                       x
                                    
                                    
                                       q
                                    
                                 
                                 −
                                 
                                    
                                       X
                                    
                                    
                                       c
                                    
                                 
                                 
                                    
                                       α
                                    
                                    
                                       c
                                    
                                    
                                       ⁎
                                    
                                 
                                 ‖
                              
                           
                        
                     
                  

Again, in HDLSS settings, data matrices are (nearly) of full rank so that a stable solution of the system in (10) can be computed efficiently using the QR factorization. Compared to NAH-svd, our approach of NAH-lsq significantly reduces computational efforts to 
                        O
                        (
                        d
                        
                           
                              N
                           
                           
                              c
                           
                           
                              2
                           
                        
                        )
                     .

Human activity recognition in consumer videos and images is a challenging problem since there are considerable variations in human pose, motion cues, scene context, camera view-point, occlusion, or lighting. Most existing research attempts to derive invariant features and to classify the resulting high dimensional descriptors. In this section, we discuss 4 well known datasets containing videos, images or Kinect depth data (Table 1
                     ) and provide details as to which features are extracted from each dataset.

HMDB [16] is one of the largest and most versatile datasets for action recognition in videos. It contains 6766 video sequences of 51 action categories such as body movements or human interactions. The UCF50 data [20] consists of 6681 YouTube videos with 50 action categories. For all 50 categories, the videos are split into 25 groups. For both of these datasets, we use action bank templates [21] as features for classification since they have shown very good performance in combination with linear SVM classification. Feature extraction is based on spotting different motion templates in the multiple scale spatio-temporal cuboids and results in a 
                           14
                           ,
                           965
                         dimensional feature vector.

The Berkeley Multimodal Human Action Database (MHAD) [19] consists of 660 sequences of 11 actions performed repeatedly by 12 people and captured from different sensors. Here, we used two modalities: skeleton information from a motion capture system and depth information from a Kinect sensor. In each case, we divide the video in 
                           
                              
                                 N
                              
                              
                                 s
                              
                           
                         overlapping temporal segments and quantize all frames within a segment. We build a vocabulary of 
                           
                              
                                 N
                              
                              
                                 w
                              
                           
                         skeletal or visual words using k-means and represent entire action sequences in terms of vectors of length 
                           K
                           =
                           
                              
                                 N
                              
                              
                                 s
                              
                           
                           ×
                           
                              
                                 N
                              
                              
                                 w
                              
                           
                        .

The MHAD-Mocap data is acquired by tracking the relative 3D positions of 43 LED markers placed on different body parts. We represent each activity as a sequence of skeletal vectors of length 129. First, we sample 
                              100
                              ,
                              000
                            skeletal vectors from all the data and build a vocabulary of 
                              
                                 
                                    N
                                 
                                 
                                    w
                                 
                              
                              =
                              60
                            skeletal words. We set 
                              
                                 
                                    N
                                 
                                 
                                    s
                                 
                              
                              =
                              40
                            and represent each action sequence by a vector of length 2400.

Depth videos are first divided into 8 disjoint Depth-Layered Multi-Channel (DLMC) videos. We used only channel C-3 DLMC videos since almost all subjects and their movements occur in this depth range. Our representation for this modality is based on motion histograms features presented earlier in [8]. Given a sequence of gray scale depth images 
                              I
                              =
                              
                                 
                                    I
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    I
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              
                                 
                                    I
                                 
                                 
                                    n
                                 
                              
                           , a set of motion energy images 
                              D
                              =
                              {
                              
                                 
                                    D
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    D
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              
                                 
                                    D
                                 
                                 
                                    (
                                    n
                                    −
                                    1
                                    )
                                 
                              
                              }
                            is obtained where 
                              
                                 
                                    D
                                 
                                 
                                    i
                                 
                              
                              =
                              
                                 
                                    I
                                 
                                 
                                    (
                                    i
                                    +
                                    1
                                    )
                                 
                              
                              −
                              
                                 
                                    I
                                 
                                 
                                    i
                                 
                              
                           . Each difference image is divided into a grid of cells and the average motion energy is estimated for each cell. We consider cells of size 
                              20
                              ×
                              20
                            and transform the 2D grid of motion energies into a vector of length 
                              
                                 
                                    N
                                 
                                 
                                    b
                                 
                              
                              =
                              768
                           . For a Bag-of-Words (BoW) representation, we sample 
                              100
                              ,
                              000
                            feature vectors from all the data, build a vocabulary of 
                              
                                 
                                    N
                                 
                                 
                                    w
                                 
                              
                              =
                              100
                            words, and set 
                              
                                 
                                    N
                                 
                                 
                                    s
                                 
                              
                              =
                              20
                            to represent each action sequence by a 2000 dimensional vector.

The Ikizler action dataset [14] contains 2458 still images found on the internet. They show the 5 actions of dancing, playing golf, sitting, running, and walking. We used VLFeat
                           1
                        
                        
                           1
                           
                              http://www.vlfeat.org/.
                         to extract local SIFT descriptors over multiple scales to build a code book of size 1024. Each image was then divided into a three-level spatial pyramid and quantization was carried on each gird component resulting in 
                           13
                           ,
                           312
                         dimensional feature vector.

We evaluate the NAH-lsq and other geometric classification methods (discussed in Section 3) and compare their performance to traditional approaches such SVM, kNN, and LDA. For the two parametric methods kNN and SVM, we report the best results over choices of hyper-parameters. For linear SVMs, the optimal penalty parameter was determined within the range between 0.1 and 
                        100
                        ,
                        000
                     . For kNN classifiers, the parameter k was varied from 1 to 
                        
                           
                              N
                           
                           
                              c
                           
                        
                     . Every method discussed in Sections 3 and 4 was implemented in Python. For other methods, we used implementations and wrappers provided by the Python based machine learning library Scikit-learn.
                        2
                     
                     
                        2
                        
                           http://scikit-learn.org.
                      All experiments were carried out on a PC with 16 GB RAM using a single core.

For each dataset, we apply cross validation and report average results over all iterations. For HMDB, we use the original three train-test splits [16] where in each case for each action 70 videos are used for training and another 30 for testing. For UCF50, we apply a 5-fold Leave-Five-Groups-Out approach and for MHAD datasets we use Leave-One-Actor-Out scheme. For the Ikizler dataset, we randomly sample 100 images from each class for training and use the rest for testing.


                        Table 2
                         compares recognition accuracies obtained from the different classifiers tested. While NHD exhibits good performance on some datasets, it shows poor performance in cases where number of classes is high. The piecewise boundaries of NCH also do not generalize well enough to compete with SVMs. All instance-based methods that bound the class regions (kNN, NCH, MNP, and NHD) suffer from loss of performance in one way or another. On the other hand, NAH methods which represent classes as affine subspaces, are least affected by artifacts due to high dimensional neighborlessness. Notice that NAH-lsq and NAH-svd achieve similar accuracies. In short, non-parametric NAH based classifiers outperform other instance-based methods in all cases and show better results than the optimal SVM in some cases.

Instance-based methods usually do not require any training and model fitting is deferred to the classification phase. For example, to compute distance of a d-dimensional query instance from an affine set, the proposed NAH-lsq costs 
                           O
                           (
                           d
                           
                              
                                 N
                              
                              
                                 c
                              
                              
                                 2
                              
                           
                           )
                         whereas NAH-svd costs 
                           O
                           (
                           
                              
                                 d
                              
                              
                                 2
                              
                           
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                           )
                        . A naive implementation of kNN involves computing distances of the query to all N data points and sorting those distances to determine k-nearest neighbors. This leads to a computational cost of order at-least 
                           O
                           (
                           d
                           N
                           )
                         where 
                           N
                           =
                           
                              
                                 ∑
                              
                              
                                 c
                                 =
                                 1
                              
                              
                                 C
                              
                           
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                        . A common enhancement to kNN algorithm is to employ KD-tree data structures. However, our experiments using Python library scikit-learn show KD-trees become very slow when dimensionality increases. Other traditional classifiers such as LDA, SVM, and Decision Trees/Random Forests explicitly learn a model from the training data in an offline manner. Thus, they are efficient during classification. For example, LDA learns lower order feature transforms at a computational cost 
                           O
                           (
                           d
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                           )
                         and is highly efficient at classification. Note that the classification time also depends on the complexity of the decision surface. For example, for C classes, the decision surface of SVM-OAO will consist of 
                           C
                           (
                           C
                           −
                           1
                           )
                           /
                           2
                         different d-dimensional hyperplanes whereas SVM-OAA will have only C hyperplanes. However, determining those C hyperplanes is often more time consuming than fitting an SVM-OAO since the complexity of SVM training is between 
                           O
                           (
                           
                              
                                 N
                              
                              
                                 2
                              
                           
                           )
                         and 
                           O
                           (
                           
                              
                                 N
                              
                              
                                 3
                              
                           
                           )
                         and the number of required examples N can be large for an SVM-OAA.


                        Fig. 5
                         plots overall training and testing times (in CPU seconds) for all methods with optimal parameters on all 5 datasets. The non-zero training time of instance-based methods NAH-lsq, NAH-svd, kNN, and NCH reflects an overhead due to pre-processing (indexing and storage) of the training data. Notice that, in all cases, the proposed least squares approach to NAH is more efficient than any other hull based method and is competitive to kNN classification. In particular, NAH-lsq gains significantly over NAH-svd. It also shows a competitive classification time when compared to the optimal SVM-OAO. While LDA shows promising efficiency, its accuracy degrades on complex multiclass datasets (see Table 2). Notice also how the training time of SVM-OAA rises for large datasets i.e. HMDB and UCF50 in the second row.


                        Table 3
                         summarizes several theoretical, structural, and empirical aspects of a number of classifiers. The last four columns are based directly on the complexity of decision surfaces, training and testing time, and accuracy on the 5 datasets. Here, a • indicates a high value and each column compares different classifiers based on qualitative (relative) or quantitative results. For example, kNN has the most complex decision surface (due to its Voronoi nature) whereas NAH and OAO-SMV have least complex surface i.e. a single affine subspace or a separating hyper-plane corresponding to each class. With respect to accuracy, methods based on NAH and SVM achieve the highest or competitive to the highest on each of the 5 datasets whereas NHD-based methods compete well in 3 cases and perform poorly otherwise. Similarly OAA-SVM, OAO-SVM, and LDA are seen to exhibit a slower training but faster classification – especially on large data sets.

Our empirical results indicate the effectiveness of NAH-lsq in high dimensional classification. The only competitive classifier is the SVM. The theoretical foundations, geometrical simplicity, and computational efficiency of NAH-lsq as compared to SVMs illustrate the power of linear models in high dimensional data processing. In practice, we observed that almost all the training samples form the support of the decision surfaces. Table 4
                         shows the percentage (to the nearest integer) of training data of a class used to determine piecewise boundaries (point-hull and class-class) and overall decision surfaces. For example, in the case of the HMDB dataset, a single binary decision surface of SVM-OAO was observed to require, on average, 34% of the data of each of the two corresponding classes as support vectors and, for a given class, almost every instance became a support to one or more binary decision surfaces.

On the one hand, these observations confirm the lack of structure within class regions. On the other hand, they justify the use of affine subspaces in classification. They also hint at an advantage of using NAH-lsq in online settings. Note that online SVMs would require (nearly) complete retraining when most data had previously been selected as support vectors. Let N be the number of instances, S be the number of support vectors, and 
                           R
                           ≤
                           S
                         be the number of support vectors such that 
                           0
                           ≤
                           
                              |
                              
                                 
                                    α
                                 
                                 
                                    i
                                 
                              
                              |
                           
                           ≤
                           C
                        . If 
                           N
                           ≡
                           S
                           ≡
                           R
                         then the training time of online SVM is the same as that for a regular SVM [3]. We empirically evaluated the performance of a popular online learning scheme SVM-SGD that combines SVM with stochastic gradient descent optimization [4]. A brief description of SVM-SGD is provided in Appendix A.

We compare NAH-lsq and one-against-all SVM-SGD in online settings for our two large datasets UCF50 and HMDB. For UCF50, we chose videos belonging to all groups from g6 to g25 for training and the rest for testing. For HMDB, we use the split-1 [16] for training and testing. We train each classifier initially on 40% of the training data and subsequently add equal amounts of the remaining data in 10 episodes. While NAH-lsq explicitly fits a new model on arrival of new data, SVM-SGD takes a warm start from the current solution and iterates until convergence or until a maximum number of iterations is reached. Optimal hyper-parameters of SGD were determined through offline cross validation.

The results are shown in Fig. 6
                        . In each case, the first row shows that the accuracy of NAH-lsq consistently improves with an increase in training data while the performance of SVM-SGD may occasionally suffer, e.g. when the distribution of new data points differs significantly from the one for which the previous solution w was estimated. The second and third rows plot training and test time respectively after each episode. While NAH-lsq spent most of their time on lazy classification, adding new data almost always caused a complete retraining of SVM too. Usually, in online settings, the model estimation costs are not distinguished for training and classification phases. The last row in Fig. 6 plots combined average time, per instance, for model fitting in training or classification phases. It can be seen that the NAH-lsq emerges to significantly outperform SVM-SGD in terms of efficiency.

@&#CONCLUSION@&#

We investigated geometric approaches to real-world human activity recognition based on HDLSS data. We could affirm the lack of neighborliness in HDLSS data. On the one hand, neighborliness negatively affects the performance of neighborhood-based classification methods such as kNN, NCH, or NHD; on the other, it causes over fitting artifacts for SVMs. Our results show that representing classes as affine subspaces spanned by their members can remedy this situation. We proposed NAH-lsq, a least square- and QR factorization-based approach that significantly reduces time and memory requirements of previous NAH methods. Given its parameter-free model fitting, smooth decision surfaces, efficiency, and accuracy in multiclass scenarios, NAH-lsq appears to be an appropriate classifier for HDLSS data. On several challenging image and video datasets, we found the NAH-lsq classifier to show competitive or superior performance than the widely used SVMs. Moreover, despite its lazy classification approach, NAH-lsq appears to be a faster approach when compared to SVMs in online settings (a major application area for content-based image and video retrieval). To conclude, we (a) provided an empirical insight into the complexity of the multiclass HDLSS activity recognition problem and (b) proposed a simple yet powerful classification approach.

Our approach to classification of HDLSS data is arguably the first such attempt in the domain of human activity recognition. In our future work, we are mainly interested in extending efficiency and applicability of NAH-lsq to scenarios involving millions of images and videos. In this line we aim at representing each class by an ensemble of affine hulls that are built over random or clustered subsets of data. Our proposed approach can be tailored to other domains such as video set classification in Big Data (e.g. video portals). For example, large-scale classification of weakly labeled YouTube videos using video co-watch data can benefit from affine hull-based representation. Note that several image set classification methods (e.g. [13] and [25]) already employ affine hull representation to project image subsets as points on a Grassmanian manifold for classification. Harandi et al. [12] modeled Auto Regression Moving Average (ARMA) features along a simple video as an affine subspace, represented it as a point on a Grassmanian manifold, and employed discriminant analysis for action recognition. However, their frame-by-frame feature representations may become cumbersome for unconstrained videos where the compact bag-of-features representation is more practical. There are hardly any approaches to video set classification, in particular to human action recognition in Big Data. However, with access to large amount of (weakly) labeled video data, affine hull-based representation may be of great impact in future research on Big Vision.
                        3
                     
                     
                        3
                        
                           https://sites.google.com/site/bigvision2012/.
                     
                  

@&#ACKNOWLEDGEMENTS@&#

This work was carried out in the project “automatic activity recognition in large image databases” which is funded by the German Research Foundation (DFG).

Stochastic gradient descent based optimization, in combination with popular classifiers such as SVMs and CRFs, has gained popularity for its applicability to large scale online classification problems [4,28]. Here we give a brief description of a popular online learning approach that combines SVM classification with stochastic gradient descent optimization [4]. Given a training set 
                        {
                        (
                        
                           
                              x
                           
                           
                              i
                           
                        
                        ,
                        
                           
                              y
                           
                           
                              i
                           
                        
                        )
                        ,
                        
                           
                              x
                           
                           
                              i
                           
                        
                        ∈
                        
                           
                              R
                           
                           
                              d
                           
                        
                        }
                     , several supervised classification approaches aim to determine a decision function 
                        f
                        (
                        x
                        )
                        =
                        
                           
                              w
                           
                           
                              T
                           
                        
                        x
                        +
                        b
                      by minimizing an error function of the form:
                        
                           (A.1)
                           
                              E
                              (
                              w
                              ,
                              b
                              )
                              =
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 n
                              
                              L
                              (
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                              ,
                              f
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              )
                              )
                              +
                              α
                              R
                              (
                              w
                              )
                           
                        
                      where L and R are loss and penalty functions, respectively.

In the case of SVMs, the error function is composed of the Hinge loss function and the L2 norm of w. A stochastic gradient descent algorithm is an iterative procedure to determine the optimal w by applying the following update rule:
                        
                           (A.2)
                           
                              w
                              ←
                              w
                              −
                              η
                              
                                 {
                                 α
                                 
                                    
                                       ∂
                                       R
                                       (
                                       w
                                       )
                                    
                                    
                                       ∂
                                       w
                                    
                                 
                                 +
                                 
                                    
                                       ∂
                                       L
                                       (
                                       
                                          
                                             w
                                          
                                          
                                             T
                                          
                                       
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       +
                                       b
                                       ,
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       ∂
                                       w
                                    
                                 
                                 }
                              
                           
                        
                      where η is the learning rate.

@&#REFERENCES@&#

