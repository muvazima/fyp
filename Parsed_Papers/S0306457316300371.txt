@&#MAIN-TITLE@&#Fast top-k preserving query processing using two-tier indexes

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present a new query processing method for text search.


                        
                        
                           
                           We extend the BMW-CS algorithm to now preserve the top-k results, proposing BMW-CSP.


                        
                        
                           
                           We show through experiments that the method is competitive when compared to baselines.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Information retrieval

Query processing

Inverted index

Search system

BMW

@&#ABSTRACT@&#


               
               
                  In this paper we propose and evaluate the Block Max WAND with Candidate Selection and Preserving Top-K Results algorithm, or BMW-CSP. It is an extension of BMW-CS, a method previously proposed by us. Although very efficient, BMW-CS does not guarantee preserving the top-k results for a given query. Algorithms that do not preserve the top results may reduce the quality of ranking results in search systems. BMW-CSP extends BMW-CS to ensure that the top-k results will have their rankings preserved. In the experiments we performed for computing the top-10 results, the final average time required for processing queries with BMW-CSP was lesser than the ones required by the baselines adopted. For instance, when computing top-10 results, the average time achieved by MBMW, the best multi-tier baseline we found in the literature, was 36.29 ms per query, while the average time achieved by BMW-CSP was 19.64 ms per query. The price paid by BMW-CSP is an extra memory required to store partial scores of documents. As we show in the experiments, this price is not prohibitive and, in cases where it is acceptable, BMW-CSP may constitute an excellent alternative query processing method.
               
            

@&#INTRODUCTION@&#

Systems that allow fast access to information available in large textual collections are nowadays an integral part of day to day activities of billion of users around the world. Such activities include using web search engines, e-commerce search systems and textual contents of any modern database management system, among others. Users of such systems are usually satisfied with only a few, highly relevant results for each submitted query. Thus, many researchers have been working to develop newer methods to speed-up the computation of only the top results of a query, which are the ones usually presented to the users.

Modern search systems usually deploy a number of different sources of relevance evidence for determining the top-k results for answering each query. For instance, an e-commerce search system may take into account not only the textual description of each product, but also its popularity and price. Citing another example, in addition to the page contents itself, web search engines use information such as the titles of the pages and link structure. The sources of relevance evidence are combined using a myriad of approaches, such as learning to rank techniques, in order to produce the final ranked results.

The combination of multiple sources of evidence is usually expensive, and thus it is not applied to the full set of documents that suffice the query being processed (Carvalho, Rossi, de Moura, Fernandes, & Silva, 2012; Rossi, de Moura, Carvalho, & Silva, 2013). Usually, only a small subset of documents selected from an initial process of ranking is considered in a second, more sophisticated ranking phase. The initial process of computation of the ranking consists of applying a basic IR model, such as BM25 (Robertson & Walker, 1994) or the Vector Space Model (Salton, Wong, & Yang, 1975), to compute an initial rank of top results. Most of the research aimed at reducing computational costs in query processing is related to the reduction of number of documents considered while creating this initial ranking.

In this paper we propose and evaluate Block Max WAND with Candidate Selection and Preserving Top-K Results algorithm, or BMW-CSP, a fast query processing method that ensures that the top-k results will have their rankings preserved. This method is an improved and extended version of BMW-CS, Block Max WAND with Candidate Selection, which was also previously proposed by us (Rossi et al., 2013) .

Although very efficient, BMW-CS has the disadvantage of not guaranteeing to preserve the top-k results for a given query. In our preliminary study, we show through experiments that the use of BMW-CS does not cause significant loss in quality of results when applied in a search system, still the property of preserving top results is considered important in literature. Further, the use of BMW-CS is particularly risky, since it may completely remove important results from the final ranking. The new algorithm proposed here, BMW-CSP, maintains the time performance advantages presented by BMW-CS, while presenting the advantage of guaranteeing that the top-k ranking results will remain unchanged. In addition, our experiments indicate that BMW-CSP is faster than the best baselines we found in the literature.

The remainder of this article is structured as follows: Section 2 presents the background and related research necessary to better understand the proposed methods. Section 3 presents our method. Section 4 presents the experimental results. Finally, Section 5 presents the conclusion and prospective future research.

Search systems that deal with large textual collections use index structures for allowing fast query processing. One of the most adopted index structures is known as inverted file (Baeza-Yates & Ribeiro-Neto, 2011). An inverted file contains, for each term t, where a term is usually a word, the list of documents where it exists. For each document in the list, it also stores information used to compute the importance of the document in the list, or its weight. Inverted lists usually store the frequency of the term in the document to compute such importance. This list of pairs of document and term frequencies is called an inverted list of t and is used to measure the relative importance of terms in the stored documents. Each document is represented in these lists by a value named document ID, referred to as docId in this article. Depending on the size and number of documents in the system, the inverted files may become huge, being stored in a compressed format to save memory space.

Queries are usually processed by traversing the inverted files either in a Term-At-A-Time (TAAT) or a Document-At-A-Time (DAAT) query processing. In the TAAT strategy, the inverted lists are sorted by term impact in non-increasing order and the query results are obtained by sequentially traversing one inverted list at a time. In the DAAT strategy, the inverted lists are sorted by docIds, which allows the algorithms to traverse all the inverted lists related to a query in parallel.

The TAAT approach takes advantage of the fact that sequentially accessing each inverted lists may lead to a speed up in query processing. Its main disadvantage is to require the usage of large amounts of memory to store partial scores achieved by each document when traversing each inverted list. These partial scores should be stored to accumulate the score results obtained by each document when traversing each inverted list. The final ranking can be computed only when all the inverted lists are processed.

Several authors proposed methods to discard partial scores, thus reducing the amount of memory required to process queries in the TAAT mode (Anh & Moffat, 2006; Anh, de Kretser, & Moffat, 2001; Strohman & Croft, 2007). Some of the benefit of the TAAT approach is lost when the queries are processed in the main memory, since the gains in performing sequential access to the lists, instead of random access, are smaller in main memory than while performing disk accesses.

Our algorithm here adopts the DAAT approach. In the DAAT, the full scores of documents are computed while traversing the lists in parallel. Since the lists are ordered by docIds, it is possible to skip documents that are unlikely to be relevant. The memory requirements are fairly smaller than in TAAT, since DAAT approach requires only the current top-k answers to be stored at each moment while processing a query. The price is that the algorithm changes from list to list all the time, thus performing less sequential accesses when compared to TAAT.

As in TAAT approach, several authors have proposed algorithms and data structures to accelerate the query processing in the DAAT approach. For instance, data structures to allow fast skipping in the inverted lists, named as skiplists (Moffat & Zobel, 1996), are adopted to accelerate the query processing. Skiplists divide the inverted lists into blocks of entries and provide pointers for fast access to such blocks, so that a scan in the skiplist determines the block, in which a document entry may occur in the inverted list.

Several DAAT approaches can be considered related to our proposal. Broder, Carmel, Herscovici, Soffer, and Zien (2003) proposed a successful strategy for query processing, known as WAND. In WAND, a heap of scores is created to maintain the top-k documents with larger scores during each step of the query processing. The smallest score present in the heap at each moment is taken as a discarding threshold to accelerate the query processing. A new document is evaluated and inserted in the heap only if it has a higher score than the discarding threshold.

While processing queries, WAND tries to discard documents before evaluating their actual scores. Documents have their maximum possible score evaluated using the information of maximum score of each list where they may occur. The full score of a document is evaluated only if its maximum possible score is greater than the current discarding threshold.

At each moment in the DAAT query processing, there is a pointer to the next document to be processed in each inverted list associated with the query. For instance, if we have a query with terms hot, chili and peppers, there will be a pointer to the next document processed in each of their lists. Fig. 1
                      gives an example where the currently pointed documents in each list are shown in bold circles. In the figure, documents 69, 466 and 466 are currently pointed in lists hot, chili and peppers, respectively. The WAND algorithm assures that the previous occurrences in each list were already examined and each of the pointed documents represent the smaller docId in the list that was not yet processed (Broder et al., 2003). Since the lists are organized by docIds, at this stage, we know that the smaller docId (69) occurs only in the list of the term hot, while the document 466 occurs in the lists of chili and peppers and might as well occur in the list of the term hot.

At this point, we can estimate the maximum scores which can be obtained by each of the currently pointed documents while processing the query: 69 can reach a maximum possible score equal to the maximum score that a document may achieve in the list of hot, which is 1.8 in the example. Document 466 can reach the sum of maximum scores of the three query terms, thus achieving at most 9.4. We, thus, may discard the documents which are not able to reach the discarding threshold, i.e., cannot achieve a score higher than the minimum score among the top-k results which have been already processed at this moment.

The entry with the smaller docId among the ones that reach a maximum possible score higher than the current discarding threshold is then chosen as a candidate to be included in the answer. This entry is known as pivot. We then move all pointers of all lists to point to docIds of at least the same value as the pivot.

Only, the pointers of the lists where the current docIds are smaller than the pivot, require a movement. If one of these lists does not have the pivot document, the document is discarded, a new pivot is selected and the process is repeated. Otherwise, if all these lists contain the document, its actual score is then computed. If the actual score of the pivot is higher than the discarding threshold, it is included in the answer set and the discarding threshold is updated. After processing the pivot, we move all the pointers of the lists to the next document with the docId greater than the pivot, and start the process again.


                     Ding and Suel (2011) revisited the ideas presented in the WAND method and proposed an even faster solution named Blockmax WAND (BMW). In BMW, the entries of the inverted lists are grouped into compressed blocks that may be skipped without any decompression of their content. Each block contains information about the maximum impact among the entries found in it. Whenever such maximum impact is not relevant to change the results for a given query, the whole block is discarded, which avoids high query processing costs.

BMW is based on the WAND algorithm and uses the same approach for selecting a document pivot during the query processing. In BMW, the pruning of entries is performed using two main pieces of information: (i) the maximum score of the whole inverted list, which is also adopted in WAND, as illustrated in Fig. 1; and (ii) the maximum impact found in each of the blocks which are pointed by the skiplist entries, known as the block max score, so that before accessing a block, it is possible to predict the maximum impact of an entry among those, found in such a block. The block max score is used to accelerate query processing considerably when compared to the strategy proposed in WAND.

The algorithm also allows skipping entire blocks of inverted list entries based on the block max score present on the skiplists, a procedure referred to as shallow movement. Contrary to the regular movement present in the inverted lists, shallow movement accesses only the skiplist entries, which avoids the cost of decompressing blocks of the inverted lists while processing queries.

Before decompressing and evaluating the pivot document, BMW makes a shallow movement to align the inverted lists over the blocks that possibly have the document. After the alignment, the algorithm uses the information of the block max score stored in the skiplists to estimate a local upper bound score of the candidate document. If the upper bound score is lower than a given pruning threshold, the document is discarded and one of the lists is advanced.

Similar to WAND, the pruning threshold is dynamically updated according to the score of the evaluated candidate. We choose to adopt BMW as one of the baselines in our experiments, since BMW, alike our method, was developed considering in-memory query processing and since it has a multi-tier version which results in performance gains.

The authors present experiments that show that BMW achieves significant reductions in query processing times compared to previous studies, comparing their method not only to DAAT query processing strategies, such as WAND, but also to TAAT strategies, such as the one proposed by Strohman and Croft (2007). The authors concluded that their method was by far the fastest query processing method when it was proposed.

Special optimizations to the DAAT methods, including BMW, were proposed by Dimopoulos, Nepomnyachiy, and Suel (2013). Authors propose the use of a set of block-max and posting-bitmap structures that can be used to transparently accelerate any DAAT index traversal algorithm. They also propose an optimized implementation of their mechanisms that exploits SIMD instructions of modern CPUs and restricts the critical data structures to L1 cache. As our method is also a DAAT algorithm, it can also take advantage of these ideas. Even considering the restricted use of machines that support SIMD instructions, we plan to study and implement the optimizations proposed by the authors in a future work to assert their impact to our proposed method.

In a parallel development, Chakrabarti, Chaudhuri, and Ganti (2011) also developed document-at-a-time algorithms for computing top-k query results using block max scores to improve the pruning strategies. The authors formally study the optimality characteristics of the proposed algorithms and present a discussion about the costs related to query processing and how to set block sizes.


                     Shan, Ding, He, Yan, and Li (2012) show that the performance of BMW is degraded when static scores, such as Pagerank, are added to the ranking function. In their work, they assume that the ranking function consists of query-independent features, e.g., PageRank, and query-dependent features, e.g., BM25, and the final ranking is a linear combination of such elements. One of the main problems they found while using BMW in this scenario was that the maximum score of documents may become extremely high, since it is expected that documents containing both a high BM25 score and a high Pagerank, are likely to exist in the inverted lists of a term. Authors propose small modifications in the BMW algorithm to deal with this scenario. The main change was to select the pivot by using the local block max information, instead of using max score. The same study can be done to check the performance of our proposed method in the scenario of combining query dependent and query independent features, and the same adaptation of using block max instead of max score could also be applied. Further questions about the use of state-of-art query processing algorithms in practical web search scenarios, such as using other static features besides Pagerank or using distinct ways of combining multiple sources of relevance evidence, are still open. We let these questions shape our future work.

Previous work proposed the splitting of the inverted index into more than one tier in an attempt to accelerate query processing. In these architectures, the query processing starts in a small tier and only if necessary it proceeds to larger tiers.


                        Risvik, Aasheim, and Lidal (2003) divided the index into three tiers with the goal of achieving better scaling when distributing the index. According to static and dynamic sources of evidence, more relevant documents are selected to compose the smaller tier. The query processing starts in this tier and only if the result set is not satisfactory, according to an evaluating algorithm, the query processing proceeds to the next tier. Performance gains are achieved when the processing does not visit the larger tiers. There is no guarantee that the result set is the exact set if compared to exhaustive query processing.


                        Ntoulas and Cho (2007) presented a two-tiered query processing method that avoids any degradation of the quality of results and always guaranteeing the exact top-k results set. The first tier contains the documents which are considered most important to the collection, selecting the entries by using static and dynamic pruning strategies that remove non-relevant documents and terms. The second tier contains the complete index. They propose to use the first tier as a cache level to accelerate query processing. Whenever the method detects that the results of the first tier assures that the top results will not be changed, it does not access the second tier, based on the results from the first tier only. Otherwise, they process the query using the second tier.

To guarantee that their method preserves the top answer/results when compared to a system without pruning, the method evaluates the ranking function while processing the first tier, assigning the maximum possible score that could be achieved when processing the full index for entries that are not present in the first tier. The authors demonstrate how to determine the optimal size of a pruned index and experimentally evaluate their algorithms in a collection of 130 million Web pages. In their experiments, the presented method achieved good results for first-tier index sizes varying from 10% to 20% of the full index.

The two-tier strategy is also adopted in our article, but instead of using the first tier as a cache, we use it as a candidate selection layer. Another important difference is that we always process the queries using both tiers.


                        Skobeltsyn, Junqueira, Plachouras, and Baeza-Yates (2008) evaluates the impact of including a cache in the system while using the two-tier method presented in Ntoulas and Cho (2007) and shows that the query distribution workload is affected by the cache system. Their conclusion is that, using the multi-tier for processing queries twice didn’t improve much when combined with a cache system.

We presented a preliminary version of our method in a previous work, where we proposed the Block Max WAND with Candidate Selection method (Rossi et al., 2013), or BMW-CS. It is an algorithm based on a two-tiered index organization in which the first tier is a small index created using the entries with the highest impacts from each term list, while the second tier is a larger index containing the remaining entries. By highest impact, we mean the entries with higher final contribution to the ranking, considering the similarity function adopted. For instance, in BM25, we compute the contribution of each entry by using the BM25 formula, including all factors taken by BM25. The idea is to use the first tier to accelerate the query processing in the second tier (i.e., the larger index) while computing the final ranking. The two tiers are disjoint, which means that, the high-impact entries in the first tier are not present in the second one.

To select the entries for the first tier, we compute a global threshold, so that the size of the first tier is about Δ% of the full index. The parameter Δ provides an estimation of the final size of the first tier index. We keep the size of each inverted list to a minimum of 1000 entries to prevent any individual list from becoming smaller than the typical sizes of top-k lists required in practice. For each term in the collection, the highest impact (max score) in the whole inverted list and the highest impact in the second tier are computed and stored along with the term information. The highest impact in the list at the second tier is used to set the upper bound of contribution for the entries that are not present in the first tier, but appear in the inverted list in the second tier.

The idea behind BMW-CS is to process the queries in two distinct phases. In the first phase, the method obtains from the first tier, a list of documents that are candidates for the top-k results, according to the ranking model adopted.

The candidate list is obtained by traversing the first tier and computing the scores of the documents found in the inverted lists of query terms. Since the inverted lists are not complete, whenever a document has a chance to become part of the top-k results, BMW-CS estimates the maximum score this document would achieve if it occurs in the second tier for all those terms where it does not appear in the first tier. The maximum score of a document in the second tier is added to compute its maximum potential score, and if this maximum potential score is higher than the current discarding threshold, which is the smaller score among the current top-k results, it is included in the candidate list.


                        Fig. 2
                         gives an example of an index divided into two tiers. In this example, document 69 occurs only in the list of the term hot in the first tier. If we were checking whether document 69 could be a candidate to be included in the top-k answers, we would add 1.7 to its score, since 1.7 would be the maximum score it would receive if it occurs in the second tier in lists of chili and peppers. The final score obtained would be the maximum potential score for document 69 in the collection. If this score is higher than the discarding threshold, document 69 would be saved as a candidate document to be considered while processing the second tier.

The second phase traverses the second tier to compute the complete score of each candidate document. Entries for other documents that are not in the candidate list are discarded, which considerably accelerates the traversal of the second tier. For instance, the complete score of document 69 would be obtained only while traversing the second tier. If we consider that in the example it does not occur in the second tier, its score would be the same as obtained in the first tier. If document 466 were also selected as a candidate, its score would be increased while processing the second tier, since it occurs in the second tier for the term peppers.

After processing the second tier, the method ensures that the right scores for each candidate document are computed, thereby obtaining a ranking that almost preserves the top-k results. However, some of the top-k answers for the query may be lost, since documents that do not occur in any of the query term lists in the first tier would not be taken into account while processing the final answer. If one of such documents is part of the top-k results, then the BMW-CS algorithm would miss it. For instance, in the example of Fig. 2, document 5 appears only in the second tier and would be discarded even if it were one of the top-k results. We further stress that document 5 would not appear in the answer at any position, given that it would never be selected as a candidate, which represents a serious flaw in our previous proposal.

We presented experiments in our previous article (Rossi et al., 2013) showing that BMW-CS could be several times faster than BMW. However, the weighing function adopted in our previous article was different from the standard adopted in the literature. This difference affected the number of index entries pruned by the experimented methods, and also their final time performance. In this article we have fixed this problem in the experimental protocol, BMW-CS still being up to two times faster than BMW, while computing top-10 results.

In the next section we describe an extension of BMW-CS, which we call BMW-CSP, that avoids the loss of answers in the top-k results.

In this section we present our method for fast query processing, called Block Max WAND with Candidate Selection and Preserving Top-k results algorithm, or BMW-CSP. It modifies BMW-CS to avoid situations where documents could be wrongly removed from the final answer. BMW-CSP includes a third phase of query processing that is activated in case there is chance for a document whose entries occur only in the second tier to be present at the final ranking. Such situation occurs when the sum of max scores of the query terms in the second tier is higher than the discarding threshold after processing the second tier. In the example of Fig. 2, this sum is 2.1, and then the third phase of query processing would be activated in case the discarding threshold after processing the second tier was smaller than 2.1.

The third phase performs a second traversal in the second tier to search for new documents to be included in the top-k results. The top scores computed in the first two phases are used to speed up the third phase, since they increase the pruning threshold. While at a first glance performing a third phase seems to be expensive, our experiments show that it does not cause significant changes in the final query processing times while considering the best configuration scenarios, since it is rarely activated. On the other hand, with this final step our query processing method preserves the top-k results.

Besides the change to guarantee the top-k results, we also modified BMW-CS in an attempt to reduce both memory requirements and time for computing queries. The three main changes include:

                        
                           (i)
                           Storing the k-th higher score of each term in the index, k being the number of top results desired. We use this information to compute an initial threshold while selecting candidates. The initial discarding threshold in BMW-CS is zero, with this change we start with a higher and safe initial threshold that discards more documents at the beginning of the query processing. The initial threshold is set to the highest k-th score of the query terms, since it is a clear lower bound value for the smaller score among the top-k results.

The usage of the block max information from the second tier index while selecting candidates in the first phase. With this change, we obtain a better estimation of the maximum potential score of each candidate document at the first phase, thus reducing the chance of including candidates that will not be present at the final top-k results. In BMW-CS, such estimation was based on the maximum score of the term in the second tier, instead of using the block max score. This change not only may speed up the query processing, but also may reduce the number of candidates, and thus the memory overhead required by the method.

Taking advantage of the threshold, already computed while processing the first tier as an initial threshold limit to be used while computing the complete score in the second phase. This simple strategy was not used in our original BMW-CS and again it may allow for speeding up the query processing.

We present experiments comparing the performance of the original BMW-CS and the performance of the first two-phases of BMW-CSP using the changes mentioned above. The results indicate that the changes led to a small reduction in both memory requirements and time for processing queries. While examining the impact of the three strategies, we realized that the strategy of selecting an initial threshold was the one that provided the highest impact in the performance. The other two strategies resulted in almost no gain in performance. Still we decided to report them, as we consider that the information about the possible changes in the method is useful for the reader.


                     Listing 1
                      presents an overview of the BMW-CSP algorithm, further illustrating its functioning. In its first phase, BMW-CSP uses the first tier to select documents that are candidates to be present in the top results. Initially, the set of candidate documents, denoted by 
                        
                           A
                           ,
                        
                      is generated by the function SelectCandidates, which computes the candidates for the top-k results of a query (Listing 2
                     ). We detail the candidate selection phase in Section 3.1.

Since the two tiers are disjoint, missing occurrences of documents in the first tier may appear in the second one. An example is given in Fig. 2, where we can see that document 466 occurs only in the second tier for the term peppers. Notice that while selecting candidates in the first phase we do not know whether 466 occurs in the second tier. Thus, during the candidate selection phase, the algorithm may discard a document that could have a high enough score when the full inverted lists are evaluated.

To avoid this possibility, the maximum score that a document can achieve after processing the inverted list in the second tier is estimated while computing the pivot. Considering again the example of Fig. 2, the maximum score value added to document 466 would be 0.4, which is the block max score of the block where it occurs in the second tier. In the original BMW-CS proposal, the algorithm would use the maximum score of the second tier as its estimation, thus choosing value 1.1 in the example, instead of 0.4. This is one of the improvements we proposed here. As a result, our new strategy provides more accurate estimates, in an attempt to reduce not only the number of candidates, but also the query processing times. The access to the block max of the second tier is not so expensive. This information is obtained by reading only the skip lists of the second tier, thus does not require access to the full index of the second tier while selecting candidates.

During the candidate selection, a minimum heap is used to store the top-k documents with a higher score. The lowest score of the heap is used as the discarding threshold to dynamically prune entries with no chance to be a part of the final top-k results. All evaluated documents that have a score higher than the discarding threshold while processing the first tier are added to the set of candidate documents.

In the second phase, the second tier is processed to compute the complete scores of all the candidate documents. After computing the complete scores, the algorithm checks whether the result provided by the second phase preserves the top-k ranking results or not. This is checked by computing the maximum possible score that a document with entries only in the second tier would achieve, which is represented by the sum of all max scores of terms in the second tier. This sum is then compared to the smaller score in the top-k answers computed till now. If the sum is higher than the smaller top-k score, this means that there is a possibility for a document with entries only in the second tier to be present in the final top-k answer. In this case, a third phase is required.

The third phase guarantees that the top-k results of the ranking will be preserved by performing a second pass in the second tier(function guaranteeTopKResults), when necessary. This second pass updates the top-k entries with documents whose entries occur only in the second tier. Since the initial results computed in the first two phases are used in the third phase, and given that the scores in the second tier are usually low, the third phase does not represent a prohibitive time overhead. After finishing it, we ensure that the top-k results are preserved.

With third phase we assure top-k results will be preserved. To prove that, let us assume a document d with score α is supposed to be among top-k results, thus α ≥ θ, where θ is the final pruning threshold adopted in BMW-CSP, which is always taken from the smaller score among the top-k results. Notice that the pruning threshold θ never decreases when processing a query, starting with value zero and reaching its maximum value at the end of query processing. As BMW-CSP computes the full score of each document that was not discarded, the only hypothesis to have d outside the final result is to suppose that it was discarded by BMW-CSP. However, d would never be discarded, since, by definition, the discarding procedure does not remove documents with scores higher than or equal to θ. We then come to a contradiction, since α ≥ θ. As a conclusion, all documents in top-k results will be taken into account, and BMW-CSP computes the full scores of all of them, thus preserving their ranking.

We have presented a general outline so far, but in the following sections we present the implementation in greater detail to make the reproduction of the method easier for the reader.


                        Listing 2 describes the algorithm for selecting candidates, which corresponds to phase 1 of our method. It starts by selecting the inverted lists to be processed (Line 6), which are the lists that represent each query term. The discarding threshold θ, is initially set to the maximum value among the k-th highest score found in each list (Line 7). Line 8 makes each of the inverted lists point to their first document. The function Next(l, d) searches in the skiplist associated with list l for the block where there is the first occurrence of a docId equal to or larger than d, setting the current block to the found position. Then, it moves the pointer to the current document of the list, l.curDoc, to the smallest entry with value greater than d.

The inverted lists of query terms shown in Listing 2 are represented by vector lists and each inverted list has an internal pointer to the docId being processed at each moment that is the current docId. In Line 10 we sort this vector in increasing order according to the current docId pointed by the internal pointers in each of these lists. Then in Line 11, we compute the next document that has a chance to be present in the top results, performing the pivoting. Our pivoting is computed by taking into consideration the possibility of some of the entries of a document being not included in the first tier, which allows us to add this information to compute the upper score (upScore), which is the maximum score, of each document while selecting the pivot. This procedure is described in Listing 3
                        . Lines 12 and 13 test the stop condition and set the current document to be analyzed by the algorithm.

Line 14 adopts the function NextShallow to move the pointers to the current document in each list. The function NextShallow is the same as presented in the original BMW proposal, and differs from function Next because it does not need to access the documents, rather accessing only the skiplists of the inverted lists to move their pointers and set a new current block in each inverted list. Using this function, we can skip entries without the need to access them. Line 15 calls function CheckBlockMax, detailed in Listing 4
                        .

The remaining algorithm checks whether a document has a score high enough to be included in the answer. Line 19 takes the incompleteness of the first tier into consideration while computing the upScore. It uses the max scores of the terms in the second tier to give an initial estimation of the upScore. The score of each document is used to include it in heap 
                           
                              H
                              ,
                           
                         as in lines 20–25. 
                           H
                         is maintained to control the discarding threshold θ. The upper score of each document (upScore) is used in Line 26 to check whether a document should be included in the candidate documents set 
                           A
                        . If the first estimation of the upScore is enough to include the document as a candidate, we compute a tighter estimation using the block max of second tier, as shown in Line 27.

The threshold θ changes as more documents are processed, so, whenever we add a document to 
                           
                              A
                              ,
                           
                         we also check if there is at least one document in 
                           A
                         with an upper score (upScore) value lower than the current value of θ. In such cases, we remove the document (line 31). This procedure avoids wasting memory by keeping elements in 
                           A
                         that will be discarded at the end of the process. After updating 
                           
                              A
                              ,
                           
                         the algorithm moves the pointers of each of the inverted lists to continue the query processing. This movement is performed in lines 34–46. Finally, the algorithm trims the set 
                           
                              A
                              ,
                           
                         removing all candidates that cannot be present in the top-k results (Line 48). This trimming decreases the cost of the second phase. By the end of the candidate selection algorithm, the list of candidate documents 
                           A
                         is returned, so that the final result can be obtained by processing the remainder of the index in the second tier.

Function CalculateCompleteScore (Listing 5
                        ) implements phase 2 of the algorithm. In this function, the scores of candidates with missing terms are evaluated using the larger index in the second tier, which contains the index entries not present in the first tier. While computing the complete scores, we set the initial value of threshold θ as the same as that obtained at the end of the candidate selection phase. This procedure allows us to discard more entries and speed up the second phase. Furthermore, to avoid unnecessary costs of decompression or accessing the list entries, the shallow movement described in Ding and Suel (2011) is used to align all the term lists. Then, a second BlockMaxScore check is made to verify whether the document can be a part of the top-k results or not. Each document is evaluated only if it has a high enough score, otherwise it is discarded. As in the first phase, we keep a minimum heap with the documents with the greatest scores evaluated, and the minimum score of this set is used as a discarding threshold to prune candidates.

To guarantee that the top-k results of the ranking will be preserved, the third phase performs a second passage in the second tier. In Listing 1, this phase is represented by function guaranteeTopKResults, which updates the top-k answers stored in the heap 
                           H
                         with the documents whose entries occur only in the second tier. This function is very similar to a new run of BMW on the second tier, thus we decided not to provide its listing. The only differences are: (i) it starts with the heap 
                           H
                         containing the top-k results computed up to that point; and (ii) the documents whose results are included in 
                           H
                         are not taken into account to avoid repetition of results. Notice that, as the scores in second tier are usually smaller than the ones in the first tier, we expect that most of the entries will be discarded and thus this third phase is expected to become faster than a regular BMW execution.

We used the TREC GOV2 collection for the experiments to compare the performance of our method to previous proposals presented in the literature. The collection has 25 million web pages crawled from the .gov domain in early 2004. It has 426GB of text, composed of HTML pages and the extracted content of pdf and postscript documents.

While extracting words with a simple parser, the vocabulary contains about 45 million words. Our indexes store the frequency of the terms as the information to compute term impacts at query processing times. We have selected a set of 10, 000 queries from the TREC 2006 efficiency query set for the experiments. We have performed most of the experiments without removing stop-words from the queries, but also present experiments with stop-words removal for the sake of completeness. During the query processing, we previously loaded into the memory, the inverted lists of all words found in the queries. This procedure was also adopted in the experiments described in Ding and Suel (2011) to avoid any possible bias during query processing time. The loading times are not taken into account. All of these setup options were chosen for being similar to those adopted in previous studies (Ding & Suel, 2011; Rossi et al., 2013; Strohman & Croft, 2007), including the baselines we adopt here. We ran the experiments in a 24-core Intel(R) Xeon(R), with X5680 Processor, 3.33 GHz and 64GB of primary memory. All the queries were processed in a single core.

All the experiments were run exclusively, with no other user being logged in or process being executed in the machine. To avoid cache effects, we have cleaned the cache before the execution of each method. Each method executed the whole set of queries without any cache clean between executions of each query. Codes were compiled using option -O2 with gcc compiler.

We used Okapi BM25 as the rank function, but our method can be adopted to compute other ranking functions. The generated skiplists have one entry for each block of 128 documents. Each skiplist entry keeps the first docId in the block, and the maximum impact registered in the block.

We performed experiments with the blocks of both types; one, that is stored in a compressed format, using Elias-γ coding (Ellias, 1975), and the other without any compression. When using the blocks with no compression, all methods achieve better performance, but memory requirements for storing the index increase. While using compression methods, we save memory space, but at the price of increased cost for accessing information from any block. Elias-γ encoding is a method for compressing sequences of small integers that presents competitive compression rates. Other compression methods could also have been adopted, such as pforDelta (Zukowski, Heman, Nes, & Boncz, 2006), which is faster for decoding and gives slightly lower compression rates. The experiments presented are good enough to compare the methods in a large range of possibilities, varying from the faster possibility of using no compression to the more aggressive compression achieved while using Elias-γ.

Another parameter evaluated in the experiments was the size of the top results required by the algorithms. We evaluated the algorithms requesting 10 and 1000 results. Retrieving the top-1000 results was included to simulate an environment where the top results are computed to feed a more sophisticated ranking method that performs a re-ranking of the results. The top-10 results were included to simulate a more common scenario where the user is interested in getting only a small list of results.

We also performed experiments varying the size of the collection to assert the impact of collection size in the final performance. We created collections with subsets of the GOV2 collections, taking 100 thousand documents, referred to as GOV100K, one million documents, referred to as GOV1M, 10 million documents, referred to as GOV10M and the full collection, referred to as GOV2.

We implemented the methods BMW and its multi-tier version, MBMW (Ding & Suel, 2011), and BMW-CS (Rossi et al., 2013) as baselines. To avoid a bias due to optimization of any version, all the codes were developed using the same basic BMW code, which was provided by one of the authors of BMW and MBMW Ding and Suel (2011). The BMW-CS and BMW-CSP codes were then achieved by modifying the original BMW code to produce the three phases of BMW-CSP and the two phases of BMW-CS. The main step of BMW-CS and BMW-CSP is the candidate selection, which is very similar to BMW, but includes the control to maintain the candidates. The access to the inverted lists was completely implemented by us, and was similar in all experimented methods. The final performance is compatible with the ones reported by the authors of MBMW and BMW. Our codes are publicly available in a git repository (http://github.com/CaioDaoud/BMW-CS).

The usage of multi-tier methods, such as BMW-CS, BMW-CSP and MBMW results in an overhead to store the maximum scores of the terms in each tier, instead of storing a single maximum score value, and pointers to indicate the beginning of skip lists for each term in each tier. Considering pointers of 64 bits, and maximum scores stored in 32 bits, this extra overhead represents 12 bytes for each term in the collection. As in our experiments the vocabulary contains about 45 million words, that means an extra space of about 0.5GB. On the other hand, our index contains about 6.7 billion entries that in the compressed form take about 6GB. Thus, when considering the compressed index, the overhead in our experiment is smaller than 8.5% of the total index size for each extra tier included in the index. If we consider the index with no compression, the overhead would be close to 1%. It is important to notice that the size of the vocabulary in natural language texts usually increases in O(Nβ
                     ), where N is the size of the collection and β < 1 de Moura, Navarro, Ziviani, and Baeza-Yates (2000). That means the overhead is supposed to become smaller as the indexed collection increases. Further, the extra space overhead in our method is the same as the ones presented in the two multi-tier baselines experimented.

@&#RESULTS@&#

The standard setup for the experiments is to index the collection without removing stop-words and using the index with no compression. We explicitly mention the changes in all the examples where this standard setup is modified.

We begin by comparing the memory and time requirements for computing the first two phases of BMW-CSP to the ones required while using the BMW-CS method. These experiments are useful to quantify the possible improvements in the first two phases achieved by the changes we proposed to the BMW-CS method. Fig. 3
                        (a) and (c) shows that BMW-CSP achieved a time performance only slightly better than BMW-CS for computing queries both at top-10 and top-1000 results. We also do not see much variation in the number of candidates selected in the first phase while comparing the methods. While examining the average number of candidates required for processing queries, we see that this number quickly decreases up to tier sizes of about 20% while computing top-10 results. The same occurs up to tier sizes of about 30% while computing top-1000 results.


                        Fig. 3 also shows an interesting behavior while considering the evolution in the number of candidates as we increase the size of the first tier. When first tier is too small, e.g. with 2% of the index, the number of candidates is small because the query results achieved in the first tier are quite incomplete. As we increase the size of first tier to intermediate sizes, e.g. from 2% to 10% while computing top-10 and from 2% to 15% while computing top-1000 results as shown in Fig. 3, the number of candidates increases, since we have more documents occurring at first tier. At these intermediate sizes, the maximum possible scores at the second tier do not allow to discard too many candidates. When we keep increasing the size of first tier, the maximum scores of terms in the second tier drop, and our estimation of results gets more accurate, allowing more candidates to be discarded. As a consequence, the number of candidates decreases for higher first tier sizes. At a limit, if we make the first tier with the whole index, the candidate selection would be able to pick only the exact set of top-k results, minimizing the size of candidate set. Thus the candidate set is smaller for very small first tier sizes, increases up to intermediate sizes and decreases again as we get bigger first tier sizes.


                        Fig. 4
                         presents the time required by each of the three phases of BMW-CSP method while varying the size of the first tier and computing the top-10 results (a) and the top-1000 results (c). Looking at the evolution of the number of candidates presented in Fig. 3 and at the results presented in Fig. 4, we see that the time required for the first phase is affected by two main factors, the number of selected candidates and the size of first tier. Second phase is highly affected by the initial increase in the number of candidates, which makes the costs increase quickly at smaller index sizes. However, as the size of first tier reaches the point where the number of candidates decreases, the cost of second phase starts to decrease, since both the number of candidates and the size of second tier get smaller.

Third phase has a distinct behavior, as we increase the size of first tier, the number of queries that go to the third phase quickly drops, as shown in Fig. 4(b and d). As a consequence, the average time for processing the third phase quickly decreases with the increase of first tier size.

We conclude that in regard to the performance of the method in the three phases, all three phases may play significant roles in the final performance of BMW-CSP. Thus, future alternatives for accelerating any of the three phases could potentially improve BMW-CSP. For instance, when computing the top-1000 results, significant improvements in the performance of third phase would make the first tier sizes from 1% to 10% more attractive, and significant improvements in the performance of second phase would make sizes from 5% to about 30% more attractive, while improving the performance of first phase would specially affect the scenarios where first tier is large.

Selecting tier-sizes that generate small number of candidates is important, since a potential disadvantage of our method compared to BMW and MBMW is the extra amount of memory required to hold information about candidate documents. Again comparing the results presented in Figs. 3 and 4, we see that the number of candidates is not so prohibitive, while considering the tier sizes that resulted in best performance for the methods. For instance, when computing top-10 results, the average number of candidates required in the best time performance option, when the first tier size was 30%, was 4225. While computing the top-1000 results, the total number of candidates was an average of 40702 in the best time performance option, when first tier size was 40%. If the query load is high, this extra memory requirement may become a serious problem. In such cases, higher first tier sizes could be adopted for reducing the average number of candidates. In extreme situations, the designer may choose MBMW instead of BMW-CSP, since MBMW does not require the storage of candidates while processing queries.

More important than comparing BMW-CSP to BMW-CS is comparing it to BMW and MBMW. In Fig. 5
                         we compare the performance of the three methods while varying the first tier sizes from 5% to 50%. In this Figure, we present the original version of BMW and MBMW proposed by the authors, and their optimized versions, BMWT and MBMWT, that adopt the proposed initial threshold we also adopted in BMW-CSP. The best results for MBMW and MBMWT were achieved at tier sizes 5% while computing top-10 results and 10% when computing top-1000 results. The best results for BMW-CSP were achieved for 30% tier sizes when computing top-10 results and 40% tier sizes while computing top-1000 results. We can also see from this Figure that, if we consider the best tier size options, both experimented multi-tier alternatives perform better than BMW using a single tier.


                        Table 1
                         presents details about the best results achieved by each method in the experiments. We can observe the performance of the algorithms BMW, BMWT, MBMW, MBMWT and BMW-CSP in terms of the number of selected pivots, the number of accessed blocks while processing the queries, and query time while processing the top-10 and top-1000 results. The time results are presented with a confidence level of 95%, and the tier size chosen for each multi-tier method are the ones that achieved the best results in Fig. 5.

We can see that BMW-CSP was considerably faster than the optimized version of the best baseline, MBMWT, while computing top-10 and top-1000 results. The adoption of BMW-CSP in top-10 would reduce the time from 35.27 ms to 19.64 ms and in top-1000 results, a reduction of time from 82.65 ms to 66.35 ms. This improvement is a consequence of a reduction of both the number of pivots and the number of accessed blocks, which constitute the main query processing costs.

In spite of the considerably high reduction in the number of accessed blocks, time reduction was more related to the proportion of reduction in the number of evaluated pivots. In these experiments, we adopted an index where the blocks are not compressed, thus reducing the costs for accessing each block.

The number of selected pivots is especially important to MBMW and BMW-CSP. For instance, in BMW-CSP, the selection of each pivot results in extra time for updating the candidates list. MBMW also presents important extra costs when selecting pivots. For instance, MBMW manages twice the number of lists managed by the other methods, since it splits each inverted list into two lists.


                        Fig. 6
                         presents an overview about the time performance of the methods BMW-CSP and MBMWT while processing all the queries in the dataset. The queries are sorted by the time for running BMW-CSP, which allows a clear distinction between its performance and the performance of MBMWT. Points above the line indicate queries where BMW-CSP was better than the baseline. Results while processing top-10 are presented in Fig. 6(a) and top-1000 are presented in Fig. 6(b). To allow a better view of the points in the graphics, we present separate graphs for queries that take less than 1 s (a) and (b), and for queries that take more than 1 s (c) and (d). Queries that take more than 1 s represent a small set in both scenarios, which constitutes exactly 15 queries in the top-10 scenario and 49 queries in the top-1000 scenario. The Figures are useful to show that BMW-CSP was faster while processing most of the queries for both top-10 and top-1000 results.

While analyzing the queries that take more time, we realized that they are mostly composed by very large queries, varying their sizes from 9 to 25 terms. Comparing the performance of the methods for those expensive queries, still BMW-CSP was slightly better than MBMWT while computing top-10 results. While computing top-1000 results, BMW-CSP was worse in 26 queries from a total of 49. We further investigated the reasons for the degradation in performance of BMW-CS for those queries and realized that they are the queries where the number of terms are extremely high. For instance, BMW-CSP was worse for almost all queries with 14 terms or more. The maintenance of the candidates set in these queries slows down the query processing in BMW-CS, specially while computing top-1000 results. A clear conclusion is that BMW-CSP is not a good option in applications where the number of terms is expected to be too high. We notice that in such scenarios, document-at-a-time query processing methods are not recommended as the best option for processing queries and the term-at-a-time methods are usually the best solutions.


                           Table 2
                            presents the performance of the algorithms while processing distinct query sizes for both the top-10 and top-1000 results computation. We can see that BMW-CSP was the fastest option in all of the query sizes. The biggest differences between our method and the baselines can be seen in the results for top-10 with queries of a single term, but this set of queries is less than 2% of the queries in the collection. We can also see that more than 80% of the queries have sizes varying from 2 to 5 terms.

An important conclusion is that, while computing top-1000 results, MBMWT is more competitive, being faster for query sizes of 2 terms, which represents about 14.84% of the queries. On the other hand, BMW-CSP achieved the best performance in all the other sizes experimented.

As we see, BMW-CSP is faster than the baselines while using the index with no compression. We also performed experiments using the methods to process queries when the index is compressed with Elias-γ coding, which slows down the access to each block in the index. Table 3
                            presents the results of these experiments. As expected, the differences in time between BMW-CSP are slightly larger while using compression, since BMW-CSP decompresses fewer blocks. Time for computing top-10 results in the best configuration for BMW-CSP, thus increased from 19.64 ms to 41.64 ms, a difference of 22 ms. While looking at the results of MBMWT, the times varied from 35.25 to 87.18,ms, a difference of about 52 ms. The increase in time was smaller for BMW-CSP, since it decodes a considerably smaller number of blocks from the index. The same phenomenon occurs when we look at the best results achieved while computing top-1000 results, where the increase in time was about 71 ms for BMW-CSP and about 110 ms for MBMWT.

We notice that the ratio between the time increase for BMW-CSP and MBMWT was not completely proportional to the amount of blocks accessed by the methods. This phenomenon can be explained by machine architectural details, such as the effect of the cache system over the decoding times. For instance, while decoding more blocks, the decoded blocks become closer to each other into the main memory, thus possibly improving the caching performance. The experiments with and without compression give a good idea about the impact of the speed of the adopted compression method while decoding blocks into the final performance of the methods. They show that the gain in performance achieved by BMW-CSP will still exist while using slower compression methods, such as Elias-γ (Ellias, 1975) or faster methods, such as pforDelta (Zukowski et al., 2006).

Another variation that may occur while implementing a search system is that in certain scenarios the systems remove stop-words. Table 4
                            presents results of experiments to compare the performance of the methods in a scenario where there was stop word removal. The comparative performance of the methods in this scenario shows that BMW-CSP is still faster than the baselines. We observe that when we remove the stop-words, the differences in performance between the methods are smaller. For instance, while computing top-1000, MBMWT was only 10% slower than BMW-CSP, while the difference is larger, about 19%, when the stop-words are not removed.


                           Table 5
                            presents the impact of varying the collection sizes while processing queries with the experimented methods. The goal of this experiment was to check whether variations in the collection size affect the differences in performance between the methods. For this experiment, we have selected chunks of size 100K, 1M, 10M and the full collection GOV2. The results indicate that the differences between BMW-CSP and the baselines are slightly larger when query processing is done in the smaller portion of GOV2 collection. It decreases up to the collection of 1M and then stabilizes.

The experimental results indicate that BMW-CSP algorithm is a quite competitive alternative compared to the previous state-of-the-art query processing methods. We achieved considerable gains in time performance when computing top-10 results, reducing the average query processing times from 35.27 to 19.64 ms while compared to MBMWT. While computing top-1000 results, our method resulted in gains from 82.66 to 66.35 ms in the query processing times achieved in our experiments.

The application of our algorithm represents a trade-off between memory usage and time performance compared to MBMWT. The main memory overhead required by our algorithm is in practice the space required to store the accumulators for the candidates selected in the first phase. Our experiments show that this extra memory also, is in practice acceptable, but still MBMWT will become a better option in scenarios with memory restrictions and high query load, where several queries are simultaneously processed.

The experiments presented in this article consider a scenario where the baselines are typically applied. The GOV2 collection has been used as a standard to compare the performance of document-at-a-time query processing algorithms, such as WAND, BMW and MBMW. These algorithms, as it occurs with BMW-CSP, may not be the best options in other search scenarios not considered here. For instance, Fontoura et al. (2011) performed a study in collections typically processed by web advertising systems, where the documents are extremely small and the queries are long. In this scenario, they concluded that term-at-a-time solutions, or hybrid approaches combining term-at-a-time and document-at-a-time algorithms may be the best alternative for processing queries.

As future work, we plan to investigate the possible advantages of using BMW-CSP in a scenario where the index does not fit into main memory. In such cases, the property of performing the first phase entirely using the smaller index may represent an interesting advantage while comparing BMW-CSP to MBMW and BMW. This new scenario requires further detailed studies, since other factors, such as the possibility of using caching methods, may affect the conclusions about the possible benefits of using BMW-CSP. We expect however that BMW-CSP may present even more competitive performance in such scenario.

@&#ACKNOWLEDGMENTS@&#

We thank Constantinos Dimopoulos for sending us the source codes of BMW, which was used as the seed for our implementation. This research was sponsored by project E-vox pesquisa/FAPEAM, by CAPES scholarship (Caio Moura Daoud) and by individual CNPq fellowship Grants for Edleno S. de Moura is 308130/2014-6 and for Altigran Soares da Silva is 31433/2014-6.

@&#REFERENCES@&#

