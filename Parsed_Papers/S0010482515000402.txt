@&#MAIN-TITLE@&#No-reference hair occlusion assessment for dermoscopy images based on distribution feature

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A novel local adaptive hair detection method is presented.


                        
                        
                           
                           The proposed hair detection method works well on both sparse hair and dense hair.


                        
                        
                           
                           Hair distribution features based on quantity, position and dispersion are extracted.


                        
                        
                           
                           An objective assessment metric for the degree of hair occlusion is designed.


                        
                        
                           
                           Our assessment method can effectively evaluate the degree of hair occlusion.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Dermoscopy image

Hair occlusion

Hair detection

Image quality

No-reference assessment

@&#ABSTRACT@&#


               
               
                  The presence of hair is a common quality problem for dermoscopy images, which may influence the accuracy of lesion analysis. In this paper, a novel no-reference hair occlusion assessment method is proposed according to the distribution feature of hairs in the dermoscopy image. Firstly, the image is adaptively enhanced by simple linear iterative clustering (SLIC) combined with isotropic nonlinear filtering (INF). Then, hairs are extracted from the image by an automatic threshold and meanwhile the postprocessing is used to refine the hair through re-extracting omissive hairs and filtering false hairs. Finally, the degree of hair occlusion is evaluated by an objective metric based on the hair distribution. A series of experiments was carried out on both simulated images and real images. The result shows that the proposed local adaptive hair detection method can work well on both sparse hair and dense hair, and the designed metric can effectively evaluate the degree of hair occlusion.
               
            

@&#INTRODUCTION@&#

Dermoscopy, as a non-invasive skin imaging technique which allows a better visualization of the skin surface and subsurface structures [1], is beneficial to diagnosing many skin diseases in clinical applications [2,3]. In the early diagnosis of malignant melanoma (MM), dermoscopy images play a significant role to increase the survival rate of patients. However, the automatic analysis of skin lesions is heavily impacted by hairs covering them. As shown in Fig. 1
                     , hair pixels in dermoscopy images occlude some information of lesions such as border and texture, which will lead to imprecise segmentation and wrong classification. To overcome the hair occlusion problem [4], an effective hair detection and removal method is required.

In 1997, Lee et al. [5] proposed the DullRazor hair-removal algorithm which used morphological closing operation to identify the locations of dark hairs and replaced them by linear interpolation. Kiana et al. [6] improved the DullRazor method for detecting dark and light-color hairs. In [7], Xie et al. used morphological top-hat operator and statistic threshold to obtain the hair binary image. Then hairs were extracted according to the elongated state of connected region and removed from the image by the partial differential equations (PDE) inpainting algorithm. Abbas et al. [8] proposed the hair lines detection scheme based on the 2-D derivatives of Gaussian function in the CIE L⁎a⁎b⁎
                      color space and then used the morphological operator to obtain smooth hair lines which were inpainted by the fast marching inpainting method. A similar approach was described in [9], which improved the canny method to roughly detect and remove hairs from dermoscopy images through a multi-resolution coherence transport inpainting method.

However, most of the algorithms above only deal with the case of mild hair occlusion. For dermoscopy images with dense hair like the rightmost one in Fig. 1, they perform badly due to (i) hardly extracting correct hair pixels and (ii) bad inpainting results in the crowded hair region. Hence, an effective assessment method for the degree of hair occlusion is necessary. If dermoscopy images with serious hair occlusion can be suggested to be abandoned or recaptured before they get into the automatic analysis system, the accuracy of automatic diagnosis for skin diseases will be effectively improved.

The presence of hair is a common quality problem for dermoscopy images. Although some hair detection and removal methods have been proposed, the hair occlusion assessment is still not addressed. Since there is no reference image for the dermoscopy image with hair, a no-reference assessment method is needed here. In the last few decades, lots of no-reference image quality assessment (IQA) methods have been developed for different purposes. In these IQA methods [10–12], the considered quality problems are mainly caused by distortions such as blur, noises, JPEG and JPEG2000 compression. As a real substance, the hair is not a distortion. Therefore, traditional IQA methods cannot be used for hair occlusion assessment.

In this paper, hairs in dermoscopy images are extracted by the local adaptive hair detection method firstly, and then the degree of hair occlusion is evaluated according to the hair distribution features. The remainder of this paper is organized as follows. In Section 2, the local adaptive hair detection approach is described in detail. In Section 3, we introduce the assessment method for the degree of hair occlusion. Experimental results and analysis are presented in Section 4. Finally, Section 5 gives the conclusions.

Generally, hair detection contains two steps: hair enhancement and threshold segmentation. In most of state-of-the-art hair detection methods, the enhancement and the thresholding are usually global, for example the methods described in [7,8]. However, there are disparities in the color and texture between the healthy skin region and the lesion region. Similarly, relative to regions of the same background, the region with dense hair tends to have a different contrast from the region with sparse hair. Therefore the global hair detection method cannot adapt to the local variety of color and texture in dermoscopy images. In this paper, a local adaptive hair detection method is proposed for dermoscopy images, as shown in Fig. 2
                     . The image is clustered into sub-regions using a simple linear iterative clustering (SLIC) algorithm [13] firstly, then hairs in each sub-region are enhanced through isotropic nonlinear filtering (INF) [14] and extracted by the proposed adaptive threshold.

Superpixel algorithms are very useful as a preprocessing step for computer vision applications like object class recognition and medical image segmentation [13,15]. As a superpixel method, the SLIC algorithm [13] is simple to implement and outputs better quality superpixels that are compact and roughly equally sized. Let 
                           
                              
                                 [
                                 
                                    
                                       l
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       a
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       b
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 ]
                              
                              
                                 T
                              
                           
                         be the 5-dimensional space, where 
                           
                              
                                 [
                                 
                                    
                                       l
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       a
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       b
                                    
                                    
                                       i
                                    
                                 
                                 ]
                              
                              
                                 T
                              
                           
                         represents CIE L⁎a⁎b⁎
                         color space and 
                           
                              
                                 [
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 ]
                              
                              
                                 T
                              
                           
                         is the pixel position. According to [13], a distance measure D
                        
                           S
                         is defined to enforce color similarity as well as pixel proximity in this 5-D space as follows:
                           
                              (1)
                              
                                 
                                    
                                       D
                                    
                                    
                                       S
                                    
                                 
                                 =
                                 
                                    
                                       d
                                    
                                    
                                       lab
                                    
                                 
                                 +
                                 
                                    
                                       h
                                    
                                    
                                       S
                                    
                                 
                                 
                                    
                                       d
                                    
                                    
                                       xy
                                    
                                 
                              
                           
                        where the lab distance d
                        
                           lab
                         equals 
                           
                              
                                 
                                    
                                       (
                                       
                                          
                                             l
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          
                                             l
                                          
                                          
                                             k
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       (
                                       
                                          
                                             a
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          
                                             a
                                          
                                          
                                             k
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       (
                                       
                                          
                                             b
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          
                                             b
                                          
                                          
                                             k
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , the xy distance d
                        
                           xy
                         equals 
                           
                              
                                 
                                    
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          
                                             x
                                          
                                          
                                             k
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       (
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       
                                          
                                             y
                                          
                                          
                                             k
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        , 
                           
                              
                                 [
                                 
                                    
                                       l
                                    
                                    
                                       k
                                    
                                 
                                 ,
                                 
                                    
                                       a
                                    
                                    
                                       k
                                    
                                 
                                 ,
                                 
                                    
                                       b
                                    
                                    
                                       k
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       k
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       k
                                    
                                 
                                 ]
                              
                              
                                 T
                              
                           
                         is the 5-D feature of the kth cluster center, S is the grid interval which decides the number of clusters and h is the compactness parameter. The greater the value of h is, the more spatial proximity is emphasized and the more compact the cluster is. The algorithm obtains the clustering result by iteratively repeating the process of associating pixels with the nearest cluster center and recomputing the cluster center.

In this paper, the value of S is a quarter of the width of the image and h is set to 80. Fig. 3
                         shows an example of clustering result for a dermoscopy image using the SLIC method. The dermoscopy image with hair is clustered into several uniform sub-regions, which can be approximately classified into 4 types: only healthy skin, only lesion, healthy skin with hair and lesion with hair. Compared with the whole image, the color and texture become simple in each sub-region which is of benefit to the accuracy of hair detection.

For the effective extraction of hairs, the INF method is introduced into this paper to enhance hairs in each sub-region. Different from most existing edge and line enhancement methods which use directional derivatives, INF can enhance a wide line completely without any derivative [14]. For a pixel 
                           (
                           
                              
                                 x
                              
                              
                                 0
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 0
                              
                           
                           )
                        , the value of INF response m can be defined as
                           
                              (2)
                              
                                 m
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       0
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       0
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             x
                                          
                                       
                                       
                                          
                                             ∑
                                          
                                          
                                             y
                                          
                                       
                                       s
                                       (
                                       x
                                       ,
                                       y
                                       ,
                                       
                                          
                                             x
                                          
                                          
                                             0
                                          
                                       
                                       ,
                                       
                                          
                                             y
                                          
                                          
                                             0
                                          
                                       
                                       ,
                                       t
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             x
                                          
                                       
                                       
                                          
                                             ∑
                                          
                                          
                                             y
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       (
                                       x
                                       −
                                       
                                          
                                             x
                                          
                                          
                                             0
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       (
                                       y
                                       −
                                       
                                          
                                             y
                                          
                                          
                                             0
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                                 ≤
                                 
                                    
                                       r
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        where r is the radius of circular mask centered at 
                           (
                           
                              
                                 x
                              
                              
                                 0
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 0
                              
                           
                           )
                        , (x,y) is the coordinate of any other pixel within the mask, s represents the brightness similarity between the center pixel 
                           (
                           
                              
                                 x
                              
                              
                                 0
                              
                           
                           ,
                           
                              
                                 y
                              
                              
                                 0
                              
                           
                           )
                         and the element (x,y) and can be calculated as
                           
                              (3)
                              
                                 s
                                 (
                                 x
                                 ,
                                 y
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       0
                                    
                                 
                                 ,
                                 
                                    
                                       y
                                    
                                    
                                       0
                                    
                                 
                                 ,
                                 t
                                 )
                                 =
                                 {
                                 
                                    
                                       
                                          
                                             1
                                          
                                          
                                             if
                                             
                                             I
                                             (
                                             x
                                             ,
                                             y
                                             )
                                             −
                                             I
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   0
                                                
                                             
                                             ,
                                             
                                                
                                                   y
                                                
                                                
                                                   0
                                                
                                             
                                             )
                                             ≤
                                             t
                                             ,
                                          
                                       
                                       
                                          
                                             0
                                          
                                          
                                             if
                                             
                                             I
                                             (
                                             x
                                             ,
                                             y
                                             )
                                             −
                                             I
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   0
                                                
                                             
                                             ,
                                             
                                                
                                                   y
                                                
                                                
                                                   0
                                                
                                             
                                             )
                                             >
                                             t
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           I
                           (
                           x
                           ,
                           y
                           )
                         is the brightness value, which is normalized between 0 and 1, t is the brightness contrast threshold.

When the r is large enough and an appropriate t is given, the response m of hair pixel will be much smaller than that of skin pixel. In [14], r has proved to be more than 1.25 times the width of line and the value of t is suggested to be the standard deviation of grayscale values. In this paper, r is set to 20 according to the real width of thick hair, and t is the standard deviation of grayscale values in the sub-region. Fig. 4
                         is an instance of hair extraction based on sub-regions, where (b) shows the sub-regions clustered by SLIC, and (c) presents the enhancement results of the sub-regions in (b) using INF. It can be seen that, most of hairs are effectively enhanced, but in the crowded hair region, the enhancement effect is dissatisfactory (see the blue circle in the rightmost of Fig. 4(b)), which may lead to omissive hairs.

Now, an automatic threshold is needed for each sub-region to extract hairs from the enhanced image. From Fig. 3, it can be seen that the backgrounds of sub-regions are simple (only lesion or healthy skin in most of the sub-regions) and the quantity of hairs is very different for different sub-regions. The Otsu method can segment an image automatically, but there would be over-segmentation for sub-regions with few hairs and under-segmentation for sub-regions with dense hair. In [7], dermoscopy images with hair are segmented by the threshold method with 5% ratio after enhanced. This method cannot work well for the image with lots of hair.

It is obvious that there are two main factors to influence the optimal threshold: gray value distribution of the enhanced sub-region and the quantity of hairs in this sub-region. However, it is difficult to obtain the exact quantity of hairs from the enhanced sub-region because of the information loss. Hair has the characteristic of curve, and the gradient information of original image can reflect the amount of hairs to some extent. In addition, the gradient information is from the original grayscale. According to the analysis above, we infer that the optimal threshold is related to three values: grayscale and gradient means of sub-region in the original image and INF response mean of sub-region in the enhanced image. Next, we analyze the correlations of the three means and the optimal threshold.

We obtained 646 sub-regions from 35 dermoscopy images with different degrees of hair occlusion using the SLIC algorithm. In the 646 sub-regions, there are 282 sub-regions without hair which are so many as to influence the correlation analysis result. Hence, we discarded 146 sub-regions without hair and 500 sub-regions were saved to balance the sample sizes among sub-regions with sparse hair, sub-regions with dense hair and those without hair. For each sub-region, its three means (grayscale and gradient means in the original image and INF response mean in the enhanced image) are calculated, and the optimal threshold is manually obtained from the enhanced image. The distributions of the three means are shown in Fig. 5
                           . The linear correlation coefficients among the three means and the optimal threshold are 0.1855, 0.6327 and 0.3812. Obviously, the optimal threshold has higher correlation with both the gradient mean and the INF response mean. For a sub-region, we model the optimal threshold y as the linear combination of the gradient mean and INF response mean as follows:
                              
                                 (4)
                                 
                                    y
                                    =
                                    
                                       
                                          ax
                                       
                                       
                                          1
                                       
                                    
                                    +
                                    
                                       
                                          bx
                                       
                                       
                                          2
                                       
                                    
                                    +
                                    c
                                 
                              
                           where x
                           1 is the gradient mean of the sub-region in the original image, which reflects the quantity of hairs, x
                           2 is the INF response mean of the sub-region in the enhanced image, a, b and c are the combination coefficients.

In order to obtain optimal coefficients a, b and c, we use the manual thresholds of the 500 sub-regions to train the model in Eq. (4) through the linear regression method. The coefficients a, b and c are optimized to be 1.4553, 1.1384 and −0.5775 respectively. To avoid too large threshold, an upper bound operator is added to Eq. (4), and the final adaptive threshold formula is
                              
                                 (5)
                                 
                                    
                                       
                                          y
                                       
                                       
                                          ^
                                       
                                    
                                    =
                                    min
                                    {
                                    1.4553
                                    
                                       
                                          x
                                       
                                       
                                          1
                                       
                                    
                                    +
                                    1.1384
                                    
                                       
                                          x
                                       
                                       
                                          2
                                       
                                    
                                    −
                                    0.5775
                                    ,
                                    λ
                                    }
                                 
                              
                           where λ is the upper bound of threshold, which is 0.85 in this paper.

For Eq. (5), the INF response is combined with the gradient information to achieve the optimal threshold which can adaptively vary with the image content. The process is shown in Fig. 4. For each sub-region, the gradient mean and the INF response mean are calculated from Fig. 4(b) and (c) respectively, and putting them into Eq. (5), the adaptive threshold is obtained. Using the adaptive threshold to segment the sub-region in (c), the hairs are detected in (d), and (e) is the final hair detection result. Clearly, most of hairs are extracted. At the same time, some non-hair noises are also detected.

From Fig. 4(d), it can be seen that some true hairs are missed and some noises are detected as hairs. Therefore, the detected hairs need to be refined.

The omissive hairs are mainly located in the region with dense hair, and their brightness values are similar to those of the extracted hairs in the sub-region. Therefore, the omissive hairs can be re-extracted through comparing the brightness values between the un-extracted pixels and the extracted hair pixels in the sub-region. Because the border pixels of the extracted hairs are not always accurately corresponding to the edge of the real hairs, it is more reliable to use the brightness values of the pixels corresponding to the skeleton of extracted hairs to represent the real hair brightness. Considering that there are often some non-hair noises (for example the dot regions in Fig. 6
                           (b)) which influence the confirmation of the brightness value, the brightness median of the pixels on the skeleton of extracted hairs is finally taken to represent the brightness of the hairs in the sub-region. Therefore, for a sub-region, the main steps of the re-extraction of omissive hairs in this paper include
                              
                                 (i)
                                 Obtaining the skeleton of extracted hairs through the thinning method proposed in [16].

Taking the skeleton as mask and calculating the brightness median of the pixels.

Scanning all the un-extracted pixels in this sub-region, if the brightness difference between the un-extracted pixel and the median is less than a threshold, which is experimentally set to 0.05 in this paper, the un-extracted pixel is re-extracted as hair pixel.

The re-extraction procedure is shown in Fig. 6, where (c) is a sub-region with omissive hairs and (e) is the corresponding original grayscale image, (f) is the re-extracted result. The re-extraction procedure is carried out on all the sub-regions to generate the final result shown in (g). Comparing (g) with (b), it can be seen that the omissive hairs are re-extracted successfully.

In the binary images of extracted hairs, for example Fig. 6(g), there are many connected regions (black regions) including non-hair noise, single hair and multiple hairs crossing each other. These non-hair noises (false hair objects) need to be filtered out, and the process is on the whole image here. We used 8-connected neighborhood to extract these connected regions. And for each connected region, its circularity is defined as
                              
                                 (6)
                                 
                                    
                                       
                                          F
                                       
                                       
                                          c
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                A
                                             
                                             
                                                c
                                             
                                          
                                       
                                       
                                          π
                                          
                                             
                                                R
                                             
                                             
                                                c
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           where A
                           
                              c
                            and R
                           
                              c
                            are the area and the minimum circumscribed circle radius of the connected region respectively. The value of F
                           
                              c
                            is between 0 and 1, and a small F
                           
                              c
                            indicates a linear region. The crowded hair region also has a large F
                           
                              c
                           , but it usually has a larger R
                           
                              c
                            than the false hair. Therefore, if 
                              
                                 
                                    F
                                 
                                 
                                    c
                                 
                              
                              >
                              
                                 
                                    λ
                                 
                                 
                                    f
                                 
                              
                            and 
                              
                                 
                                    R
                                 
                                 
                                    c
                                 
                              
                              <
                              
                                 
                                    λ
                                 
                                 
                                    r
                                 
                              
                           , the region is filtered out as non-hair noise. In this paper, λ
                           f and λ
                           r are experimentally set to 0.18 and two-fifth of the image width respectively. Fig. 7
                            is the removal result of the false hairs for Fig. 6(g).

For dermoscopy images, the degree of hair occlusion is related to the hair distribution in three aspects: (i) the quantity or area of hairs. The more the quantity or area of hairs is, the more seriously the information is occluded. (ii) The location of hairs in the image. Lesion is more important than healthy skin for dermoscopy image analysis. Therefore, hairs inside the lesion region occlude the information more seriously than those in the healthy skin region. (iii) The dispersion of hairs. Crowded Hairs have stronger occlusion capability than scattered hairs. According to the distribution feature, the evaluation metric for the degree of hair occlusion is proposed in this paper.

Given an image with size of M×N, the coverage rate C
                     
                        c
                      is defined as
                        
                           (7)
                           
                              
                                 
                                    C
                                 
                                 
                                    c
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          ω
                                       
                                       
                                          1
                                       
                                    
                                    
                                       
                                          A
                                       
                                       
                                          hairLesion
                                       
                                    
                                    +
                                    
                                       
                                          ω
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          A
                                       
                                       
                                          hairHealth
                                       
                                    
                                 
                                 
                                    M
                                    ×
                                    N
                                 
                              
                           
                        
                     where A
                     
                        hairLesion
                      and A
                     
                        hairHealth
                      are hair area inside the lesion region and in the healthy skin region respectively, ω
                     1 and ω
                     2 
                     
                        (
                        
                           
                              ω
                           
                           
                              1
                           
                        
                        >
                        
                           
                              ω
                           
                           
                              2
                           
                        
                        )
                      are the occlusion weights for A
                     
                        hairLesion
                      and A
                     
                        hairHealth
                     . C
                     
                        c
                      is related to two factors including hair area and position, which indicates the proportion of the hairs area with different occlusion weights to the total image area. The larger the value of C
                     
                        c
                      is, the more seriously the information is occluded by hairs.

The dispersion degree of hairs is defined as
                        
                           (8)
                           
                              
                                 
                                    C
                                 
                                 
                                    d
                                 
                              
                              =
                              
                                 
                                    1
                                 
                                 
                                    n
                                 
                              
                              
                                 
                                    ∑
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    n
                                 
                              
                              
                                 
                                    
                                       
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          −
                                          
                                             
                                                x
                                             
                                             
                                                ¯
                                             
                                          
                                          )
                                       
                                       
                                          2
                                       
                                    
                                    +
                                    
                                       
                                          (
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                          −
                                          
                                             
                                                y
                                             
                                             
                                                ¯
                                             
                                          
                                          )
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           
                        
                     where n is the pixel number of all the hairs in the image, 
                        (
                        
                           
                              x
                           
                           
                              i
                           
                        
                        ,
                        
                           
                              y
                           
                           
                              i
                           
                        
                        )
                      is the coordinate of the ith hair pixel, 
                        (
                        
                           
                              x
                           
                           
                              ¯
                           
                        
                        ,
                        
                           
                              y
                           
                           
                              ¯
                           
                        
                        )
                      is the center coordinate of all hair pixels, 
                        
                           
                              x
                           
                           
                              ¯
                           
                        
                        =
                        (
                        1
                        /
                        n
                        )
                        
                           
                              ∑
                           
                           
                              i
                              =
                              1
                           
                           
                              n
                           
                        
                        
                           
                              x
                           
                           
                              i
                           
                        
                      and 
                        
                           
                              y
                           
                           
                              ¯
                           
                        
                        =
                        (
                        1
                        /
                        n
                        )
                        
                           
                              ∑
                           
                           
                              i
                              =
                              1
                           
                           
                              n
                           
                        
                        
                           
                              y
                           
                           
                              i
                           
                        
                     . The larger the value of C
                     
                        d
                      is, the more scattered the distribution of hairs is. Thus, we define the metric for the degree of hair occlusion as
                        
                           (9)
                           
                              C
                              =
                              
                                 
                                    
                                       
                                          C
                                       
                                       
                                          c
                                       
                                    
                                 
                                 
                                    
                                       
                                          C
                                       
                                       
                                          d
                                       
                                    
                                 
                              
                           
                        
                     From Eq. (9), it can be seen that, for a dermoscopy image, the more and the denser hairs are, the larger the value of C is, which means that the information in this image is seriously occluded.

For Eq. (7), the hair pixel position needs to be identified whether in the lesion region or not. Therefore segmentation is done before calculating the metric C. Considering hairs in the image usually causes to imprecise segmentation result, we use Otsu׳s thresholding [17] to segment the non-hair regions in gray space to obtain the lesion region and the healthy skin region. Fig. 8
                      is the segmentation procedure of the dermoscopy image with hair, where (a) is the original image, (b) is the grayscale image, (c) is extracted hairs by the method described in Section 2, (d) is the mask generated by inversing the (c) and the black part (non-hair regions) indicates which elements of (b) need to be segmented, (e) is the segmentation result for (b) with the mask (d) by Otsu׳s thresholding. According to the position of lesion and the coordinates of hair pixels, the metric C can be calculated by using Eq. (9).

The proposed method is tested on real dermoscopy images and simulated data respectively. For the real data, there are 110 dermoscopy images including 40 images without hair, 40 with sparse hair, and 30 with thicker and dense hair. For the simulated data, the simulation of dermoscopy image with hair is shown in Fig. 9
                     . We first drew several curves arbitrarily, and randomly stretched, rotated or distorted these curves using Photoshop software to generate simulated hairs, and then overlapped these simulated hairs on each other to obtain 6 hair masks with a different degree of hairs shown in Fig. 9(b). The 6 simulated masks were added to 15 reference images without hair, and in total, 90 simulated images were obtained. All of the images including 110 real images and 15 reference images are xanthoderm with size 752×560 from General Hospital of the Air Force of the Chinese People׳s Liberation Army, and they are not crossed with the 35 dermoscopy images used for the optimal threshold model in Section 2.3. All steps of the proposed method were implemented using Matlab R2013a on the PC with 3.40GHz 
                        
                           
                              Intel
                           
                           
                              ®
                           
                        
                      
                     
                        
                           
                              Core
                           
                           
                              ™
                           
                        
                      i7 processor and 8GB DDR3 SDRAM.

DullRazor [5] is a hair removal software which enhances hairs through the morphological closing operator first and extracts the hair regions through determining a pixel inside the hair if the longest line in one direction through the pixel is longer than 50 pixels and the lines in other three directions through the pixel are all shorter than 10 pixels. Xie et al. [7] used the morphological top-hat operator to enhance the image and segmented the enhanced image by a threshold, then used elongate function to filter noises. In this paper, the image is clustered into sub-regions firstly, and in each sub-region, the hairs are enhanced and extracted. The proposed method is compared with DullRazor
                           1
                        
                        
                           1
                           
                              http://www.dermweb.com/dull_razor/dullrazor_wins.zip.
                           
                         and Xie׳s method
                           2
                        
                        
                           2
                           
                              http://www.sa.buaa.edu.cn/UploadFiles/Download/LC2014/11/HairDetectionAndRemovalDemo.zip.
                           
                         on the images with sparse hair and dense hair respectively. Fig. 10
                         is a group of hair detection instances for sparse hair, where the red lines are detected hairs by the three automatic methods. For the image in the second row of Fig. 10, all of the three methods give satisfactory extraction results. However, only our method achieves best effect for the images in the first (having fewer hairs) and the last (having weak hairs inside the lesion) rows. Fig. 11
                         is a group of instances for dense hair, and our algorithm can obtain much better extraction results than the other two methods. Therefore, our algorithm has better performance of hair detection than the two compared methods.

For quantitatively evaluating the hair extraction veracity, manual hairs, drawn from the original image as ground truth (GT), are compared with the hairs extracted by the three automatic algorithms. The percentage of hair-detection error (HDE) is defined as
                           
                              (10)
                              
                                 HDE
                                 =
                                 
                                    
                                       FP
                                       +
                                       FN
                                    
                                    
                                       TP
                                       +
                                       FP
                                       +
                                       FN
                                    
                                 
                                 ×
                                 100
                                 %
                              
                           
                        where TP, FP and FN stand for true positive, false positive, and false negative respectively. Definitions of true/false positive/negative are given in Table 1
                        .

A small value of HDE indicates a good hair detection result. Table 2
                         gives the average HDE of three methods on 40 images with sparse hair and 30 images with dense hair respectively. From Table 2, it can be seen that our approach has the lowest HDE of 17.6% for sparse hair and 12.0% for dense hair. For DullRazor, because there is a limitation to the length and width of hair, too thick or too weak hairs and crowded hairs are difficult to be detected successfully. And for Xie׳s method, the top 5% brightest pixels in the enhanced image are regarded to be hair pixels. When the hair area in the image is more than 5%, the method cannot work well. While the proposed method is based on the sub-region, which can adaptively extract both sparse hair and dense hair very well.

Good robustness in hair detection is necessary for the assessment of hair occlusion. In order to validate the robustness of our hair detection method, 40 images without hair from our database are tested and the detection error is calculated for three methods. The false hair error (FHE) [7] is given by
                           
                              (11)
                              
                                 FHE
                                 =
                                 
                                    
                                       Non
                                       -
                                       hair
                                       
                                       noise
                                       
                                       area
                                    
                                    
                                       Image
                                       
                                       area
                                    
                                 
                                 ×
                                 100
                                 %
                              
                           
                        where Image area is the pixel number of image, and Non-hair noise area is the pixel number of the non-hair noises extracted by the automatic method, which are mainly some hair-like textures inside the lesion. A low FHE value indicates a good robustness for the hair detection method. Table 3
                         is the statistic result for the three methods. Obviously, the proposed method has the best result.

In this paper, an objective assessment metric is proposed for dermoscopy images according to the distribution of hairs including quantity, position and dispersion. And the simulated hairs can quantitatively reflect the influence of hair distribution on the degree of hair occlusion. We extracted hairs from the simulated images using the local adaptive hair detection method described in Section 2, and calculated metric C with Eq. (9) (
                           
                              
                                 ω
                              
                              
                                 1
                              
                           
                           =
                           2
                           ,
                           
                              
                                 ω
                              
                              
                                 2
                              
                           
                           =
                           1
                        ). Fig. 12
                         is the result for 90 simulated images which are generated from 15 reference images with 6 hair masks, shown in Fig. 9. From Figs. 9 and 12, it can be seen that for the leftmost three masks in Fig. 9(b) which have the similar amount of hairs, the values of C corresponding to them are very close. The first two masks in Fig. 9(b), which are corresponding to the lowest two levels of hair occlusion in Fig. 12 (see the red and the green line), have the same amount of hairs, and the value C is increased when these hairs are moved from the healthy region to the lesion region. With an increase in the quantity of hairs in the mask, the value C also becomes large. And for a different degree of hairs, the value of C is separate enough. Therefore, the proposed metric can be used to evaluate the degree of hair occlusion correctly.

Linearity experiment tests the linear correlation between objective metric C and subjective scores. 70 real dermoscopy images with hair from our database are tested. For each image, the objective metric C is calculated by Eq. (9), and the quality ground truth is obtained through subjective experiments. The evaluation criteria includes Pearson linear correlation coefficient (LCC) and Spearman rank-order correlation coefficient (SROCC). The LCC between actual quality score and the algorithm predicted score is used to evaluate the prediction accuracy. And the SROCC estimates the agreement between the rank orders of actual quality score and algorithm predictions. The closer to 1 their values are, the better the performance of the algorithm is. The mapping relationship is shown in Fig. 13
                         and the values of LCC and SROCC are 0.9039 and 0.9749 respectively. Therefore, our method is in high conformity with subjective evaluation.

@&#CONCLUSIONS@&#

For dermoscopy images, the hair may influence the segmentation and classification so as to yield error aided diagnosis result. The degree of hair occlusion needs to be evaluated to suggest user recapture for the image with serious hair problem, which can ensure that the image getting into the computer-aided diagnosis system of skin disease is eligible. Although some hair detection and removal methods have been proposed, the hair occlusion assessment is still not addressed. In this paper, a novel no-reference assessment method for the degree of hair occlusion is proposed according to the distribution of hairs in the image. Firstly, the image is clustered into sub-regions by SLIC, and in each sub-region, INF is employed to enhance hairs. Then the automatic threshold, which is modeled as the linear combination of the gradient mean and the INF response mean in this paper and can adaptively vary with the quantity of hairs, is taken to extract hairs from the image. The proposed local adaptive hair detection method can not only deal with sparse hair but also work well on dense hair. In the assessment stage, a no-reference metric is proposed according to the hair distribution and the calculation procedure for the metric is given. A series of experiments was carried out on both simulated images and real images. The analysis results show that the proposed assessment method can effectively evaluate the degree of hair occlusion.

None declared.

@&#ACKNOWLEDGMENTS@&#

This work was supported by the National Natural Science Foundation of China (Grant nos. 61471016, 61371134 and 61271436).

@&#REFERENCES@&#

