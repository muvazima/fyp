@&#MAIN-TITLE@&#Motion boundary based sampling and 3D co-occurrence descriptors for action recognition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A motion boundary based sampling strategy is proposed for dense trajectory.


                        
                        
                           
                           A set of 3D co-occurrence descriptors is developed to describe cuboids.


                        
                        
                           
                           Two decomposition strategies are presented to further improve performance.


                        
                        
                           
                           We achieve state-of-the-art results on several human action datasets.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Dense trajectory

Action recognition

3D co-occurrence descriptors

Motion boundary

Bag of Features

@&#ABSTRACT@&#


               
               
                  Recent studies witness the success of Bag-of-Features (BoF) frameworks for video based human action recognition. The detection and description of local interest regions are two fundamental problems in BoF framework. In this paper, we propose a motion boundary based sampling strategy and spatial-temporal (3D) co-occurrence descriptors for action video representation and recognition. Our sampling strategy is partly inspired by the recent success of dense trajectory (DT) based features [Wang et al., 2013] for action recognition. Compared with DT, we densely sample spatial-temporal cuboids along a motion boundary which can greatly reduce the number of valid trajectories and preserve the discriminative power. Moreover, we develop a set of 3D co-occurrence descriptors which take account of the spatial-temporal context within local cuboids and deliver rich information for recognition. Furthermore, we decompose each 3D co-occurrence descriptor at pixel level and bin level and integrate the decomposed components with a multi-channel framework, which can improve the performance significantly. To evaluate the proposed methods, we conduct extensive experiments on three benchmarks including KTH, YouTube and HMDB51. The results show that our sampling strategy significantly reduces the computational cost of point tracking without degrading performance. Meanwhile, we achieve superior performance than the state-of-the-art methods. We report 95.6% on KTH, 87.6% on YouTube and 51.8% on HMDB51.
               
            

@&#INTRODUCTION@&#

Automatic recognition of human action in videos has been an active research area in recent years due to its wide range of potential applications, such as smart video surveillance, video indexing, and human–computer interface. Though various approaches have been proposed and significant progresses have been made, action recognition still remains a challenging task due to the high dimension and complexity of video data, the large intra-class variations, clutter, occlusion and other fundamental difficulties [2].

A fundamental problem in action recognition is how to represent an action video. The approaches for action video representation can be roughly divided into five categories: (1) dynamic model based approaches which apply statistical sequential models such as HMM and Bayesian network to describe the temporal states of actions [3,4]; (2) human pose based approaches which utilize pose structure information [5,6]; (3) global action template based approaches which construct global templates to capture appearance and motion information of the whole motion body [7–9]; (4) local feature based approaches which mainly extract spatial-temporal cuboids [10–17] or motion parts [18,19]; and (5) supervised feature learning based methods which learn the representation by hierarchical networks or other models [20–23].

Among the state-of-the-art methods, the representation of local spatial-temporal feature with Bag-of-Features (BoF) framework [24] is perhaps the most popular and successful one for action recognition. Local features are usually obtained by cuboid detectors and descriptors. Laptev [25] developed space-time interest points (STIP) detector by extending the Harris detector to 3D domain. Dollar et al. [10] detected space-time salient points by applying 2D spatial Gaussian and 1D temporal Gabor filters. Willems et al. [26] utilized Hessian matrix to extract scale-invariant spatial-temporal interest points in videos. Wang et al. [14] densely sampled cuboids at regular positions and scales. For descriptors, well-known approaches include HOG/HOF [11], Cuboids [10], HOG3D [13], 3D-SIFT [27], and so on.

Recently, Wang et al. [15] proposed dense trajectory for sampling spatial-temporal interest points and introduced a novel descriptor named motion boundary histogram (MBH) for action recognition. The motion boundary is defined by the gradient magnitude of optical flow which is initially introduced in the context of human detection [28]. Extensive experiments on nine popular human action datasets have demonstrated the excellent performance of this approach [1]. Despite its great power, the DT based representation is expensive in memory storage and computation due to the large number of densely sampled trajectories.

In this paper, we first develop a motion boundary based sampling strategy named DT-MB to reduce the computation and storage consumption of the previous DT based method. We start from densely sampled patches with grids in a frame. Meanwhile, motion boundary (Fig. 1
                     ) is derived from optical flow and a binary mask is estimated from motion boundary. Then we remove those sampled regions that have very few overlaps with foreground in the mask. Central points of the rest patches are refined by averaging the location of occupied foregrounds within the patches. Our DT-MB is partly motivated by the fact that those trajectories on motion boundary are the most meaningful ones. This is also implied by the superior performance of the MBH descriptor [1]. Using our sampling method, the number of DTs can be sharply reduced without hurting the performance.

In addition, to further enhance the discriminative power of DT based representation, we propose a set of spatial-temporal (3D) co-occurrence descriptors to describe the local appearance and motion information along trajectories. This is partly inspired by the success of co-occurrence feature in image domain [29–31]. In [30], a descriptor based on co-occurrence HOG (CoHOG) is presented for human detection. In [29], gray-level co-occurrence matrix (GLCM) is introduced to extract textural features for image classification. Our motivation is that the spatial-temporal co-occurrence features, which depict the local tiny context of motion and appearance in videos, can provide important cues for action recognition. The novel descriptors are composed of 3D-CoHOG, 3D-CoHOF and 3D-CoMBH. We find that (1) 3D-CoHOG depicts more complex structure of spatial patch and the appearance changes along with time; (2) 3D-CoHOF conveys complex motion structure and motion direction changes; and (3) 3D-CoMBH captures the complex gradient structure of optical flow and the changes of gradient orientations of flow. Furthermore, we thoroughly exploit two types of multi-channel pipelines for these descriptors, namely the pixel level pipeline and the bin level pipeline. Considering 3D-CoHOG in a given cuboid aligned by trajectory, we set several offsets at horizontal, vertical and temporal axes for each point, and the co-occurrence matrices in all the offsets are vectorized and concatenated to form 3D co-occurrence descriptors. For pixel level multi-channels of 3D-CoHOG, we vectorize the co-occurrence matrices for each offset and model them by the BoF pipeline individually, and then combine all the BoF pipelines by a multi-channel kernel SVM. For the bin level, we split the co-occurrence matrices into several channels by their co-occurrence bins for each offset. The idea of using multi-channels for 3D co-occurrence is partly inspired by the fact that MBHx and MBHy perform differently [1] and the complementarities can be better investigated in a multi-channel way as shown in [32].

To evaluate our sampling strategy and the proposed descriptors, we perform action classification with a standard BoF framework and a kernel SVM classifier [11] on three widely-used datasets, namely KTH [33], YouTube [34] and HMDB51 [35]. Our framework is illustrated in Fig. 1. We investigate our DT-MB sampling strategy over that of the original dense trajectory [1] in the view of computation and memory cost. We evaluate the improvement of our new descriptors over original HOG, HOF and MBH [1]. Furthermore, we provide a theoretical analysis on the advantages of using co-occurrence feature.

The main contributions of this paper are summarized as follows:
                        
                           1)
                           we develop a motion boundary based sampling strategy to reduce the number of dense trajectories which can save memory and computation without degrading performance;

we propose a set of 3D co-occurrence descriptors, namely 3D-CoHOG, 3D-CoHOF and 3D-CoMBH, which can depict the spatial-temporal contextual information within local cuboids;

we present two decomposition strategies for 3D co-occurrence descriptors (pixel level and bin level) and integrate the decomposed components with a multi-channel framework, which can further improve the performance;

we achieve state-of-the-art results on several widely-used human action datasets.

It's worth noting that our new descriptors are independent of the spatial-temporal cuboid detectors (e.g., DT [1], STIP [25], dense cuboids [14]). Though we mainly discuss our novel descriptors with dense trajectory, one can easily extend them with other detectors as well. The analysis and results presented here extend our preliminary work in BMVC 2013 [36]. Here, we develop more general spatial-temporal co-occurrence descriptors and further improve the performance by exploiting their multi-channel versions. We also provide an information theory analysis to validate the advantages of using co-occurrence descriptors.

The rest of this paper is organized as follows. In Section 2, we give a brief review of dense trajectory based method and present our DT-MB method in detail. In Section 3, we present our 3D co-occurrence descriptors. The two decomposition strategies for 3D co-occurrence descriptors are presented in Section 4. Section 5 shows the experimental results and gives a comprehensive comparison for each individual descriptor. We conclude our work in Section 6.

In this section, we first give a brief review of dense trajectory method [1] and explain the advantage of DT from a view of human visual fixation system. Then, we present our new sampling strategy based on motion boundary in details.

Dense sampling strategy has been widely used in extracting local image features and achieved great success in image classification. This fact inspires researchers to develop dense sampling approaches for video based action recognition, which can yield richer description of action than spare interesting points. Two successful examples are dense cuboid [14] and dense trajectory [1]. Dense trajectory based method mainly consists of the following steps.

Feature points are sampled in the current frame on a grid by a step size w at S spatial scales. To track successfully, points in homogeneous image areas are filtered out by examining the eigenvalues of their auto-correlation matrices.

Dense points are tracked by median-filtered optical flow on each spatial scale separately. Tracked points in successive frames at scale s are concatenated to form trajectories: (P
                           
                              t
                           
                           
                              s
                           , P
                           
                              t
                              +1
                           
                              s
                           ,…), where P
                           
                              t
                           
                           
                              s
                           
                           =(x
                           
                              t
                           
                           
                              s
                           ,
                           y
                           
                              t
                           
                           
                              s
                           ) represents the spatial position. To prevent the trajectories from drifting, the length is limited to L frames. Once a trajectory's length reaches L, its mean position drift and variation will be checked. Trajectories with tiny or large mean drift and variation will be pruned since they usually correspond to static or erroneous trajectories.

There are four types of descriptors for each cuboid aligned by trajectories [1]. The trajectory shape is described by a sequence (ΔP
                           
                              t
                           
                           
                              s
                           ,…, ΔP
                           
                              t
                              +
                              L
                           
                           
                              s
                           ) of displacement vectors ΔP
                           
                              t
                           
                           
                              s
                           
                           =(x
                           
                              t
                              +1
                           
                              s
                           
                           −
                           x
                           
                              t
                           
                           
                              s
                           ,
                           y
                           
                              t
                              +1
                           
                              s
                           
                           −
                           y
                           
                              t
                           ). Usually, this vector is normalized by the ℓ
                           1-norm. Therefore, we obtain a 2L-dimensional shape descriptor. To catch the motion and structure information, HOG, HOF and MBH are extracted within a space-time cuboid whose size is N
                           ×
                           N
                           ×
                           L aligned with the trajectory. HOG and HOF are among popular descriptors which yield excellent results on many datasets [11]. The MBH is derived from the gradients of optical flow which is originally introduced for human detection [28]. To embed more structure information, we usually subdivide the cuboid into a spatial-temporal grid of size n
                           
                              σ
                           
                           ×
                           n
                           
                              σ
                           
                           ×
                           n
                           
                              τ
                           . Assuming n
                           
                              bin
                            is the number of quantized bins for HOG and MBH, and n
                           
                              bin
                           
                           +1 (1 for static) for HOF, then we can obtain a n
                           
                              σ
                           
                           ×
                           n
                           
                              σ
                           
                           ×
                           n
                           
                              τ
                           
                           ×
                           n
                           
                              bin
                            feature vector for HOG, n
                           
                              σ
                           
                           ×
                           n
                           
                              σ
                           
                           ×
                           n
                           
                              τ
                           
                           ×(n
                           
                              bin
                           
                           +1) for HOF and n
                           
                              σ
                           
                           ×
                           n
                           
                              σ
                           
                           ×
                           n
                           
                              τ
                           
                           ×
                           n
                           
                              bin
                            for MBHx and MBHy, respectively.

Dense trajectory based approaches whose features are extracted along with trajectories are consistent with human visual fixation system as illustrated in the 3rd row of Fig. 2
                           . Visual fixation refers to the maintaining of the visual gaze on a single location, also known as smooth pursuit or temporal slowness [37]. A number of species, including humans, other primates, cats and rabbits can perform this mechanism by three categories of eye movements: micro-saccade, ocular drift, and ocular micro-tremor. There are two basic properties for smooth pursuit from a view of video representation. On the one hand, it makes the feature robust to velocity change. Obviously, the appearance features in a cuboid with smooth pursuit can be very similar despite the difference of motion velocity. Motion features also remain similar after normalization. On the other hand, as shown in the 4th row of Fig. 2, more meaningful appearance and motion information are captured with temporal slowness. For these reasons, DT can always outperform dense cuboids with identical parameters [1] in theory.

A limitation of DT is that too many points need to be tracked in the original dense sampling criterion [1]. However, only a few of them may lead to valid trajectories. We notice that those points on the motion boundary are the most discriminative ones. This is indeed partly implied by MBH descriptor [1] and motion boundary contour system (BCS) in neural dynamics of motion perception [38]. In this paper, we introduce motion boundary based sampling strategy which constrains the sampled points to the sharp regions of motion boundary.

The implementation of DT-MB is straightforward. Different from the original DT, it needs two successive frames to sample points. A comparative example is illustrated in Fig. 3
                        . Fig. 3(d) shows an example of motion boundary image. We calculate the gradient magnitudes for both the horizontal and the vertical components of optical flow, and set the maximum of them as motion boundary image. After a thresholding operation on the motion boundary image, a mask is generalized to refine the original DT sampled points. Particularly, we estimate the mask by Otsu's algorithm [39] empirically. Those regions outside the foreground of mask will be removed. Central points of the remaining patches will be refined by the average location of foregrounds. It's worth noting that the motion boundary image is a middle result of DT, so we do not need to add complexity. As shown in the 2nd column of Fig. 3, our approach removes a large number of points which are not on the motion foreground. The 3rd column of Fig. 3 exhibits the trajectories from historical points by DT and DT-MB. The red marks are the end points of trajectories. Note that we do not force all the points of trajectories on the motion boundaries in case of inaccurate tracking. Our DT-MB can be viewed as an effective strategy to reduce the influence of camera motion in the early stage. The detailed analysis of complexity and performance are given in Section 5.

Generally, there always exist strong correlations among spatial-temporal neighborhoods of pixels. Traditional HOG, HOF and MBH descriptors are statistical histograms counted pixel-wise which ignore the correlation of pairwise pixels. To jointly encode the spatial-temporal correlations of pixels, we present 3D co-occurrence descriptors which consist of 3D-CoHOG, 3D-CoHOF and 3D-CoMBH.

The spatial CoHOG (2D-CoHOG) is initially introduced in the context of pedestrian detection [30]. Specially, it uses pairs of gradient orientations as units and employs the co-occurrence matrix for image representation. As for 3D-CoHOG in video domain, offsets in time domain are taken into account, which is an extension from 2D. The co-occurrence matrix expresses the joint distribution of gradient orientations between anchor points and offset points over a cuboid as illustrated in Fig. 4
                        , and it can jointly depict more complex structure of spatial patch and the appearance changes along with time. A straightforward co-occurrence strategy is to obtain the statistics of all the possibilities of joint occurrences. This results in a 
                           
                              n
                              bin
                              
                                 
                                    n
                                    offset
                                 
                                 +
                                 1
                              
                           
                         co-occurrence matrix where n
                        
                           offset
                         is the number of offset points and n
                        
                           bin
                         is the quantized bins for HOG. This strategy leads an excessively redundant matrix whose high dimensionality not only increases the computation and storage cost but also makes the classification expensive. To overcome this problem, we use pairwise co-occurrence representation. Considering three offsets shown as the red points in Fig. 4, each pair of which will vote for one co-occurrence matrix. After voting, we vectorize all co-occurrence matrices and concatenate them into a vector for each cell of grid, and then concatenate these vectors cell-wise to yield the final descriptor for a cuboid. Given a cuboid with grid size n
                        
                           σ
                        
                        ×
                        n
                        
                           σ
                        
                        ×
                        n
                        
                           τ
                        , the final 3D-CoHOG descriptor is a n
                        
                           bin
                        
                        ×
                        n
                        
                           bin
                        
                        ×
                        n
                        
                           offset
                        
                        ×
                        n
                        
                           σ
                        
                        ×
                        n
                        
                           σ
                        
                        ×
                        n
                        
                           τ
                         dimensional vector.

We can also apply the above 3D co-occurrence strategy to HOF and MBH descriptors. The implementations of these are very similar with 3D-CoHOG except for the inputs. 3D-CoHOF applies spatial and temporal pairs of optical flow orientations as units, and 3D-CoMBH utilizes spatial and temporal pairs of the gradient orientations in the horizontal and vertical flow components, separately. So there will be two 3D-CoMBH components, namely 3D-CoMBHx and 3D-CoMBHy.

In our case, considering the computation and discriminative ability, we use three offsets (i.e., (2,0) and (0,2) for spatial offsets, and Δt
                        =2 for temporal offset) and process the trajectory-aligned cuboids pixel-wise with a grid of size n
                        
                           σ
                        
                        ×
                        n
                        
                           σ
                        
                        ×
                        n
                        
                           τ
                        . Specially, a co-occurrence matrix C over a M
                        ×
                        N
                        ×
                        T cell I, parameterized by an offset (x,y), is defined as:
                           
                              (1)
                              
                                 
                                    
                                       C
                                       
                                          x
                                          ,
                                          y
                                       
                                    
                                    
                                       p
                                       q
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             t
                                             =
                                             1
                                          
                                          T
                                       
                                       
                                    
                                    
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          M
                                       
                                       
                                    
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          N
                                       
                                       
                                    
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         G
                                                         t
                                                      
                                                      
                                                         i
                                                         j
                                                      
                                                      +
                                                      
                                                         G
                                                         t
                                                      
                                                      
                                                         
                                                            i
                                                            +
                                                            x
                                                            ,
                                                            j
                                                            +
                                                            y
                                                         
                                                      
                                                   
                                                   2
                                                
                                                ,
                                                
                                                if
                                                
                                                
                                                   O
                                                   t
                                                
                                                
                                                   i
                                                   j
                                                
                                                =
                                                p
                                                ,
                                                
                                                   O
                                                   t
                                                
                                                
                                                   
                                                      i
                                                      +
                                                      x
                                                      ,
                                                      j
                                                      +
                                                      y
                                                   
                                                
                                                =
                                                q
                                                ;
                                             
                                          
                                          
                                             
                                                0
                                                ,
                                                
                                                otherwise
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where p and q are the quantization bins, Gt
                        (i,j) is the gradient magnitude and Ot
                        (i,j) is the assigned bin (e.g., gradient orientation, flow orientation) at position (i,j) of the t-th frame. As shown in Eq. (1), the average gradient magnitude is used to weight co-occurrence matrices. To reduce the boundary effects, we also apply linear interpolation for voting the co-occurrence matrix. For example, given a pre-quantized gradient bin 1.4, we would vote this for both bin 1 and bin 2 with weights 0.6 and 0.4, respectively.

One can imagine that it needs Δt
                        +1 frames at least to calculate the co-occurrence matrices for 3D-CoHOG and Δt
                        +2 for 3D-HOF and 3D-MBH. The offsets in time domain we used are aligned by trajectories. It is worth noting that tracking is a necessary step in dense trajectory based approach, so our descriptors can benefit from the computational process of DT.

Here, we give an information theory analysis for co-occurrence descriptors and explain why co-occurrence can yield extra information for classification and how to select the offsets.

Suppose we have K categories denoted by set C
                        ={c
                        1,c
                        2,…, cK
                        }. The prior probabilities of C can be denoted as p(C)={p(c
                        1),
                        p(c
                        2),…, p(c
                        
                           K
                        )}. We utilize the mutual information to analyze the different discrimination between the distributions yielded by individual and co-occurrence (pairwise) units or pixels.

Suppose we have N bins for individual unit distribution H
                        ={h
                        1,…, h
                        
                           N
                        } (e.g., HOG) and the probability of the ith element hi
                         is denoted by p(hi
                        ). For histogram features, each bin is assumed to be independent. Thus, for an individual unit H, its contribution to the classification can be defined as the mutual information:
                           
                              (2)
                              
                                 
                                    I
                                    
                                       H
                                       C
                                    
                                    =
                                    I
                                    
                                       C
                                       H
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             n
                                             =
                                             1
                                          
                                          N
                                       
                                       
                                    
                                    
                                    
                                       
                                          ∑
                                          
                                             k
                                             =
                                             1
                                          
                                          K
                                       
                                       
                                    
                                    
                                    p
                                    
                                       
                                          h
                                          n
                                       
                                    
                                    p
                                    
                                       
                                          
                                             c
                                             k
                                          
                                          |
                                          
                                             h
                                             n
                                          
                                       
                                    
                                    log
                                    
                                       
                                          p
                                          
                                             
                                                
                                                   c
                                                   k
                                                
                                                |
                                                
                                                   h
                                                   n
                                                
                                             
                                          
                                       
                                       
                                          p
                                          
                                             
                                                c
                                                k
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The joint distribution between H and another individual unit distribution H′={h
                        1′,
                        h
                        2′,…, h
                        
                           M
                        ′} can be denoted as p(
                           F
                        )={p(f
                        1,1),
                        p(f
                        1,2),…, p(f
                        
                           N,M
                        )} where p(f
                        
                           n,m
                        )=
                        p(h
                        
                           n
                        ,
                        h
                        
                           m
                        ′). Then, the information gain of co-occurrence feature 
                           F
                         with respect to H is given by,
                           
                              (3)
                              
                                 
                                    
                                       
                                          I
                                          
                                             F
                                             C
                                          
                                          −
                                          I
                                          
                                             H
                                             C
                                          
                                          =
                                          
                                             
                                                ∑
                                                
                                                   n
                                                   ,
                                                   m
                                                
                                             
                                             
                                          
                                          
                                          
                                             
                                                ∑
                                                k
                                             
                                             
                                          
                                          
                                          p
                                          
                                             
                                                f
                                                
                                                   n
                                                   ,
                                                   m
                                                
                                             
                                          
                                          p
                                          
                                             
                                                
                                                   c
                                                   k
                                                
                                                |
                                                
                                                   f
                                                   
                                                      n
                                                      ,
                                                      m
                                                   
                                                
                                             
                                          
                                          log
                                          
                                             
                                                p
                                                
                                                   
                                                      
                                                         c
                                                         k
                                                      
                                                      |
                                                      
                                                         f
                                                         
                                                            n
                                                            ,
                                                            m
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                p
                                                
                                                   
                                                      c
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          −
                                          
                                             
                                                ∑
                                                n
                                             
                                             
                                          
                                          
                                          
                                             
                                                ∑
                                                k
                                             
                                             
                                          
                                          
                                          p
                                          
                                             
                                                h
                                                n
                                             
                                          
                                          p
                                          
                                             
                                                
                                                   c
                                                   k
                                                
                                                |
                                                
                                                   h
                                                   n
                                                
                                             
                                          
                                          log
                                          
                                             
                                                p
                                                
                                                   
                                                      
                                                         c
                                                         k
                                                      
                                                      |
                                                      
                                                         h
                                                         n
                                                      
                                                   
                                                
                                             
                                             
                                                p
                                                
                                                   
                                                      c
                                                      k
                                                   
                                                
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

Recall,
                           
                              (4)
                              
                                 
                                    p
                                    
                                       
                                          h
                                          n
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                          m
                                       
                                       
                                    
                                    
                                    p
                                    
                                       
                                          f
                                          
                                             n
                                             ,
                                             m
                                          
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                          m
                                       
                                       
                                    
                                    
                                    p
                                    
                                       
                                          h
                                          n
                                       
                                       
                                          
                                             h
                                             m
                                             ′
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    p
                                    
                                       
                                          
                                             c
                                             k
                                          
                                          |
                                          
                                             h
                                             n
                                          
                                       
                                    
                                    =
                                    
                                       1
                                       
                                          p
                                          
                                             
                                                h
                                                n
                                             
                                          
                                       
                                    
                                    
                                       
                                          ∑
                                          m
                                       
                                       
                                    
                                    
                                    p
                                    
                                       
                                          f
                                          
                                             n
                                             ,
                                             m
                                          
                                       
                                    
                                    p
                                    
                                       
                                          
                                             c
                                             k
                                          
                                          |
                                          
                                             f
                                             
                                                n
                                                ,
                                                m
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Then, we can rewrite Eq. (3) to,
                           
                              (6)
                              
                                 
                                    
                                       
                                          
                                          I
                                          
                                             F
                                             C
                                          
                                          −
                                          I
                                          
                                             H
                                             C
                                          
                                          
                                          =
                                          
                                             
                                                ∑
                                                k
                                             
                                             
                                          
                                          
                                          
                                             
                                                ∑
                                                
                                                   n
                                                   ,
                                                   m
                                                
                                             
                                             
                                          
                                          
                                          p
                                          
                                             
                                                f
                                                
                                                   n
                                                   ,
                                                   m
                                                
                                             
                                          
                                          p
                                          
                                             
                                                
                                                   c
                                                   k
                                                
                                                |
                                                
                                                   f
                                                   
                                                      n
                                                      ,
                                                      m
                                                   
                                                
                                             
                                          
                                          
                                             
                                                log
                                                
                                                   
                                                      p
                                                      
                                                         
                                                            
                                                               c
                                                               k
                                                            
                                                            |
                                                            
                                                               f
                                                               
                                                                  n
                                                                  ,
                                                                  m
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      p
                                                      
                                                         
                                                            
                                                               c
                                                               k
                                                            
                                                            |
                                                            
                                                               h
                                                               n
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          =
                                          
                                             
                                                ∑
                                                
                                                   n
                                                   ,
                                                   m
                                                
                                             
                                             
                                          
                                          
                                          p
                                          
                                             
                                                f
                                                
                                                   n
                                                   ,
                                                   m
                                                
                                             
                                          
                                          KL
                                          
                                             
                                                p
                                                (
                                                
                                                   c
                                                   k
                                                
                                                |
                                                
                                                   f
                                                   
                                                      n
                                                      ,
                                                      m
                                                   
                                                
                                             
                                          
                                          
                                             
                                                ,
                                                p
                                                
                                                   
                                                      
                                                         c
                                                         k
                                                      
                                                      |
                                                      
                                                         h
                                                         n
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where KL represents the Kullback–Leibler divergence [40] between two distributions. We conclude two important properties from Eq. (6):
                           
                              1)
                              Since KL(;)≥0, we have I(
                                    F
                                 ;
                                 C)−
                                 I(H;
                                 C)≥0. This suggests the co-occurrence feature can provide extra information for classification. In fact, this property can be interpreted by the so-called Data Processing Inequality (DPI) [41] as well. DPI states that post-processing cannot increase information [41]. In particular, if Z
                                 =
                                 f(Y), then I(X;
                                 Y)≥
                                 I(X;
                                 f(Y)), where X, Y, and Z are random variables and f is a (probabilistic) function. Eq. (4) shows that H can be seen as a function of 
                                    F
                                 , then we have I(
                                    F
                                 ;
                                 C)≥
                                 I(H;
                                 C) from the view of DPI.

One key parameter to design our co-occurrence descriptor is spatial and temporal offsets. Considering Eq. (6), p(c
                                 
                                    k
                                 |h
                                 
                                    n
                                 ) denotes the probability distribution by a given special bin. It is an approximate uniform distribution in our investigation, which means an individual bin has very limited discriminative power. Thus, the information gain in Eq. (6) is mainly dependent on the distribution p(c
                                 
                                    k
                                 |f
                                 
                                    n,m
                                 ). The gain is zero when p(c
                                 
                                    k
                                 |f
                                 
                                    n,m
                                 ) tends to be uniform. Intuitively, p(c
                                 
                                    k
                                 |f
                                 
                                    n,m
                                 ) will tend to be uniform in two cases: very small offset and large offset. The extreme case for small offset is zero offset, which yields uniform distribution obviously. Large offset tends to be independent with the anchor point, then the joint distribution can be rewritten as p(c
                                 
                                    k
                                 |h
                                 
                                    n
                                 )p(c
                                 
                                    k
                                 |h
                                 
                                    m
                                 ′), which is near to a uniform distribution as well. We evaluate the offset experimentally in Subsection 5.3.3.

In this section, we first present the multi-channel scheme at pixel level for 3D co-occurrence descriptors. Then, we revisit our previous spatial-temporal context descriptors. Finally, we give the details of bin level multi-channel scheme.

Inspired by the fact that MBHx and MBHy perform differently and their combination leads to better results [1], we also split all the 3D co-occurrence descriptors according to the offsets and integrate them in a multi-channel scheme with BoF model. In our case, we get four types of descriptors to split, namely 3D-CoHOG, 3D-CoHOF, 3D-CoMBHx and 3D-CoMBHy. As shown in Fig. 5
                        , we split each 3D co-occurrence descriptor along with offset
                           x
                        , offset
                           y
                         and offset
                           t
                        . After voting in the spatial-temporal grids, we apply BoF model for each vectorized co-occurrence matrix, and leverage multi-channel kernel SVM to combine channels for each 3D co-occurrence descriptor. The complementarity among each co-occurrence pair can be effectively exploited by multi-channel SVM as shown in [32].

In a previous work [36], we have developed another type of spatial and temporal context descriptors which employ spatial co-occurrence and temporal co-occurrence, separately. They are composed of spatial co-occurrence HOG (S-CoHOG), S-CoHOF, S-CoMBH, temporal co-occurrence HOG (T-CoHOG), T-CoHOF and T-CoMBH. Each S-Co descriptor is yielded by concatenating the co-occurrence matrices from offset
                           x
                         and offset
                           y
                        . As the previous case in Fig. 4, only the blue and red co-occurrence matrices will be vectorized and concatenated for S-Co descriptors. T-Co descriptors are designed to depict the appearance and motion changes from successive patches as illustrated in Fig. 6
                        . These spatial and temporal context descriptors can be seen as spatial channel and temporal channel of the 3D co-occurrence descriptors.

The diagram of bin level decomposition is illustrated in Fig. 7
                        . The co-occurrence matrices can be seen as the interchanges of gradient orientations or flow orientations between anchor points and their offsets. Inspired by the multi-channels of Motion Interchange Pattern [42], we also decompose each co-occurrence matrix according to the relative angle of pairwise bins. As shown in Fig. 7, the relative angles are 0°, 90°, 180° and 270° from channel ℓ
                        1 to channel ℓ
                        4, respectively. Although the use of angle channels in [42] is aimed at computing conveniently, we find this strategy to significantly improve the performance in our case, especially by the channels from offset
                           t
                        . We explain that different angle channels can reflect the difference of action categories. Take the offset
                           t
                         of 3D-CoHOF for example, high value can occur in the ℓ
                        3 channel for action boxing, while strong values happen in the ℓ
                        2 and ℓ
                        4 channels for action waving.

@&#EXPERIMENTS@&#

We evaluate the performance of the proposed methods on three popular human action datasets, namely KTH [33], YouTube [34] and HMDB51 [35]. In this section, we first give a brief introduction for these datasets, and then compare the performance and complexity between DT and DT-MB. Finally, we give a comprehensive comparison between our descriptors and other descriptors.

These datasets we used are collected from controlled experimental setting or web videos. Some sample frames are illustrated in Fig. 8
                        . We totally evaluate more than 10,000 video clips for our experiments.

The KTH dataset [33] is one of the most popular datasets in action recognition, which consists of 2391 video clips acted by 25 subjects. It contains 6 action classes: walking, jogging, running, boxing, hand-waving, and hand-clapping. Actions are recorded at 4 environment settings: outdoors, outdoors with camera motion, outdoors with clothing change, and indoors. We follow the experimental settings in [33] where clips are divided into the training set (16 subjects) and the testing set (9 subjects).

The YouTube dataset [34] is collected from YouTube videos. It contains 11 action categories: basketball shooting, volleyball spiking, trampoline jumping, soccer juggling, horseback riding, cycling, diving, swinging, golf-swinging, tennis-swinging, and walking (with a dog). A total of 1168 video clips are available. Following [34], we use Leave-One-Group-Out cross-validation and report the average accuracy over all classes.

The HMDB51 dataset [35] is a large action video database with 51 action categories. Totally, there are 6766 manually annotated clips which are extracted from a variety of sources ranging from digitized movies to YouTube. It contains facial actions, general body movements and human interactions. It is a very challenging benchmark due to its high intra-class variation and low video quality. We follow the experimental settings in [35] and report the mean average accuracy over all classes.

We employ the standard BoF framework [24] to represent videos. In particular, we set the parameters (w,
                        S,
                        N,
                        L,
                        n
                        
                           σ
                        ,
                        n
                        
                           τ
                        ) mentioned in the previous sections to be (5, 8, 32, 15, 2, 3) following [1]. After calculating the descriptors of videos in training set, we construct a codebook with the size of 4,000 using k-means from a subset of 100,000 randomly selected features for each channel. Then we quantize the local descriptors with the codebook and employ the statistical histograms of code words as video representations.

For classification we use the RBF–SVM with a multi-channel χ
                        2 kernel which is slightly different from [11]. The multi-channel Gaussian kernel is defined by:
                           
                              (7)
                              
                                 
                                    K
                                    
                                       
                                          H
                                          i
                                       
                                       
                                          H
                                          j
                                       
                                    
                                    =
                                    exp
                                    
                                       
                                          −
                                          α
                                          
                                             
                                                ∑
                                                
                                                   c
                                                   ∈
                                                   C
                                                
                                             
                                             
                                          
                                          
                                          
                                             1
                                             
                                                A
                                                c
                                             
                                          
                                          
                                             D
                                             c
                                          
                                          
                                             
                                                H
                                                i
                                                c
                                             
                                             
                                                H
                                                j
                                                c
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where D
                        
                           c
                        (H
                        
                           i
                        
                        
                           c
                        ,
                        H
                        
                           j
                        
                        
                           c
                        ) is the χ
                        2 distance between two video representations, Hi
                         and Hj
                         in the cth feature channel. Ac
                         is the average value of all the distances in training set for the cth channel. α is a scaling factor ranging from 0 to 1 which is obtained by cross-validation, and we set it to be 1 when dealing with single channel feature. For multi-class classification, we use the one-against-rest approach and select the class with the highest score.

Our first purpose is to investigate the effect caused by constraining dense trajectories on motion boundary. We compare the recognition accuracy, frame rate in the whole process of feature extraction including the time of I/O (fps), the average tracking time of dense points per frame (T
                           
                              track
                           ) and the average number of trajectories per video clip between DT and DT-MB. Specially, we quantize orientations into eight bins (an additional zero bin is added for HOF) with full orientation and weight the bins with magnitudes. For the accuracy comparison, we only report the combination performance of all the raw DT descriptors. We evaluate the fps and T
                           
                              track
                            within 10 videos randomly selected from each dataset and the run-time is obtained on an Acer laptop with a 2.5GHz Intel Core i5 CPU and 4GB RAM.

The comparison between DT and DT-MB is shown in Table 1
                           . The computational cost decreases significantly for tracking points by using DT-MB (see the bold in Table 1). It is about 6 times less than that of DT on the YouTube dataset. The numbers of valid trajectories also fall dramatically on all datasets. Specially, it is reduced by about 4 times on the HMDB51 dataset. The average class accuracies of DT and DT-MB on all the three datasets are very similar and it is even better than DT on the YouTube dataset. When comparing the confusion matrices of DT and DT-MB on the YouTube dataset, we find out that the accuracies degrade only for those actions which are strongly related to the backgrounds like tennis-swinging and volleyball spiking.

More examples of DT-MB are depicted in Fig. 9
                           . As it is noted the motion boundaries in Fig. 9 are less pure than the one shown in Fig. 3 and salient regions can exist in background motion boundaries due to large irregular camera motion. Nevertheless, as illustrated in Figs. 9 and 3, our DT-MB is able to reduce most of the irrelevant trajectories, which can save memory and computation cost and preserve the accuracy.

We evaluate all the proposed descriptors using our DT-MB sampling scheme since it is a good alternative to the original DT sampling strategy. We quantize orientations into four bins except an additional bin for HOF in this evaluation. One can certainly use eight bins which is not the key point in our evaluation. We fix the offsets for direction x, y , and t to 2. The comparisons with original DT descriptors are shown in Table 2
                            and Fig. 10
                           . The results for MBH are obtained by combining x and y channels. The results for ST-Co are achieved by combining S-Co and T-Co channels.

As for the HOG type of feature, all of the co-occurrence descriptors extended from the original HOG achieve better results than the original HOG on the three datasets, the improvements for “3D-CoHOG:Lv2” (bin level multi-channel pipeline of 3D-CoHOG) are 6.14%, 5.56% and 13.75%, respectively. As illustrated in Fig. 10, the improvements of 3D-CoHOG, ST-CoHOG and “3D-CoHOG:Lv1”(pixel level multi-channel pipeline) are very similar and present a rising trend. The significant difference between “3D-CoHOG:Lv2” and other pipelines indicates the complementarity of different co-occurrence bins, which can be better explored by multiple channel pipelines. One impressive result of “3D-CoHOG:Lv2” is achieved on the HMDB51 dataset where the performance is similar with the original MBH as shown in Table 2. This indicates that the 3D co-occurrence for HOG is very effective. This can be ascribed to the fact that 3D-CoHOG can depict the spatial and temporal structure changes which suggest motion information implicitly.

The multi-channel versions (i.e., level 1 and level 2) of 3D CoHOF perform consistently better than the original HOF on all the datasets. The direct 3D-CoHOF seems to slightly degrade the performance. We argue that the high dimension of 3D-CoHOF makes Euclidean distance measure unstable which counteracts the advantage of co-occurrence. That's why our reproduced result (i.e., 32.53% by 5 bins) of HOF is superior than that (i.e., 31.5% by 9 bins) in [1].

It is beneficial to incorporate 3D co-occurrence information to MBH descriptor, especially on the YouTube dataset and HMDB51 dataset where the maximum improvements are 3.43% and 6.45%, respectively. As for the KTH dataset, all the versions of MBH perform similarly. Observing recent related publications [43,44,1], we find the approximate accuracy of 95% on the KTH dataset might be the upper bound by using low-level features since there are some confused videos even difficult for human to classify.

Overall, our co-occurrence schemes can boost performance on all the datasets, and the multi-channel versions of “3D-Co” always perform better than the original “3D-Co”. Compared with “3D-Co”, the multi-channel versions of “3D-Co” own several advantages: first, splitting the long vectors into short ones and performing BoF separately have the similar effect as increasing the codebook size significantly for long vectors, which is beneficial to performance. In fact, when using the multi-channel version, the codebook of the original “3D-Co” is defined as Cartesian product, which increases the codebook size exponentially [45]; second, the mutual influence among different channels can be reduced by splitting the “3D-Co” into multi-channels. Though multi-channel versions could lead to perform BoF for more times, all the implementations can be easily conducted in parallel and the processing speed is faster since the dimension of feature is lower than the original one.

To evaluate the key parameter (i.e., offset) for co-occurrence features, we conduct a verification experiment using the channel x and channel y of 3D-CoHOG on the KTH dataset. We show the information gains (IGs) and performance with different offsets in Fig. 11
                           . For the statistics of IGs, we randomly select 10 videos
                              1
                           
                           
                              1
                              The filenames of selected videos is available at http://mmlab.siat.ac.cn/personal/pxj/. All the source code will be released as well.
                            for each action class and count the distributions mentioned in Subsection 3.3 pixel-wise within trajectories, and then compute IGs for different offsets using Eq. (6). Specially, IGs are computed for each channel at pixel level separately because we use pairwise co-occurrence scheme. For the performance, we evaluate it on the whole dataset with the same parameters as Subsection 5.3.2 except for the offsets. As shown in Fig. 11, co-occurrence descriptors can bring information gain on all the offsets we used. Increasing the offset decreases the information gains of both channels when offsets are above 2, which validates our qualitative analysis in Subsection 3.3. We achieve the highest IGs and performance at offset
                           =2. The trend of performance (right of Fig. 11) is not as clear as that of IGs because there are some uncertain factors for the process of BoF like the sampling of descriptors and the generation of codebooks by K-means. In the rest of this paper, the offsets for x, y, and t channels are fixed to 2 unless otherwise stated.

We show the results of channels x, y and t (i.e., the spatial and temporal offsets) for all the 3D co-occurrence descriptors separately in Table 3
                           . The channels x and y show similar performance for all kinds of descriptors in both levels of multi-channel pipeline. The bin level pipeline outperforms the pixel level pipeline significantly. Interestingly, the channel t for 3D-CoHOG is superior to the channels x and y in both pipelines but the results are inverse for 3D-CoHOF. A possible explanation is that temporal co-occurrence for spatial-aware descriptors contains motion information implicitly, and spatial co-occurrence for temporal-aware ones can capture more complementary information than their temporal co-occurrence.

To further investigate the effects of bin level pipelines, we show the results of different bin levels of 3D-CoHOG in Table 4
                           . The four channels {ℓ
                           1,…, ℓ
                           4} denote the gradient orientation changes from 0° to 270° between the co-occurrence pairs as shown in Fig. 7. There is no evident trend except that the ℓ
                           3 channel in temporal co-occurrence outperforms other channels. A possible explanation is that this channel reflects the tiny changes that occurred on edges of patches aligned by trajectory, since inverse orientations usually exist beside edges and the changes on edges or boundaries are discriminative.

We also conduct experiments to examine the performance of the combination of different descriptors. Table 5
                            reports the results of several combinations for previous descriptors and our proposed co-occurrence ones using Eq. (7). The baseline [1] is the combination of trajectory, HOG, HOF and MBH; we re-implement it in our evaluation. The combinations of our new descriptors consistently outperform that of original descriptors. Without the trajectory descriptor, the improvements of our bin level pipelines compared to the original one are 1.16%, 2.91% and 5.88% on KTH, YouTube and HMDB51, respectively. The best combination is “S-CoMBH+T-CoMBH” for KTH, the “trajectory+3D-Co:Lv2” for YouTube, and the “3D-Co:Lv2” for HMDB51. An additional finding is that the trajectory descriptor is not important for all the combinations on these three datasets. We explain that the trajectory descriptor is actually the optical flow information of a successive point, which is implicitly included in the HOF (HOF contains neighbor flows as well).


                        Table 6
                         presents the comparison between our best results and several recent results on all datasets. For fair comparison, we do not show the spatio-temporal pyramids (STP) post-processing results for Wang's approach which is also inferior to ours. Our method outperforms all these previous methods. In particular, the improvement over the best reported result to date
                           2
                        
                        
                           2
                           
                              http://serre-lab.clps.brown.edu/resources/HMDB/eval/.
                         is 5.2% on the HMDB51 dataset, and it is 3.5% on the YouTube dataset.

@&#CONCLUSION@&#

This paper first introduced a new dense sampling strategy (i.e., DT-MB) for dense trajectories. This scheme constrains sampled points on the motion boundary which can significantly save memory and time cost without degrading performance. Another important contribution is that we propose a set of 3D co-occurrence descriptors, namely 3D-CoHOG, 3D-CoHOF and 3D-CoMBH, which can depict the spatial-temporal contextual information within local cuboids. We also exploit these 3D-Co descriptors by using the decomposition at pixel level and bin level, respectively. The comparisons of the individual descriptors demonstrate that our new features are beneficial. Finally, our method improves the performance of the state-of-the-art action recognition methods on several challenging datasets.

@&#ACKNOWLEDGMENTS@&#

This work is partly supported by the construct program of the key discipline in Hunan province, Natural Science Foundation of China (91320101, 60972111), Shenzhen Basic Research Program (JC201005270350A, JCYJ20120903092050890, JCYJ20120617114614438), 100 Talents Program of CAS, and Guangdong Innovative Research Team Program (201001D0104648280).

@&#REFERENCES@&#

