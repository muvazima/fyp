@&#MAIN-TITLE@&#FIRME: Face and Iris Recognition for Mobile Engagement

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Face and iris authentication on mobile


                        
                        
                           
                           Fusion strategy driven by response reliability


                        
                        
                           
                           Spoofing detection for mobile face recognition


                        
                        
                           
                           Best sample selection for higher accuracy


                        
                        
                           
                           Optimization for Android


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Android

Face authentication

Iris authentication

Identity management

Mobile computing

Open source

Pervasive computing

Security

Spoofing detection

@&#ABSTRACT@&#


               
               
                  Mobile devices, namely phones and tablets, have long gone “smart”. Their growing use is both a cause and an effect of their technological advancement. Among the others, their increasing ability to store and exchange sensitive information, has caused interest in exploiting their vulnerabilities, and the opposite need to protect users and their data through secure protocols for access and identification on mobile platforms. Face and iris recognition are especially attractive, since they are sufficiently reliable, and just require the webcam normally equipping the involved devices. On the contrary, the alternative use of fingerprints requires a dedicated sensor. Moreover, some kinds of biometrics lend themselves to uses that go beyond security. Ambient intelligence services bound to the recognition of a user, as well as social applications, such as automatic photo tagging on social networks, can especially exploit face recognition. This paper describes FIRME (Face and Iris Recognition for Mobile Engagement) as a biometric application based on a multimodal recognition of face and iris, which is designed to be embedded in mobile devices. Both design and implementation of FIRME rely on a modular architecture, whose workflow includes separate and replaceable packages. The starting one handles image acquisition. From this point, different branches perform detection, segmentation, feature extraction, and matching for face and iris separately. As for face, an antispoofing step is also performed after segmentation. Finally, results from the two branches are fused. In order to address also security-critical applications, FIRME can perform continuous reidentification and best sample selection. To further address the possible limited resources of mobile devices, all algorithms are optimized to be low-demanding and computation-light.
               
            

@&#INTRODUCTION@&#

The term “mobile” referred to capture equipment for different kinds of signals, e.g. images, has been long used in many cases where field activities required special portability and flexibility. As an example we can mention mobile biometric identification devices used by the U.S. army for different kinds of security tasks. Due to the critical task involving them, such devices have to offer remarkable quality, in terms of resolution and quality of the acquired data. Notwithstanding this formerly consolidated reference for the term mobile, nowadays, it is most often referred to modern phones, tablets and similar smart devices, for which new and engaging applications are designed. For this reason, from now on, the term mobile will refer only to such devices. As a matter of fact, phones, which are the most widespread representatives of this class of equipment, are no longer used to merely make phone calls. On the contrary, the main use of “smart” mobile devices has moved from a role of simple remote communication tools to a more complex set of functions that, though based on communication, go well beyond it. From one side, their increasing and wide diffusion has spurred a keen competition among producers, with a continuous challenge to offer more and more advanced features and services. From the other side, the number and quality of new functions and gadgets have encouraged a growing consumers' interest. These new functions are especially supported by Web2.0 technologies and applications, and most of all by further Web3.0 advances, which are expected to support not only a higher degree of personalization of services and contents, but also the symmetric creation and enforcement of novel and ubiquitous patterns of social interaction. The reverse of the medal of such improved capacity of handle and share information is the equally increased vulnerability of users to fraud, due to the growing use of mobile devices to store and exchange sensitive information. Services offered in the mobile world must be therefore appropriately secured. Biometrics is able to address most issues of this composite scenario, with advantages for both security, and for comfort and ease of use. Both verification (one-to-one matching) and identification (one-to-many matching) are supported. The former one certifies a claimed identity for tasks requiring legal and secure access, e.g., handling email, access to personal medical records, or banking transactions, and also for continuous recognition after a first secured access. The latter one aims at determining the identity of an individual in a group, when it is not expressly stated, and can facilitate and support social activities, e.g., tagging of photos, profiling of users for customized applications and services, and targeted advertising. The achievable accuracy of identification obviously depends on the size of the group of subjects to handle.

The application FIRME (Face and Iris Recognition for Mobile Engagement) for mobile devices is based on face and iris biometrics due to a mix of reasons: a) the wide potential extension of the audience to which the application is addressed calls for a good acceptance; b) resolution, accuracy and speed that the hardware available on mobile devices can provide, condition implementation and deployment; and c) biometric traits such as fingerprint require dedicated sensors to be added to current equipment. Face and iris share the properties required, because they are contact-less, can be processed by light-weight algorithms in terms of space and computational time, and just require the webcam available on all mobile devices. FIRME implements the entire chain of operations needed by a mobile biometric system: face and iris localization, spoofing detection, correction of face samples (pose and illumination), feature extraction and matching. In particular, FIRME can implement both a verification and an identification scheme.

FIRME has been implemented on the Android system, one of the most popular open source operating system nowadays, with an expected exponential market growth. FIRME was extensively tested, to evaluate both its performance in terms of accuracy, and its acceptability and usability.

A report about the market for mobile communications equipment published in late 2012 [20] anticipates a final overall growth for 2012 of more than 12%. Mobile handsets and tablets play a crucial role in such technological spread. A further report [16] deals with mobile penetration in the world, with 10 countries with over 100million mobile subscriptions.

One of the most appealing outcomes for mobile penetration and growth is represented by mostly unanticipated patterns for sharing both information and services, through social networks and related frameworks. However, like in the beginnings of Internet, only a fraction of smartphones and tablets are protected by security-oriented software. As a consequence, there is an increase of viruses, worms and malicious hacker software targeted at mobile devices. To this regard, BullGuard has identified 2500 different types of mobile malware in 2010 [1]. We can expect that this number has significantly increased since then. As a matter of fact, just few months later, IBM X-Force named 2011 “The year of the security breach”, and predicted that “exploits targeting vulnerabilities that affect mobile operating systems will more than double from 2010” [9]. Despite all dark assessment and anticipations, more than 90% smartphones and tablets in 2011 were lacking a robust security protection.

Three basic tasks make up the core of any security protocol for a (mobile) system: a) to prevent unauthorized access; b) to ensure only authorized access to the stored information; c) to ensure full availability to authorized users. While mobile face recognition is spreading as a useful feature for both authentication and social activities [24], it is also questionable how much it is presently usable by the average users [23]. As a matter of fact, according to presently available systems, it seems that one has to reach a compromise between usability and safety, which seem to present contrasting needs. FIRME provides easier and faster procedures for the validation of credentials, and at the same time increases authentication accuracy through biometrics. The authentication process is further enhanced through tools such as spoofing detection, continuous recognition, and best sample selection. Thanks to these additional possibilities, the user does not have to worry about the position of the camera: FIRME can select the best biometric samples in any capture conditions, therefore increasing both authentication and usability. Moreover, the application can detect if the camera is capturing a 2D photo in place of a real user. Relaxation of strict pose requirements and anti-spoofing, are both achieved with low-demanding, computation-light algorithms. This makes FIRME an interesting candidate for real world applications, specifically in a mobile scenario.

Literature about mobile biometrics is still quite poor, despite the wide development of biometric research in many fields. It may seem obvious to inherit techniques and strategies used in general settings. However, there are a number of factors to consider that can somehow change the perspective to adopt. From the one side, mobile authentication can be assumed practically always cooperative. This eliminates the need for a number of preliminary, often computationally expensive operations, such as pose normalization for faces. Illumination is still a problem, but it can be solved in a lighter way. On the reverse of the medal, storage and computational resources associated to a mobile device cannot be considered equivalent to those available on bigger devices, so that the remaining computational steps must be revised to optimize their use. While face has been among the first biometrics considered for mobile use [10], m-commerce has been one of the first foreseen applications [15]. A hash-based fingerprint remote user authentication scheme is presented in [13], while commercially available mobile device containing low-grade accelerometers are used to implement mobile gait recognition in [7]. In the first case, dedicated hardware must be added to the mobile device. In the second case, we have to observe the behavioral nature of gait, which can be affected by emotional factors, and therefore lends itself to a soft modality recognition, which is not appropriate for secure settings. When iris recognition is addressed in mobile settings, the usual problems with motion blurring and reflection may be increased. The work in [12] uses an IR-illuminator and an IR pass filter to capture detailed iris patterns. The IR pass filter is attached in front of the camera lens, but this does not seem a practical solution. One of the most recent works on mobile biometrics presents the MOBIO system, combining face and voice biometrics [22]. We rather prefer to combine face with iris to simplify capture and processing, while at the same time we avoid heavy normalization procedures, which are performed in [22]. In general the quality of captured signals, both audio and video, depends on factors, which are internal (device dependent) and external (environment dependent). Even if the concept of quality is often quite vague, we can assume for sure that resolution is one of the internal factors, which concur to it. The image resolution depends on the size and technology of the sensor, while the resolution of captured audio signals depends on the precision of the sensor and on sampling frequency. It is well known that during interaction, low quality in images is better tolerated than low quality in sound [18], because the human visual system is able to cope with more interferences than the auditory one, and this often dictates the compression parameters, which are used during signal processing. However, when recognition tasks are involved, a low quality of images plays a critical role. We observe that, in general, the quality of signals captured by a mobile device cannot be compared to that of capture devices used in static settings. Technology advances are very quickly closing this gap, but the lower quality for mobile devices may not only depend on the precision of the sensor, but also on the specific conditions. For example, movement may increase blurring in captured images and noise in captured voice signal. As for now, our system does not use voice capture, so that we concentrate on underlining differences in video capture. Considering the equipment for face acquisition, the image resolution ranges from 640×480 (with very dated equipment) to 1600×1200 and above. Iris recognition can exploit about the same resolutions, but given the acquisition settings (light reflections, occlusions, etc.) and the size of the interesting region, the final image might not lend itself to a robust recognition process.

In the modular design of FIRME, each component performs a specific task in the typical biometric workflow (see Fig. 1
                     ). The limited average performance that can be expected from current mobile technology calls for a special attention to the rationalization and optimization of acquisition, storage and computational aspects. FIRME is made up of separate modules, with a common starting and final processing, and a central part specialized for each biometrics: a) acquisition; f1) face detection, f2) face segmentation, f3) spoofing test, f4) face feature extraction, f5) face template selection, and f6) face matching; i1) iris detection, i2) iris segmentation, i3) iris feature extraction, i4) iris template selection; i5) iris matching; r) fusion of face and iris results. The overall recognition is repeated in a cyclic process to support continuous reidentification. Notice that spoofing detection is applied for face only. The two traits are captured at the same time (the eyes are included in the face), so that for the sake of computational resources we assess the most comprehensive one.

FIRME code is essentially the result of the porting of C libraries which underlie the biometric systems running on PCs (see for example FACE in [3]). The different modules exploit the OpenCV libraries for Android. Some function parameters have been modified with respect to the original ones, and parts of the code have been redesigned and rewritten to adapt to Android and to a different resource availability. Some examples follow.
                        
                           •
                           Some functions need to initialize a number of variables. An example is the function HaarDetectObjects implementing a Haar Classifier. The parameter in this case is an xml file which specifies the kind of object to recognize. This code is used three times from the detector: one time for the face and two times for the eyes (two different xml files are loaded for right and left eye). In this regard, it is worth noting that disk reading and writing are expensive operations, which can affect the overall performance. Through the mechanism of global references provided by Java Native Interface (JNI) it is possible to initialize the required variables at application starting, in order to avoid loading during other critical operations.

A further optimization related to Haar classifiers relies on the kind of interaction with the device. It is expected that the captured image will always be a close-up photo; among the parameters of OpenCV function we find the scale factor, which can significantly influence detection times. As a matter of fact, the algorithm divides the image in small areas, attempts detection in each of them and then iteratively increases the search area. Since we expect the face to occupy almost the whole image, it is not necessary to start searching from small areas, and to this aim the scale factor parameter can be suitably incremented.

The problem of disk writing/reading is also found with image handling. In this case the main issue is to export images from the Java code section to the native one, which can coexist thanks to the porting facilities provided by Android SDK. A trivial way is to save the image on disk and then load it from the native section. However, this requires a significant waste of time. Therefore, even in this case the solution was to pass the image as a parameter through an appropriate procedure which exploits OpenCV support.

As will be better detailed in the following, with respect to FACE system [3] we avoid computationally expensive pose normalization, but rather rely on simple detection of distorted pose and on best sample selection.

Nowadays, any mobile OS provides both routines and drivers to access an on-board device camera. Using them, it is quite easy to acquire and store face images, possibly coming from a continuous clip. On the other hand, specialized routines must be implemented to handle the following steps. In particular, a robust face segmentation and possible correction is critical both because it must provide the best starting point for feature extraction, and because it can provide crucial information for spoofing detection. The optimization of all procedures is of paramount importance for the efficiency and also for the usability of the application. It is therefore necessary to optimize phase wise implementation in terms of both CPU and memory.

The face detection module presently included in FIRME also locates a number of reference points used to determine if it presents a pseudo-frontal pose (see Fig. 2
                           ). The module first implements the Viola–Jones algorithm [3] for the localization of the relevant subregion of the image containing the face. Since the user is usually looking at the device screen during such operation, we can reasonably assume a frontal pose. Therefore, we can avoid computationally heavy pose normalization techniques. Those include the ones based on Active Shape Model (see for example [3]), which provides a good compromise between accuracy and computational weight, but is still too demanding for an average mobile device. However, a fast sample selection still seems appropriate. To this aim, once the face region has been cropped, Viola–Jones algorithm is reused, with obviously different parameter configurations, to locate the eyes and mouth: their positions and distances allow to discard those samples which would require pose correction before proceeding with the recognition. An image correction routine is used instead to normalize illumination. FIRME implements a Self Quotient Image (SQI) algorithm [25]. SQI relies on the Lambertian reflectance model of an ideal diffusely reflecting surface. It states that the image of a 3D object (and therefore its 2D projection) is subject to intrinsic factors, namely surface normal and albedo, and the extrinsic factor of illumination intensity. Differently from the quotient image [19] technique, which follows the same model though without addressing shadows, SQI uses two different versions of the same image (self), therefore no alignment process is needed. Moreover it is robust to shadows and to different kinds of light source, does not need training to estimate illumination direction, and does not assume that all objects (faces) have the same surface normal. This is a great advantage to the specific features of faces and of operating settings. Last but not least, SQI is also computationally much lighter than quotient image. The illumination-corrected image is obtained by applying a smoothing filter. Then the ratio between the original image and its smoothed version is computed, providing a light-invariant representation of the face image. In practice, the value of each image pixel is divided by the mean of the values in its neighborhood, represented by a square mask of size k×k (in our case k=8). These selection/correction procedures also return quality indices, which allow to discard those biometric samples that are expected to poorly support if not hinder the next recognition steps.

Traditionally, applications requiring maximal security due to the critical nature of handled data, are also those which may attract most spoofing efforts. In the mobile scenario, this is for instance the case of mobile banking. In this context, accurate recognition may not be enough, and anti-spoofing countermeasures might be worth being adopted. To address these issues, Google has recently implemented a recognition system on Android which is extended with a liveliness detection routine, which exploits eye blink detection. It processes the entire part of the image containing the eyes, and the cost of this operation is affected by the image resolution and by the kind of analysis. To avoid a too demanding computation, FIRME exploits the technique of 3D geometric invariants, which is both simpler and more robust, to estimate the structure of the face [5]. The elective application of 3D geometric invariants is the classification of 3D objects from a single 2D view. Given a 2D image of a 3D object, the technique measures the distances between pre-defined landmarks (reference points) on the object. The distances between those points, of course, change with the capture viewpoint. However, some relationships between them, under specific conditions, remain invariant to the viewpoint, and are therefore called geometric invariants. They are intrinsic to a certain object and can also be measured on a single image of it. The invariance conditions which may hold regarding, for instance, collinearity or coplanarity of the points under consideration. The anti-spoofing technique implemented in FIRME applies this principle in the reverse direction. It exploits a set of five points on the face, for which the coplanarity constraint is normally strongly violated, namely the outer corner of the right and left eyes, the extreme left and right of the face, and the nose tip. When the face is moved, the spoofing detection routine estimates the geometric invariant relative to the identified schema of points; if the invariant holds, the points comply with the constraint of coplanarity: since this should not be the case, this means that the captured face image must be a photo (spoofing); otherwise the points are not coplanar and the three-dimensionality of the face is guaranteed and thus the captured image corresponds to a real (live) user (Fig. 3
                           ). This technique calculates a ratio of determinants of matrices (more details are available in [5]), therefore it is extremely straightforward and fast.

Among the factors that make up the effort required by the user of a biometric system, and the possibly deriving discomfort, a crucial role concerns the degree of control required during the acquisition phase; considering presently available systems, iris recognition is the one generally requiring maximum control, since the user must stay immobile looking at the camera during image capture; on the other hand, face recognition is often used also in systems, such as video surveillance ones, where the subject is most often unaware of image capturing. Choosing this pair of traits, we had to handle their different requirements. For this reason, we implemented a fast strategy for best template selection, transparent to the user and able to discard samples of low quality.

Especially for currently available mobile applications, this aspect is seldom considered or is addressed only indirectly. One approach to provide the user with a greater freedom would be to implement some robust method to correct image distortion caused by pose and/or movements (Google, and Facebook tagging system). In FIRME, the acquisition phase is implemented so that the camera actually captures a high number of frames, aiming at maximizing the accuracy of the recognition. As a matter of fact, after a suitable choice procedure, only the best one will be used for the recognition process. FIRME implements a sample selection mechanism based on entropy. Of course, we avoid any further processing before such selection, to save computational resources. We also avoid any correction/normalization procedure for both pose which might be exploited in this step, as the one described in [3], to improve the quality of samples. In practice, this step occurs immediately after the operation of face localization (Fig. 4
                           ). The selection requires to calculate the correlation between all pairs of faces in the currently acquired sequence of frames. The obtained value of the correlation index (usually in the interval [−1,1]) is normalized to the range [0,1]. Given a pair of samples, this index can be interpreted as the probability that they conform to each other, and can be used in a way similar to [6]. The difference with the algorithm for best sample selection in [6], is the localized version of the correlation index that is used as similarity measure. This provides more accurate results, but is much more computationally expensive. FIRME uses this more complex version only for face matching, which will be discussed below. For this reason, though sharing a common basic technique, Best Sample Selection module and Face Recognition module are implemented in two distinct procedures. The innovation with respect to [6], is the implementation of different similarity measures according to the current goal. Returning to the presently described best sample selection, after a further appropriate normalization, the values sum up to 1. In this way, they can be treated as a probability distribution on the whole (small) set of samples under examination. This distribution can be used to calculate the entropy of that set. Afterwards, the selection module “discards” a sample at a time from the collection and recalculates the entropy of the remaining set; in addition, it calculates the difference between the entropy before and after a sample “elimination”. This is done for all the samples in the sequence (“set”) captured. That sample which, when extracted, produces the minimum difference, is permanently removed from the set, and the process iterates. The last sample left is selected as the best sample, and is submitted to the rest of the workflow towards the recognition phase.

The earliest techniques for face classification, such as Linear Discriminant Analysis [8], are too sensitive to image distortions to be reliably used in commercial applications, which imply partially or completely uncontrolled settings. Given two images I1
                            and I2
                            and their respective mean values 
                              
                                 
                                    I
                                    ¯
                                 
                                 1
                              
                            and 
                              
                                 
                                    I
                                    ¯
                                 
                                 2
                              
                            for pixel intensity, their spatial correlation is computed as:
                              
                                 (1)
                                 
                                    
                                       s
                                       
                                          
                                             I
                                             1
                                          
                                          
                                             I
                                             2
                                          
                                       
                                       =
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   0
                                                
                                                
                                                   n
                                                   −
                                                   1
                                                
                                             
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         j
                                                         =
                                                         0
                                                      
                                                      
                                                         m
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               I
                                                               1
                                                            
                                                            
                                                               i
                                                               j
                                                            
                                                            −
                                                            
                                                               
                                                                  I
                                                                  ¯
                                                               
                                                               1
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               I
                                                               2
                                                            
                                                            
                                                               i
                                                               j
                                                            
                                                            −
                                                            
                                                               
                                                                  I
                                                                  ¯
                                                               
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         0
                                                      
                                                      
                                                         n
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            ∑
                                                            
                                                               j
                                                               =
                                                               0
                                                            
                                                            
                                                               m
                                                               −
                                                               1
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        I
                                                                        1
                                                                     
                                                                     
                                                                        i
                                                                        j
                                                                     
                                                                     −
                                                                     
                                                                        
                                                                           I
                                                                           ¯
                                                                        
                                                                        1
                                                                     
                                                                  
                                                               
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         0
                                                      
                                                      
                                                         n
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            ∑
                                                            
                                                               j
                                                               =
                                                               0
                                                            
                                                            
                                                               m
                                                               −
                                                               1
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        I
                                                                        2
                                                                     
                                                                     
                                                                        i
                                                                        j
                                                                     
                                                                     −
                                                                     
                                                                        
                                                                           I
                                                                           ¯
                                                                        
                                                                        2
                                                                     
                                                                  
                                                               
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

In FIRME recognition step, such correlation is adapted to work locally, on individual sub-regions r1
                            and r2
                            of the images I1
                            and I2
                            
                           [3,6]. For each sub-region r1
                            in I1
                           , the region r2
                            that maximizes the correlation coefficient s(r1,r2
                           ) is searched, extending this search in a narrow window around the corresponding position in I2
                           . The global correlation S (I1, I2
                           ) between the two images I1
                            and I2
                            is the sum of these local maxima. As mentioned above, this approach achieves better accuracy, but it is also more computationally expensive. This is the reason for using it in a very late phase. However, it is worth noticing that the pre-calculation of some quantities involved in the formulas, some code optimization, and reduced resolution allow to perform a considerable number of verification checks (in the order of tens) in less than a second even when using a mobile architecture. These pre-calculated factors extracted from the face are stored in what is called biokey.

Differently from a verification operation, where the user claims an identity, an identification protocol requires that each image must be matched against all those stored in a certain gallery. For each registered identity gk
                            in the gallery G (k=
                           1,…,|G|), we assume to store one or more images Ij
                           , j>0. When a new query image q is submitted, FIRME compares it with all images in the system and computes the corresponding correlation indices. After that the final list of values is reorganized in decreasing order, FIRME returns the identity gk
                            with more images in the first n positions. If the gallery contains only one image per subject, FIRME returns the first retrieved identity. To be consistent with the following processing, the value of global spatial correlation, which is of course a similarity measure, is normalized in the range [0,1] and then transformed into a distance in the same range by subtracting it from 1.

As in face recognition, eye images acquisition and storage exploit routines as well as drivers provided by the mobile SO at hand. Eye capture procedure does not present special constraints, since the interaction pattern itself implicitly guides the user to maintain a frontal pose, so to avoid off-axis problems. The user has to get close to the camera till reaching a maximum distance of 10–12cm. The used segmentation algorithm is ISIS (Iris Segmentation for Identification Systems) [2]. It was especially devised and implemented to address under-controlled acquisition conditions, therefore it is well suited to be used on mobile devices. It is robust to the presence of reflections and requires a limited computational time.

ISIS algorithm implies four main phases: preprocessing, pupil location, linearization, and limbus location.
                              
                                 -
                                 Preprocessing: eye image contains many disturbing details such as sclera vessels, skin pores, or eyelashes shape; these are complex patterns that can negatively interfere with edge detection. Moreover, the same internal characteristic patterns, which are fundamental for recognition, may hinder a correct segmentation. To avoid this problem, a posterization filter FE (Enhance) is applied: a square window W is moved over the whole image, pixel by pixel, a histogram is computed for the region contained in W, and the value with the maximum frequency is substituted for the central position.

Pupil location: in this phase, a first step implies to apply Canny filtering on the preprocessed image with ten different thresholds th=0.05, 0.10, 0.15, …, 0.55. We start from the assumption that relevant circles can be detected at different thresholds (e.g. at 0.15, 0.20, and 0.25), while circles detected only for a single value of the threshold may be artifacts. Choosing a fixed step (0.05) for the threshold value allows to explore uniformly its overall admissible range, which was experimentally found. Processing the image (Canny filtering, circle fitting) with different thresholds has a little impact on the computational burden of the overall technique, but significantly increases the accuracy of the segmented result. On the other hand, an adaptive threshold technique may concentrate only on specific parts of this range; however, it is important to explore it completely, because we don't know in advance the dominant gray level of the pupil. The connected components are identified in each resulting image, and those containing a number of pixels greater than a threshold ThC
                                     are all included in a unique list L of starting candidates. Since the pupil is not a perfect circle, many approaches search elliptical shapes possibly representing it. However the presence of noise (e.g., spurious branches by Canny filter) may cause erroneous results from ellipse fitting algorithms. Therefore, ISIS detects circular objects within the image by using a precise and fast circle detection procedure presented by Taubin in [21]. Taubin's algorithm is applied to each element in L to compute the approximating circle. All the components whose corresponding circles are not completely contained inside the image are removed from L. To extract the real pupil boundary from all candidates in the final list, each remaining circle undergoes a voting procedure, according to the sum of two different measures presented in [2]: homogeneity and separability. The former represents the homogeneity of the pixels in each circular region, and is expressed as the maximum number of occurrences of a same value, suitably normalized. The latter is based on the observation that the pupil contour represents a boundary region with a quite pronounced transition from a darker to a lighter zone, and therefore measures such transition. The circle Cmax
                                     with highest score is considered as the circular shape which better approximates pupil.

Linearization: in order to perform limbus location, the image is first transformed from Cartesian coordinates to polar coordinates. The pixel with the greatest distance ρmax from the center of the identified pupil circle is selected as the starting point for the image transformation.

Limbus location: a median filter is applied to the polar image. For each column corresponding to a position θi
                                     on the horizontal axis, ranging over ρj
                                     on the vertical axis, the following weighted difference is calculated:
                                       
                                          (2)
                                          
                                             
                                                Δ
                                                
                                                   
                                                      ρ
                                                      j
                                                   
                                                   
                                                      θ
                                                      i
                                                   
                                                
                                                =
                                                φ
                                                
                                                   
                                                      I
                                                      ˙
                                                   
                                                   
                                                      ρ
                                                      j
                                                   
                                                   
                                                      θ
                                                      i
                                                   
                                                
                                                ⋅
                                                
                                                   
                                                      
                                                         I
                                                         ˙
                                                      
                                                      
                                                         
                                                            
                                                               ρ
                                                               j
                                                            
                                                            +
                                                            δ
                                                            ,
                                                            
                                                               θ
                                                               i
                                                            
                                                         
                                                      
                                                      −
                                                      
                                                         I
                                                         ˙
                                                      
                                                      
                                                         
                                                            
                                                               ρ
                                                               j
                                                            
                                                            −
                                                            δ
                                                            ,
                                                            
                                                               θ
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    where İ is the image in polar coordinates, and
                                       
                                          (3)
                                          
                                             
                                                φ
                                                
                                                   
                                                      I
                                                      ˙
                                                   
                                                   
                                                      ρ
                                                      j
                                                   
                                                   
                                                      θ
                                                      i
                                                   
                                                
                                                =
                                                
                                                   
                                                      
                                                         
                                                            1
                                                         
                                                         
                                                            if
                                                            
                                                               I
                                                               ˙
                                                            
                                                            
                                                               
                                                                  
                                                                     ρ
                                                                     j
                                                                  
                                                                  +
                                                                  δ
                                                                  ,
                                                                  
                                                                     θ
                                                                     i
                                                                  
                                                               
                                                            
                                                            −
                                                            ,
                                                            
                                                               I
                                                               ˙
                                                            
                                                            
                                                               
                                                                  
                                                                     ρ
                                                                     j
                                                                  
                                                                  −
                                                                  δ
                                                                  ,
                                                                  
                                                                     θ
                                                                     i
                                                                  
                                                               
                                                            
                                                            >
                                                            0
                                                         
                                                      
                                                      
                                                         
                                                         
                                                            and
                                                            min
                                                            
                                                               
                                                                  
                                                                     I
                                                                     ˙
                                                                  
                                                                  
                                                                     
                                                                        
                                                                           ρ
                                                                           j
                                                                        
                                                                        −
                                                                        δ
                                                                        ,
                                                                        
                                                                           θ
                                                                           i
                                                                        
                                                                     
                                                                  
                                                                  ,
                                                                  
                                                                     I
                                                                     ˙
                                                                  
                                                                  
                                                                     
                                                                        
                                                                           ρ
                                                                           j
                                                                        
                                                                        +
                                                                        δ
                                                                        ,
                                                                        
                                                                           θ
                                                                           i
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                            >
                                                            
                                                               ε
                                                               G
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            0
                                                         
                                                         
                                                            otherwise
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

This procedure allows to identify the points with the higher positive variation, which indicates the transition from a darker zone (the iris) to a lighter one (the sclera). The first inequality just selects points with a positive gradient, while the second one rules out those points between pupil and iris by prescribing that the darker pixel in the pair at hand must exceed a threshold εG ∈ [0,255] (here εG
                           =50). Points which maximize (2) for each column θi
                            in İ make up the limbus that we will call F. After this, it is possible to discard outliers by considering that, in the polar space, point of the limbus must lie approximately on a line, and therefore have an almost constant ρ component. To this aim, points with a ρi producing a relative error above a threshold ε are canceled (here ε=0.4). The relative error err is calculated as follows:
                              
                                 (4)
                                 
                                    
                                       err
                                       =
                                       
                                          
                                             
                                                
                                                   ρ
                                                   i
                                                
                                                −
                                                
                                                   ρ
                                                   med
                                                
                                             
                                          
                                          
                                             
                                                max
                                                i
                                             
                                             
                                                
                                                   
                                                      ρ
                                                      i
                                                   
                                                   −
                                                   
                                                      ρ
                                                      med
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where ρmed
                            is the median value over F.

At the end of acquisition and segmentation procedures, we obtain an image in polar coordinates where most useless information has been discarded.

The feature extraction phase has been implemented by the CSUM (Cumulative SUMs) algorithm proposed in [14]. The method is based on the analysis of local variations of gray levels in the image. It is simple to implement, and does not require high computational costs. Last but absolutely not least, it is robust to under-controlled iris acquisition conditions. These features make it appropriate for mobile processing, therefore it was chosen for FIRME. The algorithm is applied to the polar image obtained in the previous phase, and entails the following steps:
                              
                                 Step 1. Division of normalized iris image into basic cell regions for calculating cumulative sums. A cell region is 3 (row)×10 (col) pixels; the mean gray value X is used as a representative value of a basic cell region for the following calculation.

Step 2. Horizontal and vertical grouping of basic cell regions (in our case the group size is of 4 cells); the mean value 
                                       
                                          X
                                          ¯
                                       
                                     over cell representative values is calculated for each group.

Step 3. Computation of cumulative sums over each group according to the following :
                                       
                                          (5)
                                          
                                             
                                                
                                                   
                                                      
                                                         S
                                                         0
                                                      
                                                      =
                                                      0
                                                   
                                                
                                                
                                                   
                                                      
                                                         S
                                                         i
                                                      
                                                      =
                                                      
                                                         S
                                                         
                                                            i
                                                            −
                                                            1
                                                         
                                                      
                                                      +
                                                      
                                                         
                                                            
                                                               X
                                                               i
                                                            
                                                            −
                                                            
                                                               X
                                                               ¯
                                                            
                                                         
                                                      
                                                      
                                                      for
                                                      
                                                      i
                                                      =
                                                      1
                                                      ,
                                                      …
                                                      ,
                                                      4
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

Step 4. Generation of iris feature codes.

The iris code is obtained by comparing, for each group, two consecutive cumulative sums. Values 1 or 2 are assigned to a cell if the value Si contributes respectively to an upward slope or to a downward slope. Otherwise, value 0 is assigned to Si.

Matching of iris codes is computed by Hamming distance.

Depending on the required level of security, different strategies may be adopted for mobile biometrics. As a matter of fact, the more the process is distributed, i.e. implies the communication between the personal device and a remote server, the higher the risk of attacks along the path, e.g. sniffing. However, the reverse of the medal is a compromise between local and remote processing, to decrease the computational demand on the mobile device. Following these considerations, we implemented two different operation modes for FIRME. The overall process of acquisition/processing/authentication can be implemented on the mobile device, or the template can be sent over the network, and then processed by a server, which responds with a message of grant/reject. In the second case, security can be enforced by the use of cancelable templates, and by the encryption of the biometric key (DES, RSA, etc.). Anonymization for privacy purposes can be managed through an independent process, by associating the real identity to a unique code on the server (e.g., through an appropriate generation algorithm, or through a handshake protocol). Only the code is communicated over the network, and its knowledge cannot allow to recover the real identity of the user. This second mode is under development.

Mobile systems are a true challenge in terms of accuracy of the result from a matching module. This originates from two important factors which characterize mobile settings: a) the reduced resolution of images captured by on-board sensors equipping the mobile device; b) the high degree of freedom of the user in handling the device. These two factors interact so that a recognition system dealing with a single biometric trait, though robust, might not be sufficient to address the security requirements of a number of mobile applications. On the other hand, the fusion of multiple biometric traits is very often considered as one of the best, if not the only solution in those settings, where under-controlled acquisition conditions may negatively affect the performance of a unimodal biometric system at a significant extent [11]. FIRME adopts a multi-biometric fusion scheme, where two different biometric traits are integrated to improve the recognition performance of the system. When dealing with such kind of fusion, different aspects must be taken into account, among which fusion policy and fusion scheme play a crucial role. Fusion may happen in different steps of the recognition process. Some systems fuse the biometric features, so cutting the cost of the next steps and maximizing the amount of information available in the biometric signature. However, this is possible only when the features extracted from the different traits are compatible, i.e. can be expressed in a consistent form allowing to fuse them in a single vector. Alternatively, information from the different biometric systems may be fused immediately after matching, i.e., at score level. At present, this is considered as the best compromise between the complexity of the fusion process and the amount of information still available to guarantee a good result [17]. As a matter of fact, pushing fusion forward towards the decision step would make the fusion scheme even simpler (responses are of binary form and fusion rules of Boolean type), however at the expense of losing an excessive amount of information, which might still be effectively used for a more accurate result.

FIRME performs fusion of face and iris information at matching level. This allows to handle scalar values, the scores, which are easy to analyze but at the same time still retain rich information. A special attention has been devoted to the fusion scheme, that in such an under-controlled system becomes a key factor to assure effectiveness and accuracy of the fusion itself.

In FIRME, each subsystem, together with an identity, returns a pair of values. The first one is the distance, i.e. a value normalized in the interval [0,1] which indicates how much two biometric templates (from face or iris) are far from each other (0=identical templates, 1=completely different templates). The second value represents the confidence to associate to the response from the single system. In other terms, a high distance with a low confidence, indicates that the templates are very different, but the system considers the probability of a wrong answer high as well. In such a situation, the response would probably become a false reject. On the contrary, a low distance, with a low confidence, may become a false accept. The fusion scheme implemented by FIRME exploits distance and confidence from both biometrics to produce a single value, which is compared with an acceptance threshold: if it is lower, the subject is accepted, otherwise is rejected. Distance measures used for face and iris have been introduced in detail in the respective sections, we focus here on the definition of the confidence function and on how the obtained result for the current probe is integrated into the fusion scheme.

There is a substantial difference between a quality measure of an input sample and a confidence value measured as a function of the response of the biometric system. If defined independently from the distance metric used for matching, the second one can be used for any recognition system and takes into account the context in which the metric is computed, namely the whole gallery of the recognition system.

Within the proposed framework of fusion in a multi-biometric identification setting, it is appropriate to have a biometrics-independent confidence function. We considered two possible formulations of the confidence function φ defined in [4]. We used the relative distance and the density ratio, as well as a combination, to obtain the final function (for details see [4]). Given a probe p and an identification system A with a gallery G, and the set of gallery subjects returned by the system ordered by their distance from the probe, the relative distance is defined as:
                           
                              (6)
                              
                                 
                                    
                                       φ
                                       1
                                    
                                    
                                       p
                                    
                                    =
                                    
                                       
                                          d
                                          
                                             p
                                             
                                                g
                                                
                                                   i
                                                   2
                                                
                                             
                                          
                                          −
                                          d
                                          
                                             p
                                             
                                                g
                                                
                                                   i
                                                   1
                                                
                                             
                                          
                                       
                                       
                                          d
                                          
                                             p
                                             
                                                
                                                   g
                                                   i
                                                
                                                
                                                   
                                                   
                                                      G
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where d is a distance function with codomain [0,1]; if the values were not included in that range, it is possible to use a normalization function like the Quasi Linear Sigmoidal (QLS), defined in [4] (used in this work). Relative distance is based on the principle that if a person is genuine, there is a great distance between the scores assigned to the first retrieved identity and the score assigned to the immediately closets one. The density ratio, is instead defined as:
                           
                              (7)
                              
                                 
                                    
                                       φ
                                       2
                                    
                                    
                                       p
                                    
                                    =
                                    1
                                    −
                                    
                                       
                                          N
                                          b
                                       
                                    
                                    /
                                    
                                       G
                                    
                                    ,
                                 
                              
                           
                        where
                           
                              
                                 
                                    
                                       N
                                       b
                                    
                                    =
                                    
                                       
                                          
                                             g
                                             
                                                i
                                                k
                                             
                                          
                                          ∈
                                          G
                                          |
                                          d
                                          
                                             p
                                             
                                                g
                                                
                                                   i
                                                   k
                                                
                                             
                                          
                                          <
                                          2
                                          ⋅
                                          d
                                          
                                             p
                                             
                                                g
                                                
                                                   i
                                                   1
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

In practice, we consider the identities returned in a process of identification as a cloud; the more this cloud thickens near the first returned identity, the more unreliable is the answer, as there are many individuals that might be potential candidates as well; on the contrary, the response is reliable, if the cloud is scattered. In this paper we also adopt a variation of the density ratio. As one can observe in the definition of Nb in Eq. (7), the factor 2 (the cloud radius) is constant. We define here a new version Nc of the term used to compute φ where this factor is proportional to the distance of the first returned identity, so that the cloud radius depends on the relative distance according to the following definition:
                           
                              (8)
                              
                                 
                                    
                                       φ
                                       3
                                    
                                    
                                       p
                                    
                                    =
                                    1
                                    −
                                    
                                       
                                          N
                                          c
                                       
                                    
                                    /
                                    
                                       G
                                    
                                    ,
                                 
                              
                           
                        where
                           
                              (8)
                              
                                 
                                    
                                       N
                                       c
                                    
                                    =
                                    
                                       
                                          
                                             g
                                             
                                                i
                                                k
                                             
                                          
                                          ∈
                                          G
                                          |
                                          d
                                          
                                             p
                                             
                                                g
                                                
                                                   i
                                                   k
                                                
                                             
                                          
                                          <
                                          
                                             
                                                
                                                   
                                                      1
                                                      +
                                                      d
                                                      
                                                         p
                                                         
                                                            g
                                                            
                                                               i
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      1
                                                      +
                                                      d
                                                      
                                                         p
                                                         
                                                            g
                                                            
                                                               i
                                                               2
                                                            
                                                         
                                                      
                                                      −
                                                      d
                                                      
                                                         p
                                                         
                                                            g
                                                            
                                                               i
                                                               1
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             4
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Since the value of d ranges in [0,1], the maximum (absolute) value for the numerator in the second term in Eq. (8) is 4, and this explains the normalization factor.

The fusion schema adopted by FIRME exploits the confidence values produced by face and iris systems to fuse their returned distances. The two values are φF
                        (p) for the face and φI
                        (p) for the iris, where φF
                         and φI
                         represent the same function applied to the respective galleries. They are used as weights to assign to the respective distances, in order to produce a weighted sum, which corresponds to the global distance produced by the system using Eq. (9). Of course, in order to make consistent such weighted sum, the sum of the weights must be 1, so that the single values φF
                         and φI
                         are normalized with respect to their sum (φF
                        
                        +
                        φI
                        ). The obtained global distance is then compared with an acceptance threshold th, so to divide recognized subjects (with a distance lower or equal to the threshold) from the unrecognized ones (distance higher than the threshold).
                           
                              (9)
                              
                                 
                                    d
                                    
                                       p
                                       
                                          g
                                          j
                                       
                                    
                                    =
                                    
                                       
                                          φ
                                          F
                                       
                                       
                                          
                                             
                                                φ
                                                F
                                             
                                             +
                                             
                                                φ
                                                I
                                             
                                          
                                       
                                    
                                    ⋅
                                    
                                       d
                                       F
                                    
                                    
                                       p
                                       
                                          g
                                          j
                                       
                                    
                                    +
                                    
                                       
                                          φ
                                          I
                                       
                                       
                                          
                                             
                                                φ
                                                F
                                             
                                             +
                                             
                                                φ
                                                I
                                             
                                          
                                       
                                    
                                    ⋅
                                    
                                       d
                                       I
                                    
                                    
                                       p
                                       
                                          g
                                          j
                                       
                                    
                                 
                              
                           
                        
                     

where p represents the probe at hand and gj
                         is the j-th element in the system gallery. After this, the subjects are ordered according to the increasing distance from the probe. The first one returned is one sample per subject id present in the gallery, otherwise the one with more samples in the first n positions.

FIRME can be applied in many different scenarios, depending on the provided level of security. At present, the banking sector seems to be an especially interesting application for mobile technologies, even because banks show continuous change from both the technological and organizational points of view. Mobile banking services are presently provided through four “technological platforms”: SMS, Sim Toolkit, Mobile Site, and App. Almost all banks offering mobile banking services exploit SMS, e.g. for charge notification. Apps are rapidly spreading alongside the Mobile Site optimized for mobile devices. FIRME has been used for mobile banking (Fig. 5
                     ), according to two different protocols:
                        
                           -
                           Both the acquisition of the image and the generation of the corresponding biokey are executed on the mobile device; in this case, after spoofing detection and best template selection, a verification operation is rather required, which only entails a 1:1 matching;

The acquisition of the image, spoofing detection, best template selection and feature extraction are all carried out on the mobile device, afterwards the biokey is sent to the server that performs an identification operation (1:N matching) to recognize the user.

In the first approach, there is a higher computational demand on the mobile device, which must be appropriately equipped. In the second approach, security plays a more critical role, since the user has to communicate sensitive data with the server.

It is worth noticing that the two biometrics might also be used, without fusion, in different moments of a transaction. As an example, face might be used in a first phase for authentication, while iris might be used as a the equivalent of a further transaction password.

FIRME system was evaluated from both a quantitative (performance) and qualitative (usability) point of view. The experiments included testing operations performed by users, followed by the completion of a questionnaire. Performance was measured in terms of False Acceptance Rate (FAR) and the False Rejection Rate (FRR), commonly used to estimate the accuracy of biometric systems, and widely accepted by the biometrics community.

Tests were run on a Samsung Galaxy Tab 2.0 with 10.1-in. screen and 1280×800 pixel resolution, equipped with a dual core 1GHz processor, 1GB RAM, a back camera with 3.2Megapixel resolution, a front camera with 640×480 resolution, Operating system Android 4.0 Ice Cream Sandwich.

Forty-nine subjects participated in the tests. The dataset used to test the performance of FIRME is composed of images of their faces and irises. For each biometric trait and for each subject, 2 samples were acquired indoor and 2 outdoor, for a total of eight samples per subject. To simulate system operation, images of the dataset have been divided into two sets: Probe and Gallery. The Gallery contains the enrollment database, while the Probe contains the samples submitted for recognition. We executed different experiments by modifying from time to time the composition of Probe and Gallery. We underline that the iris images are the starting set of a much larger dataset which will be available on demand for the upcoming competition MICHE (Mobile Iris CHallenge Evaluation – for details see http://www.journals.elsevier.com/pattern-recognition-letters/call-for-papers/special-issues-on-mobile-iris-challenge-evaluation/).

We can identify two classes of experiments: indoor and outdoor. In indoor tests, both Probe and Gallery contain templates acquired indoor. In outdoor tests, the Gallery contains templates acquired indoor, while templates in Probe are acquired outdoor. In both cases the experiments follow open set modality, where Probe contains templates of subjects which are not included in Gallery. This simulates the case of not registered subjects who attempt to access the system.

The tests where first executed separately for iris and face, and then a fusion was performed. We summarize the two protocols defined above:
                        
                           -
                           All processing is performed on the mobile device; in this case, a verification operation is rather required, which only entails a 1:1 matching;

After all preliminary processing, the biokey is sent to the server that performs an identification operation (1:N matching) to recognize the user.

Tests presented in this section were performed according to the second one. The sequence of experiments is described below. We will call idXX the first sample for a biometric trait for subject XX, and with idXX_2 the second sample. In order to obtain more reliable results from open set experiments, we repeated them twice with a different selection of images for the Probe, once using samples acquired indoor and then using samples acquired outdoor. This procedure is summarized in Table 1
                     .

Before starting the tests, the users were trained in the use of the device and of the application. At the end of the tests, the users completed a usability questionnaire, focused on both enrollment (“registration”) of a user in the system, and authentication of a previously registered user.

The presence of false negatives, while might be disturbing for the user, does not undermine the security of the system. On the other hand, after a negative response, a second attempt is automatically started on the following frame, in a way transparent to the user, therefore reducing possible discomfort. We chose a quite high similarity threshold for the matching of two faces, in order to make the system safe. Ensuring an accurate recognition obviously causes an increase of false rejections. We remark again that a procedure for the evaluation of face image quality may discard “worse” frames, while normalization of face pose may support more accurate recognition, but both would require more computation.

We estimated the time required by the application to complete the operations of registration (4.82s.) and authentication (5.04s). Note that the average execution time to perform the operations is 5s.

Users also filled a questionnaire of twelve questions on the friendliness of FIRME. Actually, only 47 over 49 participated this last step. Most questions allowed four possible answers: a) strongly disagree, b) moderately disagree, c) moderately agree, and d) strongly agree. Some questions may seem to suggest an answer, but this is quite usual for this kind of questionnaires, since the user has to express a level of agreement with a precise statement. Table 2
                      shows the questions, their response options, and for each of them the percentage of users who chose the single entry.

The results of the questionnaire reveal some occasional problem due to false rejections. Despite this, users are satisfied in terms of both ease and speed. The suggested modifications mainly pertain to interface layout, and are quite easy to implement in the next versions of the system.

The first experiment aims at evaluating the performance obtained by the single biometrics, both in terms of Cumulative Match Curve (CMC), and of di Receiving Operating Characteristic (ROC) curve. The curves in Fig. 6
                         represent the performance of the system when using samples acquired indoor as Probe.

We can observe from Fig. 6 that face offers much better results than iris. This seems to be in contrast with general reported trends, which indicates iris as a much more reliable biometrics that face. However, this result can be explained. First of all, regions containing the two biometric traits are captured at a resolution that can be sufficient for face recognition, but less appropriate for iris, whose patterns require a better resolution. We have to consider, that, in general, iris is captured by using dedicated sensors, in infrared spectrum and at a distance that assures am optimal resolution in the relevant portion of the image. In this specific case, the on-board camera does not provide an acquisition with the resolution usually exploited by iris-based systems, so that the localization and normalization processes are especially affected, and as a consequence the following matching process. A second consideration is that, while face is usually processed in the visible spectrum, iris is most often processed in infra-red, so that processing in visible light and under-controlled conditions is a less usual setting for this biometrics. Last but not least, face recognition achieves good results even because the natural interaction pattern with the device takes the user to spontaneously adopt a frontal (and therefore less disturbed) pose, with normalization and best sample selection further intervening to address possible pose problems.


                        Fig. 7
                         shows the same type of experiment, with samples acquired outdoor used as Probe.

Curves in Fig. 7 confirm the trend observed in the previous experiment, further highlighting that the available resolution better supports the face with respect to iris, which occupies a much smaller region in the image. A further factor to consider is the difficulty in focusing by the sensor, which hinders a correct acquisition of all features in the iris. However, as a final remark, we may consider that the increasing availability of high-performance sensors even on medium level mobile devices will soon turn this situation in favor of the most reliable iris biometrics.

In this experiment, modules for face and iris have been fused at score level, based on confidence values. The fusion policy is the one described in Section 5.2. The same experiment has been performed both for indoor and outdoor Probe. Results with indoor Probe are reported in Fig. 8
                        , while those with outdoor Probe are reported in Fig. 9
                        .

Each graph in Figs. 8 and 9 reports three curves. In some cases, two of them completely overlap, so that one is not visible. The experiments with outdoor probes show how the three different confidence functions φ provide very different performance. The worst one is φ3
                        . Since it is based on a cloud of subjects around the first returned one whose radius depends (is proportional) on distance between the first and second responses from the same biometric system, we can deduce that this distance is not sufficiently significant to capture enough information about the behavior of the rest of the gallery to return a useful reliability measure. The density ratio, i.e. function φ2
                        , considers a cloud of responses around the top ranked one provided by the system with a fixed radius, which seems to give a better account of how the template of the returned identity is related with the overall Gallery. As a matter of fact, if we observe the corresponding CMC and ROC curves, we can notice that FIRME performance improves significantly. Among the three, φ1
                         is the one giving more rambling results.

For the indoor tests, FIRME achieves optimal performance. On the contrary, in outdoor tests, though satisfying, performance with φ2 are slightly worse than those with face alone. However, the element to consider is the robustness of the adopted fusion technique, besides its versatility. As a matter of fact, it is able to efficaciously contain the scarce performance provided in this case by the iris. This demonstrates that the combination of more biometric techniques, when appropriately designed, is able to neutralize, on a given device, a negative behavior that might be observed from time to time, possibly due to bad acquisition, sensor problems, incorrect device orientation, etc. In this way the overall response from the system, in this case FIRME, is still reliable. This also supports the belief that, with the predictable improvement of acquisition devices and better iris images, the recognition accuracy can significantly improve, and therefore it is worth following the multi-biometric approach.

@&#CONCLUSIONS@&#

This paper describes FIRME (Face and Iris recognition for Mobile Engagement) which was implemented as an embedded application for mobile devices using Android. The potential scope and reach for FIRME is widely opened.

Due to its potential use in very different settings, FIRME provides verification and identity management for different levels of security. FIRME modular architecture includes acquisition, anti-spoofing, detection, segmentation, feature extraction, and matching for face and iris. FIRME can support for continuous recognition (reidentification) and best biometric sample selection to deal with varying pose and illumination. Low-demanding and computation-light algorithms are accurately designed. FIRME includes a novel and practical solution for both face spoofing detection, based on 3D collinearity invariants, and for best sample selection, using iterative entropy evaluation. Questionnaires administered to users show user acceptance, ease of use, reliability; and adequate privacy and security. Experimental results demonstrate that an accurate design of multi-biometric recognition can address different hardware performance of devices and sensors, especially when it cannot be anticipated in advance which biometric trait will provide the most reliable responses.

@&#REFERENCES@&#

