@&#MAIN-TITLE@&#MODS: Fast and robust method for two-view matching

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel algorithm for wide-baseline matching called MODS is presented.


                        
                        
                           
                           View synthesis for affine detectors boosts performance.


                        
                        
                           
                           Iterative scheme “do only as much as needed” principle reduces runtime.


                        
                        
                           
                           New challenging extreme zoom and viewpoint datasets are presented.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Wide baseline stereo

Image matching

Local feature detectors

Local feature descriptors

@&#ABSTRACT@&#


               
               
                  A novel algorithm for wide-baseline matching called MODS—matching on demand with view synthesis—is presented. The MODS algorithm is experimentally shown to solve a broader range of wide-baseline problems than the state of the art while being nearly as fast as standard matchers on simple problems. The apparent robustness vs. speed trade-off is finessed by the use of progressively more time-consuming feature detectors and by on-demand generation of synthesized images that is performed until a reliable estimate of geometry is obtained.
                  We introduce an improved method for tentative correspondence selection, applicable both with and without view synthesis. A modification of the standard first to second nearest distance rule increases the number of correct matches by 5–20% at no additional computational cost.
                  Performance of the MODS algorithm is evaluated on several standard publicly available datasets, and on a new set of geometrically challenging wide baseline problems that is made public together with the ground truth. Experiments show that the MODS outperforms the state-of-the-art in robustness and speed. Moreover, MODS performs well on other classes of difficult two-view problems like matching of images from different modalities, with wide temporal baseline or with significant lighting changes.
               
            

@&#INTRODUCTION@&#

The wide baseline stereo [1] problem—the automatic estimation of a geometric transformation and the selection of consistent correspondences between view pairs separated by a wide baseline—has received significant attention in the last 15 years [2,3]. State-of-art local feature detectors [4–7] and descriptors [6–8] allow to match images of a scene with a viewing angle difference up to 60° for planar objects [9] and 30° for non-planar 3D objects [10]. Fast detectors [11,12] and binary descriptors [13–15] make matching significantly faster at the cost of decreasing tolerance to scale, rotation and affine changes. At the other end of the spectrum of wide baseline problems, the ASIFT matching scheme [16,17], increased the range of handled viewing angle differences up to the 80° at the cost of a significant slow-down.

We propose a novel two-view matching algorithm called MODS—matching on demand with view synthesis—that handles viewing angle difference even larger than the state-of-the-art ASIFT algorithm, without a significant increase of computational costs over “standard” wide and narrow baseline approaches. The performance gain is achieved by introducing a number of improvements to the wide-baseline matching process.

First, MODS employs a combination of different detectors. It is known that different detectors are suitable for different types of images [9] and that some detectors are complementary in the type of structures in the image they respond to Aanaes and Dahl [18]. Moreover, we show that the combination of the different detectors allows increasing the average speed of the matching and to match pairs of images which cannot be solved by any of the detectors alone. The results indicate that searching for the “best” detector leads to data- and problem-specific outcomes and that exploiting multiple detectors is superior to any single one.

Second, we introduce an iterative scheme which follows the “do only as much as needed” principle. Progressively more powerful yet slower detectors and descriptors are applied, together with more images synthesized on-demand, until sufficient support for a two-view geometry estimate is obtained. Such on demand approach finesses an apparent robustness vs. speed trade-off, avoiding the slowdown for easy wide baseline problems brought by the time-consuming operations needed for solving the most challenging pairs.

Third, a novel tentative correspondences generation strategy is presented which generalizes the standard first to second closest distance ratio [6]. The selection strategy which shows performance superior to the standard method is applicable to any vector descriptor like SIFT [6], LIOP [19] and MROGH [20].

The parameters of the MODS algorithm were optimized and its performance thoroughly evaluated. The optimization included the selection of the particular sequence of feature detectors, the choice of the number and parameters of images synthesized to facilitate matching and the parameter setting of the individual detectors.

The performance of the MODS algorithm was validated on several publicly available datasets and it was compared to the state-of-the-art in both speed and robustness, i.e. the ability to recover the two-view geometry reliably. We show that MODS significantly outperforms prior approaches in both robustness and speed. We have collected a set of image pairs for evaluating MODS on wide baseline problems with very large angular difference between views. These form the Extreme View Dataset. The dataset with the ground truth and the source code of the MODS algorithm is available on the authors web-page.
                        1
                     
                     
                        1
                        Available at http://cmp.felk.cvut.cz/wbs/index.html.
                     
                  

@&#RELATED WORK@&#

The standard wide baseline matching pipeline (see Algorithm 1) begins with the detection of local features, computation of descriptors, generation of tentative correspondences and ends with geometric verification using the homography or epipolar constraint. Matching images of a scene with viewpoint difference up to 60° for planar objects [2] and 30° for non-planar 3D objects [10] was reported. The execution time varies from a fraction of a second to seconds for 800 × 600 images [9].
                  

The idea of generating synthetic views to improve a local feature based wide baseline matching pipeline was first explored by Lepetit and Fua [21]. They synthesized views to find distinctive keypoints repeatedly detectable under affine deformations. Synthetic views provided a training set for learning a random forest classifier that labeled individual feature points. Feature points in different images with the same label were assumed to be in correspondence. The simple keypoint detector of Lepetit and Fua is very fast, but invariant only to translation and rotation and thus the number of views necessary to achieve acceptable repeatability was high. The method was tested on pairs undergoing significant affine transformations, but the final representation did not scale and cannot be easily used for indexing.

Recently, Morel et.al. [16] proposed a new matching pipeline—see Algorithm 2
                     . The authors showed that view synthesis extends the handled range of viewpoint differences. The ASIFT algorithm starts by generating synthetic views (described in Section 3.1) for both images. Next, feature detection and description are performed using standard SIFT [6] in each synthesized view. Tentative correspondences are formed for all pairs of views synthesized from the first and second image. The matching stage thus entails n
                     2 independent matching problems, where n is the number of synthesized views per image. The set of correspondences between the images is the union of results for all synthesized pairs. The duplicate filtering stage of ASIFT prunes correspondences with small spatial distance (2 pixels) of local features in both images—all such correspondences except one (random) are eliminated from the final correspondence set. “One-to-many” correspondences—correspondences of features which are close to each other (are situated in radius of 
                        
                           2
                        
                      pixels) in one image while spread in other synthetic views are also eliminated, despite the fact that some of the pairs can be correct. Finally, geometric verification is performed by ORSA [22]. ORSA is a RANSAC-based method, which exploits an a-contrario approach to detect incorrect epipolar geometries. Instead of having a constant error threshold, ORSA looks for matches that have the highest “diameter”, i.e. matches which cover a large image area. ASIFT was shown to match images of a scene with viewpoint difference up to 80° for planar objects [16]. Computational costs are in the order of tens of seconds to a few minutes.

The latest extensions of wide-baseline matching pipeline are limited to modifications of the ASIFT algorithm. Liu et al. [23] synthesized perspective warps rather than affine. Pang et.al [24] replaced SIFT by SURF [7] in the ASIFT algorithm to reduce the computation time.

The main idea of the proposed iterative MODS algorithm (see Algorithm 3
                     ) is to repeat a sequence of two-view matching procedures, until a required number of geometrically verified correspondences is found. In each iteration, a different and potentially complementary detector is used and a different set of views synthesized. The algorithm starts with fast detectors with limited invariance proceeding progressively with more complex, robust, but computationally costly ones. MODS is thus capable of solving simple matching problem fast without loosing the ability to deal with very difficult cases where a combination of detectors is employed to extend the state-of-the-art.

The adopted sequence of detectors and view synthesis parameters is an outcome of extensive experimental search. The objective was to solve the most challenging problems in the development set, i.e. to correctly recover their two-view geometry, while keeping the speed comparable to standard single-detector wide-baseline matchers for simple problems. Details about the selected configuration and the optimization process are given in Section 4. The rest of the section describes the steps involved in the iterations of the MODS algorithm, which is compared to the standard two view matching and ASIFT pipelines.

MODS (Algorithm 3) starts by synthetic view generation. It is well known that a homography H can be approximated by an affine transformation A at a point using the first order Taylor expansion. The affine transformation can be uniquely decomposed by SVD into a rotation, skew, scale and rotation around the optical axis [26]. In [16], the authors proposed to decompose the affine transformation A as

                           
                              (1)
                              
                                 
                                    
                                       
                                          A
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                H
                                                λ
                                             
                                             
                                                R
                                                1
                                             
                                             
                                                (
                                                ψ
                                                )
                                             
                                             
                                                T
                                                t
                                             
                                             
                                                R
                                                2
                                             
                                             
                                                (
                                                ϕ
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             λ
                                             
                                                (
                                                
                                                   
                                                      
                                                         
                                                            cos
                                                            ψ
                                                         
                                                      
                                                      
                                                         
                                                            −
                                                            sin
                                                            ψ
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            sin
                                                            ψ
                                                         
                                                      
                                                      
                                                         
                                                            cos
                                                            ψ
                                                         
                                                      
                                                   
                                                
                                                )
                                             
                                             
                                                (
                                                
                                                   
                                                      
                                                         t
                                                      
                                                      
                                                         0
                                                      
                                                   
                                                   
                                                      
                                                         0
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                
                                                )
                                             
                                             
                                                (
                                                
                                                   
                                                      
                                                         
                                                            cos
                                                            ϕ
                                                         
                                                      
                                                      
                                                         
                                                            −
                                                            sin
                                                            ϕ
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            sin
                                                            ϕ
                                                         
                                                      
                                                      
                                                         
                                                            cos
                                                            ϕ
                                                         
                                                      
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where λ > 0, R
                        1 and R
                        2 are rotations, and Tt
                         is a diagonal matrix with t > 1. Parameter t is called the absolute tilt, ϕ ∈ ⟨0, π) is the optical axis longitude and ψ ∈ ⟨0, 2π) is the rotation of the camera around the optical axis (see Fig. 1). Each synthesized view is thus parameterized by the tilt, longitude and optionally the scale and represents a sample of the view-sphere respectively view-volume around the original image.

The view synthesis proceeds in the following steps: at first, a scale synthesis is performed by building a Gaussian scale-space with Gaussian 
                           
                              σ
                              =
                              
                                 σ
                                 base
                              
                              ·
                              S
                           
                         and downsampling factor S (S < 1). Then, each image in the scale-space is in-plane rotated by longitude ϕ with step 
                           
                              Δ
                              ϕ
                              =
                              
                                 Δ
                                 
                                    ϕ
                                    base
                                 
                              
                              /
                              t
                           
                        . In the third step, all rotated images are convolved with a Gaussian filter with 
                           
                              σ
                              =
                              
                                 σ
                                 base
                              
                           
                         along the vertical and 
                           
                              σ
                              =
                              t
                              ·
                              
                              
                                 σ
                                 base
                              
                           
                         along the horizontal direction to eliminate aliasing in a final tilting step. The final tilt is applied by shrinking the image along the horizontal direction by factor t. The synthesis parameters are: the set of scales {S}, Δϕ
                        base—the longitude sampling step at tilt 
                           
                              t
                              =
                              1
                              ,
                           
                         the set of simulated tilts {t}.

The second step of MODS is detection and description of local features. It is known that different local feature detectors are suitable for different types of images [9] and that some detectors are complementary in the image structures they respond to Aanaes and Dahl [18]. Our experiments show (see Section 5) that combining detectors improves the overall robustness and speed of the matching procedure.

MODS combines a fast similarity covariant FAST (in ORB implementation) detector and affine covariant detectors MSER and Hessian-Affine. The normalized patches are described by the binary descriptor BRIEF [13] (in ORB implementation) and a recent modification of SIFT [6]—the RootSIFT [27]. The local feature frames computed on the synthesized views are backprojected to the coordinate system of the original image by the known affine matrix A and associated with the descriptor and the originating synthetic view. MODS steps configuration are specified in Table 1
                        .

For the MSER and Hessian-Affine detectors, the fast affine feature extraction process from [28] was applied.

The next step of the MODS algorithm is the generation of tentative correspondences. Different strategies for the computation of tentative correspondences in wide-baseline matching were proposed. The standard method for matching SIFT(-like) descriptors is based on ratio of the distances to the closest and the second closest descriptors in the other image [6]. While the performance of this test is in general very good, it degrades when multiple observations of the same feature are present. In this case, the presence of similar descriptors will lead to the first to second SIFT ratio to be close to 1 and the correspondences will “annihilate” each other, despite the fact they represent the same geometric constraints and are therefore not mutually contradictory (see Fig. 2
                        ). The problem of multiple detections is amplified in matching by view synthesis since covariantly detected local features are often repeatedly discovered in multiple synthetic views.

To address this problem, we propose a modified matching strategy denoted first to first geometrically inconsistent—FGINN . Instead of comparing the first to the second closest descriptor distance, the distance of the first descriptor and the closest descriptor that is geometrically inconsistent with the first one is used. We call descriptors in one image geometrically inconsistent if the Euclidean distance between centers of the regions is ≥ n pixels (default: 
                           
                              n
                              =
                              10
                           
                        ). The difference of the first-to-second closest ratio strategy and the FGINN strategy is illustrated in Fig. 2.

The last step of the MODS is the geometric verification. It consists of three substeps.

The redetection of covariant features in synthetic views results in duplicates in tentative correspondences. The duplicate filtering prunes correspondences with close spatial distance ( ≈ 5 pixels) of local features in both images—all these correspondences except one—with smallest descriptor distance ratio—are eliminated from the final correspondences list. The number of pruned correspondences can be used later for evaluating the quality (probability of being correct) in PROSAC-like [30] geometric verification.

The LO-RANSAC [31] algorithm searches for the maximal set of geometrically consistent tentative correspondences. The model of the transformation is set either to homography or epipolar geometry, or automatically determined by a DegenSAC [25] procedure.

Since the epipolar geometry constraint is much less restrictive than a homography, wrong correspondences consistent with some (random) fundamental matrix appear. The local affine frame consistency check (LAF-check) eliminates virtually all incorrect correspondences. The procedure uses coordinates of the closest and furthest ellipse points from the ellipse center of both matched local affine frames to check whether the whole local feature is consistent with estimated geometry model (see Fig. 3
                           ). The check is performed for the geometric model obtained by RANSAC. Regions which do not pass the check are discarded from the list of inliers. If the number of correspondences after the LAF-check is fewer than the user defined minimum, matcher continues with the next step of view synthesis.

In this section, we discuss the tilt-rotation-detector setups of the MODS algorithm, and threshold selection for the first to first geometrically inconsistent—FGINN matching strategy validation.

The two main parameters of the view synthesis, tilt {t} sampling and the latitude step Δϕ
                        base, were explored in the following synthetic experiment.

A set of simulated views with latitudes angles 
                           
                              θ
                              =
                              (
                           
                        0°, 20°, 40°, 60°, 65°, 70°, 75°, 80°, 85°), corresponding to tilt series 
                           
                              t
                              =
                              (
                           
                        1.00, 1.06, 1.30, 2.00, 2.36, 2.92, 3.86, 5.75, 11.47)
                           2
                        
                        
                           2
                           Assuming that the original image is the fronto-parallel view.
                         was generated for each of 150 random images from the Oxford Building Dataset
                           3
                        
                        
                           3
                           Available at http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/.
                         
                        [32]. Example images are shown in Fig. 4
                        . The ground truth affine matrix A was computed for each simulated view using equation (1) and used in the final verification step. The original image was matched against its warped version, and the running time and number of inliers for each combination of the detector, tilt and rotation (see Table 3
                        
                        ) were computed. In all, 84 setups for each of the 8 detectors on the 150 image pairs were evaluated. As an example, we show the relation between the density of the view-sphere sampling and the number of images matched for the DoG detector in Fig. 5
                        .

Since our goal is to find a variety of detector-tilt-rotation configurations operating with different matching ability—run-time trade-offs, we defined “easy”, “medium” and “hard” problems on the synthetic dataset. Successful two-view matching was defined as recovering n ≥ 50 ground truth correspondences on a synthetically warped image. The threshold is set high—synthetic warping of an image is underestimating the reduction of the number of matchable features induced by the effects of a corresponding viewpoint change, e.g., due to non-planarity of the scene or illumination changes. The matcher is considered to solve an “easy” problem if percentage of the matched images f ≥ 50% of total images, “medium” if f ≥ 90% of images matched and solved “hard” if f ≥ 99% of the images are matched.

The experiment with the synthetically warped dataset gives a hint about the limits of configurations. Three configurations that solved the maximum tilt difference for each case fastest for a given detector were selected for evaluation. The configurations are specified in Table 4
                        .

The average time necessary to match a given synthetic tilt difference for different detectors with the optimal configuration is shown in Fig. 6
                        . The computations were performed on the Intel i7 3.9 GHz (8 cores) desktop with 8 Gb RAM with parallel processing.

Note that view synthesis significantly increases the matching performance of all detectors, but not uniformly. The left plot of Fig. 6 shows that a very sparse viewsphere sampling greatly improves matching at almost no computational cost for all detectors. However, after reaching a certain density, additional views do not add correspondences in the hardest cases—see the right graph of Fig. 6. The ORB detector-descriptor clearly outperforms other detectors in terms of speed, but fails to match all images with the maximum tilt difference. The Hessian-Affine shown the best performance and it matched all pairs.

The following protocol was used to find the thresholds and to evaluate the performance of the proposed first geometrically inconsistent nearest neighbor FGINN strategy. First, similarity covariant regions were detected using the DoG detector (we also tried Hessian-Affine, MSER and SURF, with very similar results) and described using four popular descriptors—RootSIFT, SURF, LIOP [34] and MROGH [20] which are typically matched with the second-nearest region SNN strategy. Then for each keypoint descriptor, the first, second and first geometrically inconsistent descriptors in the other image were found. The matching keypoints were then labeled as correct if their Sampson error was within 1 pixel of the ground truth location given by homography for the image pair, and incorrect otherwise.

The experiment was performed on 26 image pairs of the publicly available datasets [9,33] (image pairs 1–3, precise homography provided) and [35] (the homography was estimated using provided precise ground truth correspondences). The recall-precision curves for correspondences from all images were plot with a varying ratio threshold from 0 to 1 in Fig. 7
                        . The FGINN curves for SIFT and SURF slightly outperform standard SNNs, while for LIOP and MROGH the difference is much more significant. The significantly higher benefit of the FGINN rule for LIOP and MROGH can be explained by their lower sensitivity to keypoint shift which in turn means that undesirable suppression of keypoints happens in a larger neighborhood. The lower sensitivity to shifts was experimentally verified.

@&#EXPERIMENTS@&#

We have tested MODS and, as a baseline, ASIFT
                        4
                     
                     
                        4
                        Reference code from http://demo.ipol.im/demo/my_affine_sift.
                      and single detector configurations specified in Table 4 on seven public datasets [10,29,36–40].

The kd-tree algorithm from FLANN library [41] was used to efficiently find the N-closest descriptors. The distance ratio thresholds of the FGINN matching strategy were experimentally selected based on the CDFs of matching and non-matching descriptors.

The MODS algorithm allows to set the minimum desired number of inliers which have a very low probability to be a random result as a stopping criterion. The recommended value—15 inliers to the homography—did not produce a false positive results in experiments. Computations were performed on Intel i3 CPU @ 2.6 GHz with 4 Gb RAM with 4 cores.

To evaluate the
                         performance of matching algorithms, we introduce a two-view matching evaluation dataset
                           5
                        
                        
                           5
                           Available at http://cmp.felk.cvut.cz/wbs/index.html.
                         with extreme viewpoint changes, see Table 5. The dataset includes image pairs from publicly available datasets: adam and mag 
                        [16], graf 
                        [9] and there 
                        [33]. The ground truth homography matrices were estimated by LO-RANSAC using correspondences from all detectors in view synthesis configuration 
                           
                              
                                 {
                                 t
                                 }
                              
                              =
                              
                                 {
                                 1
                                 ;
                                 
                                    2
                                 
                                 ;
                                 2
                                 ;
                                 2
                                 
                                    2
                                 
                                 ;
                                 4
                                 ;
                                 4
                                 
                                    2
                                 
                                 ;
                                 8
                                 }
                              
                              ,
                           
                        
                        
                           
                              Δ
                              ϕ
                              =
                              
                                 72
                                 ∘
                              
                              
                              /
                              t
                           
                        . The number of inliers for each image pair was ≥ 50 and the homographies were manually inspected. For the image pairs graf and there precise homographies are provided by Cordes et al. [33]. Transition tilts τ were computed using equation (1) with SVD decomposition of the linearized homography at center of the first image of the pair (see Table 5). Oxford [9] dataset with 42 image pairs (1-2,
                           
                              
                              …
                              
                           
                        , 1–6) was used for easier wide baseline problems.

The evaluated algorithms matched image pairs and the output keypoints correspondences were checked with ground truth homographies. The image pair is considered as solved, when at least 10 output correspondences are correct.


                              Fig. 8 compares the different view synthesis configurations. Note that no single detector solved all image pairs. The Hessian-Affine, MSER, Harris-Affine and DoG successfully solved, respectively, 13, 13, 12 and 13 out of the 15 image pairs however, at the expense of the high computational cost. We also noticed that if one would know the suitable detector and configuration for each image, it is possible to match all image pairs.

The MODS algorithm with more time-consuming configurations solves all image pairs and does it faster than a suitable configuration for each image pair—see Fig. 9
                              . We have tested several variants of the MODS configurations, stated in Table 2. Experiments shows that the proposed MODS configuration is very fast on the easy WBS problems as in Oxford dataset (see Fig. 9, right graph) and has very little overhead on the harder EVD dataset—it is the second best after configuration without ORB steps. The results of the MODS medium configuration—without first sparse synthesis step—shows fruitfulness of the progressive view synthesis.

ASIFT is able to match only 6 image pairs from the dataset. The ASIFT algorithm generates a lower number of correct inliers and works slower than our identical DoG configuration (which has the same tilt-rotation set). The main causes are the elimination of “one-to-many”, including correct, correspondences, the inferiority of the standard second closest ratio matching strategy and a simple brute-force algorithm of matching used in ASIFT (Table 6
                              ).


                              Fig. 10
                               shows the breakdown of the computational time. The most time consuming parts—detection and description (including the dominant orientation estimation)—take 40% and 35% respectively of the all time. Without applying the fast SIFT computation from [28], the SIFT description takes more than 50% of the time. The ORB is an exception—the synthesis is not so profitable, since it takes more time than detection and description itself. Note that the whole process is almost linear in the area of the synthesized views. The only super-linear part, matching, takes only 10% of the time.

The evaluation dataset consists of 35 image sequences taken from the Turntable dataset [10] (“Bottom” camera) shown in Table 7
                           . Eight image sets contain objects with relatively large planar surfaces and the remaining ones are low-textured, “general 3D” objects.

The view marked as “0°” in the Turntable dataset was used as a reference view and 0°–90° and 270°–355 ° views with a 5° step were matched against it using the procedure described in Section 5, forming a 
                              
                                 [
                                 −
                                 
                                    90
                                    ∘
                                 
                                 ,
                                 
                                    90
                                    ∘
                                 
                                 ]
                              
                            sequence. Note that the reference view is not usually the “frontal” or “side” view, but rather some intermediate view which caused asymmetry in results (see Fig. 11 and Table 8).

The output of the matchers is a set of the correspondences and the estimated geometrical transformation. The accuracy of the matched correspondences was chosen as the performance criterion, similarly to the protocol in [42]. For all output correspondences, the symmetrical epipolar error [26] 
                           e
                           SymEG was computed according to the following expression:

                              
                                 (2)
                                 
                                    
                                       
                                          e
                                          SymEG
                                       
                                       
                                          (
                                          F
                                          ,
                                          u
                                          ,
                                          v
                                          )
                                       
                                       =
                                       
                                          
                                             (
                                             
                                                v
                                                ⊤
                                             
                                             F
                                             u
                                             )
                                          
                                          2
                                       
                                       ×
                                       
                                       
                                          (
                                          
                                          
                                             1
                                             
                                                
                                                   
                                                      (
                                                      F
                                                      u
                                                      )
                                                   
                                                   1
                                                   2
                                                
                                                +
                                                
                                                   
                                                      (
                                                      F
                                                      u
                                                      )
                                                   
                                                   2
                                                   2
                                                
                                             
                                          
                                          +
                                          
                                             1
                                             
                                                
                                                   
                                                      (
                                                      
                                                         F
                                                         ⊤
                                                      
                                                      v
                                                      )
                                                   
                                                   1
                                                   2
                                                
                                                +
                                                
                                                   
                                                      (
                                                      
                                                         F
                                                         ⊤
                                                      
                                                      v
                                                      )
                                                   
                                                   2
                                                   2
                                                
                                             
                                          
                                          
                                          )
                                       
                                       
                                       ,
                                       
                                       
                                       
                                       
                                    
                                 
                              
                           where F—fundamental matrix, u, v—corresponding points, 
                              
                                 
                                    (
                                    F
                                    u
                                    )
                                 
                                 j
                                 2
                              
                           —the square of the jth entry of the vector Fu.

The ground truth fundamental matrix was obtained from the difference in camera positions [26], assuming that turntable is fixed and the camera moved around the object, according to the following equation:

                              
                                 (3)
                                 
                                    
                                       
                                          
                                             F
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                   K
                                                   
                                                      −
                                                      ⊤
                                                   
                                                
                                                R
                                                
                                                   K
                                                   ⊤
                                                
                                                
                                                   
                                                      [
                                                      K
                                                      
                                                         R
                                                         ⊤
                                                      
                                                      t
                                                      ]
                                                   
                                                   ×
                                                
                                                ,
                                             
                                          
                                       
                                       
                                          
                                             R
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                   (
                                                   
                                                      
                                                         
                                                            
                                                               cos
                                                               ϕ
                                                            
                                                         
                                                         
                                                            0
                                                         
                                                         
                                                            
                                                               −
                                                               sin
                                                               ϕ
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            0
                                                         
                                                         
                                                            1
                                                         
                                                         
                                                            0
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               sin
                                                               ϕ
                                                            
                                                         
                                                         
                                                            0
                                                         
                                                         
                                                            
                                                               cos
                                                               ϕ
                                                            
                                                         
                                                      
                                                   
                                                   )
                                                
                                                ,
                                                
                                                K
                                                =
                                                
                                                   (
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  m
                                                                  f
                                                               
                                                               
                                                                  FR
                                                                  X
                                                               
                                                            
                                                         
                                                         
                                                            0
                                                         
                                                         
                                                            
                                                               m
                                                               2
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            0
                                                         
                                                         
                                                            
                                                               
                                                                  n
                                                                  f
                                                               
                                                               
                                                                  FR
                                                                  Y
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               n
                                                               2
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            0
                                                         
                                                         
                                                            0
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                   
                                                   )
                                                
                                                ,
                                             
                                          
                                       
                                       
                                          
                                             t
                                          
                                          
                                             =
                                          
                                          
                                             
                                                r
                                                
                                                   (
                                                   
                                                      
                                                         
                                                            
                                                               sin
                                                               ϕ
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            0
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               1
                                                               −
                                                               cos
                                                               ϕ
                                                            
                                                         
                                                      
                                                   
                                                   )
                                                
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           where R is the orientation matrix of the second camera, K—the camera projection matrix, t—the virtual translation of the second camera, r—the distance from camera to the object, ϕ—the viewpoint angle difference, FR
                              X
                           , FR
                              Y
                           —the focal plane resolution, f—the focal length, m, n—the sensor matrix width and height in pixels. The last five parameters were obtained from EXIF data.

One of the evaluation problems is that background regions, i.e. regions that are not on the object placed on the turntable, are often detected and matched influencing the geometry transformation estimation. The matches are correct, but consistent with an identity transform of the (background of) the test images, not the fundamental matrix associated with the movement of the object on the turntable. In order to solve this problem, the median value of the correspondence errors was chosen as the measure of precision because of its tolerance to the low number of outliers (e.g., the above-mentioned background correspondences), and its sensitivity to the incorrect geometric model estimated by RANSAC.

An image pair is considered as correctly matched if the median symmetrical epipolar error on the correspondences using ground truth fundamental matrix is ≤  6 pixels.

@&#RESULTS@&#


                           Fig. 11 and Table 8 show the percentage and the number of image sequences respectively for which the reference and tested views for the given viewing angle difference were matched correctly.

The difference between easy, medium and hard configurations is small for structured scenes—unlike planar ones. Difficulties in matching are caused not by the inability to detect distorted regions but by object self-occlusions. Therefore synthesis of the additional views does not bring more correspondences.

Experiments with view synthesis confirmed [10] results that the Hessian-Affine outperforms other detectors for matching of structured scenes and can be used alone in such scenes. MODS shows similar performance, but is slower than the Hessian-Affine configuration.

The computations
                            were performed on Intel i5 3.0 GHz (4 cores) desktop with 16 Gb RAM. Examples of the matched images are shown in Fig. 12
                           .

We introduce the extreme zoom dataset (EZD), which is a small subset of the retrieval dataset used in [40]. It consists of six sets of images with an increasing level of zoom (see examples in Fig. 13
                           ). The state-of-the art matcher—ASIFT [16] and registration algorithm DualBootstrap [43] as well as results for MSER, ORB and Hessian-Affine matchers without view synthesis were compared to MODS. Image pairs are matched with tested algorithms. An image pair is considered solved when at least 10 output correspondences are correct. Results are shown in Table 9.

The MODS performance was evaluated on the city-from-air dataset from [36]. The dataset comprises 30 pairs (examples shown in Fig. 14) of photographs of buildings taken from the air. The view points difference is quite large, the images contain repeated structures, illumination differs. The authors proposed a matcher based on HoG [44] descriptor with view synthesis and compare it to ASIFT and D-Nets [45] for skyscraper frontal face matching. We follow the evaluation protocol which considers a pair matched correct only if the facade plane is matched (≥75% correct inliers). If the output homography was ground/roof, it is considered incorrect. The results are shown in Table 10.

Note that no special adjustment is done in MODS for homography selection, so the reported performance is a lower bound.

Despite being designed for (extreme) wide baseline stereo problems, MODS performance was evaluated on other datasets: GDB-ICP [38] (modality, viewpoint and photometry changes), SymBench [37] (photometrical changes and photo-vs-painting pairs), and MMS [39] (infrared-vs-visible pairs)—see Table 11
                           
                           
                           
                           . The state-of-the art matcher—ASIFT [16] and registration algorithm DualBootstrap [43] as well as results for MSER, ORB and Hessian-Affine matchers without view synthesis were compared to MODS.

Image pairs are matched with the tested algorithms. Output keypoints correspondences were checked against the ground truth homographies. An image pair is considered solved when at least 10 output correspondences are correct Our primary evaluation criterion is the ability to find sufficiently correct geometric transformations in a reasonable time; accurate geometry can be found in consecutive step.

The computations were performed on Intel i3 3.0 GHz desktop (4 cores) with 4 Gb RAM.

Results are shown in the Table 12. MODS is the fastest method and it is able to match the most image pairs in GDB-ICP and SymBench datasets without using symmetrical parts or other problem-specific features. Images from the MMS dataset (as well as other thermal images) produce a small number of features as they do not contain many textured surfaces, and have very short geometrical baseline. Those are the main reasons why area-based method—Dual-Bootstrap—work significantly better than the feature-based methods.

After lowering the threshold for detectors allowing to detect more feature points and using the orientation restricted SIFT [46] in addition to the RootSIFT, MODS-IR solved 83 out of 100 image pairs from MMS dataset.

@&#CONCLUSIONS@&#

An algorithm for two-view matching called MODS algorithm was introduced. The most important contributions of the algorithm are its ability to adjust its complexity to the problem at hand, and its robustness, i.e. the ability to solve a broader range of wide-baseline problems than the state of the art. This is achieved while being fast on simple problems.

The apparent robustness vs. speed trade-off is finessed by the use of progressively more time-consuming feature detectors, and by on-demand generation of synthesized images that is performed until a reliable estimate of geometry is obtained. The MODS method demonstrates that the answer to the question “which detector is the best?” depends on the problem at hand, and that it is fruitful to focus on the “how to combine detectors” problem.

We are the first to propose view synthesis for two-view wide-baseline matching with affine-covariant detectors, which is superficially counter-intuitive, and we show that matching with the Hessian-Affine or MSER detectors outperforms the state-of-the-art ASIFT. View synthesis performs well when used with simple and very fast detectors like ORB, which obtains results similar to ASIFT but in orders of magnitude shorter time.

Minor contributions include an improved method for tentative correspondence selection, applicable both with and without view synthesis and a modification of the standard first to second nearest distance rule increases the number of correct matches by 5–20% at no additional computational cost.

The evaluation of the MODS algorithm was carried out both on standard publicly available datasets as well as a new set of geometrically challenging wide baseline problems that we collected and will make public. The experiments show that the MODS algorithm solves matching problems beyond the state-of-the-art and yet is comparable in speed to standard wide-baseline matchers on easy problems. Moreover, MODS performs well on other classes of difficult two-view problems like matching of images from different modalities, with large difference of acquisition times or with significant lighting changes.

@&#ACKNOWLEDGMENT@&#

The authors were supported by The Czech Science Foundation Project GACR P103/12/G084.

@&#REFERENCES@&#

