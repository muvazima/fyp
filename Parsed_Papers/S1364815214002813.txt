@&#MAIN-TITLE@&#An overview of methods to evaluate uncertainty of deterministic models in decision support

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We review different types of uncertainty present in environmental modelling.


                        
                        
                           
                           We review methods to evaluate uncertainty related to model results.


                        
                        
                           
                           Best way to evaluate uncertainty depends on the models and available information.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Environmental modelling

Deterministic

Probabilistic

Uncertainty

Decision support

@&#ABSTRACT@&#


               
               
                  There is an increasing need for environmental management advice that is wide-scoped, covering various interlinked policies, and realistic about the uncertainties related to the possible management actions. To achieve this, efficient decision support integrates the results of pre-existing models. Many environmental models are deterministic, but the uncertainty of their outcomes needs to be estimated when they are utilized for decision support. We review various methods that have been or could be applied to evaluate the uncertainty related to deterministic models' outputs. We cover expert judgement, model emulation, sensitivity analysis, temporal and spatial variability in the model outputs, the use of multiple models, and statistical approaches, and evaluate when these methods are appropriate and what must be taken into account when utilizing them. The best way to evaluate the uncertainty depends on the definitions of the source models and the amount and quality of information available to the modeller.
               
            

@&#INTRODUCTION@&#

Environmental problems like climate change, overfishing, erosion, and reduction of biodiversity, are typical textbook examples of “wicked problems” (Rittel and Webber, 1973; Churchman, 1967). They are difficult to define, unique, their results are not true/false but rather better/worse, and the possible discrepancy regarding the problem or its parts can have several plausible explanations. Nevertheless, many environmental problems are proving their urgency, and despite the difficulty, we must aim to find solutions to these problems.

The research community has embraced the challenge and researchers around the world are working to shed light on the complex interactions of the environmental systems. Modelling in its various forms is an increasingly popular method to approach these problems (e.g. Hilborn and Mangel, 1997; Jackson et al., 2000; EPA, 2008; Aguilera et al., 2011; Laniak et al., 2013). The common way to build a quantitative environmental model is to describe the relationships between model variables using mathematical equations with deterministic values (Jackson et al., 2000). These values are found either in literature, by fitting equations to data, or, if no such information is available, through iterative search in which the model outputs are compared to observed system behaviour (Jackson et al., 2000). Modelling can clarify our understanding of the human–nature interactions, point out where the largest gaps in our knowledge lies, and distinguish between competing hypotheses.

In natural resource management, understanding the average processes is often not sufficient, however, and decision-makers are increasingly interested to understand the uncertainties of the models (O'Hagan, 2012). Several divergent, imperfectly known processes (physical, biological and social), will affect the events, and it is impossible to predict with certainty what the result of each management decision will be (Dietz, 2003; Reichert and Borsuk, 2005). In all non-trivial cases, in the heart of the management problem there is a trade-off between maximizing the human benefit while minimizing the harm caused to the nature and avoiding disastrous outcomes such as extinctions, stock collapses, loss of ecosystem services, etc. (Burgman, 2005; Fenton and Neil, 2012).

In environmental decision-making, potential risks may include irreversible damage such as destruction of habitats or even extinction of unique populations or species, or other ecologically severe and economically expensive consequences. The aim of probabilistic modelling and decision analysis is to explicitly spell out these risks. By evaluating the nature and extent of the uncertainties in the system, the model can provide decision-makers with a realistic picture of the possible outcomes (Burgman, 2005; Power and McCarty, 2006). Formal decision analysis can be helpful in structuring the problems, integrating knowledge, and visualizing the results (Kiker et al., 2005). They ease the work of decision-makers by helping them make consistent and justifiable choices. In recent decades, decision support and analysis tools have increasingly been applied to management of natural resources (e.g. Varis, 1997; Marcot et al., 2001; Borsuk et al., 2004; Dorner et al., 2007; McIntosh et al., 2011).

The probabilistic, integrative decision support models can derive their data from three types of sources: first-hand data, expert knowledge, or pre-existing (probabilistic or deterministic) models. Of these approaches, using field-observation data is in many cases straightforward, and expert elicitation has been covered by excellent reviews, e.g. by O'Hagan (2012) and Martin et al. (2012). Probabilistic modelling, i.e. modelling approaches that incorporate uncertainty into the model calculations at all stages of the modelling, are increasing in popularity (e.g. Aguilera et al., 2011; Mantyniemi et al., 2013). Yet, a large part of existing ecological models does not incorporate uncertainty directly, and the uncertainty related to their output must be evaluated separately. While literature exists proposing methodology to do this (e.g. Bates et al., 2003; Gronewold and Borsuk, 2009; Baroni and Tarantola, 2014; Gal et al., 2014), we are not aware of an overview reviewing the various methods that exist (however see Bennett et al. (2013) for a review of methods to evaluate the performance of environmental models).

Therefore, this paper reviews methods to evaluate the uncertainty that is associated with the results of deterministic models. The aim is to give an overview of methodology that has been or could be applied for this purpose, and give sufficient references so that worked examples of these methods can be found in the literature. In chapter 2, we explore the definition of uncertainty, in chapter 3, justify why uncertainty has to be accounted for in decision support modelling, and in chapter 4, review various ways to do this. Chapter 5 draws short conclusions.

From the management point of view, uncertainty is, quite simply, the lack of exact knowledge, regardless of what is the cause of this deficiency (Refsgaard et al., 2007). Each decision or set of decisions has associated gains or losses which are usually dependent on several random factors and thus highly uncertain (Fenton and Neil, 2012). Typically, uncertainty is present on every step of the environmental management analysis, from the randomness in the environmental response to the society's definition of risk, opportunity, and whose gains or losses should be considered in decision-making. Burgman (2005) states that “in most circumstances, the best use of models is to interrogate options, resulting in choices that are robust to a range of assumptions and uncertainties”. Decision analysis should aim to provide the decision maker with as realistic picture of the current knowledge and its deficiencies as possible, by utilising all the relevant information available. Uncertainty is often expressed in the form of probability distribution that indicates how likely each of the possible outcomes is (Fig. 1
                     ).

Divergent, overlapping uncertainty classifications can be found in literature, the typology varying remarkably depending on the context and scope (see e.g. Regan et al., 2002; Walker et al., 2003; Refsgaard et al., 2007; Skinner et al., 2014). It is common to divide uncertainty into two or three categories according to their basic nature: aleatory uncertainty, i.e. inherent randomness and natural variability; epistemic uncertainty, resulting from imperfect knowledge, and linguistic uncertainty, arising from language issues. The first is typically seen as irreducible, whereas the two latter can be quantified and reduced.

Uncertainty stems from various sources. Following the classification of Regan et al. (2002), uncertainty (excluding linguistic uncertainty) can be divided into 6 classes:
                        
                           -
                           
                              Inherent randomness. However well we know the process and the initial (starting) conditions, we cannot be more certain of what the outcome will be. This randomness-type of uncertainty can be thought as being inherent to nature. Randomness can often be quantified very well, and is easy to deal with in probabilistic models.


                              Measurement error. Measurement error causes uncertainty about the value of the measured quantity. The measurement error can be estimated by statistical methods, if several samples are taken. If the extent of the measurement error can be estimated, it can be relatively easily dealt with in probabilistic models.


                              Systematic error in the measurements results from a bias in the sampling, and is more difficult to quantify, or even notice. If systematic error goes unnoticed, it may have cumulative effects in the models that are built on the data.


                              Natural variation. The natural systems change in time and place, and so do the parameters of interest. Therefore, despite measurements, there is always uncertainty about natural conditions. This kind of uncertainty can be quantified, but it requires some careful consideration; we need to estimate the possible range and relative probabilities of the unknown quantities.


                              Model uncertainty. Models are always abstractions of the natural system. Some less important variables and interactions are left out, and the shapes of the functions are always abstractions of the real processes. Science may also have insufficient knowledge about the relevant processes, the shapes of the functions and their parameter values. Uncertainty of the model parameters can be accounted for in probabilistic models much the same way as natural variation, with careful consideration of the range of possible values and their probabilities; while uncertainty about the model's structure, i.e. uncertainty about the cause-and-effect relationships, is often very difficult to quantify.


                              Subjective judgement based uncertainty occurs due to interpretation of data, especially when the data are scarce or error prone.

When constructing an environmental decision analysis model, many types of uncertainty are typically present, and it is often impossible to distinguish, let alone separate, the various types of uncertainty from each other conclusively. Being aware of the various sources of uncertainty may, however, help the modeller make justified decisions about how to account for this uncertainty.

Decision support models are built to help the decision-maker evaluate the consequences of various management alternatives (Borsuk et al., 2004; Fenton and Neil, 2012; Holzkamper et al., 2012). In order to be most useful, the decision support model should also include information about the uncertainties related to each of the decision options, as the certainty of the desired outcome may be a central criterion on the selection of the management policy. In addition, an otherwise attractive management scheme may also include an increased probability of an extremely undesired outcome (such as the collapse of a habitat-building species), and the decision maker may prefer to choose another decision option that reduces this risk even if the expected benefits would decrease as well. Estimates of the probabilities of possible outcomes are required to evaluate these trade-offs.

The policy and management options that are of interest to the decision-maker are often influenced by environmental factors which cover a wide range of processes and dynamic interactions in various temporal and geographical scales, such as short and long term ecosystem state, a general geographical overview and certain hot spots, etc., which are rarely covered by one, coordinated research project (Borsuk et al., 2004; Barton et al., 2012). Separate models describing the different parts of the system, and operating on different temporal and geographical scales, often exist. These models' outputs can be combined in the decision support model so that it serves as a meta-model that draws together the predictions of the various dynamic models and summarizes their combined message (Borsuk et al., 2004; Jolma et al., 2011; 2014).

When utilizing outputs of the existing models, the decision support modeller may face a difficult problem, however. The existing models are often deterministic, giving just a single output value for each variable (possibly repeated over several time steps and spatial units), without any indication of the amount of uncertainty or expected variation around this value. The modeller/analyst then needs to come up with an uncertainty estimate to go along with each of the model predictions. This task is of crucial importance, since it has a large impact on how the decision support model will behave, and therefore has potential for changing the management recommendations that are drawn from the model. This problem is, to some extent, analogous to uncertainty analysis or sensitivity analysis of a single modelling framework: all sources of uncertainty need to be identified and characterized using all available information, including measurements, expert opinion, and boundary considerations (Baroni and Tarantola, 2014).

There is, however, an additional challenge when utilizing the results of one model as an input in another one. It is crucial to understand whether the definitions of the variables in the two models actually are compatible or not. If the model gives an estimate of Chlorophyll a concentration, what do we actually get? Is it the value in a specific point in time and space, or is it an estimate of the average Chlorophyll a concentration? If so, average of what exactly: over a time period in a single point in space, over a defined area but a single point in time, or an average over both time and space? These questions can be critical in correctly defining the probability distributions, or the uncertainties, in the probabilistic model. The variability present in the outcomes of the deterministic model may vary widely depending on the exact quantity being modelled, and how the probability distributions are defined based on that output should vary accordingly. Therefore, examining the models' definitions should be the first step in these exercises in order to avoid answering the wrong question, as suggested also by Bennett et al. (2013) for model performance evaluations.

There is no clear-cut answer how to estimate uncertainties in any cases more complex than a well-known random process such as coin toss. Below, we present some approaches that have been or could be employed in order to estimate the uncertainty related to the outcomes of deterministic models. For a wider-scope reviews of uncertainty in environmental modelling in general, see e.g. Refsgaard et al. (2007), Matott et al. (2009), and Tebaldi and Knutti (2007). As with model performance evaluation (Bennett et al., 2013), the most fitting choice is always case-dependent, depending on the available models as well as the decision problem at hand.

Expert assessment is an established methodology for obtaining estimates of relationships that cannot be or are too expensive or impractical to be observed directly, such as hypothetical scenarios (e.g. Krueger et al., 2012 and references therein). It can also be used to obtain estimates of the variance around model parameters (O'Hagan, 2012) and model-predicted values, although estimating variance or variability is a challenging task, especially if there are several conditioning environmental factors that need to be taken into account simultaneously (O'Hagan et al., 2006).

Expert assessment is used for example in the general framework for uncertainty and global sensitivity analysis proposed by Baroni and Tarantola (2014) in an informal manner in conjunction with data; they state that “In particular, the uncertainty of each source should be characterized using all available information: measurements, estimations, physical bounds considerations and expert opinion”, and define the uncertainties of the model components by evaluating field-collected data. If such data is available, it may be used to help the experts evaluate also the uncertainties related to the model outputs; however, if the modelling results are based on scenarios that have not yet taken place, the experts need to be careful on how much they rely on current data in evaluating these yet-unseen conditions.

When using expert knowledge to estimate the uncertainty around model-predicted values, the following aspects need to be considered (see also O'Hagan, 2012): Which experts to choose? The modellers who have made the original deterministic model, or other scientists, who are familiar with the domain, but not necessarily with the model? Knowledge of the workings of the deterministic source model is not necessary for the estimation task; the relevant thing is to understand the dynamics of the system and factors that may affect it. On the other hand, it is important that the expert understands the definitions of the variables and the general description of the system as represented in the models; otherwise, they may evaluate a different quantity from that intended to be represented in the decision support model. To guarantee this, the decision modeller and the domain expert need to take enough time for the task and discuss the assumptions and restrictions of the decision support model. Several experts can be used, and each of them can evaluate the uncertainties related to their area of expertise. Then again, if several experts separately estimate the uncertainty of the same variable, a technique for combining these estimates need to be decided. Several techniques for interviewing the experts and combining expert estimates have been proposed; we refer the reader to, for example, Morgan and Henrion (1990), O'Hagan et al. (2006), and O'Hagan (2012). It is also possible to include the various views of the experts as a set of distributions (Rinderknecht et al., 2012) or as an auxiliary variable (Uusitalo et al., 2005; Lehikoinen et al., 2012), which enables the analysis of the relevance of the difference in expert opinions from the decision analytic point of view.

Expert judgement as the source of uncertainty estimates can easily be criticized as subjective. However, lacking data to estimate the variances by, the experts who have devoted their careers to study these questions might be better sources of information than any hasty quantitative models made for this purpose. Various methods exist to help and support the experts in the evaluation task. However, the facilitator also has to be careful to make sure that the experts in fact evaluate the desired quantity, not something related but distinct (O'Hagan et al., 2006). The experts' task may be eased by investigating areas, species, or cases that are deemed sufficiently similar, and upon which data exists. For example, values from adjacent but separate watersheds, forests, or sea areas may be useful here. The range of values that have been observed in distinct but ecologically relevant cases informs about the plausible range of values this variable can get, and therefore may also indicate how large uncertainty is associated to the prediction of the deterministic model.

An example of this situation from our own experience is the case of salmon (Salmo salar) density in their juvenile areas in rivers (Uusitalo et al., 2005). Although in this research the expert knowledge were used to estimate the whole probability distributions, and no modelling results were available to set the averages, the question is essentially the same. In Baltic Sea rivers, the estimates of maximum salmon smolt densities have been in the range of 0.3–1.5 individuals/100 m2, whereas in Atlantic Canada, 3 individuals/100 m2 is considered as an average. The much higher densities in Canadian rivers, which are climatologically and geographically roughly equivalent, cast some doubt to the Baltic estimates; it is possible that the maximum density in the Baltic could be much higher, as well. This would suggest a high variance, indicating high uncertainty about the true value of the maximum density.

The probable range of values of the model output can also be examined by analysing how the output value would behave if some other, fixed variable values were changed within their reasonable range or assigned a probability distribution. The most comprehensive option is to conduct an uncertainty analysis (UA) on the model output. UA is a method that is used to quantify the uncertainty in model outputs induced by uncertainty in inputs (O'Hagan, 2006). The simplest way to conduct a UA is to apply Monte Carlo (MC) methods in which configurations of model inputs are drawn randomly from their distribution, and the resulting set of model outputs can be seen as a random sample of the distribution of the output of interest (Kennedy and O'Hagan, 2001). This, however, requires usually a large number of model runs.

Another option is to apply sensitivity analysis (SA), which is a common way to explore any model. The goal of SA is to characterize how model outputs respond to changes in input, with an emphasis on finding the input parameters to which outputs are the most sensitive (Saltelli et al., 2000; Kennedy and O'Hagan, 2001). This can be achieved by using various approaches ranging from simple one-factor-at-a-time methods to more comprehensive approaches, usually based on MC methods (e.g. Saltelli et al., 2005; Cariboni et al., 2007; Yang, 2011).

The basic idea of the SA is to alter model input values (e.g. Chu-Agor et al., 2011) and/or parameters (e.g. Tomassini et al., 2007) of the model and study the subsequent changes in model output. If the output value changes only little, the output is robust to changes in parameter values within the model. In that case, it seems probable that the uncertainty about the value is relatively small. If, on the other hand, the value of the variable under interest changes markedly when we change some parameters in the model within their reasonable range, this indicates that there is large uncertainty about the variable's value. This is because it is unrealistic to assume that the values used in the model would be exactly those that take place in the nature, and if small differences in these values cause large differences in the outcome, the outcome is bound to be rather uncertain. However, as with UA, making a reasonably thorough sensitivity analysis through the process of altering the parameter and initial values may require a large number of model runs (Saltelli et al., 2010; Baroni and Tarantola, 2014). Furthermore, it is advisable to examine the different parameter values not one by one, but also combinations of them, as parameter combinations may include non-linear interactions, performing a global rather than local sensitivity analysis (Baroni and Tarantola, 2014). However, the number of the combinations increases exponentially as the number of these parameters and their possible values increase, and if the model takes a long time to run, this may render the process infeasible.

In order to minimize the number of model runs, some techniques can be used. The number of required model runs can, to some extent, be reduced by making a preliminary sensitivity analysis, and based on its result focussing on the variables that have a stronger effect on the response variable and using a sparser grid of values for the less influential variables. E.g. (Morris, 1991) presented a well-known screening method that ranks the input factors in order of importance. This method has been later revised and its sampling strategy improved (Campolongo et al., 2007).

These approaches, however, account only for the uncertainty in the model's input values and parameters (such as the slope and intercept of a linear function), not in the model's structure (i.e. existence and functional form of dependencies between variables, etc.) (e.g. O'Hagan, 2012). The structural uncertainty can be evaluated by comparing model results and real observations; however, there may not be enough data for this to be conclusive, and therefore expert assessment is seen as a key method for evaluating structural uncertainty (O'Hagan, 2012).

Model sensitivity analysis can be combined with expert assessment: the final variance estimates would be crafted by experts, aided by the results of the sensitivity analysis. This approach would combine the advantages of expert and model sensitivity assessments, namely, the quantitative rigour of the model, and the insight about the potentially relevant factors outside the model.

As mentioned above, conducting proper sensitivity or uncertainty analysis for complex simulation models requires usually a high number of model runs, which, however, can be unfeasible to conduct due to the limited time and/or other resources. In recent years, a growing number of studies have tackled the problem by using model emulation. Generally speaking, an emulator is a low-order approximation of the more complex model (Castelletti et al., 2012). O'Hagan (2006) defines an emulator to be a statistical approximation of the original simulation model. If this approximation is precise enough, the emulator can be used as a substitute for the original model, and e.g. sensitivity and uncertainty analyses can be based on the emulator instead of the more complex simulation model (O'Hagan, 2006). Although the results obtained with the emulator differ from the results that would have been obtained with the original model, the emulator as a statistical approximation offers a probability distribution for the possible outcome values of the model and thus quantifies the uncertainty related to emulator itself (O'Hagan, 2006; O'Hagan, 2012).

A common approach when building an emulator is to apply Gaussian processes (GP) (O'Hagan, 2006; O'Hagan, 2012), although other options are available as well (Villa-Vialaneix et al., 2012). GPs are statistical processes that define a probability distribution for a set of functions. More precisely, a GP is a distribution for a function, where each function value has a normal distribution, and a set of function values has a multivariate normal distribution (O'Hagan, 2012). Thus GPs have the same, mathematically convenient properties that the normal distribution. When building an emulator, a GP prior is assigned to describe the original model, after which the Bayes theorem is applied to update the prior with a set of model runs. The resulting posterior distribution is the emulator (Kennedy et al., 2006).

In the context of environmental modelling, emulators based on GPs have been used especially for sensitivity analyses of complex models covering e.g. global atmospheric aerosol dynamics (Lee et al., 2011), denitrification and decomposition processes (Qin et al., 2013), multiple physical processes in soil/vegetation/atmosphere continuum (Petropoulos et al., 2009), and the response of bird populations to landscape change (Parry et al., 2013). Kennedy et al. (2008) produced uncertainty maps over carbon flux estimates for England and Wales based on a complex deterministic model. They applied emulators in sensitivity analysis in order to find the most important variables affecting the model output, after which the uncertainty of the model outputs was quantified. As there was no field measurement data available, the approach covered only uncertainties stemming from input parameters, ignoring e.g. the uncertainty related to the model structure. A comprehensive analysis taking into account uncertainty related to the input variables as well as the model inadequacy has been presented by Kennedy and O'Hagan (Kennedy and O'Hagan, 2001) in the context of probabilistic model calibration.

Sometimes it may not be feasible to conduct a complete probabilistic analysis for the model (i.e. eliciting prior distributions for parameters and updating them with the training set). Vanhatalo et al. (2013) studied the effectiveness of different nutrient loading reduction scenarios on water quality in the Gulf of Finland. They estimated the uncertainty related to the outputs of a deterministic ecosystem model by treating the parameters of the deterministic model as known and fixed, and assessing the uncertainty arising from model inadequacy, residual variation, and observation error. The approach applied GPs, which were updated with spatiotemporal monitoring data to produce uncertainty estimates for the original model.

It is usual that the source models describing the system give results on a finer temporal or spatial scale than that used in the decision support model. The models describing the processes are developed on a fine spatial and temporal grid, while the management approach often concentrate on larger areas (lakes, river basins, gulfs, protected areas, cities, etc.) and longer-time (seasonal, yearly) averages. It is common to use observation averages as proxies for the unknown mean (Cha and Stow, 2014) and this approach has been expanded and applied to model-predicted spatial and/or temporal variability as an estimate of the possible variance, and therefore as a proxy for the uncertainty (Lehikoinen et al., 2014). The benefit of this approach is the relative straightforwardness – the results needed for the estimation can often be obtained directly, without any extra effort on running several iterations of the deterministic process model. The procedure of using the model-derived values (the frequency distribution of the modelling results) as the probability distribution is also easy to understand even for those not familiar with probabilistic modelling.

However, before applying this approach, the modeller needs to consider carefully whether the spatiotemporal variance, i.e. the natural variance in time and space, actually is a good proxy for the uncertainty of the interest variable; it might equally well turn out to be an under- or over-estimate of the uncertainty, depending on the definition of the variable in the probabilistic model. We may end up with an underestimate of the uncertainty (i.e. the real uncertainty is larger than our model indicates), if the variable in the decision support model describes the value at a single point in time and space, i.e. the value of a single observation we might see if we went out to get a sample. This is because the uncertainty related to each of these model-generated spatio-temporal observations should be included in the estimate as well (see also discussion in Cha and Stow, 2014). On the other hand, if the variable in our probabilistic model represents the average value over space and time, this approach is likely to overestimate the uncertainty– the Law of Large Numbers kicks in, and the uncertainty about the mean may be rather narrow even if the variance of the observations were large.


                        Lehikoinen et al. (2014) used the results of a deterministic ecosystem model to predict the likely ecosystem status classes within four large water areas on the Finnish coast of the Gulf of Finland, Eastern Baltic Sea in year 2015, given certain nutrient abatement scenarios on the catchment area. A deterministic ecosystem model provided point estimates of water quality variables for each cell in a 5 × 5 km grid (Kiirikki et al., 2001, 2006). Each cell-specific value within a larger area was treated as one possible observation under the defined nutrient load conditions, and the probability distributions for the large areas were formed based on these values.

The system and model uncertainties can, to some extent, be addressed by using multiple models developed to describe the same domain. Simplifications, assumptions about dependencies between the variables, and various parameterizations are always made whenever a model is constructed. If these choices are made independently for each model, there is a possibility to use these separate models in structural uncertainty assessment (e.g. Tebaldi and Knutti, 2007; Bormann et al., 2009; Kronvang et al., 2009; Exbrayat et al., 2010).

If different models end up with very different estimates for the same scenario and similar parameterization, it may be a safe assumption that the structural uncertainty related to the estimate is large. However, it is again crucial to make sure that the variables presented in the model are equal or reasonably comparable. If the models give very similar results, it can be concluded that the uncertainty is small; however, caution is advised in making this conclusion. The model parameters have been fitted to predict the observed data well, so depending on which kinds of scenarios are tested, it may only be natural that the model predictions are relatively similar. This also might be due to the improper description of some key processes in the model, and due to lack of such parameterizations, the model is insensitive for the manipulation of some parameters which it includes.

While qualitative analysis of modelling results can give us an idea of the extent of uncertainty related to them, various ways have been proposed in the literature to combine several models. The term ensemble modelling is used both for running a single model multiple times with different sets of initial values (single model ensemble), and for using multiple models (multiple model ensemble). Ensemble modelling has mainly been used in two ways: to produce one “best” predictive model (e.g. Kronvang et al., 2009; Grenouillet et al., 2011; Trolle et al., 2014), and to evaluate the predictive uncertainty (e.g. Georgakakos et al., 2004; Gal et al., 2014).

The models can also be weighed according to their likelihood, which is evaluated using observed data (Hilborn and Mangel, 1997). Bayesian model averaging (Hoeting et al., 1999) can be used to combine and weigh several Bayesian models using their posterior probability as the weighting factor. An extension of this approach, applicable to deterministic models has been proposed (Raftery et al., 2005; McLean Sloughter et al., 2013). However, as in these methods the weighing is based on models' performance over a training period, applying them may require a substantial amount of model runs.

The weighting of models can also be based on expert opinion. Lehikoinen et al. (2012) evaluated the environmental risk caused by oil shipping. In their work, the uncertainty arising from multiple alternative models that predictied the emergence of hull breach in the case of a tanker collision was considered. The model-specific zero-leak probability estimates, the sources of which varied from statistical models to sophisticated dynamic impact models, were weighed based on an expert opinion, taking into account e.g. the specifications of the models and their suitability to the study area.

In some cases, e.g. within the meteorological and hydrological domains, enough data are available about the modelled phenomenon that a statistical assessment of the uncertainty related to the model outputs can be performed (Van Steenbergen et al., 2012). It is also possible to utilize data from cases that are not similar, but where some commonality can be assumed, such as across species or geographical areas. This approach has been used for estimating the parameter values, i.e. the probability distributions (e.g. Borsuk et al., 2001; Pulkkinen et al., 2011), but can similarly be used to evaluate the expected uncertainty around the model-predicted expected value. Model performance testing methods such as cross-validation and bootstrapping can also be considered to explore the variability in the model predictions and hence the model's robustness to variability in the data (see e.g. Bennett et al., 2013).

The data-based approach requires that the data utilized can be assumed to be representative of the modelled variable and situation. Therefore, this approach must be used with care with models that simulate the consequences of yet-unseen management options, climate scenarios, etc.

@&#CONCLUSIONS@&#

Decision support models can prove very valuable in the management of complex environmental problems, as they may effectively summarize and bring together various, distinct consequences related to alternative management measures. As the decisions should be made based on prevailing knowledge but also acknowledging the gaps in it, transparent representation of uncertainty is recommendable on each level of these models. Those decision support models that include the uncertainties related to the management options may be of considerable added value for the decision maker. Here, the decision support modeller has to be very careful, however, to account for the uncertainties as honestly as possible. Including part of the uncertainties while ignoring others may lead to results that are not only incorrect but also severely misleading for the decision maker, and therefore lead to unintended and undesirable management decisions. Therefore, it needs to be considered carefully whether the new model is actually an improvement to how the decisions have been made previously. If incomplete or biased decision support model replaces the use of common sense and caution exercised earlier, the result might actually be for worse rather than for the better. Therefore, when moving from deterministic to uncertainty-specific environmental decision support models, it is important to evaluate the extent of uncertainties in a way that is justified and transparent, and to consider carefully the assumptions behind both the decision-support model and the models that are used to provide inputs to it, to make sure that the variables in the models are compatible. In this paper, we have provided guidelines as to what these approaches could be, and what needs to be taken into account when applying them.

Every model is just “a stylized representation or a generalized description used in analysing or explaining something” (Hilborn and Mangel, 1997). Thus, how much effort and resources should be put on translating a deterministic model to probabilistic form is clearly a case specific question. A cost-effective solution might be some kind of tiered approach, segregating the analysis in tiers where the level of model complexity or resolution is gradually increased (Smith et al., 2007; Hobday et al., 2011; Tighe et al., 2013). It could also mean starting with only light prior assumptions about the qualitative causalities in the model. With the simpler model version(s) the decision modeller could then analyse, e.g. by conducting a value of information analysis (Clemen and Winkler, 1985), what elements in the model actually are those affecting the ranking order of the analysed decisions most. That result could be used for directing the resources during the rest of the project.

@&#ACKNOWLEDGEMENTS@&#

The writing of this paper was partly supported by the TALENTS project at the Finnish Environment Institute and the project MIMIC at the University of Helsinki, which is gratefully acknowledged. The authors wish to express their gratitude to Dr. Mark Borsuk and three anonymous reviewers for insightful and constructive comments that helped improve the manuscript considerably, and to Dr. Jarno Vanhatalo for checking the model emulation sub-chapter.

@&#REFERENCES@&#

