@&#MAIN-TITLE@&#Detecting tympanostomy tubes from otoscopic images via offline and online training

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A new system is developed to detect tympanostomy tubes in otoscopic images.


                        
                        
                           
                           Image features are derived to reflect the characteristics of tympanostomy tubes.


                        
                        
                           
                           A 3-layer cascaded classifier is trained in an offline training process.


                        
                        
                           
                           A real-time refinement process is designed to improve the classifier at the point of patient care.


                        
                        
                           
                           The proposed system achieves high detection accuracy in an empirical study.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Object detection

Tympanostomy tube

Otoscopic image

Cascaded classifier

Support vector machine

@&#ABSTRACT@&#


               
                  Graphical abstract
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Tympanostomy tube placement is the most common outpatient surgical procedure in the United States. Each year more than 650,000 children younger than 15 years of age receive tympanostomy tubes [1]. Tympanostomy tubes are commonly inserted because of persistent middle ear fluid affecting hearing, frequent ear infections or ear infections that persist after antibiotic therapy [2]. All of these conditions are composed of the term otitis media, which is second in frequency only to upper respiratory infection as the most common illness diagnosed in children by pediatric health providers. Most children will experience at least one episode of acute otitis media by the age of 3years and by the age of 6years, nearly 40% have experienced three or more infections [3]. Otitis media with effusion (OME) can resolve spontaneously, however for patients in which, there is persistent middle ear fluid for at least 3months with the decrease in hearing, further treatment in the form of tympanostomy tubes insertion may be recommended [2]. Fig. 1
                      illustrates the placement of the tympanostomy tube on the ear drum.
                        1
                     
                     
                        1
                        The left graph of Fig. 1 is downloaded from www.kidshealth.org.nz.
                     
                  

Since first described in 1954 by Armstrong, tympanostomy tube placement has become the surgical treatment of choice for otitis media [4]. Placement of tympanostomy tubes improves hearing significantly in the presence of otitis media with effusion, reduces the incidence of recurrent acute otitis media (AOM), and provides a mechanism for drainage and administration of topical antibiotic therapy for acute otitis media. The latter has gained significant importance since it allows localized treatment for ear infections rather than systemic antibiotics use. The insertion of tympanostomy tubes involves aspiration of the middle ear fluid leading to instant improvement of hearing thresholds. The tympanostomy tubes are designed to extrude naturally from the tympanic membrane normally within 6months to 1year following the procedure.

Following tympanostomy tube placement, regular scheduled follow-ups are recommended by the American Academy of Otolaryngology every 6months to check the status of the ear tubes [5]. This follow up care is currently performed by an ear, nose and throat specialist resulting in increased cost compared to the cost of a visit to a general practitioner [6]. The overall complexity of the procedure is low because it mostly identifies the presence and patency of the tympanostomy tube. If this procedure can be performed by an automated computer program, it can significantly reduce cost as well as enhance the clinical efficiency of both ear nose and throat specialists and general practitioners.

Computer vision techniques have been widely used to automatically detect an object in natural images such as human faces [7–10], cars [11–13] and pedestrians [14–16]. The related techniques are also used to analyze medical images which have largely shown to facilitate the diagnosis and treatment decisions [17–23]. In this study we have developed a computer vision system to predict on an image if a tympanostomy tube is in place and collect feedback to adaptively improve the system classification performance. In this system, we extract a set of image features including RGB intensity features, edge-map based features and other advanced features such as Scale-invariant Feature Transform (SIFT) [24], and Histograms of Oriented Gradients (HOG) features [25]. We use them to learn a discriminative model to determine if an otoscopic image presents a tympanostomy tube. A cascaded classifier is constructed by the support vector machine (SVM) [26] algorithm from labeled image patches. The prediction of this classifier, when applied to a test image, is visualized. A refinement process is designed to refine this trained classifier at the point of patient care according to user feedbacks. Extensive experimental results demonstrate the effectiveness and efficiency of the proposed approach.

@&#RELATED WORKS@&#

Most object detection techniques are composed of two major aspects, feature extraction and model learning, for each of which various methods have been proposed. Readers can consult with [27,28] for an overview and challenges of the field. In the subsequent paragraphs, we briefly review several of the most relevant methods.

Region based and edge-map based features are the two types of widely used features for object detection. For the first type, the features are generated from colors [29–32], or the varying distribution of intensities [33,34,24,25]. Among these features, the SIFT [24] and HOG [25] features are the most widely used nowadays. Other algorithms have considered texture information, such as gray level co-occurrence matrix (GLCM) [35], local binary patterns (LBP) [36] and wavelet texture [37,12]. We explored these features in our system. The edge-map based features are used to capture contour shapes by computing the most representative edge fragments [38–41]. These features are robust to occlusion but are not invariant to illumination conditions. Geometrical shapes and the structure of lines and arcs in an edge map were also studied in [42–44], which facilitated to obtain more reliable features. It has been shown that combining the region based features and edge-map features may lead to robust detection [45,46]. We hence implemented feature extraction methods in both of the categories.

Various machine learning algorithms have been applied to build probabilistic models for prediction of object classes. Typical methods include Bayesian classifiers [47], expectation maximization [48], k-Nearest Neighbor [49], logistic regression [50] and support vector machines (SVM) [26,51]. These methods perform comparably although some may serve certain specific purposes, such as selecting features for use in the model. In this work, an SVM algorithm with linear classifiers was used and served as a good learning model for detecting the tubes. The SVM algorithm solves a quadratic programming optimization problem for a predictive classifier that has an optimal margin to separate different classes of examples.

Besides the two major components discussed above, online learning methods that sequentially take user feedback in building a classifier have been used to detect and track moving objects [52–55]. Our system also consists of a real-time refinement process. Although similar to an online learning method, our refinement targeted at the unseen examples that the offline models fail to predict. After the system highlights the predictions on an image, the users can correct the prediction results on the image. Our system is capable of recording this feedback and retraining the related classifiers.

@&#MATERIAL AND METHODS@&#

In this section, we describe our approach to automatically predicting if an otoscopic image of ear drum has a tympanostomy tube mounted. Fig. 2
                      provides an overview of our detection system. In this system, an image is first converted into a feature vector in the feature extraction process. Then a classifier that has been pre-trained and stored in a database (which we call knowledge database) is applied to the image features to predict the presence and location of a tube. The results are then visualized and highlighted in the image. Users can give feedback by correcting the predictions if they are inaccurate. This feedback message is then recorded and used in an automatic process to refine the classifiers stored in the knowledge database. This system is constructed based on two processes: an offline learning process to construct the classifiers in the knowledge database; and an online refinement process that takes in user correction information to refine the classifiers.

The construction of a knowledge database aims to incorporate domain knowledge and data-driven classifiers derived from labeled images. During the offline classifier training, we collected otoscopic images of patients from Connecticut Children׳s Medical Center (CCMC). These images are labeled manually with respect to whether or not it shows a tympanostomy tube. We build classifiers, as functions of image features, to distinguish images with a tube from images without. The usefulness of the various features extracted from the images is examined by building classifiers with different combinations of features. We propose a new cascaded classifier which is composed of three layers. These layers are designed according to the domain knowledge. For instance, we have observed that many tympanostomy tubes are in green color. It is hence important to design an efficient classifier to detect green tubes which might also be easier given the target is more characterized and focused on the green channel of the image. Each layer aims to detect tympanostomy tubes of a different characteristic. For each layer of the cascaded classifier, the sample of labeled images was split to have training and test sets.

During the real-time system refinement, a new image is given to the system, which first converts it to a feature vector. The classifiers in the database can make a prediction based on the feature vector. Then the system yields a marked image that highlights the results. If the classifier fails to point out the presence of a tube (false negative) or predicts as it contains a tube but gives a wrong location, a user may mark the correct location of the tube. If the classifier claims a tube in the image but there is actually not (false positive), a user can remove the incorrectly located markers. Then this image (actually its feature representation) will be added to the training set stored in the knowledge database and all the three layers of the cascaded classifier will be retrained.

We describe the three sets of features that are extracted from the otoscopic images: color-based features, edge-map based features and a set of advanced features.

Tympanostomy tubes are built with conspicuous colors that are different from the tissues normally seen in the ear. For example, green, purple, blue or white are often used for the tympanostomy tubes. For an image of true color, each pixel is typically represented by the component intensities of red, green and blue. We compute the intensity of the three channels in the image and then measure the “directional” difference of the intensities between any two channels. For instance, the difference from the red channel to the green channel is calculated by subtracting the intensity of the red channel from that of the green channel. If the difference along a direction is a negative value, we re-set it to 0. We exclude the pixels in the completely dark regions in the redundant margin of the image. We hence obtain 6 quantitative values of the difference at each pixel. The mean, maximum and standard deviation of each of the 6 differences are computed over all valid pixels in an image, which gives us 3×6=18 features for each image. These features are used to predict tympanostomy tubes on the basis that the images containing a tympanostomy tube have different RGB intensity distributions than the normal ear images.

The RGB intensity features may be most useful for distinguishing a specific kind of tubes. For instance, we notice that most of our clinical images show tympanostomy tubes of green color. Hence, we could consider detecting the green tubes in the first layer of the cascaded classifier. Fig. 3
                            illustrates the RGB features by an example of the original image and two images of the difference computed using the intensities of the green channel minus those of the red and blue channels, respectively.

An obvious characteristic of the tympanostomy tube is its circular shape if it is viewed from the side, which is also the view of the tube in the ear images. We hence design features to capture circular structures presented in an image. For each pixel, we calculate the entropy value based on its 9×9 neighborhood, which measures the “business” of that region. The pixels along an edge are expected to have higher entropy values than those far from an edge. All entropy values are normalized to the range of 
                              [
                              0
                              ,
                              1
                              ]
                           . Since we care about the pixels that have high entropy values, we set a threshold of 0.8. More precisely, if a pixel has an entropy of lower than 0.8, the pixel׳s entropy will be set to 0; or otherwise set to 1. This step converts an ear image into a binary image showing the edges, which is called an edge map. We then compute the Hough-transformation [56] of the edge map and apply the Phase-Coding algorithm [57] to detect circles.

For detecting the center of a circle, we restrict the search to pixels at the center area of the image that is within 70% of its length and width. We search for circles of varying radius. Let the length of the short side of the image be L. The maximal limit of the radius is set to 
                              L
                              ×
                              [
                              
                                 
                                    2
                                 
                                 
                                    −
                                    1
                                 
                              
                              ,
                              
                                 
                                    2
                                 
                                 
                                    −
                                    2
                                 
                              
                              ,
                              
                                 
                                    2
                                 
                                 
                                    −
                                    3
                                 
                              
                              ,
                              
                                 
                                    2
                                 
                                 
                                    −
                                    4
                                 
                              
                              ]
                            and the corresponding minimal length is one-third of the maximal limit. Empirical experiments show that the circles with radius in the range of 
                              [
                              L
                              /
                              10
                              ,
                              L
                              /
                              3
                              ]
                            can match the size of tympanostomy tubes in most of the images. By setting up these limits, we can screen out most of the unwanted circles. Fig. 4
                            shows an exemplar image (Fig. 4(a)) that is transformed to a edge map (Fig. 4(c)) via entropy computation (Fig. 4(b)) and a circle shows the detected location of a tympanostomy tube.

To further prune circles that do not correspond to a tympanostomy tube, we crop image patches within each detected circle, and extract the complete set of RGB intensity features from each patch. Let T
                           
                              i
                            denote the number of detected circles in the i-th image, and 
                              
                                 
                                    α
                                 
                                 
                                    i
                                 
                                 
                                    j
                                 
                              
                            contains the RGB features for the j-th patch in the i-th image. Let 
                              
                                 
                                    β
                                 
                                 
                                    i
                                 
                                 
                                    j
                                 
                              
                            denote the RGB features of the rest area in the image. We quantify a numerical feature for each image i based on the circle detection as 
                              max
                              {
                              
                                 Euclidean
                                 (
                                 
                                    
                                       α
                                    
                                    
                                       i
                                    
                                    
                                       j
                                    
                                 
                                 ,
                                 
                                    
                                       β
                                    
                                    
                                       i
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 ,
                                 
                                 j
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 
                                    
                                       T
                                    
                                    
                                       i
                                    
                                 
                              
                              }
                           , which is to compute the Euclidean distance between the RGB feature vectors of each cropped patch and the complement of the patch, and then take the maximum distance across all detected circles. This feature reflects that the RGB intensity of the tube area should differ the most from its ambient surrounding. For the circle that attains this maximum distance, its center location and radius are also used as numerical features for the image. If no circles are detected, the distance feature is set to 0, the center coordinates are set to 
                              (
                              −
                              1
                              ,
                              −
                              1
                              )
                           , and the radius is set to 0.

Besides the features computed from RGB intensities and edge-maps to capture the color and shape of a tube, HOG [25] and SIFT [24] features are used by adopting the Bag of Visual Words method [58]. For HOG features, we extract 31 default HOG features from each cell of 8×8 pixels. Then, a cluster analysis via k-means partitions the cells of all images into 20 clusters. The centroids of the clusters form the words in a vocabulary. By counting the number of cells in each of the 20 clusters, each image is converted to 20 features. For SIFT features, we first compute 128 SIFT features from patches of 20×20 pixels using the default setting of SIFT. Then the same cluster analysis procedure is used to compute 20 SIFT-based features.

We extract texture-based features using the gray level co-occurrence matrix (GLCM). Four features are computed to characterize the GLCM including “Contrast”, “Correlation”, “Energy” and “Homogeneity” [35]. Local Binary Patterns (LBP) [36] and wavelet texture might also be useful for the detection of tubes. Fifty eight LBP features are extracted. For wavelet features, at first we perform a single-level two-dimensional wavelet decomposition of an image, which yields an approximation coefficient matrix, as well as horizontal, vertical, and diagonal coefficient matrices. We extract 8 wavelet features by computing the average values and standard deviations of the four matrices.

Besides the feature vectors of the training images, the knowledge database also stores a cascade of classifiers, which are first trained in an offline process based on collected training data, and then refined by an online process based on user feedback at the point of care.

The SVM algorithm is employed to train each layer of the cascaded classifier. Given n images, each characterized by a data point 
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                              )
                            where 
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    m
                                 
                              
                            is the feature vector of image i and y
                           
                              i
                            is the binary label indicating whether it shows a tube. A linear model 
                              
                                 
                                    w
                                 
                                 
                                    ⊤
                                 
                              
                              x
                              +
                              b
                            is used to classify images, where 
                              w
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    m
                                 
                              
                            and b are the weight vector and the bias term of the model, respectively. The SVM method minimizes the regularized loss function 
                              λ
                              ‖
                              w
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                              +
                              
                                 
                                    ∑
                                 
                                 
                                    i
                                 
                              
                              
                                 
                                    ξ
                                 
                                 
                                    i
                                 
                              
                            for the best 
                              (
                              w
                              ,
                              b
                              )
                            where the losses ξ
                           i are defined by constraints 
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                              (
                              
                                 
                                    w
                                 
                                 
                                    ⊤
                                 
                              
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              +
                              b
                              )
                              ≥
                              1
                              −
                              
                                 
                                    ξ
                                 
                                 
                                    i
                                 
                              
                           , and 
                              
                                 
                                    ξ
                                 
                                 
                                    i
                                 
                              
                              ≥
                              0
                           , 
                              i
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              n
                            and λ is the regularization parameter. This optimization problem formulates a quadratic program, and can be solved using CPLEX solvers [59]. After the linear model is constructed, it can be used to determine the label of a new image represented by 
                              
                                 
                                    x
                                 
                                 
                                    ^
                                 
                              
                            by computing 
                              sign
                              (
                              
                                 
                                    w
                                 
                                 
                                    ⊤
                                 
                              
                              
                                 
                                    x
                                 
                                 
                                    ^
                                 
                              
                              +
                              b
                              )
                           .

The cascaded classifier consists of three layers as shown in Fig. 5
                           . Classifiers of the three layers are trained separately with different sets of features. The first layer is designed to detect the green-color tubes. Since the green tubes are widely used and commonly seen in otoscopic images, a simple classifier capable of detecting them would facilitate the detection task and improve the efficiency. RGB features related to the green channel are extracted from the original images and used in the SVM training. In our evaluation, the resultant classifier proved to be effective and detected all green tubes in the otoscopic images that were collected at Connecticut Children׳s Medical Center (CCMC).

Images downloaded from the internet contained tubes of many other colors. We use all internet images and CCMC images with non-green tubes to train the second layer of the cascaded classifier. The second layer is designed to not only utilize the color property of the tympanostomy tubes but also their shape information. We train a SVM model using all RGB intensity features and the shape features from edge-maps, and hence each image is represented by a vector of 
                              18
                              +
                              4
                              =
                              22
                            features.

In the third layer of the classifier, additional features as described in Section 3.1.3 are extracted. We have also experimented with each type of the advanced features by combining them individually with the basic RGB and shape features in training the third classifier.


                           Fig. 6
                            shows the test process after the cascaded classifier is constructed. If the first classifier classifies the image as a positive example, meaning that it contains a tube (likely a tube of green color), then the system outputs the prediction, and directly goes to the visualization step. Otherwise, this image is moved to the second classifier, and then to the third. If any of the classifiers accepts this image as containing a tube, the system will come to a final decision. If all three classifiers predict the image negative, then the system will report it as not containing a tube.

The real time refinement step allows the user to adjust the classifier by providing feedbacks to the system. The motivation of designing this refinement step comes from the fact that otoscopic images may show tympanostomy tubes with significantly varying colors, shapes, illuminance and scales when they are taken by different clinicians, or collected from different healthcare organizations. Clinicians may have different gestures when shooting the pictures. As a result, tubes are presented distinctly in angles and scales in the images from different practices. It is hence desirable to have a system that is able to adapt the classifiers to the specific setting of an organization.

However, this refinement step is not a necessary component for the system to perform. The refinement will be triggered only when a user corrects the system prediction on a test image. Hence, it runs in a semi-automatic mode in the sense that as soon as a user makes correction and saves it, the system will store the corrected data into the database and retrain the classifiers.


                           Fig. 7
                            shows the refinement procedure. If the system misclassifies an image that actually contains a tube, the user can change the label of the image to be “+1”, and specifies the center and radius of a circle to properly mark out the tube region. If the system predicts correctly but fails to mark the tube location correctly, there exist two cases. The first case is that the tube is largely visible on the image but the system marks a wrong position. The user can correct those markers. The second case is that the tube is obscured badly by wax or other structures. In this case, no circles may be seen on the image, but the user may still leave a marker to the area that he perceives to contain the tube. If an image does not contain a tube, but the system makes a false positive prediction, the user can just correct the label and remove all the markers on the image. Based on the feedbacks, the system will automatically refine the extracted features by adjusting the parameters used in feature extraction algorithms.

The three layers of the cascaded classifier will be retrained using the augmented training set that includes the test image. Note that to retrain classifiers, the system does not need to hold the raw images in the knowledge database but only the extracted feature vectors of all training images and the specific test image. The database may increase over time when more and more test images receive corrections, but the storage of the image features only requires a moderate amount of space. The size and model parameters of the classifiers will be updated in the knowledge database once retrained.

The proposed system was validated on otoscopic images collected from two sources: 235 images of real patients collected from CCMC and 40 images downloaded from internet. The CCMC images were taken with high quality and high resolution, and 77 of them contained a tube. The internet images from Google or YouTube had overall low quality: low resolutions with higher level of artifacts and distortion, and 27 of them showed a tube.

We randomly selected 40 images to form a hold-out set in order to evaluate the system performance, including evaluating both the offline training and the online refinement. We excluded 20 images from offline classifier training and used them to simulate the online refinement process. The rest of 215 images were partitioned into 3 even subsets for use in a three-fold cross validation (CV) procedure. We stratified the partition so that each of the above sets of images (totally, 5 of them) had equal ratios of positive versus negative images and CCMC versus internet images.

To train a classifier, the regularization parameter λ used in the SVM algorithm was tuned using the three-fold CV process where two of the CV subsets were used to train a classifier with the pre-chosen value of λ, then the remaining subset was used to test the resultant classifier. We ran the CV process with each choice of λ from the pre-chosen values [
                           
                              
                                 2
                              
                              
                                 −
                                 14
                              
                           
                           ,
                           
                              
                                 2
                              
                              
                                 −
                                 8
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 2
                              
                              
                                 0
                              
                           
                           ,
                           
                              
                                 2
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 2
                              
                              
                                 14
                              
                           
                        ]. The choice of λ that gave the best test classification performance was used in the SVM algorithm to train the final classifier with the full training data. The three classifiers in the cascade were all trained separately using the above CV process. Note that the first-layer classifier was trained with positive images of only green tubes whereas the second-layer classifier was trained with those positive images excluded. All 215 images were used to train the third-layer classifier.

In the classifier cascade, the first layer used 6 RGB intensity features, and the second layer used 22 features including 18 RBG features and 4 edge-map based features. For the third layer, 20 HOG, 20 SIFT, 58 LBP, 4 GLCM and 8 Wavelet features were extracted. Together with the 22 baseline features used in the second layer, we considered totally 132 features in this layer. However, we observed that using all of the advanced features did not necessarily improve the detection accuracy. Hence, we experimented with each set of the advanced features by combining the 22 baseline features with one set at a time to find the best classifier for the third layer.

@&#RESULTS@&#

In the first two layers of the classifier cascade, final decisions were made only for those images that were classified as positive by any of the two classifiers. Hence, sensitivity (true positive rate) was evaluated at the end of the second layer, but the final specificity (true negative rate) would have to be evaluated until the full cascade was applied. We observed that the 22 baseline features were useful because the first two classifiers, by themselves, already achieved a sensitivity of 78% as estimated in the CV (averaged over the test sets of the three CV folds), and also an overall accuracy 87% if treating all non-classified images as predicted negative at the second layer.


                        Table 1
                         shows the CV performance of the full cascaded classifier where we tested the classifier separately on the set of high quality CCMC images and the enlarged set including low quality internet images. The best validation error rates were in bold fonts. From Table 1, we see that each of the advanced feature sets can improve the classification accuracy from the baseline features (from the sensitivity 78%). The classifiers using additional HOG and Wavelet features achieved the same validation error on the CCMC image set but one had slightly better sensitivity and the other slightly better specificity. The classifier using additional Wavelet features had slightly better overall performance when tested on the enlarged image set.

We also tested classifiers that were based only on the advanced features with a variety of feature combinations. The classification errors ranged from 30% to 45%. This showed that although these advanced features improved our classifiers, they were by themselves not as effective as the baseline features that we proposed to use (because the first two classifiers using the proposed features achieved an overall accuracy of 87%).

To further measure the classifier performance, the receiver operating characteristic (ROC) curves were drawn to show validation performance in the CV in Fig. 8
                        . The ROC plot is widely used to characterize the performance of a classifier where the area under the curve (AUC) is a statistic indicating the overall classification performance. We observed that the classifier with SIFT features showed comparable performance on the high quality images (shown in Fig. 8(a)), but dropped its sensitivity on the enlarged image set at the region of low false positive rate (shown in Fig. 8(b)). Other classifiers performed similarly on the enlarged data set.

To tune the hyper-parameter in the SVM algorithm, Fig. 9
                         shows how the classification performance varies when the regularization parameter changes in the range from 
                           
                              
                                 2
                              
                              
                                 −
                                 14
                              
                           
                         to 
                           
                              
                                 2
                              
                              
                                 14
                              
                           
                         for each layer of the classifier. We used the parameter choices that gave the best CV performance for each feature set in the experiments and also used them in the refinement system. After a proper value of λ was chosen, a classifier was obtained by running SVM on the full training data, and then tested on the hold-out set with the test performance shown in Table 2
                         (top section).

The classifier was then refined using the 20 images we held for the refinement purpose. We conducted a simulation of how the system will be refined when deployed into a clinical use. The 20 images were randomly ordered and shown to the system in sequence. After each image was tested, and if a user (we) made correction, the system adjusted the features based on the user feedback on the image and retrained the classifiers. After all 20 images were used, the resultant classifier was tested on the hold-out set again. Table 2 (bottom section) shows the refinement performance.


                        Table 2 shows that the offline-trained classifier with additional HOG features achieved the best classification error rate and best specificity. Then, the refinement step based only on 20 test images could already improve the detection performance by 3–5%. The best classification error rate was obtained by the classifier using the baseline and HOG features. This classifier׳s accuracy was increased by 7% in sensitivity and 1% in specificity by the refinement step.


                        Fig. 10
                         visualizes exemplar images after the system makes predictions for an image. The identified tubes were marked by the red circles detected from edge-maps. It can be seen that our system was able to detect the tubes of a variety of colors, as well as tubes in different orientations or partially covered by obstructs. Two images that were errors of the offline-trained system are also shown in Fig. 10 and the errors were corrected during the refinement phase. Fig. 11
                         shows an image used in the refinement step. The offline-trained system was not able to correctly locate the tube (shown in Fig. 11(a)), but a user marked out the tube area with a blue circle (in Fig. 11(b)). After the system retrained the classifier, the tube was correctly identified in this image.


                        Fig. 12
                         reports the run time of the proposed system when running it in a Dell Precision T3500 machine. Fig. 12(a) shows the average run time of feature extraction for an image in which we compared the run time of extracting features from RGB intensities and from edge maps. Since our empirical experiments show that the system achieves the best performance while integrating HOG features, we also included the averaged run time for computing HOG features in Fig. 12(a). Fig. 12(b) shows the run time for training each layer of the cascaded classifier when all 215 training images are used. The refinement step takes a similar cost to that of training due to its incremental nature. It can be seen that classifier training only takes a small portion of the total run time, which also ensures the efficiency of the real-time refinement. From Fig. 12, the most time-consuming step lies in the feature extraction, and specifically in the edge-map feature computation. This is because we repeatedly detect circles in an image using a variety of radius ranges.

Overall, our system was able to effectively detect tympanostomy tubes out of different sets of images with varying presence of tympanostomy tubes, and with varying degrees of image qualities. When presented with a test image, the system would mark around the area where it thought the tube was located (see Figs. 10 and 11 for examples). On the high quality images we obtained a specificity above 95% using all three classifiers. Our sensitivity was also high with only SIFT features having a sensitivity below 90% at 88%. There was a drop in both the sensitivity and specificity when the classifier was tested on a mixed set of high quality and internet images with sensitivities and specificities reducing to 89% and 95.5% on average. The classifier using the baseline plus the HOG features achieved the best performance on both the high quality CCMC image set and the enlarged image set after classifier refinement. This performance was boosted to 92% and 93%, respectively, in sensitivity and specificity on the enlarged image set. Based on the experimental results, the HOG features (as shown in Table 2) and Wavelet features (as shown in Table 1) may be more useful in detecting the tympanostomy tube than other advanced features.

@&#DISCUSSION@&#

Tympanostomy tube placement is the primary surgical intervention for otitis media, which is a pediatric health problem worldwide. In a cross-sectional questionnaire study of 40,000 Norwegians, the estimated life-time prevalence of tympanostomy tube surgery was about 12% [60]. Children under the age of 7years of age are at increased risk of otitis media due to the combination of an immature immune system and a poor function of the Eustachian tube [61]. The Eustachian tube is a slim connection between the back of the nose and the middle ear space and equalizes pressure with the external environment. Tympanostomy tubes bypass the Eustachian tube by allowing equalization of middle ear pressures with the outside environment. Placement of tympanostomy tubes significantly improves hearing, reduces the prevalence of middle ear effusion and may reduce the incidence of recurrent acute otitis media.

With well over 650,000 performed each year in the United States at an approximate cost of $2700 per surgery [62] the contribution to health care costs is approximately 1.8 billion [2]. This, however, does not account for follow up care. Follow-up of a tympanostomy tube surgery is normally performed to verify the position of the tympanostomy tube in the tympanic membrane and assess the patency of the tympanostomy tube. This examination is performed using an otoscope which is an instrument that allows illumination of the tympanic membrane with magnification which is interpreted in succession by a physician.

The American Academy of Otolaryngology and Head and Neck Surgery recommends the initial control within one month after tube placement and then at least once every six months until the tubes extrude. Current check ups are performed by ear nose and throat specialists at a significant financial cost for a low complexity visit. Austad et al. [6] studied the possibility of utilizing primary care physicians for tympanostomy tube check-up as a cost saving measure with no difference in audiological and subjective hearing outcomes two years after the surgery. There are however complications that can arise from tympanostomy tube surgery such as otorrhea (drainage coming through the tympanostomy tube) and tympanic membrane perforation, which physicians must be aware and vigilant not to miss [63].

Although computer aided diagnosis has been around for well over a decade, its role has been mostly limited to the fields of radiology and pathology with little participation in the process of real time medical decision making with patients. We believe there is the potential of computer image interpretation to perform simple tasks allowing direct participation in patient care. Identifying the presence of tympanostomy tubes may offer an alternative to a physician follow-up visit in the future for patients with tympanostomy tubes. Recent studies have revealed a low up rate of post operative follow up compliance after placement of tympanostomy tubes [64] and new commercially available technologies including adaptable otoscopes to cell phones cameras can provide those images from the comfort of the patient׳s home. A clinical trial is currently being considered to evaluate the clinical usability and efficiency of our computer vision system. In the trial, we are not proposing complete replacement of the physician by machine image recognition algorithms but for patients with an uncomplicated course it may obviate an unnecessary trip to the physician׳s office. It can be an intermediate step in between visits and reducing the cost of the follow up and it may help with the overall follow up compliance.

We have identified several factors that make the identification of tympanostomy tubes a good fit for computer vision. First tympanostomy tubes consistently have an easily identifiable circular shape and second they are usually made of bright color plastic compounds usually green, blue, or black, which are distinguishable from any human tissue. In this study we have successfully implemented a computer vision system to approach the problem of identifying tympanostomy tubes from otoscopic images. We incorporated a variety of ear tubes of various colors and in various locations within the tympanic membranes achieving adequate detection rate in up to 90% of cases. We have also used images of various quality to test reliability of our methodology even with less than ideal images.

This study represents the first attempt at incorporating computer vision as a low cost alternative for a low acuity common visit of tympanostomy tube check-up. There are, however, limitations to our technique which are common to otoscopic examinations such as limited visualization due to cerumen. The current real-time refinement step retrains the system׳s classifier as long as a user feedback is received. A more robust system may need to select among the test images corrected by users to evaluate if inclusion of such an image in classifier training will enhance system performance. Another limitation would be to adequately identifying patency of the tympanostomy tubes since adequate visualization of the lumen of the ear tube may not be satisfactorily achieved without multiple images from various angles which we have not attempted in this study. Future directions may involve utilization of multiple wavelengths to achieve different depth of penetration of light through the tympanic membrane and contrast enhancement as well as multiple images of the same ear at different angles to be able to assess for patency.

@&#CONCLUSIONS@&#

We have proposed and developed a computer vision system for automatically detecting a tympanostomy tube in an otoscopic image of the ear drum. The proposed system comprises an offline classifier training process and a real-time system refinement process. The offline learning process creates a cascaded classifier with 3 layers that is designed according to our domain knowledge and observation on the various characteristics of tympanostomy tubes. A state-of-the-art supervised learning method, support vector machine, has been used to train the cascaded classifier that is then stored in a knowledge database of the system. The refinement process allows users to interact with the system aimed at system adaptation over time to a user׳s specific setting. It seeks user feedback on the prediction result of a test image during the point of check-up. Based on the user feedback, the system can automatically augment the training set by including the features extracted from the test image to refine the classifiers stored in the knowledge database. We propose the method to extract the RGB intensity features and the combination with edge-map based features to improve the quality of image representation. Other advanced but commonly used features are also examined in our system. Our empirical results show that the offline learning system can achieve a detection accuracy reaching 90% and the real-time refinement process can improve the performance of the classifier by another 3–5%.

None declared.

@&#ACKNOWLEDGMENTS@&#

This work was supported by the Connecticut Institute for Clinical and Translational Science (CICATS). Jinbo Bi and her student Xin Wang were also supported by the research grants from National Science Foundation of United States (IIS-1320586, DBI-1356655, IIS-1407205 and IIS-1447711).

@&#REFERENCES@&#

