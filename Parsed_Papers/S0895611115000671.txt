@&#MAIN-TITLE@&#Predictive sparse modeling of fMRI data for improved classification, regression, and visualization using the k-support norm

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Application of k-support norm regularization to fMRI analysis.


                        
                        
                           
                           Experimental validation in cocaine addiction tasks.


                        
                        
                           
                           Classification and regression experiments, including challenging cross-subject tasks.


                        
                        
                           
                           Our results support the generalizability of the I-RISA model of cocaine addiction.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

FMRI

Sparsity

Regularization




                     k-Support norm

Cocaine addiction

@&#ABSTRACT@&#


               
               
                  We explore various sparse regularization techniques for analyzing fMRI data, such as the ℓ1 norm (often called LASSO in the context of a squared loss function), elastic net, and the recently introduced k-support norm. Employing sparsity regularization allows us to handle the curse of dimensionality, a problem commonly found in fMRI analysis. In this work we consider sparse regularization in both the regression and classification settings. We perform experiments on fMRI scans from cocaine-addicted as well as healthy control subjects. We show that in many cases, use of the k-support norm leads to better predictive performance, solution stability, and interpretability as compared to other standard approaches. We additionally analyze the advantages of using the absolute loss function versus the standard squared loss which leads to significantly better predictive performance for the regularization methods tested in almost all cases. Our results support the use of the k-support norm for fMRI analysis and on the clinical side, the generalizability of the I-RISA model of cocaine addiction.
               
            

@&#INTRODUCTION@&#

Functional magnetic resonance imaging (fMRI) is a widely used modality, within the field of neuroimaging, that measures brain activity by detecting associated changes in blood oxygenation. One of the goals of fMRI data analysis is to detect correlations between brain activation and a task the subject performs during the scan.

The main challenges in statistical fMRI data analysis [1–4] are (i) the curse of dimensionality, (ii) a small number of samples, due to the high cost of fMRI acquisition, and (iii) high levels of noise, such as system noise and random neural activity.

A general approach for analyzing functional magnetic resonance imaging (fMRI) data is based on pattern recognition and statistical learning. By predicting some cognitive variables related to brain activation maps, this approach aims at decoding brain activity. This approach takes into account the multivariate information between voxels and is a way to assess how precisely some cognitive information is encoded by the activity of neural populations within the whole brain. However, this approach relies on a prediction function that is plagued by the curse of dimensionality, since there are generally far more features (voxels) than samples. To address this problem, different methods have been proposed, such as, among others, univariate feature selection and regularization techniques [5].

Sparsity regularizers are key statistical methods for improving predictive performance in the event that the number of observations is substantially smaller than the dimensionality of the data while the underlying signal is known to be sparse. This is the case in fMRI analysis where brain activity is known to occur in only a subset of regions for a given task. In this paper we compare the most frequently applied sparsity regularizer developed in the statistics literature, LASSO [6] and its extension the elastic net [7], with the k-support norm [8], a recently introduced method which tends to retain correlated variables while simultaneously enforcing sparsity.

The k-support norm has an intrinsic parameter, k
                     ∈{1, …, d}, where d is the dimensionality of the data, that controls the degree of sparsity. When used with squared loss, k-support regularization specializes to the LASSO when k
                     =1 and ridge regression when k
                     =
                     d. The k-support norm has previously been used in [8] for classification. We first evaluate the k-support norm in an fMRI volume classification setting in which we predict a binary task, based on an fMRI volume. We then extend this analysis to a regression problem, predicting a task-variable based on the fMRI volume.

We focus on comparing LASSO and elastic net with the k-support norm in order to establish the latter regularizer's superiority in analyzing fMRI data in the context of a classification task. We then consider a regression setting and use two loss functions, namely the squared error and the absolute error functions. The advantage of the absolute error loss is that it is more robust, in that it penalizes outliers less than the squared loss, while still retaining convexity, which guarantees finding the global optimum. In this setting we compare ℓ1 regularization with the k-support norm and demonstrate marked improvement. We compare the methods not only in their predictive accuracy but also in the interpretability and stability of their results which are critical in fMRI data analysis.

This article is based on [9,10] and extends the presentation of k-support norm regularization of fMRI data into a single unified framework. Although we consider a specific neuroscience application of validating a model of human drug addiction, this approach is more generally applicable and can be used in many other neuroscience studies involving interpretation of fMRI data.

The primary neuroscientific motivation for most of our experiments in this article is the exploration of human drug addiction. Basic studies have led to a theoretical model of human drug addiction, characterized by Impaired Response Inhibition (RI) and Salience Attribution (SA) (hence, I-RISA) [11]. According to the model, the skew in SA is predictive of impaired RI, together contributing to excessive drug use and relapse, core clinical symptoms of cocaine addiction. We use the fMRI data from a SA task (drug Stroop) in order to predict behavioral data in a RI task (color-word Stroop) collected at a different time, hence providing further evidence to support the I-RISA model.

@&#METHODS@&#

A basis of statistical inference is the application of regularized risk, in which a loss function is evaluated over a sample of data and is linearly combined with a regularizer that penalizes some norm of the prediction function as in Eq. (1), where the first term is the loss function and the second is the penalty term:
                        
                           (1)
                           
                              
                                 min
                                 w
                              
                              
                              f
                              (
                              w
                              ,
                              X
                              ,
                              y
                              )
                              +
                              λ
                              J
                              (
                              w
                              )
                              .
                           
                        
                     
                  

Here we denote by 
                        X
                        ∈
                        
                           
                              
                                 ℝ
                              
                           
                           
                              n
                              ×
                              d
                           
                        
                      the design matrix of n samples each with d dimensions; we denote by 
                        y
                        ∈
                        
                           
                              
                                 ℝ
                              
                           
                           n
                        
                      the vector of targets. In the sequel, we assume that we have a sample of labeled training data 
                        {
                        (
                        
                           x
                           1
                        
                        ,
                        
                           y
                           1
                        
                        )
                        ,
                        …
                        ,
                        (
                        
                           x
                           n
                        
                        ,
                        
                           y
                           n
                        
                        )
                        }
                        ∈
                        
                           
                              (
                              
                                 
                                    
                                       ℝ
                                    
                                 
                                 d
                              
                              ×
                              
                                 
                                    ℝ
                                 
                              
                              )
                           
                           n
                        
                      where x
                     
                        i
                      is the output of a fMRI scan, and y
                     
                        i
                      is a ground truth label that we would like to be able to predict. The scalar parameter λ
                     >0 controls the degree of regularization and J is a scalar valued function monotonic in a norm of 
                        w
                        ∈
                        
                           
                              
                                 ℝ
                              
                           
                           n
                        
                     . Sparsity regularization is a key family of priors over linear functions that prevents overfitting and aids interpretability of the resulting models [6,8].

One of the most important sparsity regularizers is the LASSO [6], where 
                        J
                        (
                        w
                        )
                        =
                        ∥
                        w
                        
                           ∥
                           1
                        
                      and f corresponds to squared loss. In many learning problems of interest, LASSO has been observed to shrink too many of the 
                        w
                      variables to zero. In the presence of a group of highly correlated variables, LASSO may prefer a sparse solution. However including all correlated variables in the model could potentially lead to higher predictive accuracy [8] and more stable support recovery. The k-support norm address this problem by providing a way of calibrating the cardinality of the regression vector 
                        w
                      so as to include more variables.

In order to create a model which can make inferences about labels for new fMRI samples and provide a map of the key voxels we must specify an appropriate loss function. This specifies the prediction properties we are interested in obtaining. We must then specify an appropriate sparse regularizer which captures the a priori structure of the data. Finally, we must optimize the objective specified in Eq. (1) in order to obtain a brain map which can predict labels for new samples and provide insights on which brain regions are most associated with this prediction. Below we discuss each of these steps in detail.

Key to the mathematical understanding of sparsity regularizers is their interpretation as convex relaxations to quantities involving the ℓ0 norm, which simply counts the number of non-zero elements of a vector. The ℓ1 norm, which is the sum of the absolute values of the vector, is the convex relaxation of the ℓ0 norm, meaning it is the tightest sparsity norm that retains convexity, which is key for computational tractability. While the ℓ1 norm can therefore be interpreted as employing the convex hull of the ℓ0 sparsity regularizer, the elastic net is looser than the convex hull of a norm that combines ℓ2 regularization with sparsity [8]. However, one may employ the k-support norm, which is exactly the convex hull of that hybrid norm. A visualization of the k-support norm unit ball is given in Fig. 1
                        . We see that there is a non-differentiability of the norm, which restricts the set of optimization strategies that we may employ (cf. Section 2.3). The k-support norm can be computed as
                           
                              (2)
                              
                                 ∥
                                 w
                                 
                                    ∥
                                    k
                                    sp
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   k
                                                   −
                                                   r
                                                   −
                                                   1
                                                
                                             
                                             
                                                
                                                   (
                                                   |
                                                   w
                                                   
                                                      |
                                                      i
                                                      ↓
                                                   
                                                   )
                                                
                                                2
                                             
                                             +
                                             
                                                1
                                                
                                                   r
                                                   +
                                                   1
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                            
                                                               i
                                                               =
                                                               k
                                                               −
                                                               r
                                                            
                                                            d
                                                         
                                                         |
                                                         w
                                                         
                                                            |
                                                            i
                                                            ↓
                                                         
                                                      
                                                   
                                                
                                                2
                                             
                                          
                                       
                                    
                                    
                                       
                                          1
                                          2
                                       
                                    
                                 
                              
                           
                        where 
                           |
                           w
                           
                              |
                              i
                              ↓
                           
                         is the ith largest element of the vector and r is the unique integer in {0, …, k
                        −1} satisfying


                        
                           
                              (3)
                              
                                 |
                                 w
                                 
                                    |
                                    
                                       k
                                       −
                                       r
                                       −
                                       1
                                    
                                    ↓
                                 
                                 >
                                 
                                    1
                                    
                                       r
                                       +
                                       1
                                    
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       k
                                       −
                                       r
                                    
                                    d
                                 
                                 |
                                 w
                                 
                                    |
                                    i
                                    ↓
                                 
                                 ≥
                                 |
                                 w
                                 
                                    |
                                    
                                       k
                                       −
                                       r
                                    
                                    ↓
                                 
                                 .
                              
                           
                        
                     

We summarize the regularizers considered in this work in Table 1
                         below

The k-support norm is closely related to the elastic net, in that it can be bounded to within a constant factor of the elastic net, but it leads to different sparsity patterns. One can see from Eq. (2) that the norm trades off a squared ℓ2 penalty for the largest components with an ℓ1 penalty for the smallest components.

A difficulty in using sparse regularizers is that they tend to lead to non-smooth functions which can cause difficulties when using gradient based convex optimization procedures. For this class of functions proximal methods are a very popular way to quickly find optimal solutions with the bottleneck generally being the computation of the proximal mapping. Among many advantages of the k-support norm, it has an easy to compute proximal operator given in [8].

While initial experiments have shown promising results with the k-support norm for a range of machine learning problems [8], to the best of our knowledge the studies discussed here are the first applications to fMRI.

For classification we consider squared loss: 
                           f
                           (
                           w
                           ,
                           X
                           ,
                           y
                           )
                           =
                           ∥
                           y
                           −
                           Xw
                           
                              ∥
                              2
                              2
                           
                        . Here we set the labels for the discriminative task to y
                        ∈{−1, 1} and predict new examples, x
                        
                           n
                        , as 
                           
                              y
                              n
                           
                           =
                           sign
                           (
                           
                              x
                              n
                           
                           w
                           )
                        . In the regression setting we consider two loss functions: the squared error and the absolute error 
                           f
                           (
                           w
                           ,
                           X
                           ,
                           y
                           )
                           =
                           ∥
                           y
                           −
                           Xw
                           
                              ∥
                              1
                           
                        . Here y corresponds to the output task-variable. In practice, we approximate the absolute error with a Huber type smoothing around zero to ensure differentiability as described in [12]. The advantage of the absolute error loss in regression is that it is more robust, in that it penalizes outliers less than squared loss, while still retaining convexity which guarantees finding the global optimum.

Optimization of objectives containing sparse regularizers is not trivial since they generally contain non-smooth terms which are not compatible with classic optimization techniques such as stochastic gradient descent. Optimization of the LASSO and elastic net has been extensively studied in the literature [13,14]. The k-support norm is a relatively new approach and does not have extensive analysis with regard to optimization. However a proximal operator is provided in [8]. This is a fundamental building block of many non-smooth optimization techniques a popular one being Fast Iterative Threshold-Shrinkage Algorithm (FISTA) [8,15–17]. The method is designed for optimizing the sum of a smooth and non-smooth convex function. It requires only the gradient of the smooth function, a proximal operator for the non-smooth function, and an upper bound on the Lipschitz constant of the gradient of the smooth function. For each of the loss functions considered here, these quantities are known, and source code is available for download [12].

@&#EXPERIMENTAL RESULTS@&#

Results are presented on three fMRI datasets. The first consists of fMRI scans of a subject viewing a movie. The second and third datasets each consists of fMRI scans from control and cocaine-addicted subjects [1,18].

This dataset consists of a set of fMRI scans from a healthy subject in a free-viewing setting. Data collection was previously described in [19,20], while the pre-processing followed [21]. The discriminative task in the first data set is the prediction of a “Temporal Contrast” variable computed from the content of a movie presented to the subject [22]. This dataset was employed for preliminary quantitative evaluation due to its larger sample size.

The overall neuropsychological experiment, referred to as the fMRI drug Stroop task [23], follows a block design with each subject (either control or cocaine-addicted) performing the same task repeatedly, during a total of six sessions where there are two varying conditions: (i) the monetary reward, as well as (ii) the word that cues the task (which can be a drug word or a neutral word). The sessions consist of an initial screen displaying a monetary reward and then presenting a sequence of 40 words in 4 different colors (yellow, blue, red or green). The subject was instructed to press one of four buttons matching the color of the word they had just read. The subjects were rewarded for correct performance depending on the monetary condition. In our experiments we use sessions with the same monetary reward (50¢) and the only varying condition is the type of cue words shown (drug words or neutral words) leading to a total of two sessions per subject. The discriminative task is to determine whether a subject is cocaine-addicted or a healthy control subject [1,18].

The overall neuropsychological experiment follows a block design with each subject (either control or cocaine-addicted) performing the same task repeatedly, during a total of eight sessions where there are two varying conditions: monetary reward and cue word (drug word or neutral word). Individual sessions follow the same protocol as described in the Cocaine classification dataset. In this experiment the monetary reward varies (50¢, 25¢, 1¢ and 0¢) as well as the type of cue words shown (drug words or neutral words) resulting in a total of eight sessions per subject.

We use the behavioral responses of the same subjects in a color-word task [24], a classic task of inhibitory control. In this task the subjects indicated the ink-color of color-words printed in either their congruent or incongruent colors [24, Fig. 1(a)]. Four colors and words (red, blue, yellow and green) were used in all possible combinations. Both congruent and incongruent stimuli were presented randomly. The subjects performed four consecutive runs of this task. As there were 12 incongruent events in each run of 200 events, each subject's data contained up to 48 incongruent events. For 38 control subjects and 74 cocaine abusers, we use the fMRI data from the drug-word task, to predict color-word behavioral variables such as the difference in subject performance accuracy between congruent and incongruent events.

In our first experiment we use the free-viewing dataset in a classification task [22]. The performance of the different sparse regularization techniques, shown in Fig. 2
                        , is evaluated as the mean correlation over 100 trials of random permutation of the data described in [21]. In each trial, 80% of the data are used to train the method, while the remaining 20% are used to evaluate the performance. More specifically, Fig. 2(a) shows the mean correlation between LASSO and elastic net against the number of non-zero variables (i.e. voxels), while Fig. 2(b) shows the mean correlation for the k-support norm against different k values – which are correlated with the number of non-zero coefficients. LASSO achieves a maximum mean correlation of 0.1198 for 44 non-zero variables, elastic net a maximum mean correlation of 0.1189 for 866 non-zero variables, while k-support norm a maximum of 0.129 for k
                        =800. This is substantially higher than was previously reported in [22].

Next we evaluate interpretability in the classification setting for the cocaine classification dataset. We use 16 cocaine addicted individuals and 17 control subjects. These were the subjects that complied to the following requirements: motion <2mm translation, <2° rotation and at least 50% performance of the subject in an unrelated task [18]. We visualize the brain regions predicted when applying the LASSO and the k-support norm to these data. For each, we have selected slices through the brain that maximize the sum of the absolute values of the weights predicted by the respective methods. These results are presented in Fig. 3
                         and discussed in the next section.

The main area of activity shown in Fig. 3(b) is the rostral anterior cingulate cortex (rostral ACC). It has been shown to be deactivated during the drug Stroop as compared to baseline in cocaine users vs. controls. This is even when performance, task interest, and engagement are matched between the groups [18] and its activity is normalized by oral methylphenidate [25] – which similarly to cocaine blocks the dopamine transporters increasing extracellular dopamine – an increase that was associated with lower task-related impulsivity (errors of commission). This region was responsive (showed reduction in drug cue reactivity) to pharmacotherapeutic interventions in cigarette smokers [26,27], and may be a marker of treatment response in other psychopathology (e.g., depression). The LASSO does not show a meaningful sparsity pattern (Fig. 3(a)).

To further understand the differences in brain activity of addicted and not addicted patients we next extend our analysis to the cocaine regression dataset.

In this section we present our regression experiments on the cocaine dataset. Our experiments aim at providing empirical evidence for the support of the I-RISA model.

We use the cocaine regression dataset described in Section 3 in two experiments both predicting color-word behavioral variables.

In experiment 1 we use the fMRI contrast drug>neutral words, averaged over monetary reward condition, to predict the conflict effect in the subjects’ reaction time on the color-word task, defined as the difference in time between correctly performing the task for congruent and incongruent events. We use the insula, hippocampus complex, amygdala and ACC, part of the brain's limbic (emotion) circuit, as regions of interest (ROIs) for this experiment. These regions are chosen on the basis of previous studies on independent datasets that showed limbic system modulation by drug-related cues, e.g. drug words [28].

In experiment 2 we use the fMRI contrast 50¢>0¢, averaged over word type condition (drug or neutral), in order to predict the subjects’ responses on the color-word task, defined as the difference in percent accuracy between performing the task for congruent and incongruent events. We use the basal ganglia and thalamus, part of the brain's reward circuit, as ROIs for this experiment. We chose these ROIs on the basis of previous studies on independent datasets that showed reward system modulation by primary and secondary reinforcers, including money [29].

For each experiment we perform 500 trial with an 85%/15% random split between training and test sets. For each trial we perform model selection on the training set. That is, for each combination of parameters (λ
                        ∈{10
                           i
                        
                        :
                        i
                        =−2, …, 8} for LASSO, λ
                        ∈{10
                           i
                        
                        :
                        i
                        =−2, …, 8}
                        k
                        ∈{1, 2, 3, 6, 12, 100, 200, 300, 600} for k-support norm), we do a leave-one-subject-out cross validation on the samples that constitute the training set. We measure the correlation between the predicted and the true response variables on the training set. The parameter setting that leads to the highest correlation is used on the whole training set in order to learn a set of weights for each method, which are then applied on the test set. Finally, we measure the correlation between the predicted and the true response variables on the test set. We report the mean correlation on the holdout test samples and its standard error across the 500 random permutations. We note that the same sample randomization is used for both LASSO and k-support norm.

We compare the performance of the two methods in Table 2
                         for the first experiment and Table 3
                         for the second experiment.

With the squared loss function, the k-support norm outperforms LASSO for almost all cases, while when combined with the absolute loss function, the regularizers do not significantly differ in their predictive performance. The absolute loss function, for both regularizers, leads to correlations that are significantly higher than those with the squared loss function in almost all cases.

We report the fraction of non-zero weights that were selected by each method for over 50% of the 500 trials in Tables 4 and 5
                        
                         for the first and the second experiments respectively.

We average the weights assigned to the voxels over the 500 permutations and then compute the cumulative distribution function (CDF) for those weights. We threshold the CDF at 0.9 and visualize the weights of the voxels up to that threshold in Fig. 4
                        . The overly sparse solutions of the LASSO (Fig. 4(b) and (d)) lead to models that cannot be interpreted as easily as the solutions of the k-support norm method (Fig. 4(a) and (c)).

In the presence of correlated features, the degree of sparsity of the solution can be tuned with the k-support norm in order to include several highly correlated features. In contrast, LASSO tends to pick one representative feature with no guarantee of consistency in feature selection across different splits of the data samples into training and test sets. In all cases the fraction of non-zero weights selected by the k-support norm is higher than that of LASSO, indicating that the k-support norm method leads to more stable solutions as compared to those obtained with LASSO.

@&#DISCUSSION@&#

In our classification experiments we have shown that the k-support norm can give better predictive performance than the LASSO and elastic net, while having favorable mathematical and computational properties. Furthermore, the brain regions implicated in addiction by the k-support norm coincide with previous results on addiction indicating that the k-support norm is additionally useful for generating sparse, but correlated, regions suitable for interpretation in a medical-research setting.

In our regression experiments, in almost all cases, the k-support norm outperforms LASSO in predicting the behavioral measures given fMRI data when combined with squared loss, while when combined with the absolute loss, the predictive accuracy of the two regularizers does not differ significantly. The absolute loss led to higher predictions than squared loss for both regularizers for almost all cases. The LASSO leads to sparse solutions, since it tends to pick one feature per group of correlated features. On the other hand, the k-support norm allows calibrating the cardinality of the solutions and thus can select more interpretable groupings of correlated features and also leads to more stable results across different training sets. Thus, our results support the further exploration of the k-support norm for fMRI analysis. Furthermore, we demonstrate that we can predict real valued behavioral variables measured in an inhibitory control task given fMRI data from a different task, designed to capture emotionally salient reward.

On the medical side, we also provide further evidence to support the I-RISA model of drug addiction, whereby the skew in SA in cocaine abusers, as indexed by fMRI response to drug words and monetary rewards, two motivationally salient stimuli, is predictive of RI, as indexed by response slowing and accuracy on a task requiring inhibitory control (the color-word Stroop). Specifically, we show that in cocaine users, response to drug words in voxels located in limbic brain regions, such as the anterior insula and ACC implicated in emotion processing and emotion regulation, was predictive of slower responses on the RI task (Exp. 1), while response to money in voxels located in reward-related brain regions, such as the putamen implicated in habits, was predictive of lower accuracy on the RI task (Exp. 2).

@&#CONCLUSIONS@&#

In this work, we have investigated the applicability of sparsity regularizers in fMRI analyses. We have shown that the k-support norm can give better predictive performance than the LASSO and elastic net, while having favorable mathematical and computational properties. Furthermore, the brain regions implicated in addiction by the k-support norm coincide with previous results on addiction, indicating that the k-support norm is additionally useful for generating sparse, but correlated, regions suitable for interpretation in a medical-research setting.

@&#ACKNOWLEDGEMENTS@&#

This work was supported in part by NIA 
                  1R21DA034954-01, DIGITEO 
                  2013-0788D – SOPRANO, and the European Commission through ERC Grant 259112, and FP7-MC-CIG 334380.

@&#REFERENCES@&#

