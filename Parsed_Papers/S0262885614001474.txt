@&#MAIN-TITLE@&#Using a Discrete Hidden Markov Model Kernel for lip-based biometric identification

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We model a lip biometric approach based on shape information.


                        
                        
                           
                           This system is working with static lip on three public datasets.


                        
                        
                           
                           We develop a kernel based Hidden Markov Model (DHMMK).


                        
                        
                           
                           The use of DHMMK obtains discriminative information.


                        
                        
                           
                           The use of DHMMK on lip gets a robust approach for identification.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Discrete Hidden Markov Model Kernel

Image processing

Lip-based biometrics

Pattern recognition

@&#ABSTRACT@&#


               
               
                  In this paper, a novel and effective lip-based biometric identification approach with the Discrete Hidden Markov Model Kernel (DHMMK) is developed. Lips are described by shape features (both geometrical and sequential) on two different grid layouts: rectangular and polar. These features are then specifically modeled by a DHMMK, and learnt by a support vector machine classifier. Our experiments are carried out in a ten-fold cross validation fashion on three different datasets, GPDS-ULPGC Face Dataset, PIE Face Dataset and RaFD Face Dataset. Results show that our approach has achieved an average classification accuracy of 99.8%, 97.13%, and 98.10%, using only two training images per class, on these three datasets, respectively. Our comparative studies further show that the DHMMK achieved a 53% improvement against the baseline HMM approach. The comparative ROC curves also confirm the efficacy of the proposed lip contour based biometrics learned by DHMMK. We also show that the performance of linear and RBF SVM is comparable under the frame work of DHMMK.
               
            

@&#INTRODUCTION@&#

Personnel security is becoming increasingly important in today's modern world [1]. Biometric-based access control is one of the most important technologies for cyber–physical security, and has received increasing attention over the past two decades. In the competitive business world of today, the need and demand for a biometric physical security solution have never been higher. The biometric market is increasing each year and this trend is set to continue, due to the increasing need for security at borders, and in buildings, airports, etc. [2]. At its core, it aims to identify a person with one or more of their body features, such as their face, hand, fingerprint, or voice [1–3]. These biometric modalities can be deployed for different applications including; searching for people, remote access control, and secure corridors in airports.

To date, there has been a large amount of work done in biometrics, with most of it focusing on using a single biometric mode. Recently, the trend has been to build robust person identification systems based on multimodal approaches, i.e., a combination of biometric features. However, to obtain a robust multimodal solution, it is of benefit to employ individual modalities which have good performance in isolation. Furthermore, there is still much room for improvement with respect to single mode approaches.

Human recognition through distinctive facial features supported by an image database is still an appropriate subject of study as already mentioned. We should not forget that this problem still presents various difficulties. For example, what will happen if an individual's haircut is changed? Is make-up a determining factor in the process of verification? Would it significantly distort facial features? For these reasons, the study of different parts of a face still merits investigation in order to improve identification. Consequently, the analysis of lip contours is receiving greater attention [4,5], as it is particularly well-suited to deployment on mobile phone platforms. The importance of lip features as biometrics is reported in [6], where numerous lip-based features are evaluated. Therefore in this work, an approach based on the shape of lips is presented.

@&#RELATED WORK@&#

In this section, we briefly review work closely related to ours. Early work in this area involved the tracking of lips, using features extracted from color distributions around the lip area [7]. The resulting feature dimensionality was reduced using principal component analysis (PCA), and classification was performed by linear discriminant analysis. By combining this approach to lip movement analysis with speech analysis, a significant improvement in speaker verification in noisy conditions was demonstrated. In [8], the modeling of lip movements by hidden Markov models (HMMs) is presented. Each lip movement clip is represented by 2D discrete cosine transform (DCT) coefficients of the optical flow vectors within the mouth region. In [9], speech, lip movements and face images are combined to give robust person identification. In this work, DCTs of intensity normalized mouth images were employed to provide static features. These were then combined with an HMM to classify the speaker via log-likelihood.

Rather than recognizing a speaking person, research by Newman and Cox tries to determine the language a person is talking in by recognizing their lip movements when speaking a specific passage of text [10]. For this, they use Active Appearance Models (AAM) to locate the face and mouth, and produce a vector that represents the lip shape for each video frame. They obtain recognition results of 100% for seventy-five different languages for a single speaker. Subsequently, Newman and Cox [11] modified the classification system to obtain speaker independent language recognition, obtaining 100% classification accuracy for five bilingual speakers — even with a viseme classification accuracy of as low as 40%.

Another field in which lip contour extraction is used is in facial expression recognition as described by Raheja et al. [12]. They studied three facial expressions by processing an image of a face. To do this, they extract the lip contour by edge detection, generate a binary image, post-process to fill in holes, and perform a histogram analysis of the binarized image for classification. Using this system, they achieve a recognition rate of up to 95%.

There have been various investigations into recognizing a person from their lips. In one of these by Mehra et al. [13], PCA is used to obtain feature vectors of reduced dimension, which are then input to a neural network for classification. They achieve an accuracy rate of 91.07%. In [14], a novel ordinal contrast measure, called Local Ordinal Contrast Pattern, is proposed for representing video of the mouth region of a speaker while talking. This has been used in a three orthogonal plane configuration as input to a speaker verification system. Verification was accomplished using the chi-squared histogram distance or LDA classifiers, obtaining a half total error rate of less than 1%. Wang and Liew [6] studied the roles of different lip features, related to both physiological and behavioral properties of lips, in personal identification, and demonstrated that though dynamic features achieve higher recognition accuracy, both dynamic and static features are promising biometrics for verification. In [15], a new approach to speaker verification using video sequences of lip movements is proposed, in which a Motion History Image is used to provide a biometric template of a spoken word for each speaker. A Bayesian classifier is used for classification, obtaining an average recognition rate of 90% at a false alarm rate of 5%. In another work, a new motion based feature extraction technique for speaker identification using orientation estimation in 2D manifolds is reported [16]. The motion is estimated by computing the components of the structure tensor from which normal flows are extracted. By projecting the 3D spatiotemporal data to 2-D planes, projection coefficients are obtained which are used to evaluate the 3-D orientations of brightness patterns in TV like image sequences. An implementation, based on joint lip movements and speech, is presented along with experiments demonstrating a recognition rate of 98% on the publicly available XM2VTS database.

There exist a number of approaches for lip contour extraction, or lip corner detection, for visual speech/speaker recognition. For example, [17] used a monochrome image histogram to detect lip corners. However, it is more common to use color images, such as RGB [6] and HSV images [18]. Prewitt and Sobel operators are employed to detect lip edges in [19]. In [20], a manifold based approach is introduced to extract the lip contour. The red exclusion method [17] is widely used, due to its simplicity and efficiency. Similar to the approach in [21,22] is based on an RGB transformation of the lip regions. The resulting transformation and the b component of the CIELAB color space, are then used for the clustering phase. The task is formulated as finding the optimum partitioning of a given color image into lip and non-lip regions. The partitioning utilizes multispectral information in a color image, instead of just the limited information in a single component gray-scale image.

In [23], an approach is presented for a system based on lip reading. In [24,25], HMM based visual speech recognition is described. Several studies have found that a multimodal approach to speech recognition, combining speech, mouth and face features, gives significantly improved performance over a single mode approach under trying test conditions [1,3,26–28].

To date, most of the existing work concentrates either on lip movement, or the combination of lip movement and static features. While multimodal lip biometrics could be likely made more successful [6], there is little dedicated attention sufficiently paid into static lip shape alone. It would be easy to get a high accuracy via a multimodal approach with more biometrical information; however, developing a high performance system based on solely one modality is usually a more challenging task. In cases when another type of biometric features is not accessible, e.g., only a static lip image present while lip dynamics are unavailable, the high performance of a single biometric modality would be vital to maintain the good performance of the whole system.

In this work, we focus on the single mode approach, based on static shape information. Motivated by our previous work on modeling the shapes of offline signatures using HMMs [29], and the recent successful use of shape for other applications [30–33], we present a novel biometric identification approach based on lip contours encoded by an HMM kernel, and learned by an SVM. Though the proposed technique is somewhat incremental, the difference in results is significant. We would like to emphasize that the proposed approach achieves an improvement in classification accuracy of up to 53%, using only two training images, compared to the standard HMM approach. To the best of our knowledge, a study in lip-based biometrics using DHMMK has not been carried out before.

The rest of this paper is organized as follows. Section 2 introduces the lip extraction method, and Section 3 presents the lip descriptor. Section 4 describes our classification scheme and our kernel. Experiments and results are given in Section 5. Discussions and conclusions are presented in Section 6.

Our approach consists of three main parts; lip extraction, DHMMK parameterization, and classification. A block diagram of our method is presented in Fig. 1
                     . The first stage of our approach is to extract lips from an image by firstly detecting the face, then the mouth and finally the lips. Fig. 2(a) shows a typical color face image from one of our datasets. Face detection is achieved using the popular Viola and Jones face detector [34], which gives detection rates of 99% over the three datasets we use.

Once the face is detected, our next step is to localize the mouth region. It has previously been shown that face parts such as the eye, nose and mouth usually have very strong geometrical relationships [26]. For example, the mouth region is always located in the lower part of a detected near-frontal face. This fact makes it possible to use a simple heuristic approach to localize the mouth region without the need to build an advanced mouth detector. So, we take the lower half region of the detected face as a rough estimate of the mouth region. To remove any boundary effects, we further exclude a few pixels from the face boundary. The segmented mouth region is indicated by the red bounding box shown in Fig. 2(b). Though this ‘guessing’ is heuristic, it does remove some background facial regions in preparation for the next step, lip contour extraction.

Motivated by the work in [5,17], we use an RGB transformation to enhance the lip region based on the fact that lip color usually has a stronger red component than other parts of the mouth [17]. Therefore, we apply the following simple transformation on the mouth region to convert the color image into gray scale [35]:
                        
                           (1)
                           
                              I
                              =
                              R
                              −
                              2.4
                              ⋅
                              G
                              +
                              B
                              .
                           
                        
                     
                  

The effect of this transformation is clearly shown in Fig. 2(c). The lip region becomes much brighter than the background pixels of the mouth region. We then employ the Otsu binarization method [36] to segment the lips in the enhanced image. The lip contour is obtained by dilating the segmented lips with the following 3×3 morphological operator se
                     =[1 0 1; 1 1 1; 1 0 1]. The undilated segmented image is then subtracted from the dilated one leaving only the lip contour, Fig. 2(d). Figs. 2 and 3
                      show the whole of the lip contour extraction process using exemplar face images from each of the three datasets. From these examples, it is clear that our simple lip extraction approach works very well, even on faces with extensive facial hair, as shown in Fig. 3.

Following extraction of the lip contour, we create a lip descriptor. For this we consider two different geometrical–sequential features on two grid layouts; rectangular and polar. This step transforms the 2D contour into a one-dimensional feature vector. The rectangular grid features are the Euclidean distances from sample points along the vertical and horizontal axes to points on the lip contour. For the vertical axis, we equally sample 180 points, while for the horizontal axis we sample 300 points (Fig. 4a). Thus, the resulting normalized feature vector has 480 elements after concatenation.

The polar grid features are motivated by the work of [29]. These have been proven to be a powerful descriptor for offline signature verification. The set of polar features is calculated from the centroid of the lip contour. The orientation is sampled from 0 to 360° with a sampling interval of one degree. For each angle, the radius is computed as the distance between the point of intersection on the contour and the center (Fig. 4b). Hence, the resulting normalized feature vector consists of 360 elements after concatenation. We have found that this feature size gives better accuracy rates.

In this section, classification approaches are described. We used two classification approaches Hidden Markov Model (HMM) [37] and support vector machine (SVM) [38]. HMM has been deployed in order to model the sequential information from our features, in a similar fashion to [37]. Finally, these features have been transformed based on an HMM kernel learnt by SVM.

HMMs have become increasingly popular over the past two decades. They are theoretically sound and usually perform very well in real applications such as speech recognition [39]. In this section we review the theoretical aspects relating to our work. An HMM has two associated stochastic processes; dynamics and observations. The former is not visible and is usually modeled by the probability of transition between hidden states. The observation process is modeled by the probability of obtaining an observed value given a hidden state (see [37,40] for a complete treatment of HMM). In our paper, we use a discrete HMM (DHMM) [37] since it forms the basis of our DHMMK in the next section. A DHMM consists of the following parameters:
                           
                              1)
                              The number of states N
                              

The number of different observations M
                              

The transition probability matrix a(N,N)

The initial state probability π(N,1)

The observation probability matrix b (N,M).

A DHMM is very powerful at modeling time sequences where events at different times have some causal relationship. However, it is also possible to extend this concept to modeling the dependencies between parts of a shape. In particular, N is going to represent a little sequence or segments of widths and/or heights. An example of this usage is for offline signature verification, where the shape of a signature is modeled by a DHMM [29]. We view the offline signature shape and lip shape is a similar problem. The online signature clearly offers both temporal information, i.e., a clear temporal ordering of the points, and the geometrical dependence of those points. However, the offline signature is a shape, thus only offers the geometrical dependence of those points. This has been verified by our previous work of using HMM for offline signature verification using the polar grid features [29]. Under the sample umbrella, lip contour can be considered as a signature of face, and we consider that there exists potentially dependence between the points on the lip contour. Lip contour could also be considered as a one-dimensional manifold, and is an ordered sequence in this space. All of this motivates us to use the DHMM to model our lip contour descriptors. Hence, “left to right” DHMMs turn out to be especially appropriate for lip contours because the transition through the states is produced in a single direction. This equips the model with the ability to maintain a certain ordering with respect to the observations produced, where each sequential element of geometry distance is among the most representative changes (see Fig. 4).

In the DHMM approach, the first step is to quantize the feature vector elements. We use the K-Means algorithm for clustering to create a set of symbols that are required by the DHMM as in [41]. These symbols construct a set of states (with a “left to right” structure) that a discrete observation at a step t could be taken from. For our case, t represents the step between the feature vector elements. This approach is similar to the one used by [29,40]. The number of observed symbols, M, is determined by experimentation. We use thirty-two in all of our experiments. Based on Markov assumptions, given an observation sequence X
                        =[x
                        1, x
                        2, x
                        3, …, xt
                        ] with step t, and a trained DHMM λ
                        =[a,b,π], the probability of X given λ can be calculated as follows:
                           
                              (2)
                              
                                 P
                                 
                                    
                                       X
                                       /
                                       λ
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                       S
                                    
                                    
                                       
                                          
                                             ∏
                                             
                                                i
                                                =
                                                1
                                                :
                                                t
                                             
                                          
                                          
                                             P
                                             
                                                
                                                   
                                                      x
                                                      i
                                                   
                                                   /
                                                   
                                                      s
                                                      i
                                                   
                                                   ,
                                                   a
                                                
                                             
                                             ⋅
                                             P
                                             
                                                
                                                   
                                                      s
                                                      i
                                                   
                                                   /
                                                   
                                                      s
                                                      
                                                         i
                                                         −
                                                         1
                                                      
                                                   
                                                   ,
                                                   b
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where S
                        =[s
                        1, s
                        2, s
                        3, …, st
                        ] denotes the variables for the hidden states. Thus for a given sequence, the optimal HMM can be selected by maximizing the posterior probability over a set of C trained HMMs:
                           
                              (3)
                              
                                 
                                    λ
                                    0
                                 
                                 =
                                 
                                    max
                                    
                                       j
                                       ∈
                                       C
                                    
                                 
                                 P
                                 
                                    
                                       
                                          λ
                                          j
                                       
                                       /
                                       X
                                    
                                 
                                 .
                              
                           
                        
                     

Though the HMM has achieved great success in many applications, the learning of this model is based on maximizing the marginal probability of the observations over the hidden states. Thus, it is a generative method and does not fully utilize the inter-class discriminative information presented in the training set. A natural way to address this weakness is to incorporate it into a discriminative learning framework, such as an SVM. This can be achieved by computing the Fisher score of an observation sequence over the learned DHMM parameters, which is calculated by the gradient of the parameter space [42].

From [42], we can calculate that gradient of the logarithm of the probability in Eq. (2) with respect to the HMM parameter λ, as follows (we call this a DHMM kernel):
                           
                              (4)
                              
                                 
                                    U
                                    
                                       
                                          X
                                          /
                                          α
                                       
                                    
                                 
                                 =
                                 
                                    ∇
                                    
                                       α
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                 
                                 log
                                 P
                                 
                                    
                                       X
                                       /
                                       λ
                                    
                                 
                                 =
                                 
                                    
                                       ξ
                                       
                                          x
                                          
                                             s
                                             j
                                          
                                       
                                    
                                    
                                       α
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                 
                                 −
                                 ξ
                                 
                                    
                                       s
                                       j
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    U
                                    
                                       
                                          X
                                          /
                                          β
                                       
                                    
                                 
                                 =
                                 
                                    ∇
                                    
                                       β
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                 
                                 log
                                 P
                                 
                                    
                                       X
                                       /
                                       λ
                                    
                                 
                                 =
                                 
                                    
                                       ξ
                                       
                                          
                                             s
                                             i
                                          
                                          
                                             s
                                             j
                                          
                                       
                                    
                                    
                                       β
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                 
                                 −
                                 ξ
                                 
                                    
                                       s
                                       j
                                    
                                 
                              
                           
                        where ξ(x,
                        s
                        
                           j
                        ) represents the number of times a certain symbol x is generated by a state sj
                         in a sequence, while ξ(s
                        
                           i
                        ,
                        s
                        
                           j
                        ) denotes the frequency of the joint occurrence of two states si
                         and sj
                         at two adjacent time intervals over a sequence, and α and β are the forward and backward variables, respectively. These are in fact the sufficient statistics for the emission probability a
                        
                           i,j
                         and transition probability b
                        
                           i,j
                         given a sequence. ξ(s
                        
                           j
                        ) represents the frequency of state sj
                         occurring in a sequence [37,42]. These values can be directly obtained from the forward–backward algorithm [42].

The DHMM kernel vector UX
                         for a given sequence X is simply the concatenation of the two gradient vectors calculated from Eqs. (4) and (5) respectively. The length of the resulting feature vector for a sequence is N
                        ×(M
                        +
                        N).

Thus, the similarity of two sequences with a learned HMM could be evaluated in a kernel fashion using the corresponding Fisher score vector as follows
                           
                              (6)
                              
                                 K
                                 
                                    X
                                    Y
                                 
                                 =
                                 K
                                 
                                    
                                       U
                                       X
                                    
                                    
                                       U
                                       Y
                                    
                                 
                                 .
                              
                           
                         where K(∙) could be any type of standard kernels for an SVM. (We have used the gpdsHMM tool [41].)

@&#EXPERIMENTS AND RESULTS@&#

Our study is carried out on three different datasets; GPDS-ULPGC [30], the PIE dataset [43] and the RaFD database [44]. The GPDS-ULPGC dataset was collected by us specifically for this study. It consists of fifty users with ten samples per user (thus 500 images in total). The database is composed of 54% males and 46% female, with ages ranging from ten to sixty. Each sample is a color image of size 768×1024pixels. It is available for downloading from [30]. The PIE dataset is a publicly available dataset [43] composed of sixty-eight subjects, with eleven samples per subject (thus giving 748 images in total), where each sample is a color image of size 200×300. The main characteristic of the dataset is it contains illumination changes and different hair styles (e.g., bearded and beardless, as shown in Fig. 3). The RaFD Face Dataset is composed of sixty subjects with nine samples per subject (thus giving a total of 540 images) [44]. The image resolution is 1024×681 and the database contains eight facial expressions for each subject. Since our study focuses on static lip features, only three of the eight expressions present in the database (neutral, sadness and indifference) suit our purpose, and are used in the experiments. Furthermore, images containing non-frontal poses for subjects with their mouth open have been removed.

We use a multi-class SVM for classification, which is built using the one-versus-all strategy. The SVM_light 
                        [45] implementation is utilized in our experiment. We tried two different kernels in our experiment; linear and radius bias function (RBF). It is well-known that the RBF kernel usually works better than the linear kernel [46]. In our study, we show that the recognition performance of the linear SVM is comparable to RBF SVM with DHMMK, while the linear SVM is usually much faster than RBF. Specifically, we use the DHMMK output, UX
                        , from Eqs. (4) and (5), as the input to the SVM. For the RBF kernel, the optimal value of the gamma parameter is found by a grid search.

Our experiments are performed using the well-known hold-out cross validation [47]. We report our results in terms of classification accuracy based on ten different runs/splits separately on each of the three different datasets. For each run/split, we randomly choose a number of images per subject for training and the rest for testing. The sampling is carried out without replacement to ensure there is no overlap between the training and test sets. In this way, we have reduced the risk of overoptimistic results from traditional cross validation experiments on small sample domains [47]. The mean and standard deviation of the classification accuracy of each run over all classes are reported.

We test the performance of our algorithm with respect to three different parameter settings: 1) different features, rectangular and polar grids; 2) number of states, N; 3) number of training images. We vary the number of training images from one to five per class. Combinations of these settings are comparatively tested with DHMM, SVM and DHMMK–SVM, respectively. Results on the GPDS-ULPGC dataset are summarized in Tables 1–6
                        
                        
                        
                        
                        
                        , while Tables 7–12
                        
                        
                        
                        
                        
                         give the results obtained with the PIE and RaFD datasets. The idea is to validate our approach, which has been constructed using the GPDS-ULPGC dataset, using two independent, or blind, datasets — PIE and RaFD, thereby, demonstrating the robustness of our approach.

In order to highlight the performance of the proposed DHMM kernel based approach, we first perform experiments using the standard DHMM method without the kernel trick. Specifically, we use a similar training approach as in [29], since it proved very successful for offline signature verification. For the DHMM, we compare the performance of rectangular and polar grids by varying the number of states from 40 to 140. We use half the sample subjects for training and the rest for testing. i.e., five training samples per class. Results are shown in Table 1.

From Table 1, we can see that rectangular grid feature clearly outperforms the polar grid feature. This might be because the geometry constraints of the rectangular grid are considerably stronger than for the polar configuration. Table 1 also shows that increasing the number of states improves the accuracy for both feature types, particularly the rectangular grid feature. The highest accuracy, 80.26%, is achieved using rectangular grid features with a DHMM of 140 states and five training images per class.

Since the rectangular grid feature outperforms the polar grid, we choose the former to test the robustness of this approach to decreasing the number of training images from five to one per class. Table 2 shows that the classification accuracy decreases significantly from 80.26% with five training samples, to 59.47% with one training sample. We conclude therefore, that the DHMM based approach is not very robust to a reduction in training image number.

We perform further experiments using an SVM without the DHMM modeling process, i.e., the rectangular and polar grid features are input directly to the SVM. We tested both the linear and RBF kernels, Table 3, and found no significant difference between them. (For the RBF kernel, the optimal parameter is determined by a grid search using cross validation). As before, the rectangular grid feature performed better. Table 4 shows the results obtained by varying the number of training images from five to one. From these results we can observe that the RBF kernel works slightly better than the linear kernel. As was the case for the DHMM, the SVM is not robust to a reduction in training image number. It is interesting to note that discriminative SVM works better than the generative DHMM.


                        Tables 5 and 6 show the results of using the DHMM kernel under the same experimental settings used in Subsections 5.3 and 5.4. Table 5 shows the results of varying the number of states with five training images per class. Compared to the results in Table 1, we can see that the DHMMK significantly outperforms the DHMM for both rectangular and polar grid features. The rectangular grid feature again performs better than the polar grid. In particular, we achieve an accuracy of 100% using the rectangular grid feature with five training samples and 140 states.

Using similar settings to the DHMM, we use the rectangular grid feature to test the robustness of the DHMMK against a reduction from five to one training images per class. Table 6 shows that the proposed approach can still achieve an accuracy of 99.83% using only two training images per class. This indicates that the DHMMK approach is less sensitive to the number of training images, than either DHMM or SVM approaches.

Overall, compared to the results in Subsections 5.3 and 5.4, the DHMMK approach outperforms both SVM and DHMM. It is also interesting to note that the proposed approach is fast when classifying a test sample. Using MATLAB on a computer with a 2.66-GHz CPU, and 2GB RAM, the running speed of our approach is 400millisecondspersample, although the training time is longer than the running speed, being slightly over three hours.

In this section we test the performance of our approach using two independent public face datasets, PIE [43] and RaFD [44]. As stated before, the PIE dataset contains illumination changes with images of lower resolution, while the RaFD dataset contains images of different facial expressions but with a similar image resolution to the GPDS-ULPGC database. We have kept exactly the same experimental and parameter settings as in Subsections 5.3, 5.4, and 5.5 for the GPDS-ULPGC database. Results are summarized from Tables 7 to 12. From those results, we can draw the same conclusions as for the GPDS-ULPGC dataset. 1) Best results are obtained with the rectangular grid feature, and the DHMM based approach is not very robust to a reduction in the number of training images (Table 7). 2) The SVM works better than the DHMM. The performance gain of using a rectangular versus a polar grid with an SVM is much higher than the gain achieved with DHMM (Tables 9 and 10). This demonstrates that the SVM fully utilizes the discriminative potential of the rectangular grid feature. 3) The proposed DHMM kernel based approach works much better than either SVM or DHMM, and is less sensitive to the number of training images. With only two training images per class, the DHMMK–SVM approach gets a classification accuracy of 97.13%.

The results in Table 7 show that when increasing the number of the states, the performance of the standard DHMM decreases (e.g., in the case of rectangular grid features, 36.19% and 40.33% with 90 states vs. 33.17% and 35.62% with 140 states for the PIE and RaFD datasets respectively). This is likely to be due to over fitting with the large number of states. However, the proposed approach with a DHMM kernel and SVM is much more robust to the change of state numbers for the PIE dataset (e.g., 99.43% with 90 states vs. 99.26% with 140 states). Lastly, similar to GPDS-ULPGC, the RaFD dataset is much more resistant to over fitting.

It is worth pointing out that the proposed approach is more robust to the change of scale and resolution than simply using HMM and SVM. This is demonstrated by cross-comparing the results achieved on the RaFD and PIE dataset from Tables 8 to 11. Note that one major difference between RaFD and PIE dataset is that the image size in the PIE dataset is much smaller (1/10) (size: 200×300) than those in the RaFD dataset (size: 1024×681). This scale and resolution change posed a significant challenge for the traditional method using HMM and SVM directly. For example, from Table 9, we can see that traditional approach using linear SVM and grid layout features with 5 training samples per class achieves the mean accuracy of 56.01% on the PIE dataset compared to the 66.72% on the RaFD dataset. The performance gap is up to 10% between those two datasets. While the proposed approach with an HMM kernel under the exactly the same experimental setting achieves the mean accuracy of 99.26% on the PIE dataset compared to 99.44% on the RaFD dataset, the performance gap (~0.2%) between the two datasets under the proposed approach is neglectable. This shows that the propose approach is robust to the change of scale and resolution to a large extent. Another point worth pointing out is that nowadays, the cameras equipped by the mobile phone are not of high quality, and it is not difficult to capture high resolution images that could well match the working conditions of our approach.

Finally, we adopt a similar biometric experiment protocol to that used in [48] and evaluated the three approaches above for user authentication. Fig. 5
                         shows the receiver operating characteristic (ROC) curves for DHMMK–SVM, SVM and DHMM with the rectangular grid feature using two training images per class on the GPDS-ULPGC dataset. Figs. 6 and 7
                        
                         show the corresponding curves for the PIE and RaFD databases respectively. It is quite clear that DHMMK significantly outperforms the SVM and DHMM approaches, especially on the challenging PIE and RaFD datasets. For the PIE dataset, the low resolution of the extracted lip contour tends to degrade the performance of all three approaches by differing degrees. As shown in the classification experiments, for the baseline approach using DHMM, the performance drops significantly from 65.20% on the GPDS-ULPGC dataset to 25.92% on the PIE dataset using the rectangular grid feature. For DHMMK, the performance is only slightly lower, dropping from 99.83% to 97.13%, with two training samples. As the number of sample is increased, this effect disappears, as verified from the ROC curves in Figs. 5 and 6. This confirms that the proposed DHMMK approach tends to be much more robust with respect to loss in resolution and illumination changes. The performance comparison between the RaFD and GPDS-ULPGC datasets reveals a similar trend, with the RaFD performance being slightly lower than that obtained with the GPDS-ULPGC dataset (98.10% vs. 99.83% using two training samples). When the number of training samples is increased, this difference is reduced, giving comparable performances for the two datasets (see Fig. 7).

In this work we presented a novel and robust identification approach based on a DHMMK from static lip information. The main results are:
                        
                           •
                           The performance of the DHMMK is significantly superior to that of DHMM or SVM for all three datasets, proving that the use of static lip information is a good biometric modality for the identification.

The DHMMK performance improvement is greater for the more challenging PIE and RaFD datasets, indicating a greater robustness to changes in illumination, resolution loss and variation in facial expression.

The DHMMK requires fewer training images per subject.

The rectangular grid feature was found to provide better performance than the polar grid feature.

The proposed approach is less sensitive to HMM system parameters, such as the number of hidden states, N.

The proposed approach is found less sensitive to scale and resolution changes.

Previously, using static lip features with an HMM has not been particularly promising with regard to subject authentication [14]. However, DHMMK overturns this perception and shows great promise for biometric based access control. This is due to the fact that the DHMMK captures valuable information from the gradient of the probability of having a certain feature vector in a particular state for both the rectangular and polar grid features. Furthermore, the fact that it works well while only requiring two training images is important with respect to subject registration — an important practical consideration for large scale access control.

Our lip extraction technique is robust to the change of skin colors to some extent. This was demonstrated by the lip contour extraction results on the PIE dataset illustrated in Fig. 3. The first image presents much darker skin color than the second image. The lip contour could be extracted relatively reliable in such cases. Much darker region such as a black face might affect the lip extraction results heavier. One possible idea to overcome this is to develop a possible face detector and adapt the lip extractor for black, dark, and Caucasian faces respectively.

It is worth pointing out that in all the three datasets tested, it is not necessary to conduct rotation correction since there are no rotated lips presented. To handle the moderate rotations of the lips, one possible way is to develop a simple algorithm in the pre-processing block to detect the rotation of the lips. The final horizontal line can be detected as the principal axis and the slope of this line could be used for the rotation correction.

In future work, we propose to investigate the performance of DHMMK under different camera viewpoints, in more complex scenarios, and under complex illumination conditions. Finally, fusing static lip features with other biometric modalities such as the face [30], or lip dynamic features, to develop a robust multimodal system and tested on a larger dataset such as the FRGC [49] would be another interesting direction to investigate.

@&#ACKNOWLEDGMENT@&#

This work is partially supported by funds from The Royal Society of Edinburgh under RSE International Exchange Programme, 2012 to Carlos M. Travieso-González; and by the Spanish Government, under Grant MCINN TEC2012-38630-C04-02.

@&#REFERENCES@&#

