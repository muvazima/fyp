@&#MAIN-TITLE@&#Guided depth enhancement via a fast marching method

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a new depth enhancement method for RGB-D sensors.


                        
                        
                           
                           It extends the fast marching method to incorporate color and depth information.


                        
                        
                           
                           It outperforms state-of-the-art local methods in terms of visual and metric qualities.


                        
                        
                           
                           It achieves visually comparable results to time-consuming global methods.


                        
                        
                           
                           It provides better inputs to the applications based on RGB-D sensors.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Depth enhancement

Image inpainting

Fast marching method

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Along with recent advancements on range imaging technologies, nowadays, it is quite convenient to obtain depth and color images simultaneously. For instance, commercially available time-of-flight (TOF) cameras [1,2] can produce depth and color image pairs up to 320×240pixels and 100 frames per second. Kinect [3–5], a type of device developed by Microsoft in recent years, can capture aligned depth and color images at a frame rate of 30fps in VGA resolution. The comparably high quality, together with the real-time nature, makes these sensors attractive to all. They have therefore been extensively used in indoor applications, such as 3D scene reconstruction [6–8], activity recognition [9], and 3D object detection [10,11].

Depth data obtained by the sensors are compelling, particularly compared to those generated by stereo-vision technologies, but they are still inherently defective [7,12]. A typical example is illustrated in Fig. 1
                     . The depth map contains numerous invalid regions where range measurements are missing, due to occlusions, reflective or dark surfaces, or the limit of depth of field. Meanwhile, the alignment of the depth map to the displaced color image leads to large invalid regions along the map's boundary. In addition, the underlying range sensing technique also results in heavy noise, especially on object boundaries. These defects more or less influence the use of depth information. Therefore, a preprocessor concerning depth inpainting and denoising is desired.

Although a great number of research and practical applications using depth maps have been released, research on the enhancement of them is relatively deficient. Therefore, in this work, we study on depth map enhancement, particularly via taking advantage of aligned color images. This task, which contains depth inpainting and denoising, is referred to as guided depth enhancement by us, in that color images are employed as guidance. To achieve the task, we propose a new method that extends the simple but effective fast marching method [13,14] to incorporate color information for inpainting and apply an edge-preserving guided filter for noise reduction.

The paper is organized as follows. We briefly review the most related work in Section 2 and present the proposed inpainting method in Section 3. A guided filtering [15] method for depth denoising is presented in Section 4. Experiments on both real-world Kinect data and the Middlebury dataset [16] are demonstrated in Section 5, followed by a conclusion in Section 6.

@&#RELATED WORK@&#

Even though the research on guided depth enhancement surges recently, filling in depth holes is closely related to the traditional image inpainting problem. Hence, in this section we focus on reviewing the techniques of traditional image inpainting and the inpainting techniques with the use of additional information. This review will be helpful for motivating the proposed method and the experimental comparisons we conduct.

Conventional inpainting is to reconstruct unknown or marked regions by inferencing intensity/color from the remaining part of an image. Other than the image itself, no additional information is used. Image inpainting is a fundamental problem that has been studied for decades. Roughly speaking, the inpainting techniques can be categorized into two groups: geometry- and texture-oriented methods [17]. Geometry-oriented methods [18,13] mostly formulate the problem within a variational framework and address it by solving partial differential equations (PDEs). This type of methods is local and shows good performance in propagating level lines. Texture-oriented inpaintings, mainly referred to as exemplar-based approaches [19], are non-local and perform well in the presence of texture. When considering depth inpainting in our case, a depth map is viewed more geometrical than textural. Therefore, we are more interested in geometry-oriented methods, among which the simple but efficient fast marching method (FMM) based inpainting [13] is compelling. The proposed method for depth inpainting is inspired by it, while differently our method also takes advantage of aligned color images.

Guided inpainting refers to the inpainting of color or depth images via using additional information. Guided color inpainting mainly exists in the research field of depth image based rendering (DIBR) [20]. For instance, Ref. [21] extends the Criminisi's exemplar-based method [19] to incorporate depth information for color image inpainting. To some extent, this method shares a similar idea with our work. That is, both extend a conventional inpainting technique to integrate additional information. In contrast, it relies on an exemplar-based approach so that it is of higher complexity. Concerning that depth maps are presented more geometrically, we prefer to choose a geometry-oriented approach which is simpler and more efficient. That is the reason why we extend the fast marching method instead of other inpainting techniques.

Guided depth inpainting is a relatively new topic. It is initially taken as a post-processing step to fill-in holes existed in depth maps generated by stereo-vision techniques. Markov random fields (MRFs) and other global optimization methods are usually used, based on the coincidence of color and depth edges [22–26]. This type of methods achieves good performance in geometric accuracy, but is time-consuming. In recent several years, with the advancement of range sensors, guided depth inpainting is developed more as a pre-processing step for practical applications utilizing depth maps. Filtering based techniques, for instance, joint bilateral filter [27–34], Laplace filter [10], or mode filters [35,36] are often applied for efficiency. But when unknown regions are large, they may not be able to fill in the regions and artifacts may occur also. Adaptive or iterative schemes [34] are hence required to improve their performance.

In addition to the above-mentioned two types of methods, there is another research line based on the total variational framework, such as nonlinear diffusion methods [37] and a structure guided inpainting technique proposed very recently by Qi et al. [38]. These methods are time-consuming as well. Moreover, they do not explore the propagation order which is quite important for a local-information-based inpainting technique.

The approach proposed here is highly inspired by a simple but effective inpainting technique that is based on the fast marching method [13,39,40]. In contrast to FMM, our approach incorporates additional guidance information from an aligned color image for depth inpainting. We thereby refer to our method as the guided FMM. It includes two key components: the model of inpainting and the order of propagation. Both of them are introduced below.

Let us consider an aligned depth–color image pair, of which the depth map contains invalid regions while the color image is assumed to be indefective. The depth and the color of a pixel p
                        =(x,y) are, respectively, denoted by D(p) and I(p). A region that needs to be filled in is represented by Ω, and its boundary is ∂Ω. Fig. 2
                         demonstrates such a toy example.

When inpainting a pixel on the boundary, we need to determine the depth according to its small neighborhood 
                           
                              B
                              
                                 p
                              
                           
                         which is composed of the pixels with known depth values. Given a point q in 
                           
                              B
                              
                                 p
                              
                           
                        , together with its depth D(q) and the gradient ∇D(q), the depth value of p can be predicted by the first order approximation
                           
                              (1)
                              
                                 
                                    
                                       D
                                       q
                                    
                                    
                                       p
                                    
                                    =
                                    D
                                    
                                       q
                                    
                                    +
                                    
                                       
                                          ∇
                                          D
                                          
                                             q
                                          
                                          ,
                                          p
                                          −
                                          q
                                       
                                    
                                    ,
                                 
                              
                           
                        where 〈⋅,⋅〉 denotes the inner product operator.

Further on, summing the predictions obtained from all pixels in 
                           
                              B
                              
                                 p
                              
                           
                        , we get the estimation of D(p) as
                           
                              (2)
                              
                                 
                                    D
                                    
                                       p
                                    
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   q
                                                   ∈
                                                   B
                                                   
                                                      p
                                                   
                                                
                                             
                                             
                                                ω
                                                
                                                   p
                                                   q
                                                
                                                
                                                   
                                                      D
                                                      
                                                         q
                                                      
                                                   
                                                
                                                +
                                                
                                                   
                                                      ∇
                                                      D
                                                      
                                                         q
                                                      
                                                      ,
                                                      p
                                                      −
                                                      q
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                q
                                                ∈
                                                B
                                                
                                                   p
                                                
                                             
                                          
                                          
                                             ω
                                             
                                                p
                                                q
                                             
                                          
                                       
                                    
                                 
                              
                           
                        in which ω(p,q) is a weighting function. This local means scheme is commonly used in filtering [28,27] and inpainting [13,38,41]. The crucial part here is how to design the weighting function in order to incorporate guided color information. In the proposed method it is designed to be
                           
                              (3)
                              
                                 
                                    ω
                                    
                                       p
                                       q
                                    
                                    =
                                    
                                       ω
                                       dst
                                    
                                    
                                       p
                                       q
                                    
                                    
                                       ω
                                       clr
                                    
                                    
                                       p
                                       q
                                    
                                    ,
                                 
                              
                           
                        where
                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             ω
                                             dst
                                          
                                          
                                             p
                                             q
                                          
                                          =
                                          exp
                                          
                                             
                                                −
                                                
                                                   
                                                      |
                                                      |
                                                      p
                                                      −
                                                      q
                                                      |
                                                      
                                                         |
                                                         2
                                                      
                                                   
                                                   
                                                      2
                                                      
                                                         σ
                                                         d
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             ω
                                             clr
                                          
                                          
                                             p
                                             q
                                          
                                          =
                                          exp
                                          
                                             
                                                −
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  I
                                                                  
                                                                     p
                                                                  
                                                                  −
                                                                  I
                                                                  
                                                                     q
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                            2
                                                         
                                                      
                                                   
                                                   
                                                      2
                                                      
                                                         σ
                                                         c
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

It consists of a geometric distance term ω
                        
                           dst
                        (p,q) and a color similarity term ω
                        
                           clr
                        (p,q). The term ω
                        
                           dst
                        (p,q) indicates that the pixels geometrically closer to p have higher contributions than the points further away. ω
                        
                           clr
                        (p,q) ensures that the pixels having similar color to I(p) contribute more. σ
                        
                           d
                         and σ
                        
                           c
                         are the corresponding bandwidths determined experimentally.

It is noted that the weighting function is in the same form as the one in joint bilateral filter [28,42]. In contrast to the original FMM and our previous work in [41], we cast off a directional component and a level set distance component. The reason is that the color similarity term plays a dominant role. Moreover, our experiments show that, in some cases, the inpainting performance is slightly degraded when the abandoned terms are included.

As emphasized by Criminisi et al. [19], when propagating information from ∂Ω into Ω, the order by which the inpainting procedure takes highly influences the resulting quality. Therefore, in this work we propose a new propagation scheme such that it can make better use of guiding information.

Propagating interfaces ∂Ω is a common problem occurred in numerous vision applications such as anisotropic diffusion, denoising, and shape detection [39]. It is efficiently solved using the fast marching method when the propagation satisfies the Eikonal equation:
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             
                                                ∇
                                                T
                                                
                                                   p
                                                
                                             
                                          
                                          F
                                          =
                                          1
                                          ,
                                       
                                       
                                          s
                                          .
                                          t
                                          .
                                          
                                          T
                                          
                                             p
                                          
                                          =
                                          0
                                          
                                          for
                                          
                                          p
                                          ∈
                                          ∂
                                          Ω
                                          ,
                                       
                                    
                                 
                              
                           
                        where T(p) is the arrival time, indicating the propagation order, of the interface when it crosses the point p. F is a speed function controlling the motion of the interface. In our inpainting procedure, each time the point with minimum T is chosen for inpainting. As T is determined by F, it is crucial to build a good model for F in order to achieve high performance.

In the original FMM-based inpainting [13], F is simply set to be −1. In this case, the solution of T is the distance map of the unknown pixels to the boundary ∂Ω. It means that the points closest to known image points are inpainted first and no additional information are concerned. In order to incorporate guiding information from the reference image, we propose to use
                           
                              (6)
                              
                                 
                                    F
                                    
                                       p
                                    
                                    =
                                    −
                                    
                                       1
                                       
                                          1
                                          +
                                          
                                             
                                                
                                                   ∇
                                                   
                                                      G
                                                      σ
                                                   
                                                   ∗
                                                   I
                                                   
                                                      p
                                                   
                                                
                                             
                                             2
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        a speed function depending on the image gradient. Here, G
                        
                           σ
                         ∗ I(p) denotes the reference image convolved with a Gaussian filter with a bandwidth σ. The negative sign constrains F
                        ≤0 so that the interface evolves inward. This formulation is widely used in anisotropic diffusion [43] and edge-aware image denoising [44], but not inpainting. It assigns a high speed to a pixel having homogenous color around it, and retards the propagation from passing through edges.

During inpainting, instead of computing T for the entire depth map, FMM maintains a narrow band so that high efficiency is achieved. The band contains the points on the boundary ∂Ω. For the purpose of clarity, we here introduce the depth propagation procedure in brief.

We initially set T
                        =0 for the pixels with known depth values and some large value T
                        
                           ∞
                         for the unknown, and assign a flag f to each pixel with the value of BAND, KNOWN, or INSIDE, which respectively indicate the point on, outside, or inside the boundary. Meanwhile, a heap NarrowBand is maintained. It initially stores all initial BAND points that locate on the boundary and are of T
                        
                           ∞
                         values. During propagation, new BAND points will be inserted into NarrowBand ascendingly according to updated T. FMM propagates the T, f and depth values following Algorithm 1.

Note that in Step 4, the value T of point p
                        =(i,j) is propagated to its neighbors by solving the finite difference discretization [14] of Eq. (5), as given by
                           
                              (7)
                              
                                 
                                    max
                                    
                                       
                                          
                                             
                                                d
                                                ij
                                                
                                                   −
                                                   x
                                                
                                             
                                             T
                                             ,
                                             −
                                             
                                                d
                                                ij
                                                
                                                   +
                                                   x
                                                
                                             
                                             T
                                             ,
                                             0
                                          
                                       
                                       2
                                    
                                    +
                                    max
                                    
                                       
                                          
                                             
                                                d
                                                ij
                                                
                                                   −
                                                   y
                                                
                                             
                                             T
                                             ,
                                             −
                                             
                                                d
                                                ij
                                                
                                                   +
                                                   y
                                                
                                             
                                             T
                                             ,
                                             0
                                          
                                       
                                       2
                                    
                                    =
                                    
                                       1
                                       
                                          F
                                          ij
                                          2
                                       
                                    
                                 
                              
                           
                        where
                           
                              
                                 
                                    
                                       
                                          
                                             d
                                             ij
                                             
                                                −
                                                x
                                             
                                          
                                          T
                                          =
                                          T
                                          
                                             i
                                             j
                                          
                                          −
                                          T
                                          
                                             
                                                i
                                                −
                                                1
                                                ,
                                                j
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             d
                                             ij
                                             
                                                +
                                                x
                                             
                                          
                                          T
                                          =
                                          T
                                          
                                             
                                                i
                                                +
                                                1
                                                ,
                                                j
                                             
                                          
                                          −
                                          T
                                          
                                             i
                                             j
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        and analogous for y. Eq. (7) is solved by the upwind scheme as introduced in [39].
                           Algorithm 1
                           The Algorithm of Guided Fast Marching Method.
                                 
                                    
                                 
                              
                           


                        Fig. 3
                         illustrates the power of our proposed method by a synthetic color–depth example. The two black rectangles in the depth map indicate the regions to be filled in. As illustrated in Fig. 3(b) and (c), in which darker pixels get inpainted earlier, the original FMM determines the inpainting order by the distance to the boundaries of unknown regions, while the guided FMM estimates depth for the pixels near visual edges at last. The inpainting results exhibited in Fig. 3(e) and (f) show that the proposed method preserves edges better than the original FMM.

The guided depth inpainting introduced above assigns each pixel in unknown regions a proper depth value. However, the inpainted depth map still suffers from noise [7,32]. A denoising step hence is preferable. For this purpose, a Bilateral Filter (BF) and a Guided Filter (GF) [15] are, respectively, utilized in the work by Izadi et al. [7] and Wasza et al. [29]. Both filters are capable of preserving edges while suppressing noise, but the latter is of higher efficiency. Therefore, in this work we choose the guided filter for depth refinement and take aligned color images as guidance.

The guided filter is based on an assumption that the output image has a linear relationship to the guidance within a small window, in our case, which means
                        
                           (8)
                           
                              
                                 
                                    
                                       
                                          D
                                          ˜
                                       
                                       
                                          p
                                       
                                       =
                                       
                                          a
                                          k
                                       
                                       I
                                       
                                          p
                                       
                                       +
                                       
                                          b
                                          k
                                       
                                       ,
                                    
                                    
                                       ∀
                                       p
                                       ∈
                                       
                                          w
                                          k
                                       
                                       .
                                    
                                 
                              
                           
                        
                     
                  

Here, 
                        
                           
                              D
                              ˜
                           
                           
                              p
                           
                        
                      denotes the filtered depth value. a
                     
                        k
                      and b
                     
                        k
                      are coefficients of the linear model in the window w
                     
                        k
                     , and they are solved by
                        
                           (9)
                           
                              
                                 
                                    
                                       min
                                    
                                    
                                       
                                          a
                                          k
                                       
                                       ,
                                       
                                          b
                                          k
                                       
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          p
                                          ∈
                                          
                                             w
                                             k
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      a
                                                      k
                                                   
                                                   I
                                                   
                                                      p
                                                   
                                                   +
                                                   
                                                      b
                                                      k
                                                   
                                                   −
                                                   D
                                                   
                                                      p
                                                   
                                                
                                             
                                          
                                          2
                                       
                                       +
                                       ε
                                       
                                          
                                             
                                                a
                                                k
                                             
                                          
                                          2
                                       
                                    
                                 
                                 ,
                              
                           
                        
                     where ε is a regularization factor and D(p) is the input depth data. Since a pixel is covered by multiple windows, GF takes the mean of 
                        
                           
                              D
                              ˜
                           
                           
                              p
                           
                        
                      from all associated windows as its final output value.

@&#EXPERIMENTS@&#

In order to validate the proposed method, we have conducted a series of experiments on both real-world Kinect data and the Middlebury dataset [16] that, respectively, provide qualitative and quantitative evaluations. Meanwhile, we have also compared our approach with the original FMM and two state-of-the-art depth enhancement methods: the bilateral filtering (BF) based depth enhancement [42] and the MRF based approach [22]. Details of experiments are presented below.

Before making comparisons, we first introduce implementation details. Our algorithm is implemented in C++, running on a PC with an Intel Core2Duo 2.33GHz processor and 2GB memory. It takes 500ms or so to process a 640×480 depth map with large unknown regions, as examples presented in Figs. 4–6
                        
                        
                        . The running time varies slightly from case to case, depending on the number of inpainted pixels. In general, it is slower than the approximated bilateral filters [42], but much faster than MRF (which takes seconds to process an image).

There are a couple of parameters determined experimentally. Unless specified otherwise, throughout all experiments we set them as follows. In our guided FMM, the size of neighboring window 
                           B
                         used in Eq. (2) is 11×11; σ
                        
                           d
                        
                        =2 and σ
                        
                           c
                        
                        =10 for Eq. (3), which are also the parameter values taken in bilateral filter; σ
                        =1 in Eq. (6); and ϵ=10−4 for Eq. (9). In addition, the kernel size of BF [31] is set to be 41×41, and σ
                        
                           c
                        
                        =10 for MRF [22] with the weighting coefficient being 1, which regularizes the weight between data and smoothness terms.

We conduct experiments first on real-world scenarios which are either collected by ourselves or taken from Berkeley 3-D Object Dataset [10]. Color images and aligned depth maps are captured via Kinect sensors in indoor environments. Each image is at the resolution of 640×480. Figs. 4–6 illustrate some representative examples. The scenarios are chosen to include both slim and large holes resulted from occlusion, dark or reflective surfaces, or other factors.

From the results we can get the following observations. By taking advantage of color information, the proposed method, that is the guided FMM followed with a guided filter, outperforms the original FMM in all cases. In particular, we can tell the differences by observing the hand and the bookshelf in Fig. 4, the bookshelf in Fig. 5, and the screens in Fig. 6. Bilateral filter is not able to fill in large holes if its kernel size is not large enough, meaning that we have to change the kernel size for different cases. When large unknown regions exist, even if we enlarge the kernel size to get all inpainted, the results are still not good (It is shown in the following set of experiments). Moreover, our method achieves visually comparable results with MRF, while MRF is prone to produce blur results and takes much longer time.

It is not handy for us to get the ground truth for real-world scenarios. Hence, in order to acquire quantitative comparisons, we hereby perform the algorithms on the Middlebury stereo dataset [45]. Depth maps in the dataset are used as the ground truth. To simulate the defective effects of real range imaging sensors, noisy and incomplete maps are synthesized by first adding noise and then create certain invalid regions on the ground truth. As investigated in [46], the noise of Kinect-like sensors consists of a white Gaussian noise, together with a deterministic noise that is proportional to range. Hence, for the sake of simplicity, we adopt the model
                           
                              (10)
                              
                                 
                                    N
                                    
                                       r
                                    
                                    =
                                    
                                       κ
                                       1
                                    
                                    r
                                    +
                                    
                                       κ
                                       2
                                    
                                    f
                                    
                                       r
                                    
                                 
                              
                           
                        where N(r) denotes the noise at the range r, f(r) is a random noise drawn from a zero-mean normal distribution with a variance σ
                        
                           r
                        , and κ
                        1 and κ
                        2 are two coefficients. In our experiments we set κ
                        1
                        =0.001, κ
                        2
                        =2, and the signal-to-noise ratio (SNR) of the added white noise is 25dB. In addition, the invalid regions are created to simulate slim holes caused by occlusions and large holes resulted from surface reflection. These regions, whose thicknesses range from 20 to 70pixels, are marked in black and illustrated in Figs. 7–11
                        
                        
                        
                        
                        .

In the experiments we select four typical images, respectively, from ‘Plastic’, ‘Moebius’, ‘Baby’, and ‘Cloth’ scenarios. Two of them are structure-rich images with homogeneous regions and the others contain complex textures, as shown in Figs. 7, and 9–11. Like previous experiments, we compare our algorithm with MRF, BF, and FMM, using the same parameter settings expect that the kernel size of BF is enlarged to 81×81 for ‘Plastic’ and ‘Baby’ scenarios. The visual results are presented in Figs. 7, and 9–11 (Some zoomed parts of the results in Fig. 7 are also presented in Fig. 8 in order to show the differences clearer.). Moreover, they are quantitatively evaluated in terms of the root mean square error (RMSE) against the ground truth. Table 1
                         summarizes the comparisons.


                        Figs. 7–11 validate the observations obtained from the experiments on real data. The performances exhibit even more obviously. For instance, from Figs. 7(d) and 10(d) we can easily observe that MRF produces blur depth boundaries on the inpainted regions. From Figs. 7(f) and 9–11(f) we know that the results of FMM are not satisfied. In addition, when we enlarge the kernel size of bilateral filter for large-hole scenarios, BF is able to inpaint the invalid regions but achieves poor performance, as shown in Figs. 7(e) and 10(e). Table 1 provides an objective comparison. From it we see that our algorithm performs best among all compared approaches.

@&#CONCLUSIONS@&#

To enhance defective depth maps captured by range imaging sensors, in this paper we have proposed a novel approach based upon FMM. Our method is able to take advantage of both color and depth information for depth enhancement. Comparative experiments show that our method outperforms other state-of-the-art local methods in terms of both visual quality and RMSE. Moreover, it achieves visually comparable results to time-consuming global methods. We believe that our method can provide better inputs to the applications based on range imaging sensors.

@&#ACKNOWLEDGMENTS@&#

This research work is partially supported by the National Natural Science Foundation of China via grants 61001171, 60534070, and 90820306, and the Chinese Universities Scientific Fund.

@&#REFERENCES@&#

