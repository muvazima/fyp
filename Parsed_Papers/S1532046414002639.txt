@&#MAIN-TITLE@&#Stable feature selection for clinical prediction: Exploiting ICD tree structure using Tree-Lasso

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We model new application of Tree-Lasso for stable feature selection in healthcare.


                        
                        
                           
                           Tree-Lasso finds more stable features compared to other feature selection methods.


                        
                        
                           
                           Tree-Lasso results in better prediction accuracy compared to other methods.


                        
                        
                           
                           The features selected by Tree-Lasso are consistent with those used by clinicians.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Feature selection

Lasso

Tree-Lasso

Feature stability

Classification

@&#ABSTRACT@&#


               
               
                  Modern healthcare is getting reshaped by growing Electronic Medical Records (EMR). Recently, these records have been shown of great value towards building clinical prediction models. In EMR data, patients’ diseases and hospital interventions are captured through a set of diagnoses and procedures codes. These codes are usually represented in a tree form (e.g. ICD-10 tree) and the codes within a tree branch may be highly correlated. These codes can be used as features to build a prediction model and an appropriate feature selection can inform a clinician about important risk factors for a disease. Traditional feature selection methods (e.g. Information Gain, T-test, etc.) consider each variable independently and usually end up having a long feature list. Recently, Lasso and related 
                        
                           
                              
                                 l
                              
                              
                                 1
                              
                           
                        
                     -penalty based feature selection methods have become popular due to their joint feature selection property. However, Lasso is known to have problems of selecting one feature of many correlated features randomly. This hinders the clinicians to arrive at a stable feature set, which is crucial for clinical decision making process. In this paper, we solve this problem by using a recently proposed Tree-Lasso model. Since, the stability behavior of Tree-Lasso is not well understood, we study the stability behavior of Tree-Lasso and compare it with other feature selection methods. Using a synthetic and two real-world datasets (Cancer and Acute Myocardial Infarction), we show that Tree-Lasso based feature selection is significantly more stable than Lasso and comparable to other methods e.g. Information Gain, ReliefF and T-test. We further show that, using different types of classifiers such as logistic regression, naive Bayes, support vector machines, decision trees and Random Forest, the classification performance of Tree-Lasso is comparable to Lasso and better than other methods. Our result has implications in identifying stable risk factors for many healthcare problems and therefore can potentially assist clinical decision making for accurate medical prognosis.
               
            

@&#INTRODUCTION@&#

Recent advances in information technology has changed the way health care is carried out and documented [37]. Nowadays, not only traditional clinical narrative but also other types of data related to healthcare such as laboratory test results, medications and radiological images are automatically captured by databases in modern health centers. Clinical data describing the phenotypes and treatment of patients represents an underused data source that has a great research potential. Mining of Electronic Medical Records (EMR) can yield useful patterns that support clinical research and decision making [21]. The EMR contains rich information about a patient, including demographics, history of hospital visits, diagnoses, physiological measurements and interventions. As these data come with considerable amount of irrelevant and redundant features where only a subset of these features are useful for prediction, feature selection plays an important role to identify important features for building predictive models. For example, it is crucial to identify risk factors of cancer mortality for designing care plan and prognosis of a cancer patient. Similarly, it may be useful to find risk factors that are responsible for avoidable hospital re-admissions to reduce the cost of healthcare.

Feature selection methods can be broadly classified into three categories: (1) Filter methods such as T-test, Information Gain [7], ReliefF [52], and Chi Square [31] that assess the relevance of features by looking only at the intrinsic properties of the data. These methods consider each feature separately and ignore dependencies between features. Hence, comparing to other types of feature selection methods, they may cause long feature lists. (2) Wrapper methods such as Beam search [46], Sequential forward selection (SFS) [25], and Sequential backward elimination (SBE) [25] that utilize a supervised learning algorithm in the process of selecting feature subsets. The downside of these techniques is their high cost of computation and the risk of an over-fitted model [23]. (3) Embedded methods such as Weighted naive Bayes [10] and Lasso [49] search for an optimal subset of features, which is built within the classifier construction, and can be seen as a search in the combined space of feature subsets (joint feature selection property) and hypotheses. In this context 
                        
                           
                              
                                 ℓ
                              
                              
                                 1
                              
                           
                        
                     -norm methods such as Lasso has received an increasing attention. Using 
                        
                           
                              
                                 ℓ
                              
                              
                                 1
                              
                           
                        
                     -norm penalty, Lasso regularizes linear models and achieves automatic feature selection by driving some coefficients toward zero.


                     Beyond classifier performance, the other main objective of feature selection is to obtain a stable list of features 
                     [23,44]. The stability of feature selection methods has been used to examine their sensitivity to changes in input data and is defined as the degree of agreement of classification models produced by an algorithm when trained on different training sets. The stability is an important property in applications where the features carry intuitive meanings and actions are taken based on these features, e.g. risk factors for cancer survival or hospital readmission prediction must be stable to help a practitioner towards making clinical decisions. The need for stable feature sets has also been strongly felt in pattern recognition community [22,27].

Despite obtaining great success in many applications with high dimensional data [43,45,54], Lasso is known to be unstable when features exhibit strong correlations [56,59]. In these situations, Lasso shows acceptable predictive performance but selected predictors are quite sensitive to small changes in data and vary drastically. This variability is due to the property that among several correlated variables, Lasso penalty term (
                        
                           
                              
                                 ℓ
                              
                              
                                 1
                              
                           
                        
                     -norm) tends to select one of them randomly [49,60]. Different methods have been proposed to solve this problem. Elastic net is one of these remedies that reduce this randomness by adding a convex penalty term (squared 
                        
                           
                              
                                 ℓ
                              
                              
                                 2
                              
                           
                        
                     -norm) [60]. However, it is not capable of exploiting the correlation structure of the data. Other methods use sub-sampling techniques [2,35], where only those features are taken that are stable across several sub-samples. Group Lasso offers another solution when the features form different groups and the variables within a group are correlated. Feature selection is performed at group level by penalizing the sum of the 
                        
                           
                              
                                 ℓ
                              
                              
                                 2
                              
                           
                        
                     -norm of these groups, so that if a group is selected then all the features in that group are selected [20,56]. In many problems, features can naturally be represented using certain tree structures e.g. ICD-10 codes used in healthcare data have an intrinsic tree structure, (Fig. 1
                      shows an example of ICD-10 codes for diseases of musculoskeletal system and connective tissue). For such problems, application of group-Lasso is not straight forward.

Addressing these problems, we propose to use Tree-Lasso algorithm – a technique which has been proposed as a prediction model for classifying images where its pixels are considered to be the features lying on a tree [32]. However, the stability behavior of Tree-Lasso is not well understood. In this paper we take up this problem and study the stability behavior of Tree-Lasso. To do so, we use two different stability measures Spearman’s rank correlation coefficient and Jaccard similarity measure. We compare its stability with stability of other feature selection methods such as T-test, Information Gain (IG), ReliefF, and Lasso using a synthetic and two real-world datasets: Cancer cohort and Acute Myocardial Infarction cohort. Furthermore, we evaluate predictive performance of Tree-Lasso with other feature selection methods using several classifiers namely, logistic regression, naive Bayes, SVM, decision trees and Random Forest.

In summary, Our main contributions are:
                        
                           •
                           Introducing novel application of Tree-Lasso algorithm to obtain stable feature sets for developing healthcare predictive models from ICD codes.

An extensive experimental study that shows stability behavior of Tree-Lasso based feature selection is significantly better than Lasso and comparable with other feature selection algorithms.

Assessing thepredictive performance of models with the corresponding feature sets using several classifiers, e.g. logistic regression, naive Bayes, SVM, decision trees and Random Forest and find that under the constraint of stable feature selection, Tree-Lasso prediction performance is always better than that of many feature selection algorithms, namely T-test, IG, ReliefF, and Lasso.

Comparing the risk-factors obtained using Tree-Lasso with the list of features used by clinical experts and find that many risk factors found by Tree-Lasso are consistent with those used by domain experts.

Our findings have implications in identifying stable risk factors for many healthcare problems and therefore assist clinicians and patients to arrive at a better care plan and prognosis. Although, we have applied our model for healthcare data, it is applicable to other real-world problems where features are hierarchical in nature and stability of features is important.

The stability of a feature selection algorithm is the robustness of algorithm in selecting features in different training sets which are drawn from same distribution [29,33]. Different methods have been proposed to assess the stability of feature selection algorithms [22]. These methods can be categorized into three groups based on the representation of the selected features used by a specific feature selection algorithm. First group, known as stability by index, considers the indices of the selected features. In this category, the selected features have no particular order or corresponding relevance weight. In the second group, known as stability by weight degree of relevance of each feature is considered by a weight that is assigned to the feature. In the third group, which is called stability by rank, the features order is important in evaluation of stability. In this group, each feature is assigned by a rank that shows the feature importance.

In order to measure similarity between subsets of features we use Jaccard index. Jaccard index is a metric that measures the similarity between two sets. Given two sets 
                        
                           
                              
                                 S
                              
                              
                                 q
                              
                           
                        
                      and 
                        
                           
                              
                                 S
                              
                              
                                 
                                    
                                       q
                                    
                                    
                                       ′
                                    
                                 
                              
                           
                        
                     , the Jaccard index 
                        
                           J
                           
                              
                                 
                                    
                                       
                                          S
                                       
                                       
                                          q
                                       
                                    
                                    ,
                                    
                                       
                                          S
                                       
                                       
                                          
                                             
                                                q
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                      is defined as
                        
                           (1)
                           
                              J
                              (
                              
                                 
                                    S
                                 
                                 
                                    q
                                 
                              
                              ,
                              
                                 
                                    S
                                 
                                 
                                    
                                       
                                          q
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                              )
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   S
                                                
                                                
                                                   q
                                                
                                             
                                             
                                                ⋂
                                             
                                             
                                                
                                                   S
                                                
                                                
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   S
                                                
                                                
                                                   q
                                                
                                             
                                             
                                                ⋃
                                             
                                             
                                                
                                                   S
                                                
                                                
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     To define a stability measure using Jaccard index, we generate Q sub-samples of the training data, indexed as 
                        
                           q
                           =
                           1
                           ,
                           …
                           ,
                           Q
                        
                     . For each sub-sample, we run feature selection model and obtain a feature set, denoted by 
                        
                           
                              
                                 S
                              
                              
                                 q
                              
                           
                        
                     . Given feature sets 
                        
                           
                              
                                 S
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 S
                              
                              
                                 Q
                              
                           
                        
                     , the Jaccard stability measure (JSM) is defined as the average of Jaccard indices over each pair of feature sets, i.e. 
                        
                           J
                           
                              
                                 
                                    
                                       
                                          S
                                       
                                       
                                          q
                                       
                                    
                                    ,
                                    
                                       
                                          S
                                       
                                       
                                          
                                             
                                                q
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     . Formally, we have
                        
                           (2)
                           
                              JSM
                              =
                              
                                 
                                    2
                                 
                                 
                                    Q
                                    (
                                    Q
                                    -
                                    1
                                    )
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       q
                                       =
                                       1
                                    
                                    
                                       Q
                                       -
                                       1
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             q
                                          
                                          
                                             ′
                                          
                                       
                                       =
                                       q
                                       +
                                       1
                                    
                                    
                                       Q
                                    
                                 
                              
                              J
                              
                                 
                                    
                                       
                                          
                                             S
                                          
                                          
                                             q
                                          
                                       
                                       ,
                                       
                                          
                                             S
                                          
                                          
                                             
                                                
                                                   q
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              .
                           
                        
                     
                  

If we assume that a feature selection algorithm assigns a weight to each feature, to evaluate the similarity between two weight vectors 
                     
                        
                           β
                           ,
                           
                           
                              
                                 β
                              
                              
                                 ′
                              
                           
                        
                     , we use Pearson’s correlation coefficient:
                        
                           (3)
                           
                              PCC
                              (
                              β
                              ,
                              
                                 
                                    β
                                 
                                 
                                    ′
                                 
                              
                              )
                              =
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                       
                                    
                                    (
                                    
                                       
                                          β
                                       
                                       
                                          j
                                       
                                    
                                    -
                                    
                                       
                                          μ
                                       
                                       
                                          β
                                       
                                    
                                    )
                                    (
                                    
                                       
                                          β
                                       
                                       
                                          j
                                       
                                       
                                          ′
                                       
                                    
                                    -
                                    
                                       
                                          μ
                                       
                                       
                                          
                                             
                                                β
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                    )
                                 
                                 
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                j
                                             
                                          
                                          
                                             
                                                (
                                                
                                                   
                                                      β
                                                   
                                                   
                                                      j
                                                   
                                                
                                                -
                                                
                                                   
                                                      μ
                                                   
                                                   
                                                      β
                                                   
                                                
                                                )
                                             
                                             
                                                2
                                             
                                          
                                          
                                             
                                                ∑
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                (
                                                
                                                   
                                                      β
                                                   
                                                   
                                                      j
                                                   
                                                   
                                                      ′
                                                   
                                                
                                                -
                                                
                                                   
                                                      μ
                                                   
                                                   
                                                      
                                                         
                                                            β
                                                         
                                                         
                                                            ′
                                                         
                                                      
                                                   
                                                
                                                )
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                              .
                           
                        
                     
                     
                        
                           PCC
                           (
                           β
                           ,
                           
                              
                                 β
                              
                              
                                 ′
                              
                           
                           )
                        
                      takes values in 
                        
                           [
                           -
                           1
                           ,
                           1
                           ]
                        
                     , where 
                        
                           PCC
                           (
                           β
                           ,
                           
                              
                                 β
                              
                              
                                 ′
                              
                           
                           )
                           =
                           1
                        
                      implies that the feature weights are completely correlated, 
                        
                           PCC
                           (
                           β
                           ,
                           
                              
                                 β
                              
                              
                                 ′
                              
                           
                           )
                           =
                           0
                        
                      implies that feature weights are uncorrelated and 
                        
                           PCC
                           (
                           β
                           ,
                           
                              
                                 β
                              
                              
                                 ′
                              
                           
                           )
                           =
                           -
                           1
                        
                      implies that feature weights are anti correlated.

Given weight vectors 
                        
                           
                              
                                 β
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 β
                              
                              
                                 Q
                              
                           
                        
                     , we define PCC as the average of Pearson’s correlation coefficient over each pair of weights of feature sets i.e. 
                        
                           PCC
                           (
                           
                              
                                 β
                              
                              
                                 q
                              
                           
                           ,
                           
                              
                                 β
                              
                              
                                 
                                    
                                       q
                                    
                                    
                                       ′
                                    
                                 
                              
                           
                           )
                        
                      as follows:
                        
                           (4)
                           
                              PCC
                              =
                              
                                 
                                    2
                                 
                                 
                                    Q
                                    (
                                    Q
                                    -
                                    1
                                    )
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       q
                                       =
                                       1
                                    
                                    
                                       Q
                                       -
                                       1
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             q
                                          
                                          
                                             ′
                                          
                                       
                                       =
                                       q
                                       +
                                       1
                                    
                                    
                                       Q
                                    
                                 
                              
                              PCC
                              (
                              β
                              ,
                              
                                 
                                    β
                                 
                                 
                                    ′
                                 
                              
                              )
                              .
                           
                        
                     
                  

We construct a ranking (denoted by r) over features by sorting the weight vector β. To measure rank based similarity between any two rankings r and 
                        
                           
                              
                                 r
                              
                              
                                 ′
                              
                           
                        
                      , we use Spearman’s rank correlation coefficient:
                        
                           (5)
                           
                              SRCC
                              (
                              r
                              ,
                              
                                 
                                    r
                                 
                                 
                                    ′
                                 
                              
                              )
                              =
                              1
                              -
                              6
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                    
                                 
                              
                              
                                 
                                    (
                                    
                                       
                                          r
                                       
                                       
                                          j
                                       
                                    
                                    -
                                    
                                       
                                          r
                                       
                                       
                                          j
                                       
                                       
                                          ′
                                       
                                    
                                    )
                                 
                                 
                                    m
                                    (
                                    
                                       
                                          m
                                       
                                       
                                          2
                                       
                                    
                                    -
                                    1
                                    )
                                 
                              
                              ,
                           
                        
                     where 
                        
                           
                              
                                 r
                              
                              
                                 j
                              
                           
                        
                      and 
                        
                           
                              
                                 r
                              
                              
                                 j
                              
                              
                                 ′
                              
                           
                        
                      are the ranks of jth feature in rankings r and 
                        
                           
                              
                                 r
                              
                              
                                 ′
                              
                           
                        
                      and m is the size of the whole feature set. Similar to Pearson’s correlation, the possible range of values are 
                        
                           [
                           -
                           1
                           ,
                           1
                           ]
                        
                     , where 1 means that the two rankings are identical, 0 means that there is no correlation between two ranks, and a value of −1 means that rankings are in reverse order.

Given rankings 
                        
                           
                              
                                 r
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 r
                              
                              
                                 Q
                              
                           
                        
                     , we define SRCC as the average of Spearman’s rank correlation over each pair of ranks of feature sets, so we have:
                        
                           (6)
                           
                              SRCC
                              =
                              
                                 
                                    2
                                 
                                 
                                    Q
                                    (
                                    Q
                                    -
                                    1
                                    )
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       q
                                       =
                                       1
                                    
                                    
                                       Q
                                       -
                                       1
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             q
                                          
                                          
                                             ′
                                          
                                       
                                       =
                                       q
                                       +
                                       1
                                    
                                    
                                       Q
                                    
                                 
                              
                              SRCC
                              (
                              r
                              ,
                              
                                 
                                    r
                                 
                                 
                                    ′
                                 
                              
                              )
                              .
                           
                        
                     
                  

Based on the fact that PCC works directly on the weight vectors that are obtained using each feature selection method, which may use different scales to assign weights and so its results may not be directly comparable across different methods. Therefore, in this paper we only use SRCC and JSM to assess stability of each feature selection method.

We study the stability behavior of Tree-Lasso and compare it with various feature selection methods, namely T-test, Information gain, ReliefF and Lasso. In the following, we briefly describe Tree-Lasso and the other feature selection methods. Furthermore, we evaluate the predictive performance of obtained features using each feature selection method by using different types of classifiers such as logistic regression (LR), naive Bayes (NB), support vector machines (SVM), decision trees (DT) and Random Forest (RF). In Section 3.1 we briefly introduce feature selection methods used in this paper and in Section 3.2 we introduce classifiers used for evaluating predictive performance of each feature selection method.

In large datasets in order to determine which input features are more important, feature ranking is often used. One of the most important feature ranking measures is T-test. It calculates a ratio between the difference of two class means and the variability of the two classes. Using this ratio we can assess whether the means of two classes are statistically different from each other. In a binary classification problem, for T-test we compute the following test statistic for each feature 
                              
                                 
                                    
                                       f
                                    
                                    
                                       j
                                    
                                 
                              
                           
                           
                              
                                 (7)
                                 
                                    t
                                    (
                                    
                                       
                                          f
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      f
                                                   
                                                   
                                                      ¯
                                                   
                                                
                                             
                                             
                                                j
                                                0
                                             
                                          
                                          -
                                          
                                             
                                                
                                                   
                                                      f
                                                   
                                                   
                                                      ¯
                                                   
                                                
                                             
                                             
                                                j
                                                1
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            s
                                                         
                                                         
                                                            j
                                                            0
                                                         
                                                         
                                                            2
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            N
                                                         
                                                         
                                                            0
                                                         
                                                      
                                                   
                                                
                                                +
                                                
                                                   
                                                      
                                                         
                                                            s
                                                         
                                                         
                                                            j
                                                            1
                                                         
                                                         
                                                            2
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            N
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 
                                    
                                       
                                          
                                             f
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       j
                                       0
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       
                                          
                                             f
                                          
                                          
                                             j
                                             1
                                          
                                       
                                    
                                    
                                       ¯
                                    
                                 
                              
                            are the feature means for class 0 and class 1, 
                              
                                 
                                    
                                       s
                                    
                                    
                                       j
                                       0
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       s
                                    
                                    
                                       j
                                       1
                                    
                                 
                              
                            are the standard deviation of feature 
                              
                                 
                                    
                                       f
                                    
                                    
                                       j
                                    
                                 
                              
                            from class 0 and class 1 and 
                              
                                 
                                    
                                       N
                                    
                                    
                                       0
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       N
                                    
                                    
                                       1
                                    
                                 
                              
                            are size of class 0 and class 1, respectively. In this method after calculating t for each feature, best features (those who have p-value≤0.05) are selected as final feature set.

Information Gain (IG) [7] is one of the most important feature ranking methods, which measures dependency between a feature and a class label. IG of jth feature 
                              
                                 
                                    
                                       f
                                    
                                    
                                       j
                                    
                                 
                              
                            and class y is calculated as
                              
                                 (8)
                                 
                                    IG
                                    (
                                    y
                                    |
                                    
                                       
                                          f
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    =
                                    H
                                    (
                                    y
                                    )
                                    -
                                    H
                                    (
                                    y
                                    |
                                    
                                       
                                          f
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    ,
                                 
                              
                           where 
                              
                                 H
                                 (
                                 ·
                                 )
                              
                            is the entropy and is a measure of the uncertainty of a random variable. If we assume that we have a two-class classification problem, 
                              
                                 H
                                 (
                                 y
                                 )
                              
                            and 
                              
                                 H
                                 (
                                 y
                                 |
                                 
                                    
                                       f
                                    
                                    
                                       j
                                    
                                 
                                 )
                              
                            are defined as follows:
                              
                                 (9)
                                 
                                    H
                                    (
                                    y
                                    )
                                    =
                                    -
                                    
                                       
                                          
                                             P
                                             (
                                             y
                                             =
                                             0
                                             )
                                             logP
                                             (
                                             y
                                             =
                                             0
                                             )
                                             +
                                             P
                                             (
                                             y
                                             =
                                             1
                                             )
                                             logP
                                             (
                                             y
                                             =
                                             1
                                             )
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (10)
                                 
                                    H
                                    (
                                    y
                                    |
                                    
                                       
                                          f
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    =
                                    P
                                    (
                                    y
                                    =
                                    0
                                    |
                                    
                                       
                                          f
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    log
                                    P
                                    (
                                    y
                                    =
                                    0
                                    |
                                    
                                       
                                          f
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    +
                                    P
                                    (
                                    y
                                    =
                                    1
                                    |
                                    
                                       
                                          f
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    log
                                    P
                                    (
                                    y
                                    =
                                    1
                                    |
                                    
                                       
                                          f
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    .
                                 
                              
                           In this method, for each feature we evaluate IG independently and top K features are selected as the final feature set.

Relief [24] is a supervised feature selection algorithm for binary classification problems. It randomly samples instances from the training data and for each sample computes the nearest instance of the same class called “near-hit” and the nearest instance of the different class called “near-miss”. The score 
                              
                                 S
                                 (
                                 j
                                 )
                              
                            of the jth feature is updated in each iteration of algorithm as follows:
                              
                                 (11)
                                 
                                    
                                       
                                          S
                                       
                                       
                                          t
                                       
                                    
                                    (
                                    j
                                    )
                                    =
                                    
                                       
                                          S
                                       
                                       
                                          t
                                          -
                                          1
                                       
                                    
                                    (
                                    j
                                    )
                                    -
                                    
                                       
                                          d
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                t
                                             
                                          
                                          -
                                          
                                             
                                                nearHit
                                             
                                             
                                                t
                                             
                                          
                                          )
                                       
                                       
                                          n
                                       
                                    
                                    +
                                    
                                       
                                          d
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                t
                                             
                                          
                                          -
                                          
                                             
                                                nearMiss
                                             
                                             
                                                t
                                             
                                          
                                          )
                                       
                                       
                                          n
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 
                                    
                                       x
                                    
                                    
                                       t
                                    
                                 
                              
                            is the random instance at iteration 
                              
                                 t
                                 ,
                                 n
                              
                            is the number of randomly sampled examples, and 
                              
                                 d
                                 (
                                 ·
                                 )
                              
                            is the Euclidean distance measure. Kononeko et al. [26] proposed ReliefF by using Manhattan (
                              
                                 
                                    
                                       l
                                    
                                    
                                       1
                                    
                                 
                              
                           ) norm instead of Euclidean (
                              
                                 
                                    
                                       l
                                    
                                    
                                       2
                                    
                                 
                              
                           ) norm for finding near-hit and near-miss. For selecting final feature set using ReliefF, we compute S score for each feature and select top K features with best S score as the final selected features.

Lasso is a regularization method that is used to learn a regularized regression/classification model that is sparse in the feature space [49]. Consider a supervised learning problem consisting of N training instances denoted as 
                              
                                 
                                    
                                       
                                          (
                                          
                                             
                                                x
                                             
                                             
                                                (
                                                i
                                                )
                                             
                                          
                                          ,
                                          
                                             
                                                y
                                             
                                             
                                                (
                                                i
                                                )
                                             
                                          
                                          ,
                                          i
                                          =
                                          1
                                          ,
                                          …
                                          N
                                       
                                    
                                 
                              
                           , where each 
                              
                                 
                                    
                                       x
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                 
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       P
                                    
                                 
                              
                            is a P-dimensional feature vector and 
                              
                                 
                                    
                                       y
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                 
                                 ∈
                                 {
                                 0
                                 ,
                                 1
                                 }
                              
                            is a class label. For classification problems, Lasso is used with logistic regression [19], which models the probability distribution of the class label 
                              
                                 
                                    
                                       y
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                 
                              
                            given a feature vector 
                              
                                 
                                    
                                       x
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                 
                              
                            as
                              
                                 (12)
                                 
                                    p
                                    (
                                    
                                       
                                          y
                                       
                                       
                                          (
                                          i
                                          )
                                       
                                    
                                    =
                                    1
                                    |
                                    
                                       
                                          x
                                       
                                       
                                          (
                                          i
                                          )
                                       
                                    
                                    ;
                                    β
                                    )
                                    =
                                    σ
                                    (
                                    
                                       
                                          β
                                       
                                       
                                          T
                                       
                                    
                                    
                                       
                                          x
                                       
                                       
                                          (
                                          i
                                          )
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          1
                                       
                                       
                                          1
                                          +
                                          exp
                                          (
                                          -
                                          
                                             
                                                β
                                             
                                             
                                                T
                                             
                                          
                                          
                                             
                                                x
                                             
                                             
                                                (
                                                i
                                                )
                                             
                                          
                                          )
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 β
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       P
                                    
                                 
                              
                            is a parameter of the logistic regression model and 
                              
                                 σ
                                 (
                                 ·
                                 )
                              
                            is the sigmoid function. The parameter β is also known as classification weight vector. The Lasso regularization acts by penalizing the sum of absolute value of weights, i.e. 
                              
                                 
                                    
                                       ℓ
                                    
                                    
                                       1
                                    
                                 
                              
                           -norm of β, denoted as 
                              
                                 |
                                 |
                                 β
                                 |
                                 
                                    
                                       |
                                    
                                    
                                       1
                                    
                                 
                              
                           . The combined optimization function can be written as
                              
                                 (13)
                                 
                                    
                                       
                                          
                                             min
                                          
                                          
                                             β
                                          
                                       
                                    
                                    
                                       
                                          
                                             -
                                             
                                                
                                                   Σ
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   N
                                                
                                             
                                             log
                                             p
                                             
                                                
                                                   
                                                      
                                                         
                                                            y
                                                         
                                                         
                                                            (
                                                            i
                                                            )
                                                         
                                                      
                                                      =
                                                      1
                                                      |
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            (
                                                            i
                                                            )
                                                         
                                                      
                                                      ;
                                                      β
                                                   
                                                
                                             
                                             +
                                             λ
                                             
                                                
                                                   
                                                      
                                                         
                                                            Σ
                                                         
                                                         
                                                            j
                                                            =
                                                            1
                                                         
                                                         
                                                            p
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     β
                                                                  
                                                                  
                                                                     j
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where λ is a non-negative regularization term. The solution of the above optimization does not have a closed form and is usually found iteratively by minimizing the cost function using pathwise co-ordinate optimization [13]. By increasing the regularization parameter λ, Lasso increasingly shrinks the coefficients toward 0. A large enough λ, makes some of the weights to become exactly zero.

In many applications, the features can be naturally represented as a tree structure, e.g. ICD-10 features in healthcare data form a tree. ICD-10 is “standard diagnostic tool for epidemiology, health management and clinical purposes”.
                              1
                              
                                 http://www.who.int/classifications/icd/en/.
                           
                           
                              1
                            
                           Fig. 2
                            shows a part of ICD-10 tree relevant to Cancer dataset used in this paper. The set of diseases shown here relates to the musculoskeletal system (ICD-10 codes: 
                              
                                 M
                                 00
                              
                            up to 
                              
                                 M
                                 99
                              
                           ). According to ICD-10 hierarchy, these diseases are classified into 6 groups and each of these groups are further classified into several subgroups, giving rise to a tree-structure. We note that the grouping of codes is mostly based on disease similarity and co-occurrences causing correlations among features. Due to using a flat 
                              
                                 
                                    
                                       l
                                    
                                    
                                       1
                                    
                                 
                              
                           -penalty on features, Lasso randomly selects only one feature from every such correlated set. Although Lasso mechanism for feature selection results in selecting less features, it causes that this method to be unstable in selecting important features. This drawback of Lasso is undesirable in many real-world applications such as clinical prediction.

For classification and regression problems having hierarchical features, a more suitable model is the Tree-Lasso [32] as it can exploit the feature correlations in the form of a tree-structure. In this context, the definition of a tree is as follows. For a tree T of depth d, all the nodes corresponding to depth i are in 
                              
                                 
                                    
                                       T
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 {
                                 
                                    
                                       G
                                    
                                    
                                       1
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       G
                                    
                                    
                                       2
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       G
                                    
                                    
                                       
                                          
                                             n
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       i
                                    
                                 
                                 }
                              
                           , where 
                              
                                 
                                    
                                       G
                                    
                                    
                                       j
                                    
                                    
                                       i
                                    
                                 
                              
                            denotes the jth node at depth 
                              
                                 i
                                 ,
                                 
                                    
                                       n
                                    
                                    
                                       0
                                    
                                 
                                 =
                                 1
                                 ,
                                 
                                    
                                       G
                                    
                                    
                                       1
                                    
                                    
                                       0
                                    
                                 
                                 =
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 p
                                 }
                              
                            and 
                              
                                 
                                    
                                       n
                                    
                                    
                                       i
                                    
                                 
                                 ⩾
                                 1
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 d
                              
                           . The nodes must satisfy the following two conditions:
                              
                                 1.
                                 The nodes at the same depth should not have overlapping indices.

The index set of a child node is a subset of its parent node.

Given the above definition of feature tree, Tree-Lasso learns the classification weight vector β by minimizing the following cost function
                              
                                 (14)
                                 
                                    
                                       
                                          
                                             min
                                          
                                          
                                             β
                                          
                                       
                                    
                                    
                                       
                                          
                                             -
                                             
                                                
                                                   Σ
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   N
                                                
                                             
                                             log
                                             p
                                             
                                                
                                                   
                                                      
                                                         
                                                            y
                                                         
                                                         
                                                            (
                                                            i
                                                            )
                                                         
                                                      
                                                      =
                                                      1
                                                      |
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            (
                                                            i
                                                            )
                                                         
                                                      
                                                      ;
                                                      β
                                                   
                                                
                                             
                                             +
                                             λ
                                             ϕ
                                             (
                                             β
                                             )
                                          
                                       
                                    
                                 
                              
                           where λ is a non-negative regularization parameter. The regularization term 
                              
                                 ϕ
                                 (
                                 β
                                 )
                              
                            is given by
                              
                                 (15)
                                 
                                    ϕ
                                    (
                                    β
                                    )
                                    =
                                    
                                       
                                          Σ
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          d
                                       
                                    
                                    
                                       
                                          Σ
                                       
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          
                                             
                                                n
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    
                                       
                                          w
                                       
                                       
                                          j
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   β
                                                
                                                
                                                   
                                                      
                                                         G
                                                      
                                                      
                                                         j
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 β
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       P
                                    
                                 
                              
                            is the weight for node 
                              
                                 
                                    
                                       G
                                    
                                    
                                       j
                                    
                                    
                                       i
                                    
                                 
                              
                           , and 
                              
                                 
                                    
                                       β
                                    
                                    
                                       
                                          
                                             G
                                          
                                          
                                             j
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                              
                            is a vector composed of the entries of β with the indices in 
                              
                                 
                                    
                                       G
                                    
                                    
                                       j
                                    
                                    
                                       i
                                    
                                 
                              
                           . The other parameter in the regularization term is 
                              
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                    
                                       i
                                    
                                 
                                 
                                 (
                                 i
                                 =
                                 0
                                 ,
                                 1
                                 ,
                                 …
                                 ,
                                 d
                                 ;
                                 
                                 j
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 
                                    
                                       n
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                           , which is a predefined weight for the node 
                              
                                 
                                    
                                       G
                                    
                                    
                                       j
                                    
                                    
                                       i
                                    
                                 
                              
                           . As mentioned in [32], this parameter can be set according to importance of feature groups. In our application, since we do not have any prior knowledge about importance of feature groups, we use 
                              
                                 
                                    
                                       ω
                                    
                                    
                                       j
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 1
                              
                            for all the groups.

To solve the problem efficiently, the term 
                              
                                 ϕ
                                 (
                                 β
                                 )
                              
                            is re-formulated through Moreau–Yosida regularization as 
                              
                                 
                                    
                                       ϕ
                                    
                                    
                                       λ
                                    
                                 
                                 (
                                 v
                                 )
                                 =
                                 
                                    
                                       min
                                    
                                    
                                       β
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                1
                                             
                                             
                                                2
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      β
                                                      -
                                                      v
                                                   
                                                
                                             
                                             
                                                2
                                             
                                          
                                          +
                                          λ
                                          
                                             
                                                Σ
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                Σ
                                             
                                             
                                                j
                                             
                                          
                                          
                                             
                                                w
                                             
                                             
                                                j
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         β
                                                      
                                                      
                                                         
                                                            
                                                               G
                                                            
                                                            
                                                               j
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                            for some 
                              
                                 λ
                                 >
                                 0
                              
                           . It has been shown that the above problem admits an analytical solution. For details of the minimization, we refer the reader to [32].

Logistic regression is a linear classifier that models the posterior probabilities of the K classes via linear function in an example 
                              
                                 x
                              
                           . In logistic regression, the parameters of the model can be interpreted as changes in log odds and also the results can be interpreted in terms of probabilities [19]. Hence, logistic regression is a widely used classifier in medical domain [1,18,47]. Lasso and Tree-Lasso has built-in logistic regression algorithms. Therefore, these methods can perform feature selection and prediction, simultaneously.

Naive Bayes (NB) is a probabilistic classifier based on Bayes theorem [48]. It assumes that given a class label, all the features are independent and posterior probability that an example 
                              
                                 x
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       P
                                    
                                 
                              
                            is classified to class c is obtained as
                              
                                 (16)
                                 
                                    Pr
                                    (
                                    C
                                    =
                                    c
                                    |
                                    x
                                    )
                                    ∝
                                    Pr
                                    (
                                    C
                                    =
                                    c
                                    )
                                    
                                       
                                          
                                             ∏
                                          
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             P
                                          
                                       
                                    
                                    Pr
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                    |
                                    C
                                    =
                                    c
                                    )
                                    .
                                 
                              
                           
                        

Despite its unrealistic independence assumption, research shows that naive Bayes often works well in practice [42]. furthermore, due to its independence assumption, in naive Bayes the number of parameters (which is equal to the number of features) do not depend on the number of examples. This property helps naive Bayes to scale well for large problems.

Support vector machines (SVM) are a type of classifiers that work based on the principle of structural risk minimization (SRM) [6,50,51]. They have some advantages such as ability to handle large feature spaces, and avoidance of overfitting [55]. SVM uses inner product to measure the similarity or distance between patterns, which is known as kernel function. In our experiments, we use Gaussian RBF kernels, where the kernel width σ is of values 
                              
                                 {
                                 0.001
                                 ,
                                 0.005
                                 ,
                                 0.01
                                 ,
                                 0.05
                                 ,
                                 0.1
                                 ,
                                 0.5
                                 ,
                                 1
                                 ,
                                 2
                                 }
                              
                            and the value of box constraint C is varied between 
                              
                                 
                                    
                                       10
                                    
                                    
                                       -
                                       9
                                    
                                 
                              
                            to 
                              
                                 
                                    
                                       10
                                    
                                    
                                       5
                                    
                                 
                              
                            by factors of ten. The best parameters of σ and C are obtained using 5-fold cross validation.

Decision trees (DT) are well-known classification methods in the field of machine learning. Popular decision tree algorithms include ID3, C4.5, C5, and CART [4,38,39]. Decision trees recursively partition the data based on its features to construct a tree for the purpose of improving prediction accuracy. To achieve this, they use mathematical algorithms such as information gain (used in ID3, C4.5, C5), Gini index (used in CART), and Chi-squared test (used in CHAID) to specify the variable and its threshold that splits the data into two or more subgroups. The splitting of the data is repeated until the complete tree is constructed. Based on the favorable predictive performance, obtained from preliminary runs, in this study we chose CART as our decision tree method.

Random Forest (RF) is an ensemble classifier that generates multiple decision trees and aggregates their results [3]. Each tree is trained on a bootstrap sample of the training data. In addition, a subset of features is randomly selected to consider at each node of each decision tree. To classify an example, decisions (votes) of all trees in the forest are aggregated and the majority voting of the trees is considered as the output of the classifier.

In this paper we grow 100 trees in the forest and the number of features at each split is chosen as the square root of the number of features.

@&#EXPERIMENTS@&#

In our experiments, we have used both synthetic and real-world datasets and demonstrate the effectiveness of the proposed Tree-Lasso by carrying out the following comparisons.
                        
                           •
                           We show that stability behavior of Tree-Lasso is better compared to severalbaseline feature selection algorithms namely, T-test, IG, ReliefF and Lasso.

We compare the predictive performance of Tree-Lasso with other baseline feature selection algorithms by using them with different classifiers namely, logistic regression, naive Bayes, SVM, decision tree and Random Forest and show that under theconstraint of stable feature selection, Tree-Lasso prediction performance is constantly better than that of other baselines.

As an extra evaluation, we compare stability and predictive performance of Tree-Lasso (with built-in logistic regression) with Random Forest, which is a well-known embedded type feature selection method in machine learning and show that Tree-Lasso achieves better results in terms of both stability and prediction.

We show that the features obtained using Tree-Lasso for real-world datasets are consistent with the well-known risk factors used by experts in clinical domain.

To illustrate the stability behavior of different feature selection algorithms, we generate a synthetic data where features are grouped hierarchically in a tree structure. To keep the matter simple, we confine ourselves to shallow 2-level trees. In order to generate data, we fix the number of leaf nodes, referred to as groups. Each such group (or node) contains a set of variables such that the variables within a group are correlated to one another while uncorrelated with the variables from other groups. This is done by defining a correlation matrix C such that its 
                              
                                 
                                    
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                 
                              
                           -th element contains the correlation coefficient between i-th and j-th variables. Formally, the correlation matrix is defined as
                              
                                 (17)
                                 
                                    
                                       
                                          C
                                       
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      ρ
                                                   
                                                   
                                                      i
                                                      ,
                                                      j
                                                      
                                                      belong to the same group
                                                   
                                                
                                                
                                                   
                                                      0
                                                   
                                                   
                                                      otherwise
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           In order to generate upper layers of the tree, correlation between each pair of groups from the lower layer is calculated. For each group, we find the other group having the highest correlation with it and connect them to construct the upper layer of the tree. Given the above correlation matrix, the feature vector 
                              
                                 
                                    
                                       x
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                 
                              
                            is generated using a multivariate normal distribution having mean zero and covariance C, i.e. 
                              
                                 
                                    
                                       x
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                 
                                 ∼
                                 N
                                 (
                                 0
                                 ,
                                 C
                                 )
                                 ,
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 N
                              
                           . The true parameter vector β is
                              
                                 
                                    β
                                    =
                                    (
                                    
                                       
                                          
                                             
                                                0
                                                ,
                                                0
                                                ,
                                                …
                                                ,
                                                0
                                             
                                             
                                                ︸
                                             
                                          
                                       
                                       
                                          50
                                          times
                                       
                                    
                                    ,
                                    
                                       
                                          
                                             
                                                1
                                                ,
                                                1
                                                ,
                                                …
                                                1
                                             
                                             
                                                ︸
                                             
                                          
                                       
                                       
                                          50
                                          times
                                       
                                    
                                    )
                                    .
                                 
                              
                           Given i-th data vector 
                              
                                 
                                    
                                       x
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                 
                              
                            and the weight vector β, the label is generated as following
                              
                                 (18)
                                 
                                    
                                       
                                          y
                                       
                                       
                                          (
                                          i
                                          )
                                       
                                    
                                    =
                                    sign
                                    
                                       
                                          
                                             
                                                
                                                   β
                                                
                                                
                                                   T
                                                
                                             
                                             
                                                
                                                   x
                                                
                                                
                                                   
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                             +
                                             ∊
                                          
                                       
                                    
                                    ,
                                    
                                    ∊
                                    ∼
                                    N
                                    (
                                    0
                                    ,
                                    0.1
                                    )
                                    .
                                 
                              
                           For the results reported in this paper, we simulate 100 variables, grouped into 4 leaf nodes, i.e. the first 25 variables are part of group-1, the next 25 variables are part of group-2 and so on. Using these features, we generate 200 data samples. We generate two such datasets: one with low correlation (
                              
                                 ρ
                                 =
                                 0
                              
                           ) and the other with high correlation (
                              
                                 ρ
                                 =
                                 0.8
                              
                           ).

For experiments with real-world data, we used two hospital patient cohorts: Cancer and Acute Myocardial Infarction (AMI). A summary of statistics of the two datasets is provided in Table 1
                            and their details are described below:

This dataset is obtained from a large regional hospital in Australia.
                                 2
                                 Ethics approval obtained through university and the hospital −12/83.
                              
                              
                                 2
                               There are eleven different cancer types in this data recorded from patients visiting the hospital during 2010–2012. Patient data is acquired from Electronic Medical Records (EMR). The dataset consists of 4293 patients and their disease condition is described using 439 ICD codes (or features). Using this dataset, our goal is to predict 1year mortality of patients while ensuring the stable feature sets. We note that feature stability is crucial for clinical decision making towards cancer prognosis. This dataset has been previously used in [16].

This dataset is also obtained from the same hospital in Australia. The dataset involves patients admitted with AMI conditions and discharged later between 2007–2011. The task is to predict if a patient will be re-admitted to the hospital within 30days. The dataset consists of 2941 patients and their disease condition is described using 528 ICD codes. This dataset has been previously used in [41].

In order to compare the stability of Tree-Lasso with other feature selection algorithms, we use two different stability measures, Spearman’s rank correlation coefficient (SRCC), and Jaccard similarity measure (JSM), These stability measures are described in Section 2.

To compare the prediction performance of Tree-Lasso with other feature selection methods, we use the area under the receiver operating characteristic (ROC) curve, further abbreviated as AUC [12]. Due to its robustness across both balanced and imbalanced datasets, AUC is commonly used in clinical decision making and is becoming increasingly popular in pattern recognition community [8,53].

The Tree-Lasso model built in our experiments is based on ICD-10 codes that have intrinsic hierarchical structure. For example as it can be seen from Fig. 2 of the paper, ICD-10 codes of musculoskeletal system is in range of 
                           
                              M
                              00
                           
                         up to 
                           
                              M
                              99
                           
                        . According to ICD hierarchy, the next level of ICD tree further classifies these diseases (codes) into 6 groups 
                           
                              (
                              M
                              00
                              –
                              M
                              25
                              ,
                              M
                              30
                              –
                              M
                              36
                              ,
                              M
                              40
                              –
                              M
                              54
                              ,
                              M
                              60
                              –
                              M
                              79
                              ,
                              M
                              80
                              –
                              M
                              94
                           
                         and 
                           
                              M
                              95
                              –
                              M
                              99
                              )
                           
                        . These groups are also further classified into several subgroups. Essentially, at the leaf node, we have individual features that are grouped progressively in parent nodes as we move up in the ICD tree. So, feature encoding is such that if a patient record contains all the offspring of some parent node, we also set the parent node to 1 along with setting all offspring nodes to 1.

To assess the variability of the experiment, we randomly divide our data into two sets: 70% of our data is considered as training set and 30% as test set. For each random split, we further split the training set into two sets: derivation set (80% of the training set) and a validation set (20% of the training set). In order to be able to select the best features, this second split is randomly repeated 100 times to generate 100 sets of derivation-validation pairs. We train all the models using each derivation set while selecting the best model parameters through model performance on the corresponding validation set. This process provides us 100 feature sets. Using the ensemble of 100 feature sets, we empirically estimate the probability of presence for each feature. Given these probability estimates, we re-train a model using derivation dataset and including only those features that occur with at least probability p (a threshold that we gradually increase). Using 30% held out test set, the predictive performance of the model is evaluated using AUC while the stability of the model is computed using SRCC, and JSM.

@&#EXPERIMENTAL RESULTS@&#

In this section, we evaluate the stability performance of Tree-Lasso and compare it with other baseline feature selection methods. We also investigate the classification performance of each algorithm with different classifiers and measure their performance in terms of AUC. In addition, we study the consistency of the features obtained using Tree-Lasso for Cancer and AMI datasets with well-known risk factors in clinical domain.


                           Table 2
                            shows the stability results in terms of SRCC and JSM, for Tree-Lasso and baseline algorithms. In case of synthetic data, Tree-Lasso achieves the best stability performance in terms of both SRCC and JSM. The high value of SRCC in Tree-Lasso means that for different training sets the ranks of features does not vary a lot. On the other hand, the high value of JSM means that the feature set selected does not change significantly. In terms of JSM Tree-Lasso achieves the stability of 0.7830 (when 
                              
                                 ρ
                                 =
                                 0
                                 )
                              
                            and 0.8777 (when 
                              
                                 ρ
                                 =
                                 0.8
                              
                           ) for synthetic data which is higher compared to the other methods.

In case of Cancer dataset, Tree-Lasso again shows the best stability performance, in terms of SRCC. The SRCC results for T-test, IG and Lasso is poor, while that of ReliefF is somewhat average. When we turn to JSM Tree-Lasso is again the winner (0.7910) followed by IG (0.7863). The other methods achieve JSM value of 0.7576 (ReliefF), 0.5542 (Lasso) and 0.4553 (T-test).

For the AMI dataset, Tree-Lasso is once again the winner with SRCC=0.6274 and JSM=0.7147, followed by ReliefF and IG. The SRCC and JSM for T-test and Lasso are significantly lower.

In order to have a better understanding of the stability behavior of different feature selection methods for the datasets that containcorrelated variables i.e. synthetic data (
                              
                                 ρ
                                 =
                                 0.8
                              
                           ), Cancer data and AMI data, we show top ten features selected by various methods in different splits of data in Figs. 3–5
                           
                           
                           , respectively. From these figures not only we can visually compare the stability of different methods but also can infer which features are considered important by each algorithm. To better distinguish between stable features in these plots we use a threshold 
                              
                                 T
                              
                            and features that are selected with a probability more than 
                              
                                 T
                              
                            are shown in black color while others are in gray color. So more stable feature selection methods will have more number of black lines and less number of gray points. In our experiments, we set 
                              
                                 T
                                 =
                                 0.5
                              
                           .

In Fig. 3 (results for synthetic data, 
                              
                                 ρ
                                 =
                                 0.8
                              
                           ) features within a group are highly correlated to one another and as expected Lasso shows an unstable behavior in selecting features. On the other hand, Tree-Lasso is the most stable algorithm. Moreover, it could correctly infer the true features of the model. In this dataset, ReliefF, IG and T-test are in the next stages of stability after Tree-Lasso. However, they are unable to select true features of the model.


                           Figs. 4 and 5 show the stability behavior of each feature selection method on Cancer and AMI datasets. As it is illustrated in Fig. 4 for Cancer dataset, again Tree-Lasso is the winner followed by IG and ReliefF. T-test and Lasso show the least stable behavior in selecting features in this dataset. The visual inspection of this figure also shows that the features selected by Tree-Lasso, IG and ReliefF are approximately similar. For AMI dataset (Fig. 5), Tree-Lasso shows the best stability followed by ReliefF and IG. Again, Lasso achieves the least stability followed by T-test.

In order to compare discrimination performance of Tree-Lasso with other baseline feature selection methods, we apply the features obtained using each feature selection algorithm to different classifiers e.g. logistic regressin (LR), naive Bayes (NB), SVM, decision trees (DT) and Random Forest (RF). As we explained in Section 4.3, after estimating the probability of presence for each feature, we re-train the model using derivation set and include only those features that occur with at least probability p (a threshold that we gradually increase). In our experiments we consider features with 
                              
                                 p
                                 =
                                 0.6
                                 
                                 to
                                 
                                 1
                              
                            with a step of 0.1. This is done to show the prediction performance under average-to-high stability constraints. We evaluate the predictive performance of each method by using 30% held out set and report it in terms of AUC.

The classification performance of various algorithms is shown in Figs. 6–9
                           
                           
                           
                           . As it can be seen from these figures, irrespective of the classifier type used, AUC of Tree-Lasso is always the best and in most of the cases followed by Lasso. In terms of classifier used for each feature selection method, we can see that on average the best predictive performance is obtained using Random Forest followed by SVM and logistic regression. In addition, when we increase the stability threshold from 0.6 to 1, the AUC performance of Tree-Lasso remains stable. However, the performance of other algorithms varies a lot and that of Lasso drops suddenly. The sudden drop in performance of Lasso is due to underfitting caused by its inability to select sufficient number of stable features.

Random Forest is a promising classification method that can perform feature selection and prediction, simultaneously. In order to study and compare the stability and predictive performance of Tree-Lasso with Random Forest, we grow 100 trees in the forest and the number of features at each split is chosen as the square root of the number of features. The experimental settings is identical to the settings described in Section 4.3. In order to compare the stability of Tree-Lasso with Random Forest in selecting features, we use SRCC and JSM. These metrics are explained in details in Section 2. Table 3
                            shows the stability results for Tree-Lasso and Random Forest. As it can be seen from this table, for all datasets (synthetic and real) the stability of Tree-Lasso is better than Random Forest, in terms of SRCC and JSM.

The other comparison between Tree-Lasso and Random Forest is based on their predictive performance. To this end, obtaining the probability of presence of each feature as described in Section 4.3, the model is trained again using derivation set by including only those features that occur with at least probability p (a threshold that we gradually increase). In our experiments we consider features with 
                              
                                 p
                                 =
                                 0.6
                              
                            to 1 with a step of 0.1. Using 30% held out test set, the predictive performance of the model is reported in terms of AUC. Figs. 10 and 11
                           
                            show the predictive performance of Tree-Lasso compared to Random Forest. As it can be seen from these figures, the predictive performance of Random Forest and Tree-Lasso are approximately the same when features with average stability are used (with 
                              
                                 p
                                 =
                                 0.6
                              
                            and 0.7). However, when the stability threshold is increased, the AUC of Random Forest declines steadily while that of Tree-Lasso remains stable. In synthetic data with 
                              
                                 ρ
                                 =
                                 0
                              
                           , where the average correlation between groups of variables is around zero, reduction of AUC performance in Random Forest is less compared to other datasets (with correlated groups of variables). This shows that although Random Forest is a good classifier and shows good performance in many applications, its performance degrades in presence of correlated features. On the other hand, Tree-Lasso shows acceptable predictive performance in presence of correlated variables.

One way to deal with instability of Lasso in selecting informative features can be through expanding the selected feature set by including the features that are unselected but correlated to one or more features in the selected set. We refer to this heuristic-based method asExpanded-Lasso and compare its feature selection stability and predictive performance with those of Lasso and Tree-Lasso. Using our synthetic and real-world datasets (same as used above), we split data into training and test sets. The model is trained on the training set and evaluated on the test set. In order to specify the tuning parameters of each method, we use 5-fold cross validation. In order to compare the feature selection stability of Expanded-Lasso with Lasso and Tree-Lasso, as before, we use two stability measures: JSM and SRCC. The explanation of these metrics can be found in Section 2. Based on the fact that for Expanded-Lasso method we need to specify the level of correlation between selected and unselected features, in our experiments we use three different thresholds, i.e. 0.7, 0.8, and 0.9.


                           Tables 4–7
                           
                           
                           
                            compare Expanded-Lasso with Tree-Lasso and Lasso in terms of feature stability and predictive performance. As seen from the tables, in terms of both JSM and SRCC Tree-Lasso is the winner, followed by Expanded-Lasso. Turning to predictive performance, again Tree-Lasso achieves the best AUC and Expanded-Lasso is the runner-up. The reason behind better performance of Tree-Lasso compared to Expanded-Lasso is because of its ability to use intrinsic hierarchical information (correlation) between ICD-10 features, whereas Expanded-Lasso needs to estimate this information. However, in problems where no information about hierarchical structure of the features is available, Expanded-Lasso can be used as a remedy to increase the stability of Lasso in selecting informative features.

Identifying stable features (risk factors) can assist clinical decision making towards accurate medical prognosis. In Tables 8 and 9
                           
                           , we show that risk factors selected using Tree-Lasso (with high probability) for both Cancer and AMI datasets are consistent with well-known risk factors used in clinical domain [5,9,11,28,30,36,40,41,58]. Columns 1 and 2 of the tables show the risk factors ICD-10 code and names, respectively and column 3 shows the probability of presence of the risk factor in each split of data. For example, acute respiratory failure (J96.0) in Cancer dataset with probability equal to one means that this important risk factor (based on clinical research papers) is also considered important by Tree-Lasso and is selected in every splits of the data.

We also study why it matters that selected features be stable when the prediction accuracy is good. To this end, we investigate importance of stability in two ways:

In some applications such as healthcare, it is important that the obtained features to be interpretable over time. For example, we need to attribute the disease of a patient to certain risk factors consistently over time. However, in presence of correlated features, feature selection methods such as Lasso may select some features off and on, causing confusions and suspicions about the model. As an example, consider correlated features I20, I21 and I25 in AMI data that are related toischaemic heart disease and we expect that these features are always selected together. However, as it is shown in Table 10
                              , although Lasso selects I21 and I25 consistently, it selects I20 only 38% of the times. This may lead to a confusion about predictive value of I20 for AMI related hospital readmissions.

By using an stable feature selection method, our goal is to choose thebest explanatoryfeatures. For example, in AMI dataset features I50 and I51 are both related to heart failure and so correlated. However, I50 is a basic feature used to code “heart failure” and I51 is a more specialized feature that gives details of heart failure i.e. “complications of heart disease”. Based on the features used by clinicians I50 is more important feature than I51 and selecting latter where the former is not selected would be meaningless. As it can be seen from Table 10, Lasso chooses I51 and ignores I50 (that is more important feature). However, this is not the case in Tree-Lasso.

@&#CONCLUSION@&#

In this paper, we study the stability behavior of Tree-Lasso – a supervised learning model that is used when features are hierarchical in nature and form a tree structure. We compare its stability and prediction performance with other feature selection algorithms, T-test, Information Gain, ReliefF and Lasso. Using a synthetic and two real-world datasets (Cancer and Acute Myocardial Infarction), we show that Tree-Lasso based feature selection is significantly more stable than Lasso and comparable to other methods e.g. Information Gain, ReliefF and T-test. We further show that, using different types of classifiers such as logistic regression, Naive Bayes, support vector machines, decision trees and Random Forest, the classification performance of Tree-Lasso is comparable to Lasso and better than other methods. Our result has implications in identifying stable risk factors for many healthcare problems and therefore assists clinical decision making towards accurate medical prognosis. As a future work, it would be interesting to explore the possibilities of extending our framework to a multiple hospital setting with an aim to reduce sample selection bias of a single hospital using the ideas of supervised and unsupervised transfer learning such as shared subspace learning and multi-task learning [15,17].

@&#REFERENCES@&#

