@&#MAIN-TITLE@&#Unsupervised flow-based motion analysis for an autonomous moving system

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This paper focuses on segmenting the motion from dense optical flow fields.


                        
                        
                           
                           Two unsupervised clustering methods are presented and a model selection is proposed.


                        
                        
                           
                           A comparison between the proposed techniques with the K-means and EM is made.


                        
                        
                           
                           Experiments are conducted in a surveillance scenario with an autonomous mobile.


                        
                        
                           
                           The proposed techniques are superior in terms of robustness and computational demands


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Motion segmentation

Optical flow

Moving observer

Active surveillance

Mobile robot

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

For the generalization of robotic applications it is crucial to overcome certain problems related to the perception and interpretation of the dynamic scene, for instance, the extraction of high level information in order to increase the robots' ability to perform suitable motion detection [4,11], tracking [17], object recognition [30,24] and navigation [6,7]. It is also imperative to increase their ability to interact with the environment and, thus, the robot must be able to detect and analyze their surrounding scene.

In the scientific community, motion perception is one of the most relevant areas under discussion, and there are several models to perform motion analysis in a variety of environments. However, most of the methods cannot achieve the real-time constraints imposed by mobile robots without specialized computers. In some cases, these computer devices cannot be used due to the small size of the vehicles or they cause a higher consumption of energy which reduces the autonomy of such robots. Nowadays, there are pixel-wise techniques that have good results [5]; however, the segmentation of motion commonly takes more than a pair of seconds. Techniques for robotic applications are computationally more efficient although, this improvement is usually done at the expense of using lower resolution of images and feature-based approaches. The computational resources and the processing-time are some of the most critical aspects for vision-based techniques applied to robotics. Usually, these applications tolerate some loss of accuracy in the algorithms to ensure a fast response [15].

The work presented by this research studies the real-time motion analysis using dense optical flow fields and for a practical use in a mobile robot. Motion segmentation is the process of dividing an image into different regions in a way that each region presents homogeneous motion characteristics. The goal is to segment different objects according to their motion coherence. In particular, the current research describes a real application that is installed in a corridor with large homogeneous regions that does not have a significant amount of structural clues for feature-based techniques. Optical flow techniques provide relevant motion information about the environment around the robot [15]. Moreover, dense optical flow fields provide a good representation of the visual and apparent motion for robotic applications [27,25]; however, the analysis of these fields is a complex and challenging process that requires for sophisticated techniques.

In this article, the estimation of dense flow fields is conducted by the optical flow technique of [29] that was especially designed for small robotic applications equipped with generic computers. This technique identifies motion properties which are considered as high level information about the sequence and originates flow fields in a short period of time. The research proposes two unsupervised clustering techniques for segmenting dense flow fields: the Hybrid Hierarchical Optical Flow Segmentation (HHOFS) and the Hybrid Density-Based Optical Flow Segmentation (HDBOFS). These two techniques were designed for robotic applications with a vision system and limited computer resources. Two major and distinct phases form both methods, namely, refining and collecting. The refining stage decomposes the flow field in a set of distinctive clusters that represent image regions with different motion models and the collecting stage merges the set of clusters that were obtained in the previous phase (using a hierarchical scheme or a density-based scheme). This architecture reduces the computational requirements of the proposed methods (HHOFS and HDBOFS). An extensive and interesting comparison between parametric (K-means and EM) and the proposed non-parametric techniques (no assumptions about the distribution of the data) is presented.

In addition, this article proposes a model selection method, called Fusing Distributed Bayesian Hypothesis (FDBH), which combines a histogram-based approach with cost functions (that balance fitness and model complexity). The estimation of the number of clusters is incorporated in all methods.

Experimental considerations prove that modeling a clustering technique in a structure formed by two consecutive stages is computationally rewarding. The computational demands of the HHOFS and the HDBOFS are substantially lower than that of the EM and K-means that are conducted at flow level. The behavior of the proposed techniques can be adjusted to specific characteristics of the application. For instance, motion segmentation in surveillance operations can be appropriately performed without processing at pixel level. Therefore, the proposed techniques are completely capable of perceiving and understanding external motions in real-time and using low computational resources. However, the results have a blocky aspect which is usually tolerable by robotic moving systems.

Therefore, the contributions of this paper include:
                        
                           1.
                           Novel motion analysis methods with a reduced computational complexity: the HHOFS and the HDBOFS. The proposed architecture guides the motion analysis in both methods, enabling a reliable segmentation process while preserving the computational time requirements;

An efficient method to decompose the optical flow field into exclusive regions based on similarity properties of motion;

A model selection method to enhance the performance of the clustering techniques, the FDBH method. The Bayesian formulation combines a histogram-based analysis of the flow field in the polar space with the decay-ratio of a model selection criterion;

A comparative study of several unsupervised motion segmentation techniques (pixel-wise and block-wise) is provided;

An extensive qualitative and quantitative evaluation under realistic working conditions (with moving observers);

The article is organized as follows. Section 2 presents a brief review of motion segmentation methods that are commonly used in robotic systems to perceive motion. Section 3 shows the concept of the robotic application, named EEyeRobot. Section 4 presents the two unsupervised clustering techniques that are proposed and used in this research. Both non-parametric techniques are described with detail in Sections 4.1 and 4.2. Afterwards, a model selection method is proposed in Section 4.3. Experimental results are presented in Section 5. These results include the comparisons of the proposed techniques with the EM and the K-means. The experiments were conducted using the EEyeRobot in a real surveillance scenario. The results demonstrate that the HHOFS and the HDBOFS methods perform satisfactorily better, and can be used as a tool for motion analysis in applications with limited resources. Finally, Section 6 presents the most important conclusions of this research.

@&#RELATED WORK@&#

In the literature, it is possible to identify three motion perception methods for conventional fixed systems [19,33]: background subtraction, temporal differencing and optical flow. The most conventional techniques for motion perception consider that the visual changes are only caused by the movement of the external objects since they assume the stationary position of the observer. Therefore, they fail almost completely when the dynamic scene is captured by a non-static observer due to their inability to distinguish both motion components. For this reason, motion perception and analysis for moving observers is becoming an active research field and preliminary techniques typically use one of the following approaches: organizing the background into mosaics [6], modifying background subtraction methods [19,4] and optical flow or geometrical models [12,23].

Quian Yu and Gerard Medioni (2008) [36] focus on motion detection for a moving observer. They propose a mosaic approach that computes the homography between consecutive frames in order to compensate the egomotion. This assumes that the depth of the scene is much smaller than the distance between the object and the camera. To prevent registration errors from spreading, the authors adopt a sliding window and only a number of frames are considered. The movement of the sliding window demands for a high computational effort because all the registration processes must be executed. However, the algorithm was implemented in GPU and the time required to compute a 320×240 image is less than 100ms.

Fernández-Caballero et al. (2010) [11] present a human detection method based on a thermal infrared camera mounted on an autonomous mobile robot. The detection is accomplished using a combination of optical flow and temporal differencing. The non-pyramidal Lucas–Kanade is used when the robot moves and the temporal differencing is used to detect human candidates based on thermal signatures when the robot stops. The authors focus on detecting motion interactions; however, the thermal cameras facilitate the detection of humans. A tracking application that resorts to pyramidal Lucas–Kanade optical flow to compute the flow field is presented in Jay et al. (2011) [8]. The main goal is to detect and extract regions where the flow does not represent the UAV's egomotion, for instance, for tracking a target that moves at different velocity compared to the background. The authors compute the pure movement of the target by searching along sparse diagonal lines which limits the Lucas–Kanade incoherence assumptions and reduces the computational complexity. An approach for motion clustering and classification based on consecutive images and a free-moving camera is presented in Jiman et al. (2010) [20]. The approach uses an optical flow technique and the random sample consensus (RANSAC) which removes outliers (scattered points) in the flow field. The flow field in Cartesian coordinates is transformed into polar axis (magnitude and orientation), and then, divided into blocks. The initial number and the respective cluster center are obtained by counting the selected block and by computing the density of the moving points. The clusters are redefined using the RANSAC, where each point is assigned to the initial cluster. Iteratively, the Euclidean distances to the clusters are computed and each point is updated with the cluster that has the minimum distance. The foreground and background are classified using the eigenvalue analysis based on the scatter of the cluster distributions, because they assume that the background is more scattered than the moving objects (due to their highest number of pixels).

Marco Tagliasacchi (2007) [34] presents a genetic-based optical flow estimation algorithm. The current frame is segmented using a watershed algorithm and by grouping the pixels with the same spatial position and similar color. This approach performs better at the border of the objects when compared to the Lucas–Kanade (due to the discrete approximation of the partial derivatives); however, the computation is too complex to achieve real-time performance since it takes more than 1s to compute a 176×144 image with a Pentium M 1.6Ghz. Naoya Ohnishi and Atsushi Imiya (2006) [26] demonstrate an algorithm that computes the dominate plane using the pyramidal Lucas–Kanade optical flow. Assuming the largest area for the dominant plane and a finite distance from the camera to the plane, they prove that the matched features of consecutive frames will be based on a projection of the dominant plane that is represented with an affine model (the homography can be approximated by an affine transformation if the camera displacement is small). The affine coefficients are computed using three randomly selected pair of points and the dominant plane is detected using the difference from the optical flow and the planar flow. The planar flow that is used by the following images can be estimated by applying the least-squared method and by computing the dominant plane afterwards. The approach fails when the selected points are mismatched pairs because the affine coefficients are not properly estimated.

Kai-Kuang Ma and Hay-Yun Wang (2002) [21] present a region-based nonparametric and spatio-temporal segmentation techniques. The flow field is estimated through the Lucas–Kanade method and the segmentation method has two steps: pre-clustering (a smoother optical-flow field is obtained by blurring small textured areas and then, these areas are merged based on the dominant region of the neighborhood) and post-clustering (the spatial segmentation is conducted by a fuzzy c-means with a smoothing operation to improve the semantic meaning of homogeneous regions). The authors focus on unsupervised segmentation; hence, they provide a method to estimate the number of moving objects by analyzing the phase histogram. Dominant motions are retrieved through an adaptive threshold. The experiments were conducted with static video sequences, which justifies more or less the fact that they have despised the information of magnitude. This research is quite interesting; however, the technique cannot be applied to our context because it does not detect different objects that share a similar motion direction.

The research work presented in [13] detects salient regions on the sequence. It proposes a sparse approach since feature points are tracked over time to pursue saliency detection as violation of co-visibility. The co-visibility is defined in terms of epipolar equivalence which means, is coherent with the rigid egomotion. The optical flow of a set of features is used to estimate the velocity of the viewer and to determine the salient regions. The method was tested on aerial video sequences which is expected to have a significant amount of features. Moreover, results show that the method does not achieve a real-time computation (32.6s) since M-estimators are used to improve the segmentation procedure by removing outliers. Feature-based techniques are usually preferred due to a lower computational demand although, the realistic environment of the current paper does not provide sufficient clues for sparse approaches.

Samuel Schulter et al. (2013) [31] present a block-wise motion segmentation method. The anisotropic Huber-L1 method computes the optical flow and the motion segmentation is accomplished using the conditional random field (CRF). The camera movement is robustly estimated as an affine model via RANSAC and considering a small part of the border of the flow field. The result is robust since the temporal coherence of moving objects removes the outliers. The clustering is conducted by a bag-of-words model built on dense scale-invariant feature transform (SIFT) features. The similarity between clusters is computed using the Chi-squared distance and object categories are discovered from the videos by learning the appearance model for each cluster through a Hough forest method. A segmentation technique that uses long term point trajectories based on dense optical flow is presented in [5]. These long term point trajectories made possible the analyzation of the temporal coherence consistent of clusters over many frames. The authors define the distance between trajectories as the maximum difference of their motion. The results show that the proposed method achieves an accurate pixel-wise segmentation; however, the method takes 497s to compute 10 frames of the “people1” sequence in the Hopkins dataset. This time is not affordable by most of the robotic systems and especially by mobile robots.

Gheissari, Bab-Hadiashar and Suter (2006) [14] propose a motion segmentation algorithm that estimates the scale of the noise based on a selective statistical estimator and a model selection. The dataset is partitioned into two groups (true segment and outliers), and data segmentation is reduced to a hypothesis-testing procedure. Alexiadis and Sergiadis (2009) [2] use a weighted fuzzy c-mean clustering procedure to obtain the velocity estimate in colored sequences. Dense optical flow fields are computed using square blocks and the estimated velocity is assigned to the center of the block after the median filter. The authors separate the different types of motion into the two-dimensional hypercomplex Fourier domain and resort to an energy-minimization-based approach. They assume that the velocity of moving objects (translational motions) is smoothly time-varying.

Motion perception and analysis are extremely important problems for several mobile robotic applications [6,7,17]. Therefore, this article proposes two major clustering techniques that measure and extract motion from dense optical flow fields. The techniques were developed for an especially designed mobile robot that performs an intelligent surveillance. The major advantage of these techniques is the ability to segment different types of motion in image sequences with real-time constraints. The proposed techniques were compared to baseline clustering methods, namely expectation–maximization and K-means.

Conventional surveillance systems have problems concerning to the cooperation between sensors, for instance, synchronization, object correspondences and communications [19]. All of these aspects make traditional security applications very unpractical for some large scale environments.

This research presents a scenario where an innovative mobile robotic system was designed for active surveillance operations. The robot is currently being developed and its name is EEyeRobot. This mobile robot is equipped with a monocular camera and it moves along a rail which enhances the surveillance capability when compared to conventional systems (mainly composed by multiple static cameras). The robot has several advantages when compared to conventional systems. For instance it enhances security since blind spots are virtually eliminated, it induces a psychological effect against potential criminal activities (intimidation factor) and its navigation is not influenced by external factors that could damage the system.

The EEyeRobot uses a monocular camera to acquire information about the environment, it reports security issues and autonomously navigates along the rail with a visual motion detection capability (observations in motion). The rail framework provides a good solution to monitor corridors, medium or large retail outlets and distribution centers. The main objective of this application is to create a surveillance mobile system [28] which can be used, for instance, in patrols, in action or remotely-operated. The action mode means that the robot autonomously detects and follows external and abnormal activities. The navigation of the EEyeRobot is simple because it is not influenced by the presence of obstacles that could damage the robot. Fig. 1(a) and (b) shows the EEyeRobot concept. In Fig. 1(b), the rail is placed in a corridor at the Department of Electrical and Computer Engineering of the Faculty of Engineering of the University of Porto. This research focuses on segmenting different types of motion in dense optical flow fields. The motion information obtained with the techniques proposed in this paper is used by high level procedures to define the autonomous behavior of the robot (based on temporally consistent clusters).

The goal of clustering techniques is to group a collection of instances into subsets of clusters: similar instances (more closely related) are clustered together and different instances belong to different groups. An important notion is the similarity or dissimilarity between the individual objects being clustered. Two main types of measurements are used to estimate this relation: distance measurements (Euclidean, Minkowski) and similarity measurements (Cosine, Pearson Correlation, Dice Coefficient, Extended Jaccard) [22]. Two unsupervised clustering techniques are proposed in this section: a hybrid hierarchical method (HHOFS) and a hybrid density-based method (HDBOFS).

The hierarchical techniques create the clusters by merging observations using pairwise similarity measurements. Usually, there are two approaches to operate hierarchically-based algorithms: top-down or bottom-up. In the agglomerative clustering (bottom-up), the clusters are successively merged until the desired structure is obtained; however, the divisive clustering (top-down) successively divides the parent cluster into sub-clusters. The output is a hierarchical representation and the highest level has only one cluster. The hierarchical partitioning is commonly presented using the dendrogram.

The segmentation of flow fields groups the pixels with similar motion properties because they probably belong to the same motion model. Two operational steps, refining and collecting, form the Hybrid Hierarchical Optical Flow Segmentation (HHOFS). It is called a hybrid method because its first phase combines divisive and agglomerative clustering schemes. The refining iteratively decomposes the flow field into a set of distinctive clusters that represent image regions with different motion models. It successively splits and merges the clusters by measuring the fitness to the estimated affine model of all observations that constitute the cluster. Parameters of the affine model are initially computed considering only a set of randomly selected instances (or observations). The resulting clusters from the first stage give information about the spatial segmentation of the flow field and it is used to accelerate the convergence of the clustering process in the second stage. The collecting phase successively merges the set of clusters that was obtained in the refining phase and using a hierarchical scheme with the Mahalanobis distance. Features such as the angle and magnitude of the dominant flow vector for each cluster (average-link clustering) are considered at this phase.

The flow vector of each observation is defined along this article as w
                        =(u,
                        v) in Cartesian coordinates and w
                        
                           p
                        
                        =(m,
                        ψ) in Polar coordinates. Therefore, a single observation is x
                        =(w,
                        w
                        
                           p
                        ,
                        x,
                        y), where (x,
                        y) is the coordinate position.


                           Fig. 2
                            shows the structure of the HHOFS technique. The refining phase receives the flow field computed using a dense optical flow technique [29] and returns a set of clusters that exhibit different motion characteristics. Each cluster represents a set of observations that are spatially related and share a similar motion model. The final set of clusters is considered as a set of objects for the collecting phase, which focuses on grouping these objects using a similarity measurement. The process starts by converting the horizontal and vertical velocities of the optical flow field, w, to a Polar coordinate system w
                           
                              p
                           . The second stage is responsible for an initial partitioning of the flow field into an initial set of clusters. This means that the flow field is uniformly divided into W
                           ×
                           H non-overlapping regions, where W and H is the number of horizontal and vertical regions, respectively. The number of regions must be defined according to the smallest object that will appear.

The motion of an individual cluster can be represented using a six-parameter affine model,
                              1
                           
                           
                              1
                              Assumes that the depth variance in individual region is small enough compared with the distance from the scene to the camera.
                            Eq. (1):
                              
                                 (1)
                                 
                                    
                                       
                                          w
                                          ^
                                       
                                       
                                          
                                             a
                                             ^
                                          
                                          x
                                          y
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   
                                                      a
                                                      1
                                                   
                                                
                                                
                                                   
                                                      a
                                                      2
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      a
                                                      3
                                                   
                                                
                                                
                                                   
                                                      a
                                                      4
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   x
                                                
                                             
                                             
                                                
                                                   y
                                                
                                             
                                          
                                       
                                       +
                                       
                                          
                                             
                                                
                                                   
                                                      a
                                                      5
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      a
                                                      6
                                                   
                                                
                                             
                                          
                                       
                                       ,
                                    
                                 
                              
                           where 
                              
                                 
                                    a
                                    ^
                                 
                                 =
                                 
                                    
                                       a
                                       1
                                    
                                    
                                       a
                                       2
                                    
                                    
                                       a
                                       3
                                    
                                    
                                       a
                                       4
                                    
                                    
                                       a
                                       5
                                    
                                    
                                       a
                                       6
                                    
                                 
                              
                            are the affine coefficients or parameters of the model. A solution for these equations can be obtained using at least 3 observations (w values) and the least squares method (LSQ). Forty two observations belonging to the cluster are randomly selected in order to compute the affine parameters that describe their motion model.
                              2
                           
                           
                              2
                              For a 95% of confidence level with 15 of confidence interval and considering 64 initial clusters.
                            The LSQ used in this paper is not robust against noise or outliers; however, the robust estimation of these parameters using random sample consensus (RANSAC) or the iteratively re-weighted least squares (with the Charbonnier M-estimator) is more computationally demanding and therefore, it can compromise the real-time demands of the robotic application that is presented in this paper.

In reality, the robust estimation of the affine model is not an issue since each motion model is adjusted to the observations that constitute the corresponding cluster during the third stage. In this way, a fitting criterion is computed using the normalized residual error, see Eq. (2).
                              
                                 (2)
                                 
                                    
                                       er
                                       
                                          r
                                          sp
                                       
                                       
                                          
                                             
                                                
                                                   a
                                                   ^
                                                
                                                j
                                             
                                          
                                          j
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   
                                                      M
                                                      j
                                                   
                                                
                                                
                                             
                                             ∥
                                             
                                                w
                                                i
                                             
                                             −
                                             
                                                
                                                   
                                                      w
                                                      ^
                                                   
                                                   ij
                                                
                                             
                                             ∥
                                          
                                          
                                             M
                                             j
                                          
                                       
                                       ,
                                    
                                 
                              
                           where M
                           
                              j
                            is the number of observations of the jth cluster, 
                              
                                 
                                    
                                       a
                                       ^
                                    
                                    j
                                 
                              
                            are the affine parameters, w
                           
                              i
                            is the flow vector of the ith observation and 
                              
                                 
                                    
                                       
                                          w
                                          ^
                                       
                                       ij
                                    
                                 
                                 =
                                 
                                    
                                       
                                          u
                                          ^
                                       
                                       
                                          v
                                          ^
                                       
                                    
                                    T
                                 
                              
                            is the estimated flow vector for the ith observation and considers the 
                              
                                 
                                    
                                       a
                                       ^
                                    
                                    j
                                 
                              
                           . When the normalized error of the jth cluster is higher than a predefined threshold then, the cluster is characterized by more than a single motion model. Therefore, the cluster is split into four smaller clusters and the affine parameters of each subcluster are estimated once again.

After the process is repeated for all the clusters, the next step is to merge these motion models in the affine parameter space. This is the fourth and last stage of the refining task, see Fig. 2. The rectangular clusters c
                           
                              s
                            and c
                           
                              r
                            are merged if they are neighbors and share the same motion. To analyze the similarity of the motion properties for both clusters: Eq. (2) makes possible the fitting of 
                              
                                 
                                    
                                       a
                                       ^
                                    
                                    s
                                 
                              
                            into observations of cluster c
                           
                              r
                            and, furthermore, the model 
                              
                                 
                                    
                                       a
                                       ^
                                    
                                    r
                                 
                              
                            is fitted to observations of the c
                           
                              s
                           . This process is called by cross-validation and it originates two normalized errors that are combined as follows:
                              
                                 (3)
                                 
                                    
                                       er
                                       
                                          r
                                          mg
                                       
                                       
                                          s
                                          r
                                       
                                       =
                                       er
                                       
                                          r
                                          sp
                                       
                                       
                                          
                                             
                                                
                                                   a
                                                   ^
                                                
                                                s
                                             
                                          
                                          r
                                       
                                       +
                                       er
                                       
                                          r
                                          sp
                                       
                                       
                                          
                                             
                                                
                                                   a
                                                   ^
                                                
                                                r
                                             
                                          
                                          s
                                       
                                       ,
                                    
                                 
                              
                           where err
                           
                              mg
                           (s,
                           r) is the merging error and 
                              
                                 er
                                 
                                    r
                                    sp
                                 
                                 
                                    
                                       
                                          
                                             a
                                             ^
                                          
                                          s
                                       
                                    
                                    r
                                 
                              
                            is the normalized error considering the parameters of 
                              
                                 
                                    
                                       a
                                       ^
                                    
                                    s
                                 
                              
                            in the data of cluster c
                           
                              r
                           . Two clusters have similar motion properties when the merging error is lower than a threshold. After merging them, the affine model is re-estimated for the combined cluster in order to obtain more accurate model parameters.

Steps 3 and 4 are executed until the clusters converge or for a maximum number of iterations. Thus, the refining stage decomposes the dense optical flow field into a set of clusters. Each cluster defines a region of the flow field that shares the same affine motion model (Fig. 3).

The refining stage is a hybrid clustering scheme and the collecting phase is an agglomerative hierarchical-based scheme, where clusters obtained in the refining stage are considered as starting objects. The hierarchical clustering is conducted when the refining stage terminates without convergence and it computes the distance between two clusters using a similarity measurement in order to obtain a similarity matrix (distance between clusters).

This phase assumes that the observation is multivariate and normally distributed, and the feature vector has two dimensions x
                           ∈
                           ℜ
                           2 since it is formed by the flow vector in Polar coordinates w
                           
                              p
                           . Hence, the difference between two clusters, c
                           
                              s
                            and c
                           
                              r
                           , can be measured by a Mahalanobis squared distance of samples. The similarity between clusters is considered in terms of a normalized difference between both mean vectors [35], 
                              
                                 
                                    
                                       w
                                       ¯
                                    
                                    j
                                    p
                                 
                              
                           , and the positive-definitive covariance 
                              
                                 Σ
                                 ^
                              
                           .


                           
                              
                                 (4)
                                 
                                    
                                       
                                          Σ
                                          ^
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         M
                                                         s
                                                      
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   
                                                      
                                                         Σ
                                                         ^
                                                      
                                                      s
                                                   
                                                
                                                +
                                                
                                                   
                                                      
                                                         M
                                                         r
                                                      
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   
                                                      
                                                         Σ
                                                         ^
                                                      
                                                      r
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   M
                                                   s
                                                
                                                +
                                                
                                                   M
                                                   r
                                                
                                                −
                                                2
                                             
                                          
                                       
                                       ,
                                    
                                 
                              
                           where M
                           
                              j
                            is the number of observations and 
                              
                                 
                                    
                                       Σ
                                       ^
                                    
                                    j
                                 
                              
                            is the sample covariance matrix of the jth cluster.

The normalized difference between 
                              
                                 
                                    w
                                    ¯
                                 
                                 s
                                 p
                              
                            and 
                              
                                 
                                    w
                                    ¯
                                 
                                 r
                                 p
                              
                            is defined by 
                              
                                 
                                    w
                                    ¯
                                 
                                 
                                    s
                                    ,
                                    r
                                 
                              
                           , and computed using Eq. (5).
                              
                                 (5)
                                 
                                    
                                       
                                          
                                             w
                                             ¯
                                          
                                          
                                             s
                                             ,
                                             r
                                          
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         m
                                                         ¯
                                                      
                                                      s
                                                   
                                                   −
                                                   
                                                      
                                                         m
                                                         ¯
                                                      
                                                      r
                                                   
                                                
                                             
                                             
                                                m
                                                max
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      g
                                                      norm
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               ψ
                                                               ¯
                                                            
                                                            s
                                                         
                                                         −
                                                         
                                                            
                                                               ψ
                                                               ¯
                                                            
                                                            r
                                                         
                                                      
                                                   
                                                
                                             
                                             π
                                          
                                       
                                       ,
                                    
                                 
                              
                           where m
                           
                              max
                            is the maximum magnitude of the flow vectors, 
                              
                                 m
                                 ¯
                              
                            is the mean of the magnitude and 
                              
                                 ψ
                                 ¯
                              
                            is the mean of the angle. This equation normalizes and maintains positive the difference of the flow vectors that characterize each cluster. An important note is related to the angle, in radians. The angle subtraction is not straightforward because it must be followed by a normalization; otherwise, the distance may be misleading. Eq. (6) is used to normalize the result of the difference (or the sum) of angles, 
                              
                                 
                                    ψ
                                    ˜
                                 
                                 ∈
                                 
                                    
                                       −
                                       π
                                       ;
                                       π
                                    
                                 
                              
                           .
                              
                                 (6)
                                 
                                    
                                       
                                          ψ
                                          ˜
                                       
                                       =
                                       
                                          g
                                          norm
                                       
                                       
                                          ψ
                                       
                                       =
                                       a
                                       tan
                                       2
                                       
                                          
                                             sin
                                             
                                                ψ
                                             
                                             ,
                                             cos
                                             
                                                ψ
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Yielding the sample means 
                              
                                 
                                    
                                       w
                                       ¯
                                    
                                    s
                                    p
                                 
                              
                            and 
                              
                                 
                                    
                                       w
                                       ¯
                                    
                                    r
                                    p
                                 
                              
                            of clusters:
                              
                                 (7)
                                 
                                    
                                       
                                          Δ
                                          2
                                       
                                       =
                                       
                                          
                                             
                                                w
                                                ¯
                                             
                                             
                                                s
                                                ,
                                                r
                                             
                                             p
                                          
                                       
                                       
                                          
                                             
                                                Σ
                                                ^
                                             
                                             
                                                −
                                                1
                                             
                                          
                                       
                                       
                                          
                                             
                                                w
                                                ¯
                                             
                                             
                                                s
                                                ,
                                                r
                                             
                                             pT
                                          
                                       
                                       ,
                                    
                                 
                              
                           where Δ is a metric that evaluates the distance between two clusters by considering the mean characteristics and confidence (represented by the covariance).

The clustering operation is an iterative process that merges two similarity clusters (lower distance) and the behavior can easily be represented using a dendrogram. The iterative process can be stopped when the similarity measurement is high, since the remaining clusters have higher distances and they probably should be disjoint clusters. Therefore, the process can stop according to a pre-specified number of clusters and/or a threshold value for the similarity.

The time complexity of original hierarchical algorithms is at least O(M
                           2), where M is the total number of objects. In addition, the algorithms can suffer from sensitivity to noise and outliers according to the type of distance metric (or similarity measurement) that is chosen. However, the refining phase of the HHOFS creates a set of coherent clusters which reduces the number of objects that are used in the hierarchical scheme. This process prevents the use of the initial objects at pixel level, which reduces the computational costs of the hierarchical clustering. Therefore, advantages of the HHOFS segmentation include: no priori information about the number of clusters is required and the clustering process is conducted in a reliable and efficient manner.

This section presents a density-based technique for clustering dense optical flow fields. The structure of this technique is similar to the HHOFS and it is called Hybrid Density-Based Optical Flow Segmentation (HDBOFS). The major difference compared to the HHOFS is related to the collecting phase, which in Fig. 2 corresponds to step 5. This last stage is accomplished using the DBSCAN (density-based spatial clustering of applications with noise) clustering instead of a hierarchical clustering methodology. The DBSCAN method [10] is a density-based clustering algorithm since it finds clusters based on the density of data points inside a region. Usually, their advantage compared to hierarchical and partitioning methods is the computational complexity, which can be reduced to O(M
                        log
                        M). In addition, this method can discover clusters of arbitrary shapes [18].

The most important concept in the DBSCAN is its notion of density-reachability and density-connection [9]. These notions are defined by two parameters: the neighborhood's distance (ϵ) and the minimum number of points required to form a cluster (minPts). Consider a random point, p
                        
                           s
                        , this point will be directly density-reachable from a point p
                        
                           r
                         if the distance between both points is less than ϵ and if p
                        
                           r
                         is surrounded by at least minPts points. Thus, the point p
                        
                           s
                         is called density-reachable which is an asymmetric property. The density-connected notion will now be introduced [9]: if there is a point p
                        
                           c
                         such that the two points p
                        
                           r
                         and p
                        
                           s
                         are density-reachable from p
                        
                           c
                         then p
                        
                           r
                         and p
                        
                           s
                         are density-connected. This notion is symmetric and makes it possible to define a cluster (a set of objects that are mutually density-connected) because if p
                        
                           r
                         belongs to some cluster and p
                        
                           s
                         is density-reachable from p
                        
                           r
                        , then p
                        
                           s
                         belongs to the same cluster. The process begins with a random point and its neighborhood is obtained using the density-reachable notion. If the size of its neighborhood is at least minPts, a cluster is started. Otherwise, the point is marked as noise. For the cases where no points are density-reachable from some point belonging to a cluster, then this point is a border object. When a point is found to be a dense part of a cluster, its neighborhood also belongs to that cluster. The process is repeated until all the points are visited.

Briefly, the key idea of the DBSCAN is that for each data object, the neighborhood of a given radius (ϵ) must contain at least a minimum number (minPts) of objects. The major problem of density-based clustering algorithms is that they easily lead to memory problems when facing large datasets [18]. For this reason, the refining phase is performed initially. Thus, the proposed HDBOFS clustering method is based on the concept of space partitioning since it efficiently identifies the different densities in the dataset according to the motion properties. Next, the method performs a density-based clustering by taking into account a similarity measurement of the objects. This two phase method makes it possible to reduce the computational memory demands by providing a guideline based on coherent clusters (obtained by the refining stage) that are initially considered objects during the collecting phase.

The refining phase is similar to the HHOFS however, the collecting stage is different, for instance, the clustering is not conducted by a hierarchical scheme but by using a density-based structure instead. The similarity measurement for the neighborhood's distance (ϵ) is defined based on a feature vector with two dimensions x
                        ∈
                        ℜ
                        2 and formed by the flow vector in Polar coordinates w
                        
                           p
                        . Hence, Eq. (7) is used to measure the distance between objects. The initial objects are obtained by the refining phase.

Finding the most suitable feature space is one of the most important issues in clustering techniques. The number of clusters (K) is a necessary parameter in some of the techniques, for instance, in EM and K-means. The two techniques that are proposed in this research, HHOFS and HDBOFS, do not need this parameter; however, knowledge on the parameter is a clear advantage because it makes it possible to guide and conclude the clustering process when the desired number of clusters is reached.

This research uses the representation of the flow vectors in magnitude and angle. Fig. 4(a), (b), (c)
                           , and (d) shows the difference between features represented in the Cartesian coordinates and Polar coordinates. The histogram analysis is an efficient nonparametric technique for density estimation. It focuses on the multimodal behavior of the dataset: the peaks (maximums). For instance, a feature space is suitable when it is possible to more easily identify the number of clusters that is represented in the data.


                           Fig. 4(a) is bimodal: one modal is high and the other is very small. The detection of small modals will depend on the size of the bin and the noise of the data. Hence, the analysis of this histogram is not robust since it returns a modal with high confidence and the other with low confidence (two possible clusters). Fig. 4(b) is a unimodal histogram and thus, the information obtained by the analysis of both histograms presents many uncertainties.

The flow field represented in Polar coordinates originates histograms that reveal the characteristics of the dataset with more confidence [21]. Fig. 4(c) shows the histogram of the magnitude for the same flow field. As can be confirmed, the histogram is unimodal; however, the histogram of the angle is very revealing. It shows 3 peaks at {−
                           π,0,
                           π}; however, the peaks {−
                           π} and {π} are of the same cluster because their normalized difference is zero. After adding the peaks with small differences, the histogram of the angle gives two clusters with high confidence. The clusters represent the components of the flow field that are associated with the egomotion of the robot and the movement of external objects.

A histogram analysis with a polar representation makes it possible to estimate parameter K (the number of clusters); however, there are other techniques to estimate this parameter: the Bayesian information criterion (BIC) [32], the Akaike information criterion (AIC) [1] and the Hannan–Quinn information criterion (HQIC) [16].

This research proposes a method to estimate the value of K, see Fig. 5
                           . This method is called Fusing Distributed Bayesian Hypothesis (FDBH) for model selection and it combines the histogram analysis with the BIC (AIC or HQIC) approach using the Bayes's formulation. The method computes the histogram of the flow field in Polar coordinates, see Fig. 4(c) and (d), and determines the local maximums. The difference between the angle peaks are used to detect and avoid possible misleading peaks because when the resulting difference (Eq. (6)) is small, then the peaks are too close to each other, which means that they must be considered as one. A similar operation is conducted for the magnitude peaks: if the difference between two local peaks, directly obtained using the histogram, is small then both peaks are the same. After the confidence of each maximum is calculated, a threshold probability is considered in order to remove local maximums that have a low number of occurrences associated. This increases the robustness of the estimation in the presence of noise. Therefore, the resulting information is: several hypotheses for the number of clusters and the respective probability for the angle and magnitude, K
                           
                              ψ
                           ,
                           K
                           
                              m
                           ,
                           P
                           
                              ψ
                            and P
                           
                              m
                           .

At the same time, the EM algorithm computes the likelihood of the dataset making it possible to formulate a cost function based on BIC (or a similar model assessment criterion). The original concept of BIC is to select the model from a set of candidate models by maximizing the posterior probability and using the maximum likelihood approach. Obviously, increasing the number of clusters leads to better fitting (at the limit, when K is equal to the number of pixels then the fitting error is lower); however, the model overfits the dataset. The BIC, AIC and HQIC criteria measure the fitness and the effect of the model complexity. They are applicable in settings where the fitting is performed by maximizing the log-likelihood.

The cost function is successively evaluated for different values of K. The model with lower cost is saved (K
                           
                              min
                           ) after the maximum number of hypotheses is reached (h
                           
                              max
                           ). In the literature, the value of K
                           
                              min
                            is the result of the model selection based on the BIC approach however; the FDBH method computes the derivative of the cost function with regard to all hypotheses (the elbow method). This provides a decay function regarding the number of clusters, f
                           
                              decay
                           (K), and it is used to detect the value of K with the higher decay ratio (K
                           
                              decay
                           ), for instance, the moment that the cost value drops more. A confidence measurement is estimated using K
                           
                              min
                            and K
                           
                              decay
                           : the decays are successively accumulated for the hypotheses below K
                           
                              min
                           , measuring the total decay. Thereby, the probability of obtaining a hypothesis is given by Eq. (8).
                              
                                 (8)
                                 
                                    
                                       
                                          P
                                          v
                                       
                                       
                                          K
                                       
                                       =
                                       
                                          
                                             
                                                f
                                                decay
                                             
                                             
                                                K
                                             
                                          
                                          
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   
                                                      K
                                                      1
                                                   
                                                
                                                
                                                   K
                                                   min
                                                
                                             
                                             
                                                
                                                   f
                                                   decay
                                                
                                                
                                                   k
                                                
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

The final estimation of parameter K is calculated by combining the hypotheses obtained from the feature (magnitude and angle) analysis and the cost function. This combination is achieved using the following naive Bayes's formulation, Eq. (9).


                           
                              
                                 (9)
                                 
                                    
                                       P
                                       
                                          
                                             h
                                             |
                                             
                                                r
                                                ϕ
                                             
                                             ,
                                             
                                                r
                                                m
                                             
                                             ,
                                             
                                                r
                                                v
                                             
                                          
                                       
                                       =
                                       
                                          
                                             P
                                             
                                                h
                                             
                                             P
                                             
                                                
                                                   
                                                      r
                                                      ϕ
                                                   
                                                   ,
                                                   
                                                      r
                                                      m
                                                   
                                                   ,
                                                   
                                                      r
                                                      v
                                                   
                                                   |
                                                   h
                                                
                                             
                                          
                                          
                                             P
                                             
                                                
                                                   r
                                                   ϕ
                                                
                                                
                                                   r
                                                   m
                                                
                                                
                                                   r
                                                   v
                                                
                                             
                                          
                                       
                                       ;
                                    
                                 
                              
                           
                           
                              
                                 (10)
                                 
                                    
                                       P
                                       
                                          
                                             h
                                             =
                                             
                                                h
                                                k
                                             
                                             |
                                             
                                                r
                                                ϕ
                                             
                                             ,
                                             
                                                r
                                                m
                                             
                                             ,
                                             
                                                r
                                                v
                                             
                                          
                                       
                                       =
                                       
                                          
                                             P
                                             
                                                
                                                   h
                                                   =
                                                   
                                                      h
                                                      k
                                                   
                                                
                                             
                                             P
                                             
                                                
                                                   
                                                      r
                                                      ϕ
                                                   
                                                   ,
                                                   
                                                      r
                                                      m
                                                   
                                                   ,
                                                   
                                                      r
                                                      v
                                                   
                                                   |
                                                   h
                                                   =
                                                   
                                                      h
                                                      k
                                                   
                                                
                                             
                                          
                                          
                                             
                                                ∑
                                                
                                                   l
                                                   =
                                                   1
                                                
                                                
                                                   h
                                                   max
                                                
                                             
                                             
                                                P
                                                
                                                   
                                                      h
                                                      =
                                                      
                                                         h
                                                         l
                                                      
                                                   
                                                
                                                P
                                                
                                                   
                                                      
                                                         r
                                                         ϕ
                                                      
                                                      ,
                                                      
                                                         r
                                                         m
                                                      
                                                      ,
                                                      
                                                         r
                                                         v
                                                      
                                                      |
                                                      h
                                                      =
                                                      
                                                         h
                                                         l
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       ,
                                    
                                 
                              
                           where r
                           
                              ϕ
                           , r
                           
                              m
                            and r
                           
                              v
                            are observed features of the angle and the magnitude analysis and the cost function, respectively. The denominator does not depend on the hypothesis since it is the normalization factor that keeps the probability in the range {0, 1}. Eq. (10) shows that maximizing the posterior probability P(h
                           =
                           h
                           
                              k
                           |r
                           
                              ϕ
                           ,
                           r
                           
                              m
                           ,
                           r
                           
                              v
                           ) is equivalent to maximizing the likelihood P(r
                           
                              ϕ
                           ,
                           r
                           
                              m
                           ,
                           r
                           
                              v
                           |h
                           =
                           h
                           
                              k
                           ). This research assumes that all hypotheses share the same prior probability.

Assuming a conditional independence between r
                           
                              ϕ
                           , r
                           
                              m
                            and r
                           
                              v
                           , the likelihood of Eq. (10) can be rewritten as:
                              
                                 (11)
                                 
                                    
                                       P
                                       
                                          
                                             
                                                r
                                                ϕ
                                             
                                             ,
                                             
                                                r
                                                m
                                             
                                             ,
                                             
                                                r
                                                v
                                             
                                             |
                                             h
                                             =
                                             
                                                h
                                                k
                                             
                                          
                                       
                                       =
                                       
                                          
                                             ∏
                                             
                                                i
                                                =
                                                ϕ
                                             
                                             v
                                          
                                          
                                       
                                       P
                                       
                                          
                                             
                                                r
                                                i
                                             
                                             |
                                             h
                                             =
                                             
                                                h
                                                k
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Thus, the formulation that makes it possible to infer about the hypotheses (values of K) by considering the combination of probabilities is given in Eq. (12):
                              
                                 (12)
                                 
                                    
                                       
                                          h
                                          new
                                       
                                       ←
                                       arg
                                       
                                       ma
                                       
                                          x
                                          
                                             h
                                             k
                                          
                                       
                                       
                                       P
                                       
                                          
                                             h
                                             =
                                             
                                                h
                                                k
                                             
                                          
                                       
                                       
                                          
                                             ∏
                                             
                                                i
                                                =
                                                ϕ
                                             
                                             v
                                          
                                          
                                       
                                       
                                       P
                                       
                                          
                                             
                                                r
                                                i
                                             
                                             |
                                             h
                                             =
                                             
                                                h
                                                k
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

The estimation of the value of K is the hypothesis that maximizes the likelihood, Eq. (11). This formulation is capable of not only estimating K, but also giving a relative confidence ratio, for instance, the ratio between the best (higher likelihood) and the worse (lower likelihood) hypothesis shows the distance between both. Therefore, the method depicted in Fig. 5 receives the optical flow field and analyzes characteristics of the flow using features that mostly represent distinct motions models, combining the hypotheses with conventional model selection techniques that measure fitness and model complexity. As can be confirmed further ahead in this article, this method produces a more reliable estimation of K that is substantially better than the BIC alone. The importance of the FDBH is related to the fact that the EEyeRobot operates autonomously and therefore, the number of clusters enhances the segmentation of the different types of motion and allows a more realistic understanding of what is happening on the environment.

@&#RESULTS@&#

A comprehensive set of experiments were conducted as part of this work. They aims were to analyze and understand the behavior of the designed techniques for motion analysis in a robotic and surveillance context: segmentation of dense optical flow fields based on clustering approaches.

The first experiments focus on testing the accuracy of the model selection that was presented in Section 4.3. The estimation of the value K (number of clusters) based on this method is compared to the true value, which is manually labeled. These trials resort to real and virtual dense flow fields, for instance, several realistic sequences that capture different types of motion are used by an optical flow technique [29] to generate the dense flow field. Then, the performance of each clustering technique is presented and analyzed in detail, namely, the HHOFS and the HDBOFS. These techniques are compared to two alternative clustering methods, for instance, EM and K-means. In this way, it is possible to evaluate the pros and cons of block-wise techniques compared to pixel-wise techniques. Factors such as the computational effort and the quality of the visual segmentation are considered.

The large majority of the experiments were conducted using the EEyeRobot in a real surveillance scenario (the corridor of our laboratories). Therefore, they depict real testing conditions which means that the visual system of the mobile robot is subjected to different light conditions, reflections, diffractions, shadows and ghost effects (due to glass walls). No filtering technique was previously applied to the sequences in order to maintain the reliability and repeatability of the experiments. Otherwise, the results will be influenced by the type of the filter.

All the results in this section were obtained with an I3-M350 2.2GHz computer and without parallel programing or GPU. The methods were implemented in C++ using the commonly used OpenCV library.
                        3
                     
                     
                        3
                        Version 2.4.3 of the OpenCV.
                      The EM and the K-means are used in this research as baselines and implemented as standard functions. The real sequences were obtained using the EEyeRobot, for instance, the images have a resolution of 640×480 and were captured from a “TheImagingSourceDFK 21AU04” camera with a 4mm focal lens.

The FDBH model selection analyzes the dense flow field and predicts the number of clusters. This information is not critical for the HHOFS and the HDBOFS; however, it enhances the segmentation of these algorithms. In addition, the selection of model makes it possible to use parametric methods for clustering the flow fields such as the EM and the K-means.

Several flow fields that depict a different number of moving objects were virtually generated and the velocity (flow vector) of each moving object is randomly assigned. These virtual flow fields make it possible to study the estimation of the value K without the influence of noise or the optical flow technique, for instance, the quality of the flow field that is obtained by the technique and the aperture problem (inability to measure or to fully estimate motion in regions of the image that do not exhibit distinguishable characteristics). Fig. 6(a), (b) and (c) shows three examples of the virtual generated flow fields with 2, 4 and 5 clusters, respectively. In these images the direction and the magnitude of the flow vector are represented by arrows; however, in Fig. 6(d), (e) and (f), the same flow fields are represented in the hue–saturation–value (HSV) color space, where the orientation is the color and the magnitude of the vector is the saturation.

30 dense flow fields were generated with the value of K between 1 and 5 (6 flow fields per each K). These 30 trials are enough to prove the concept of the FDBH under no-noise conditions. In addition, 60 sequences of images were obtained from the EEyeRobot during its navigation in the corridor. The sequences were chosen according to some factors: they include the entire corridor (different positions with different light conditions) and they have a number of moving objects between 0 and 2. Detecting more than 2 moving objects in the same sequence of images is difficult due to the current spatial disposition of the robot in the narrow corridor, and due to its limited field of view. Therefore, it is specified that the robot must be able to accurately distinguish 3 different types of motion.

Two different scenarios were tested during the trials with realistic sequences: the robot views the faces or the bodies of the people. Fig. 7(a), (b), (c), (g), (h) and (i) depicts one image in the sequence that originates the respective flow field, Fig. 7(d), (e), (f), (j), (k) and (l). In a flow field figure, the saturation (intensity) of the HSV color space represents the magnitude of each flow vector and it is obtained using the maximum magnitude of all vectors. Thus, one flow field should not be directly compared to another since its representation is relative. For instance, the flow field of Fig. 7(d) appears to have flow vectors with larger magnitude when compared to Fig. 7(e); however, this is not true because the last flow field contains a moving person characterized by a flow vector with higher magnitude, which reduces the representation scale of the remaining scene (red regions). The presence of noise is evident in some of the flow fields that were generated through the optical flow technique [29]. The noise is caused by several problems that are related to the sensor noise, the aperture problem and the technique itself. Local and global differential optical flow formulations are combined by this technique and without resorting to non-quadratic penalizers, which affects the quality of the estimation but satisfies the real-time requirements of conventional robotic applications. As can be concluded from the examples above, flow fields obtained from the EEyeRobot characterize the environment where the robot is installed.

The proposed model selection method does not need the complete dataset (flow field) to estimate the number of clusters. The computation of K is a demanding procedure if the full dataset is considered since the FDBH uses the EM (or K-means) to obtain the likelihood. It returns the probability of each hypothesis which means that it has a mechanism to evaluate the confidence level of the estimation of K. A larger dataset can be used only when several hypotheses have similar probabilities and, in this way, a sampling procedure reduces the time spent during the selection of the model that fits better into the data. The results of the FDBH method that are reported in this paper use a uniform sampling process of 5% of the original dataset (more than 15,000 flow vectors) due to real-time constraints. Table 1
                         compares the accuracy of the FDBH with the EM+BIC. The FDBH outperforms the EM+BIC, even when subjected to different moving objects in several conditions. During the experiments, the FDBH is capable of estimating the number of clusters, regardless of the type of flow field. In fact, the performance of the EM+BIC was poor, which means that it presents severe difficulties when estimating the correct number of clusters (K
                        
                           gt
                        ). After a more careful interpretation, the major reason for this low accuracy is related to the trials where the robot is looking for the person's body. Motion analysis becomes even more difficult because only the legs of the person are visible as each leg can easily be interpreted as a moving object. The proposed model selection algorithm is robust enough to recognize this type of situation because the histogram analysis identifies pixels with similar motion properties, see Table 1.

When examining the trials that originate wrong estimations by the FDBH, it is possible to confirm that the confidence ratio in 3 of the 6 faulty trials is lower than 1.1. This means that the likelihood of the best hypothesis that was returned by the method is only 1.1 times higher than the hypothesis that yields the K
                        
                           gt
                        . Most of the faulty trials were obtained in real cases with K
                        
                           gt
                        
                        =1. The egomotion of the robot creates one cluster; however, the optical flow technique and the environment's condition cause visual artifacts. The optical flow technique is limited to some issues that go beyond the scope of this research.

Hence, the FDBH achieves a global accuracy in real sequences close to 90%. This value should not be interpreted as the final accuracy of the method (for that, more trials must be conducted) but it gives a good picture of the method's capacity for estimating the number of clusters based on dense flow fields. In addition, the FDBH method was implemented using the K-means in order to reduce the computational effort required to estimate K. Thus, the likelihood is computed after the K-means and the results that were obtained are similar to the EM version; however, it requires less than 30ms to compute K.

Several experiments were conducted in order to evaluate the behavior of the HHOFS and the HDBOFS. Factors such as the quality of the visual segmentation and the computational performance are considered. Both techniques are compared to the well-known EM and K-means. The EM and K-means are parametric techniques and, hence, they need the number of the clusters. For this reason, the information related to the K
                        
                           gt
                         is available during the experiments: the EM, K-means and HHOFS can be parameterized with the value of K being automatically configured by the FDBH. Instead, the HDBOFS is more complex to set up and it was manually configured in this research.

The results start by presenting the most difficult case which is depicted in Fig. 7(i) and it represents two people moving in the opposite direction. The EEyeRobot is moving in the same direction as the person on the left when capturing this scene. The flow field is shown in Fig. 7(l). This situation demonstrates the difficulty of segmenting different types of motion because the egomotion of the robot is in the same direction as one person which may mislead some clustering processes. This is why the features space and the similarity metric are so important because if they do not reflect the difference between distinct motions correctly, then the segmentation will produce poor results. In Fig. 7(l) it is possible to visualize an interaction region between both people, which leads to an area of confusion since the people are spatially close. The optical flow technique [29] also measures the apparent velocity of shadows and therefore, it creates an interaction between the two motions in the flow field. This area increases the difficulty of extracting three clusters (egomotion, person on the left and right).

Using the EM and the K-means for clustering the flow field in Polar space resulted in Fig. 8(a) and (b), respectively. The segmentation conducted by the EM produces 3 clusters. However, the clustering process does not originate a suitable segmentation because the clusters of people appear larger and they have spatially isolated regions that are meaningless (hereafter, called clustering noise). The result of the K-means is better than the EM because the clusters depict more faithfully the person's movement. In addition, the clustering noise is lower than the EM.


                           Fig. 8(e) and (d) shows the motion segmentation based on the HHOFS and HDBOFS. Both methods resort to a flow field in Polar representation and they do not deal with a single flow vector as in previous techniques. Therefore, the results have a blocky aspect due to the refining phase, see Fig. 8(c). The segmentation conducted by the HHOFS produces the best result because there is no clustering noise: only a small region in the person on the right is misclassified; however, this is not a real misclassification because the velocity of this small region is similar to the person on the left, see the flow field. The collecting phase of the HHOFS is more robust to the presence of the shadows since the clusters that were obtained only represent the people. In the last result, the parameter's configuration of the HDBOFS was only able to extract a pair of clusters: the two people are combined in a same cluster. This may be caused by the setup of the HDBOFS or by the density-based concept (collecting phase) because the region of confusion is joining both clusters and, thus, it misled the entire clustering process.

Segmentation results are presented for the flow fields in Fig. 7(e), (f) and (j). These cases reflect situations where the EEyeRobot captures moving people with different motion models. The aims are to analyze and compare the robustness of the clustering procedures. Fig. 9(a), (b) and (c) depicts the segmentation conducted by the EM and Fig. 9(d), (e) and (f) presents the results of the K-means. These results are baselines for the proposed HHOFS (Fig. 9(j), (k) and (l)) and the HDBOFS, Fig. 9(g), (h) and (i).

The visual illustration of the motion segmentation based on dense flow fields shows that the EM produces clusters affected by noise. The K-means has a visual segmentation that it is close to the EM, although with a lower amount of noise. Both methods were able to characterize the two motion models present in each trial; however, the clustering noise is higher than the results of HHOFS and HDBOFS. This is caused by the architecture presented in Fig. 2, for instance, the refining phase makes it possible to gather a set of flow vectors that are related in space and share a similar motion model. This increases the robustness to noise and reduces the computational effort during the agglomeration of motion models (collecting phase). Fig. 9(j), (k) and (l) shows that the HHOFS produces the best visual segmentation since the clustering noise is small and the resulting clusters reliably represent the different motions. In addition, the HHOFS is easy and intuitive to configure because it requires only the maximum similarity level between clusters and/or the number of clusters. This makes possible to set up the technique in running time and according to the FDBH model selection method. On the other hand, the HDBOFS is more difficult to configure since its parameters are less intuitive to setup in this context. This problem is well-known in the literature [3]. Without a proper configuration, clustering based on the HDBOFS could result in a poor visual segmentation (more frequent when more than 2 motion models are depicted in the flow field). Fig. 9(g), (h) and (i) shows the results for the HDBOFS. As it is possible to confirm, the technique returns a segmentation that is similar to the HHOFS; however; the clustering noise is higher because the collecting phase is executed based on the DBSCAN and not hierarchically. This strongly influences the clustering process since it changes the order in which the clusters are grouped (the density-reachability and the density-connected concepts). Fig. 9(g) shows the influence of this issue since the visual segmentation shows in fact 3 clusters: the smaller cluster (dark purple) on the upper right side has an optical flow (green region in Fig. 7(e)) that is quite different from the rest of the clusters. Thus, the DBSCAN isolates this cluster since it does not consider the number of clusters like the HHOFS, but considers the ϵ and the minPts instead.

From the results shown, it is possible to see that the visual segmentation of the HHOFS is suitable for robotic and surveillance applications because the parameters of the technique can be adjusted in running time. Furthermore, it is reliable and more robust to the influence of shadows that cause regions of confusion. In addition, density-based clustering approaches are not recommended for segmenting dense flow field, especially, for robotic and surveillance applications operating in a dynamic environment.

The HHOFS and the HDBOFS can have different computational requirements according to the parametrization of the refining phase, for instance, the initial division of the flow field space into non-overlapping regions. In this context, Table 2
                            presents the expected performance of the four methods during the clustering of all real cases presented in Section 5.1. The table shows that the K-means is computationally efficient since the processing time is a fraction of the time spent by the EM (with 3 iterations). As confirmed, the visual segmentation of the K-means is very close if not better than the EM and, thereby, the K-means reveals better characteristics for motion segmentation of dense flow fields in robotic applications.

A direct comparison between the HHOFS and the HDBOFS can be found in the table. As expected, the HDBOFS is computationally more efficient than the HHOFS. Several experiments were conducted considering different resolutions for the refining phase. Table 2 shows that increasing the resolution leads to a higher processing time because, commonly, there are more initial regions for the collecting phase, which increases the clustering time. Going from a 4×4 to an 8×8 resolution increases the processing time of the HHOFS and the HDBOFS on average by 18.57 and 13.3 times, respectively. In theory, the clustering time of Fig. 7(d) should remain smaller in both resolutions because the refining phase makes it possible to detect the presence of only one motion model; however, this does not happen because the flow field that was computed is not perfect due to some noise and visual artifacts.

The results have shown that pixel-wise techniques are better to discriminate the shape of moving objects but less robust to the aperture problem and less computational efficient. On the other hand, experiments prove that the HHOFS and the HDBOFS achieve a real-time performance with lower computational resources (without parallel programming). Their results share a blocky aspect which is acceptable for the EEyeRobot; however, a pixel-wise technique starting from the results of HHOFS and HDBOFS can enhance the visual segmentation of the flow field.

The HDBOFS is more computationally efficient but it has parameters that are more difficult to set up. Moreover, the visual segmentation produced by this method is usually worse when compared to the HHOFS. Therefore, the HHOFS makes it possible to segment motion from dense flow fields and in real-time. For instance, the technique took 33ms on average to analyze the flow field and to extract the set of clusters. In this way, the HHOFS seems to be a more balanced technique when considering both computational efficiency and the quality of the clustering process.

@&#CONCLUSION@&#

Motion analysis techniques based on moving observations are still in a preliminary stage when compared to static observations because the motion of the observer creates new paradigms that make the analysis even more complex and challenging.

This research studies the real-time motion segmentation based on dense optical flow fields for mobile robotic applications. It proposes two segmentation methods to extract different types of motion: the Hybrid Hierarchical Optical Flow Segmentation (HHOFS) and the Hybrid Density-Based Optical Flow Segmentation (HDBOFS). Both methods are composed of two stages: the refining phase decomposes the flow field into a set of distinctive clusters that represent image regions with different motion models and the collecting phase merges the set of clusters that were obtained by the refining phase, using the Mahalanobis distance and the hierarchical or DBSCAN scheme. A model selection method called by Distributed Bayesian Hypothesis (FDBH) is also proposed in this research. It uses the Polar representation and combines the histogram analysis with the decay ratio of the BIC (Bayesian information criterion) to infer about the number of clusters in the flow field. This information is not required by the HHOFS and HDBOFS, but it enhances the segmentation of the HHOFS.

Experiments conducted have proven the ability and flexibility of the HHOFS to segment different motions that may be present in a realistic surveillance scenario. In addition, the HHOFS and the HDBOFS proved to be computationally efficient with regard to other techniques reported in the literature and to conventional clustering techniques, for instance, expectation–maximization and K-means. In some cases, the K-means presented an interesting visual segmentation with low computational requirements. However, the HHOFS and the HDBOFS can deal with noisy flow fields since they are less affected by the quality of the optical flow estimation. The parameters of the HHOFS are very intuitive and simpler to adjust in running time. The HHOFS can obtain reliable results using only the number of clusters from the FDBH and neglecting the maximum similarity distance between clusters. On the other hand, the HDBOFS is more computationally efficient, especially for higher resolutions in the refining phase; however, their parameters are less intuitive in our context, which makes the method more difficult to setup in running time.

The proposed techniques meet the computational demands of common robotic systems since they segment dense flow fields (with a resolution of 640×480) in less than 40ms, without specialized hardware or parallel programming. Therefore, the moving robot is capable of analyzing external motions and providing important information that can be used to detect and track any danger situation namely, an intrusion, an unrecognized object, or even, for access control and person identification.

@&#ACKNOWLEDGMENT@&#

This work was partially funded by the Portuguese Government through the FCT — Foundation for Science and Technology, SFRH-BD-70752-2010.

@&#REFERENCES@&#

