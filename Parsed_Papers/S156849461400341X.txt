@&#MAIN-TITLE@&#Data stream synchronization for defining meaningful fMRI classification problems

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We challenge a popular approach to labelling fMRI data for predictive modelling.


                        
                        
                           
                           We propose a new labelling method based on data stream synchronization.


                        
                        
                           
                           We validate the proposed method experimentally on real fMRI data.


                        
                        
                           
                           We observe major classification accuracy improvement and model complexity reduction.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Pattern recognition

Machine learning

Classification

fMRI

Data stream synchronization

Smart filtering

@&#ABSTRACT@&#


               
               
                  Application of machine learning techniques to the functional Magnetic Resonance Imaging (fMRI) data is recently an active field of research. There is however one area which does not receive due attention in the literature – preparation of the fMRI data for subsequent modelling. In this study we focus on the issue of synchronization of the stream of fMRI snapshots with the mental states of the subject, which is a form of smart filtering of the input data, performed prior to building a predictive model. We demonstrate, investigate and thoroughly discuss the negative effects of lack of alignment between the two streams and propose an original data-driven approach to efficiently address this problem. Our solution involves casting the issue as a constrained optimization problem in combination with an alternative classification accuracy assessment scheme, applicable to both batch and on-line scenarios and able to capture information distributed across a number of input samples lifting the common simplifying i.i.d. assumption. The proposed method is tested using real fMRI data and experimentally compared to the state-of-the-art ensemble models reported in the literature, outperforming them by a wide margin.
               
            

@&#INTRODUCTION@&#

In the last years there has been a great deal of research on applying machine learning techniques and tools to processing of the functional Magnetic Resonance Imaging (fMRI) outputs [15,22,27]. The prospect of determining how mental states are mapped onto distributed patterns of neural activity is very attractive. The significance of this problem stems from countless potential applications in the area of neurology, Human–Machine/Brain–Computer Interfacing (HMI/BCI), or facilities for disabled and elderly people.

The research has been mostly organized around the following three key areas [19]: (1) application of classification methods to fMRI data (e.g. [1,8–11,23–26]), including combinations of classifiers also known as ensemble models (e.g. [17,28,29]), (2) dimensionality reduction techniques (e.g. [2,3,10,21,23,30]) and (3) spatio-temporal filtering (e.g. [18]). This research has been additionally stimulated and facilitated by the fMRI equipment becoming increasingly more accessible and affordable, the exponential increase in the available computational resources and unparalleled advances in the field of machine learning.

Since application of intelligent computational techniques to fMRI data is a relatively well developed area, in this study we focus on a more fundamental issue of the fMRI data preparation process and defining classification problems to be solved. According to data mining practitioners [20], data preparation
                        1
                     
                     
                        1
                        In this context ‘data preparation’ is a term encompassing integration of data from multiple sources, sampling, labelling and standard preprocessing (e.g. data transformation, editing/imputation, attribute selection).
                      can take up to 80% of the modelling efforts and is crucial for development of well-performing models. In the current research however this issue is often overlooked or at best addressed heuristically. A central problem in our view is the alignment of the fMRI data with the actual mental states (i.e. data labelling), so that it is possible to develop a predictive model which not only demonstrates a certain level of accuracy but also solve a real classification problem. In a typical fMRI experiment a subject is instructed to enter a number of mental states in sequence (e.g. ‘think of something funny’, ‘think of something sad’) and the responses are captured with a certain, fixed sampling rate, resulting in a sequence of brain activity observations (snapshots). It is not guaranteed however that the subject will indeed enter the required mental state or how fast it will happen. Hence there is an inherent uncertainty to what extent a recorded brain snapshot corresponds to the actual desired mental state. In modelling of the Blood Oxygen Level Dependent (BOLD) signal, this effect is additionally magnified by the haemodynamic response delay [13] and relatively low resolution of the images [27]. Lack of alignment of the fMRI data with mental states can contribute to the difficulty in translating between functional brain states of different subjects, so that a predictive model trained on data collected from one subject, would generalize well to others. The alignment issue is however often ignored and all snapshots taken while a certain stimulus is active are routinely averaged [27] (in some cases trimming 1–2 initial snapshots), or labelled with this particular stimulus [17]. While the former approach considerably reduces the size of the dataset, which is always small to begin with in relation to its dimensionality, it can also result in loss of valuable information and distorting it with noisy patterns. The latter, naïve labelling approach may however lead to defining a classification problem which is not meaningful (i.e. training data labels do not correspond to the mental states), rendering the subsequent analysis and modelling efforts futile or suboptimal at best. As an example, consider two brain snapshots taken while presenting two different stimuli A and B, which are more similar to each other than two snapshots taken when presenting stimulus C alone. As demonstrated later in this study, it is not an uncommon situation.

Automatic intelligent labelling of fMRI data is by no means a trivial task. Modelling of a fixed haemodynamic response function in the MRI literature [13,15] can be perceived as one attempt to address this issue, yet to the best of our knowledge no pure data-driven approach based on machine learning techniques exists. Hence in this paper we propose such an approach by viewing the problem of intelligent labelling of fMRI outputs as synchronization of the fMRI data stream and the label stream. We validate the proposed method on real, publicly available fMRI data and investigate its performance while using various measures of fit between the two data streams.

The main contribution of this paper is an original method for assigning labels to a stream of fMRI data, which results from challenging two popular approaches to fMRI data stream labelling for subsequent predictive model training. The proposed method is purely data-driven and lifts some of the restrictions of a typical signal processing based approach with a fixed haemodynamic response function. We also propose an alternative classification accuracy assessment scheme designed to make the results obtained with various fMRI data labellings more comparable.

The remainder of this paper is organized as follows. In Section “Problem setting” we further motivate and formally define the problem being addressed. We also give details of the data used in the experiments and perform its basic analysis. In Section “Baseline approach” a baseline approach taken from the literature is discussed and evaluated, with an in-depth analysis of its performance and identification of problematic areas. Section “Alternative accuracy assessment scheme and on-line predictions” presents an alternative model assessment scheme designed to make the results obtained with various fMRI data labellings more comparable and to take advantage of distributed representation of information in the on-line (i.e. prediction) mode. In Section “Synchronization criteria and optimization scheme” we present three measures of fit between the fMRI and label stream as well as an appropriate optimization algorithm for stream synchronization. The experimental results can be found in Section “Experimental results”, while the conclusions have been given in Section “Conclusions and future research directions”.

In this paper we use the data collected by Haxby within his seminal study ‘Distributed and Overlapping Representations of Faces and Objects in Ventral Temporal Cortex’ [14]. Haxby's experiment involved presenting 8 different visual stimuli to a single subject in 10 sessions. A snapshot of the brain has been taken every TR
                     =2.5s, where TR denotes a discrete time point, and each session consisted of a single presentation of every stimulus in a random order for 9 TRs, followed by a 5-TR rest period. In the reminder of this paper we refer to the brain snapshots also as TRs or samples, while the stimulus to which a given sample refers to is called the class of the sample. The data we have used has been provided with the Princeton Multi-Voxel Pattern Analysis (MVPA) Toolbox
                        2
                     
                     
                        2
                        
                           http://code.google.com/p/princeton-mvpa-toolbox/.
                      and the only preprocessing operation we have performed prior to our analysis was session-wise normalization (z-scoring).

While some of the issues resulting from labelling all samples taken while a certain stimulus is presented with this particular stimulus, and then using the obtained dataset to build and validate a classifier are well known (e.g. strong autocorrelation of samples violating the i.i.d. assumption on which most standard machine learning algorithms rely), there is another problem. In order to demonstrate it, we have calculated the within-class and between-class similarity of all collected samples (including the rest periods
                        3
                     
                     
                        3
                        The rest periods have been included here to demonstrate the high variability within a single class. In the classification experiments the rest periods have been discarded.
                     ). The results have been presented in Figs. 1 and 2
                     
                     
                     
                        4
                     
                     
                        4
                        All 43,193 voxels available in the dataset have been used for producing these figures. In order to alleviate the issue of potential concentration of the Euclidean distance (see [7]), concentration-resistant Cosine distance has also been used to confirm the findings.
                      (the definitions of these two distances are given later in Section “Synchronization criteria and optimization scheme”).

In Fig. 1(a) the median distance between samples coming from all pairwise combinations of the 9 classes (including the rest periods) has been depicted. As it can be seen there are cases, where the average within-class distance is much larger than the average between-class distance. For example, the median distance within the face class (≈295) is higher than the median distance between bottle and shoe (≈263) or shoe and scramble (≈265). Although not as clearly visible in the case of the Cosine distance in Fig. 2(a), the median distance within the scramble class (≈0.988) is still higher than the average distance between classes house and shoe (≈0.982).

The above issue is even more pronounced in Figs. 1(b) and 2(b), where the minimal between-class distance has been plotted off the diagonal, while the diagonal entries represent the maximal within-class distance. As a result, by using the naïve labelling scheme one effectively expects the predictive model to correctly discriminate between items from different classes, which are more similar to each other than the items belonging to the same class – a rather risky and counterintuitive endeavour.

For the sake of completeness, we have also performed the same calculations for the case, when all snapshots taken during a single presentation of a stimulus are averaged. As it can be seen in Figs. 3 and 4
                     
                     , the situation did not improve much – one can still observe cases, in which samples belonging to a single class are less similar to each other than samples belonging to two different classes.

As mentioned before we propose to look at the fMRI outputs as a stream of data, which needs to be synchronized with a stream of labels. An example has been depicted in Fig. 5
                     , where the arrows at the top represent the labels assigned to selected TRs within each stimulus presentation. Note, that unlike a fixed haemodynamic response lag, in the proposed setting the lag can be different for every presentation of each stimulus. This is an important feature of our approach due to the well-known variability of the haemodynamic response across subjects, sessions in a single subject and stimuli. Also, we allow multiple TRs in each presentation to be selected and labelled, to account for the effect of sampling resolution artefacts (e.g. when the blood oxygenation level peaks between two consecutive samples or the peak spreads to a flat plateau).

Formally, the stream synchronization approach can be defined as a constrained optimization problem, where one is seeking such assignment of labels to the fMRI data stream, which minimizes a chosen criterion 
                        J
                     .

Denoting by 
                        x
                     
                     
                        i
                      the ith vector of voxel activations (sample), by L
                     
                        i
                      the stimulus shown while 
                        x
                     
                     
                        i
                      was recorded (class), the input dataset can be defined as a sequence of sample/label pairs: D
                     =((
                        x
                     
                     1, L
                     1), (
                        x
                     
                     2, L
                     2), …, (
                        x
                     
                     
                        N
                     , L
                     
                        N
                     )), where N is the number of recorded snapshots. Let I
                     =(i
                     1, i
                     2, …, i
                     
                        N
                     ) be a binary selector vector and k
                     ∈{1, 2, …, N}. We define a filter function 
                        F
                      given by 
                        
                           D
                           F
                        
                        =
                        F
                        (
                        D
                        ,
                        I
                        )
                     , where D
                     
                        F
                     
                     =((
                        x
                     
                     
                        k
                     , L
                     
                        k
                     ):
                     i
                     
                        k
                     
                     =1). The optimization problem then becomes:


                     
                        
                           (1)
                           
                              
                                 
                                    
                                       
                                          argmin
                                          I
                                       
                                    
                                    
                                       J
                                       (
                                       F
                                       (
                                       D
                                       ,
                                       I
                                       )
                                       )
                                    
                                 
                                 
                                    
                                       
                                          
                                          s
                                          .
                                          t
                                          .
                                       
                                    
                                    
                                       ∀
                                       i
                                       ∈
                                       
                                          
                                             
                                                1
                                                ,
                                                2
                                                ,
                                                …
                                                ,
                                                
                                                   N
                                                   P
                                                
                                             
                                          
                                       
                                       :
                                       
                                          ∑
                                          
                                             l
                                             =
                                             (
                                             i
                                             −
                                             1
                                             )
                                             ×
                                             P
                                             +
                                             1
                                          
                                          
                                             i
                                             ×
                                             P
                                          
                                       
                                       
                                          i
                                          l
                                       
                                       ≥
                                       1
                                    
                                 
                              
                           
                        
                     where P is the length of stimulus presentation in TRs (in our case P
                     =9) and the set of constraints ensures that at least one sample from each stimulus presentation is included in D
                     
                        F
                     .

Although the approach described above can be perceived as a variant of temporal feature selection (if all TRs from a single stimulus presentations were treated as a single sample), it has an important advantage – compatibility with standard machine learning algorithms. Most machine learning algorithms have been designed to handle input vectors of a fixed size, where the meaning of each element of these vectors does not change over time. This has an important consequence for what is traditionally understood as temporal feature selection: the number of features selected must be the same for all samples (e.g. one always selects exactly 3 TRs). As this can be suboptimal, the method proposed in this paper does not have such restrictions – the number of selected TRs can be anything between 1 and P.

As a starting point we have attempted to reproduce the best results reported in [17], i.e. 73.2% 10-fold cross-validation
                        5
                     
                     
                        5
                        Cross-validation (CV) is a standard statistical technique for estimation of model generalization ability, i.e. assessing how the model will perform on new, previously unseen data [16]. In k-fold cross-validation the dataset is randomly divided into k approximately equal subsets. Each subset (called ‘fold’) is then in turn put aside as validation data, a model is built using the remaining k
                           −1 folds and tested on the validation fold. The error estimate is then calculated as a mean of all validation errors. In both [17] and this study each stimulus presentation session corresponds to a single cross-validation fold.
                      accuracy of a Random Forest ensemble with 1000 trees. Following [17], we have first selected a subset of voxels by cross-training
                        6
                     
                     
                        6
                        In k-fold cross-training each of the k base models is trained on the union of k
                           −1 folds, every time leaving a different fold aside.
                      10 Support Vector Machines (SVMs) with linear kernels and then extracting top 200 contribution weights of voxels in terms of their absolute value, from each model. The intersection of the 10 sets obtained in this way resulted in 92 voxels common for all sessions (Fig. 6
                     ), which is a slightly different result when compared to [17], where the authors have reported the intersection to contain 93 voxels. We have attributed this to slight differences in the SVM implementations from various authors. In our experiments a Support Vector Classifier (svc) from the PRTools Pattern Recognition Toolbox version 4.2.1 for MATLAB [12] has been used.

Rather than building complex ensemble models, we have first opted for a set of basic classifiers included in the PRTools toolbox – their list has been given in Table 1
                     , while their classification accuracy assessed using the same approach as in [17] has been reported in Table 2
                     .

As it can be seen, a simple linear classifier (ldc) was able to achieve an average accuracy of 74.2%, not only vastly outperforming other tested classifiers, but also outperforming the Random Forest ensemble from [17] at the fraction of computations, yet still leaving room for improvement. Hence all the remaining experiments reported in this paper are performed using ldc as a base model and focus on the influence of label stream synchronization on the classification accuracy rather than on optimization and tuning of the classifier itself.


                     Fig. 7
                      presents the TRs for which ldc produced incorrect predictions (marked with ‘x’), broken down into session/stimulus pairs. First thing to notice is that some classes seem not to pose problems to the classifier (house, chair or scramble) while other appear to be rather difficult to handle (bottle, scissors, cat or shoe).

The confusion matrix given in Table 3
                      provides some insight into this situation:
                        
                           •
                           
                              bottle is often confused with both scissors (12 TRs) and shoe (13 TRs) as well as chair (8 TRs),


                              scissors are mostly confused with bottle (11 TRs) and scramble (8 TRs),


                              shoe is mostly confused with bottle (16 TRs).

The histogram of classification errors vs. TR given in Fig. 8
                      provides further interesting insight: the first TR of each stimulus presentation is by far the most difficult to classify as it accounts for over 25% of all errors. This can be caused by natural variability in the onset of the haemodynamic response function or the initial dip in the BOLD response reported in some studies (for example [31] and references therein), resulting in a rather noisy pattern at the output of the fMRI device. On the other hand, the third and fourth TRs (between 5 and 10s) appear to be the easiest to classify on average, which is also more or less consistent with the effect of the haemodynamic lag causing the blood oxygenation level to peak around 5s after stimulus presentation [13].

At this stage it would thus be instructive to develop the following additional classification models and assess their performance:
                        
                           •
                           A model which uses all TRs except the first, most problematic TR of each stimulus presentation,

A model which uses only the third, least problematic TR of each stimulus presentation,

A model which uses both the third and fourth, i.e. two least problematic TRs of each stimulus presentation.

The problem is however, that by dropping data corresponding to all TRs but the third, we would severely affect the validation mechanism. The reason for this is that rather than having 9 validation TRs for every stimulus in every session, we would end up with a single TR per stimulus. Moreover this would also make the results difficult to interpret and to compare between the experiments with different numbers of validation TRs. Hence in the next section we propose an alternative model assessment scheme, which levels the ground for all models, regardless of the actual number of TRs used for training.

In order to make the experiments comparable between models trained using different number of TRs, we have devised the following alternative accuracy assessment scheme, embedded into standard cross-validation.

For every fold, a model is built using only the selected TRs from the 9 training folds, but for validation all TRs from the remaining fold are always used. We are however not interested in assessing prediction for every single TR of the validation data but rather for a group of TRs, which correspond to presentation of a single stimulus. Hence for every such group of 9 TRs (batch) we produce a single classification decision by summing the soft outputs
                        7
                     
                     
                        7
                        
                           ldc produces so called ‘soft’ or ‘fuzzy’ outputs, which denote the degree of membership of a given sample to each class.
                      of the classifier and selecting the class for which this sum is maximized. This allows to capture information distributed across neighbouring TRs rather than using only a single TR for casting a prediction. It also ensures robustness of our approach, as the influence of noisy samples is minimized through the way in which the classifier outputs reflect uncertainty (i.e. there is no dominating class). The procedure has been depicted in Fig. 9
                     .

The classification accuracy obtained using the above scheme has been reported in Table 4
                     , where the subscripts in the leftmost column denote the TRs of each presentation used for training. As it can be seen, the best results – 85% accuracy – have been obtained when using only TRs #3 and #4 or not using TR #1. Hence 85% accuracy becomes our new baseline.

The proposed accuracy assessment scheme can be easily extended to support a true on-line scenario, in which consecutive samples arrive one by one and are classified in real-time. The results of such experiment have been included in Section “Experimental results”.

We propose to use the following optimization criteria in Eq. (1):
                        
                           1.
                           The ratio of the average intra-class distance to the average inter-class distance. This is a measure inspired by clustering algorithms [4], designed to encourage formation of groups of samples, which are similar to each other while dissimilar to the samples in other groups. The criterion is given by the following formula:
                                 
                                    (2)
                                    
                                       
                                          J
                                          dist
                                       
                                       =
                                       
                                          
                                             C
                                             ×
                                             
                                                ∑
                                                
                                                   c
                                                   =
                                                   1
                                                
                                                C
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   
                                                      N
                                                      c
                                                   
                                                
                                             
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                
                                                   
                                                      N
                                                      c
                                                   
                                                
                                             
                                             d
                                             (
                                             
                                                
                                                   
                                                      x
                                                   
                                                
                                                ci
                                             
                                             ,
                                             
                                                
                                                   
                                                      x
                                                   
                                                
                                                cj
                                             
                                             )
                                          
                                          
                                             
                                                ∑
                                                
                                                   
                                                      c
                                                      1
                                                   
                                                   =
                                                   1
                                                
                                                C
                                             
                                             
                                                ∑
                                                
                                                   
                                                      c
                                                      2
                                                   
                                                   =
                                                   1
                                                
                                                C
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   
                                                      N
                                                      
                                                         
                                                            c
                                                            1
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                
                                                   
                                                      N
                                                      
                                                         
                                                            c
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                             d
                                             (
                                             
                                                
                                                   
                                                      x
                                                   
                                                
                                                
                                                   
                                                      c
                                                      1
                                                   
                                                   i
                                                
                                             
                                             ,
                                             
                                                
                                                   
                                                      x
                                                   
                                                
                                                
                                                   
                                                      c
                                                      2
                                                   
                                                   j
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              where C denotes the number of classes (stimuli), N
                              
                                 c
                               is the number of selected TRs of the cth stimulus presentation, x
                              
                                 ci
                               is the ith selected sample within cth presentation and d(·) is some distance measure, in our case:
                                 
                                    (a)
                                    the Euclidean distance: 
                                          
                                             d
                                             E
                                          
                                          (
                                          
                                             
                                                
                                                   x
                                                
                                             
                                             i
                                          
                                          ,
                                          
                                             
                                                
                                                   x
                                                
                                             
                                             j
                                          
                                          )
                                          =
                                          
                                             
                                                
                                                   
                                                      (
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                         
                                                         i
                                                      
                                                      −
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                         
                                                         j
                                                      
                                                      )
                                                   
                                                   T
                                                
                                                (
                                                
                                                   
                                                      
                                                         x
                                                      
                                                   
                                                   i
                                                
                                                −
                                                
                                                   
                                                      
                                                         x
                                                      
                                                   
                                                   j
                                                
                                                )
                                             
                                          
                                       
                                    

the Cosine distance: 
                                          
                                             d
                                             C
                                          
                                          (
                                          
                                             
                                                
                                                   x
                                                
                                             
                                             i
                                          
                                          ,
                                          
                                             
                                                
                                                   x
                                                
                                             
                                             j
                                          
                                          )
                                          =
                                          1
                                          −
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                      
                                                   
                                                   i
                                                   T
                                                
                                                
                                                   
                                                      
                                                         x
                                                      
                                                   
                                                   j
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                         
                                                         i
                                                         T
                                                      
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                         
                                                         i
                                                      
                                                   
                                                
                                                ×
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                         
                                                         j
                                                         T
                                                      
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                         
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    

The internal cross-validation error calculated within ldc during the regularization parameter tuning 
                                 
                                    J
                                    cv
                                 
                              .

In order to solve the problem defined by Eq. (1) we have devised an iterative greedy optimization scheme given by Algorithm 1. In our experience greedy approaches work surprisingly well (see [5,6] for example), often outperforming stochastic methods like genetic algorithms or simulated annealing, when a strict time limit is imposed on the optimization process. Also, due to their deterministic nature, all experiments are easily reproducible and there is no need to account for random factors when comparing the results of multiple experiments.


                     
                        Algorithm 1
                        
                           
                              
                                 
                                    Optimize 
                                          J
                                       .
                                 
                                 
                                    
                                    
                                       
                                          
                                             Initialization:
                                          
                                       
                                       
                                          
                                             
                                             
                                                D
                                                ←
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            
                                                               
                                                                  x
                                                               
                                                            
                                                            1
                                                         
                                                         ,
                                                         
                                                            L
                                                            1
                                                         
                                                         )
                                                         ,
                                                         (
                                                         
                                                            
                                                               
                                                                  x
                                                               
                                                            
                                                            2
                                                         
                                                         ,
                                                         
                                                            L
                                                            2
                                                         
                                                         )
                                                         ,
                                                         …
                                                         ,
                                                         (
                                                         
                                                            
                                                               
                                                                  x
                                                               
                                                            
                                                            N
                                                         
                                                         ,
                                                         
                                                            L
                                                            N
                                                         
                                                         )
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                             I
                                             ←[001000000 001000000…001000000]
                                       
                                       
                                          
                                             
                                             
                                                bestScore
                                                ←
                                                J
                                                (
                                                F
                                                (
                                                D
                                                ,
                                                I
                                                )
                                                )
                                             
                                          
                                       
                                       
                                          
                                             
                                             improvement
                                             ←
                                             false
                                          
                                       
                                       
                                          
                                             Optimization:
                                          
                                       
                                       
                                          
                                             
                                             while 
                                             improvement
                                             =
                                             false 
                                             do
                                          
                                       
                                       
                                          
                                             
                                             for all 
                                             sp← stimuli presentations do
                                          
                                       
                                       
                                          
                                               
                                             for all 
                                             iTR
                                             ←(I(sp)=0) do
                                          
                                       
                                       
                                          
                                             
                                             
                                             I
                                             
                                                temp
                                             
                                             ←
                                             I
                                          
                                       
                                       
                                          
                                             
                                             
                                             I
                                             
                                                temp
                                             (sp)←
                                             false
                                          
                                       
                                       
                                          
                                             
                                             
                                             I
                                             
                                                temp
                                             (sp(iTR))←
                                             true
                                          
                                       
                                       
                                          
                                             
                                             
                                             
                                                score
                                                ←
                                                J
                                                (
                                                F
                                                (
                                                D
                                                ,
                                                
                                                   I
                                                   temp
                                                
                                                )
                                                )
                                                )
                                             
                                          
                                       
                                       
                                          
                                             
                                             
                                             if 
                                             score
                                             <
                                             bestScore 
                                             then
                                          
                                       
                                       
                                          
                                             
                                               
                                             I
                                             ←
                                             It
                                          
                                       
                                       
                                          
                                             
                                               
                                             bestScore
                                             ←
                                             score
                                          
                                       
                                       
                                          
                                             
                                               
                                             improvement
                                             ←
                                             true
                                          
                                       
                                       
                                          
                                             
                                             
                                             end if
                                          
                                       
                                       
                                          
                                               
                                             end for
                                          
                                       
                                       
                                          
                                             
                                             end for
                                          
                                       
                                       
                                          
                                             end while
                                          
                                       
                                    
                                 
                              
                           
                        

In Algorithm 1 we start by initializing all the relevant variables (note, that we are using symbols and terminology introduced in Section “Problem setting”). This includes the binary selector vector I, which initially selects the 3rd TR of each stimulus presentation, according to our earlier argument in Section “Baseline approach”. Then, in the main loop we iteratively test various label assignments, accepting a new one only if it is better than all assignments tested before. The optimization algorithm is run 10 times following the cross-validation scheme and hence results in 10 versions of the selector vector I, with a single active TR per stimulus presentations. We then try to improve the assignment by re-running a slightly modified algorithm, this time initializing I with the result of the previous run, until no further improvement is observed.

@&#EXPERIMENTAL RESULTS@&#

The experiments have been performed in two tracks: starting with 200 pre-selected voxels, as described in Section “Baseline approach” (i.e. 10 sets of voxels selected within a 10-fold cross-validation scheme), and starting with all voxels (i.e. without the voxel pre-selection mechanism). Note that this applies to the stream synchronization criteria only, as in both cases the ldc model has been trained using the 200 pre-selected voxels due to computational tractability. For the same reason in the case of 
                        
                           J
                           cv
                        
                      the experiments on all voxels have not been performed.


                     Tables 5 and 6
                     
                      present the results for the two tracks of experiments. For each of the three synchronization criteria (e.g. 
                        
                           J
                           dist
                        
                        /
                        
                           d
                           E
                        
                     , 
                        
                           J
                           dist
                        
                        /
                        
                           d
                           C
                        
                      and 
                        
                           J
                           cv
                        
                     ) we have first allowed for a single pass of Algorithm 1, in order to assign the class labels to a single TR within each stimulus presentation only. In Tables 5 and 6 this has been denoted by TR
                     (1). The result was a very high 87.5% classification accuracy when using 
                        
                           J
                           dist
                        
                        /
                        
                           d
                           E
                        
                      on all voxels, which is already above the baseline defined in Section “Alternative accuracy assessment scheme and on-line predictions” (for convenience, if the average accuracy in Tables 5 and 6 exceeded the baseline accuracy of 85%, it has been typed in bold).

Since as a greedy approach, the proposed optimization scheme is susceptible to getting caught in local minima, in the next experiment we have used a known good solution as a starting point – the algorithm was initialized by pre-selecting TRs #3 and #4 of each presentation according to the results reported in Table 4. We have then run the optimization 3 more times in sequence, every time feeding the result of the previous run as an input. This way we have allowed the algorithm to select up to 3, 4 and 5 TRs of each stimulus presentation (TR
                     3,4+(1), TR
                     3,4+(2) and TR
                     3,4+(3) in Tables 5 and 6). The best result in our experiments – 88.8% accuracy – has been obtained for the combination of 
                        
                           J
                           dist
                        
                        /
                        
                           d
                           E
                        
                      and TR
                     3,4+(3). Although we believe the results could be further improved, for example by employing ensemble models as discussed in [17], we consider that the point has been proven and rather focus on examining the best model in more detail.


                     Fig. 10
                      shows a histogram of the TRs used by the best model. The values on the y−axis denote the percentage of stimulus presentations in which a given TR was labelled. As it can be seen, TRs #3 and #4 are absolutely crucial in this case as they have always been used in all presentations, complemented by TRs #5, #6 and #7 in ≈30% of cases but also by TR #1 in less than 5% of the cases. Although according to our previous argument TR #1 is by far the most problematic, as it can be seen, in some situations its inclusion is beneficial. This last observation emphasizes our claim that the fMRI data should be labelled in a flexible, data-driven way rather than by imposing fixed requirements, for example of not including TR #1 in the analysis at all (trimming).

An interesting thing to note from the presented results is that the data from some sessions seem to be much easier to classify than from others. In Table 5, session 4 is one such example, where the accuracy is seldom below 100%, while in case of session 9 the accuracy never exceeds 75%. Although beyond the scope of this study, it might be instructive to closely examine the setup of the fMRI experiment in [14] looking for an explanation of this phenomenon.

In the last experiment presented in this section we have investigated extending the accuracy assessment scheme proposed in Section “Alternative accuracy assessment scheme and on-line predictions” for on-line predictions of class labels of incoming samples. In this scenario we count how many consecutive TRs must be seen by the model for the classification decision to stabilize, i.e. not change when more samples from the same batch arrive. The results have been depicted in Fig. 11
                     .

First thing to note is that almost 70% of classification decisions are made after examining just 2 incoming samples (TRs). Moreover, most of these decisions are correct and 3 incoming samples suffice to reach the final decision in about 90% of the cases. In just under 4% of cases the decisions require examination of as much as 8 samples, but this is where the classifier is never correct.

Recent advances in the area of machine learning, together with the ever increasing computational power of affordable equipment allow to model almost any relationship underlying a given dataset, regardless of how spurious it is. Definition of meaningful learning problems hence arises as an important issue, relevant to any data-intensive discipline. In this paper we have explored and addressed this issue in the context of fMRI data modelling.

To this end, we have challenged two popular approaches to fMRI data stream labelling for subsequent predictive model training, demonstrating some of their weaknesses. The solution we have proposed involves casting the issue as a constrained optimization problem in combination with an alternative classification accuracy assessment scheme, applicable to both batch and on-line scenarios and able to capture information distributed across a number of input samples lifting the common simplifying i.i.d. assumption.

By employing the proposed fMRI data labelling approach and redefining the standard classification problem in terms of what we are really interested to predict, we have been able to take the initial 75% classification accuracy up to almost 89%. In the context of the dataset used in this study it translates to misclassification of only 9 out of 80 presented stimuli rather than 20 out of 80 without the stream synchronization algorithm. As noted in Section “Experimental results” this result could likely be further improved by employing more advanced modelling techniques like the ensemble models, which forms one of the future research directions.

The work can also be extended by redefining the optimization problem in order to make the objective function continuous and differentiable e.g. by relaxation. This would allow to use other, potentially more efficient optimization methods which rely on gradient information (e.g. quasi-Newton). Yet another direction we would like to pursue in the future is making the proposed approach fully incremental, to enable it to learn on the fly as new data arrives.

@&#REFERENCES@&#

