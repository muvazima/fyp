@&#MAIN-TITLE@&#Changes in the digital scholarly environment and issues of trust: An exploratory, qualitative analysis

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Digital transition had resulted in changes in researcher behaviour.


                        
                        
                           
                           It is now easier for scientists to discover and disseminate research.


                        
                        
                           
                           The way scientists exercise trust has not changed.


                        
                        
                           
                           Metrics are less important than experience and personal recommendation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Trust

Scholarly communication

Citation

Authority

@&#ABSTRACT@&#


               
               
                  The paper reports on some of the results of a research project into how changes in digital behaviour and services impacts on concepts of trust and authority held by researchers in the sciences and social sciences in the UK and the USA. Interviews were used in conjunction with a group of focus groups to establish the form and topic of questions put to a larger international sample in an online questionnaire. The results of these 87 interviews were analysed to determine whether or not attitudes have indeed changed in terms of sources of information used, citation behaviour in choosing references, and in dissemination practices. It was found that there was marked continuity in attitudes though an increased emphasis on personal judgement over established and new metrics. Journals (or books in some disciplines) were more highly respected than other sources and still the vehicle for formal scholarly communication. The interviews confirmed that though an open access model did not in most cases lead to mistrust of a journal, a substantial number of researchers were worried about the approaches from what are called predatory OA journals. Established researchers did not on the whole use social media in their professional lives but a question about outreach revealed that it was recognised as effective in reaching a wider audience. There was a remarkable similarity in practice across research attitudes in all the disciplines covered and in both the countries where interviews were held.
               
            

@&#INTRODUCTION@&#

@&#BACKGROUND@&#

This paper constitutes an output of a research project commissioned by the Alfred P. Sloan Foundation, which sought to discover whether the digital transition had led to changes in the way academic researchers placed their trust in scholarly communications (Nicholas et al., 2014). We were especially interested in the impacts that open access publications and the various types of social media might have had on what researchers used and cited and where they published. In order to understand what and why researchers trusted in the digital age there was a need to obtain a deep and personal understanding of the context of their decision making. This meant that not only was it important to understand the research process, but in particular how the researchers themselves understood it. The interviews reported on in this paper represented a second phase in the overall project. The questions presented were intended to build on and complement what we had learnt in focus groups which were the starting point of the project and at the same time enable us to optimise the list of questions provided in the international questionnaire which was the culmination of the project. The central part of the interview enabled a quantitative analysis of citation behaviour based on a critical incident approach. Another output from the project provides a detailed analysis of reasons for citation and comments on the implications for citation theory (Thornley et al., 2015). This aspect of the interviews provided much information of a more general qualitative nature which was relevant to the broader questions which are the subject under discussion. The interviews also provided insights from the answers given to the first and last two questions (see Appendix A for the wording). These questions were phrased broadly and answers could in many cases be described as discussions related to and shedding light on the topics with which this paper is concerned. (For the results of the questionnaire phase of the study refer to Tenopir et al., 2015 and Jamali et al., 2014).

The project sought to discover:

                           
                              (a)
                              How academic researchers, in the digital age, assign and calibrate authority and trustworthiness to the sources and channels they use, cite and publish in.

Whether this has changed, especially in the light of two major digital developments, open access publishing and social media.

The scholarly context in which researchers make trustworthiness decisions.

In the analysis that follows there are some instances where assertions derived from the interviews are not specifically concerned with these aims. We included them because they represented general comments made or implied by the researchers themselves which had a bearing on and provided a context for those assertions which are more directly related to the aims set out.


                        
                           
                              (a)
                              
                                 Trust and trustworthiness. If we were to achieve a full understanding of how the trustworthiness of some information source or channel is established, we needed to consider its attributes on two dimensions: its inherent attributes which, taken together, can be seen as specifying its objective quality, as well as its more extrinsic, user-attitude determined attributes, which, taken together, can be seen as specifying its subjective reliability. The approach we took saw the interplay between the inherent attributes of an information source/channel and its more extrinsic, user-attitude shaped attributes as the determinant of its trustworthiness, with quality and reliability thus becoming sub-constructs of trustworthiness. We used a schema to help us guide the study as a whole and as an aid to evaluating the data and this is provided in Table 1
                                 . However we have to explain that we explored the use of the schema particularly in the focus groups which preceded the interviews (with some slight overlap). In the focus groups we found that those taking part had a clear idea of trust and trustworthiness which incorporated the qualities set out in the table but did not respond well to attempts by those running the focus groups to analyse the concepts closely. They knew what trust in information sources meant to them. The questions on which the interviews were based were particularly concerned with the digital transition and did not lend themselves to a close analysis.


                                 Type of academic. The investigation was concerned with scholarly communication (researchers communicating with other researchers) and we were not concerned with academic researchers as teachers. However there was some discussion about outreach, reaching practitioners and indeed the general public. This discussion arose naturally from the later questions relating to social media.


                                 Scholarly communications. When we began the research project we intended to use such terms as “research output” or “channels” because we were interested in other types of output enabled by the digital transition, not just books and journals. However, we found in the focus groups that these terms were rarely understood and almost invariably researchers when interviewed saw scholarly communication almost entirely in traditional ways.


                                 Subjects covered. The funder, Alfred P. Sloan foundation, was largely concerned with science and the social sciences. Researchers in the humanities and in applied areas such as clinical medicine were not included in the investigation though a few humanists (for example teaching in business schools but researching in humanities disciplines) were interviewed.

The nature of the semi-structured interviewing technique used in the interviews meant that we often allowed the interviewee to move on to an area of what seemed of interest even when it was a divergent to the question asked. At some stage, which of course varied from interview to interview, we had to draw them back to the basic aims and scope.

@&#METHODS@&#

UK and US academic researchers were recruited to the study with the help of a number of scholarly publishers (Taylor & Francis, Sage, Elsevier, Wiley, PLOS and BioMed Central) to take part in one-to-one interviews, which contained a critical incident element concerning citation practices. These publishers e-mailed recent authors in relevant journals with an invitation to be available for interview. The nature of the interview was made clear in the invitation. Interviewers then made direct contact with those researchers who replied positively. Social scientists and older academics seemed to be keener on being interviewed than scientists and younger academics and interviewers had to select to avoid an imbalance: not all those who offered themselves for interview were selected. This mode of sampling – there were really no alternative sampling frames, which guaranteed high levels of response, did mean that all those we interviewed had published at least one paper in a peer reviewed journal and that a number has published open access papers – the last two publishers owning exclusively open access journals. In all the disciplines with which we were concerned it is difficult to envision someone at any stage of an academic research career (except pre-doctoral) not having published at least one paper and nothing prevented researchers from putting forward blogs. A small number of the researchers interviewed (4) were introduced to the interviewer visiting their institutions by researchers actually invited to be interviewed. All interviews were conducted between December 2012 and April 2013. Interviewees could have the interview conducted face-to-face at a mutually-convenient location or remotely via phone or Skype.

Researchers were contacted ahead of the interview and provided with a list of questions (see Appendix A). These questions were chosen following an analysis of the results from the focus groups previously held and because we wished both to establish further understanding about researcher understanding of behaviour change (their own and others) in an environment of digital transition and also establish a quantitative analysis in one area – citation analysis – which was reasonably discrete. The interviews were semi-structured in the sense that all the interviewees were sent the short list of questions in advance, but the interviewees were not discouraged from ranging widely in their responses especially where it was possible to arrange a face to face interview. A publication published or submitted by them was agreed usually ahead of time which would feature in the critical incident element of the interview. In a few cases the publication proposed was rejected because it was too short and replaced at the start of the interview. The critical incident element concerned references in a recent publication authored by the interviewee. The researchers were asked to choose the citations to be discussed, but not all had done this at the start of the interview and in some cases (not more than 5) the interviewer had to make an instant decision choosing at random from different points in the total list of references. Five citations were identified and interviewees were asked a number of related questions. They were asked what made them include the citation, how did they decide whether this was a reliable source, and how they found the source.

Critical incident techniques have been used before by members of the research team, for instance, in a CICS investigation (Tenopir, Volentine, & King, 2013) of social media and scholarly reading and a CIBER study of the needs and requirements of scholarly authors in the digital environment (Rowlands, Nicholas, & Huntington, 2004). Others have used the technique in other settings including investigating the role of Google in scientists’ information seeking behaviour (Jamali & Asadi, 2010), the information seeking behaviour of senior arts administrators (Zach, 2005), the information behaviour of preteens (Meyers, Fisher, & Marcoux, 2009), and in a study of the value of information to clinical decision making (Urquhart & Hepworth, 1996).

Interviews were conducted with university academics across the US and the UK (42 in the USA and 45 in the UK). Of the 87 who were interviewed 35 were seen face-to-face and the remainder contacted by phone. There were 60 males and 27 females with the majority (51) “mature” (aged 30–50), 37 older including some “emeritus” and 9 under 30. In this paper, we have used the term “established researchers” to describe those interviewed who had established careers in academic life. There were 51 social scientists and 36 scientists; the latter divided between 20 life scientists and 16 physical scientists (see Table 2
                     ). The composition of the social sciences group reflected the disciplines that are considered core: economics, psychology, sociology, politics and geography. Clearly the sample of researchers cannot be claimed to be wholly representative, the numbers and means of selection did not allow this. However, this was a qualitative strand to the study, which sought to obtain deep insights in order to help frame the questionnaire and help explain its results. The questionnaire delivered the quantitative data and better levels of representativeness, and, as it turned out, the results of the interviews data closely matched those of the questionnaire (Jamali et al., 2014, Tenopir et al., 2015).

The data collected in the interviews was analysed using iterative coding in which the core meaning of responses was established reductively by generalising from the language used by a respondent to a normalised set of terms across responses (see Mostyn, 1985). The analytical framework adopted is used to structure the result section of this paper.

@&#RESULTS@&#

In this section we have summarised results gleaned from the interviews which relate to this topic. Where we have not instanced exceptions, it can be assumed that the behaviours mentioned appear to be the norm across all disciplines in the sciences and social sciences within the interview sample and across different levels of researcher status. Where we have instanced exceptions, it can be assumed that the different behaviour described was common to all those in the discipline whom we interviewed. The narrative in the following sections derives from the way in which the researchers presented their behaviour not only just in their use of citations but as users and authors.

It became apparent during the interviews that for most researchers their network of trusted contacts, virtually maintained, was thought to be invaluable as a source of information and a strong contributor to trust behaviour. This was implicit in answers provided to questions about citing behaviour but also explicit in the more general discussions related to the other questions in the interview schedule. Most networks were initially established face-to-face at conferences or through laboratory visits and this has probably always been the case. We were not told of any changes resulting from digital transition. Indeed, we discovered that attendance at conferences has maintained its importance as a place where you decided to trust individuals or groups or not trust them. In particular, the point was made that when you saw researchers present their work and deal well with questions and comments then you can be confident of their expertise. What has changed is how such networks were maintained. E-mails remained important, but social media was used by some researchers to keep in touch. Skype could be useful, but was not seen as replacing travel to conferences.

Established researchers, because of the number and prestige of their contacts, were often key exploiters of such networks. They knew what was going on in the laboratories before the research was published. Some of the small numbers of younger researchers interviewed, after they had emerged from mentoring, were good at building their own groups aided by a wider and more assiduous use of the social media. Others remained in the group that they started in as research students. Established researchers were confident at being able to keep up to date via their existing social networks, but a few realised that they might be missing out on extending their connections to embrace new people doing new work. It is probable that on the whole new groups or individuals from emerging countries have not yet broken into the circles of trust; probing suggested this, but more research is needed.

The social networks previously described were as much concerned with what was going on in the laboratories (scientists) or the gradual unfolding of ideas or early results from data analysis. There has always been a lot of interaction among researchers both pre-publication and post-publication. Before the web the mode of interaction was e-mail taking over from letters and to some extent telephone calls. This change in modes of communication was particularly mentioned in relation to question one and among older researchers who had lived through the change. Of course there were also meetings at conferences and visits to the laboratories or departments of other researchers. Now social media can add another and more flexible dimension, especially for younger scholars. Social scientists did talk about this type of interaction to some extent. Scientists on the whole did not talk about this sort of interaction. Information about new results or publications might come through twitter from people within the ‘circle of trust’, but also from others. Some UK researchers mentioned the use of journal “alerts” but none of those interviewed in the USA did so. In the social sciences, it was often ideas, rather than information, which were being looked for and they could well come from social media (blogs for example).

As a route to discovery Google (Scholar) was preferred over Web of Science or SCOPUS because it delivered more results. Biomedical researchers sometimes favoured Medline when the context was a purely medical one. There was an implicit distinction between a trusted system and a best source of trustworthy information (alongside less trustworthy information). Once again a strategy of deciding what was and what was not trustworthy based on a careful examination of such features as methodology used properly or not came into play when information needed for a specific purpose was found. In the US the ease of searching did carry with it a concern about missing out on high quality sources – for example social scientists were concerned that they were sacrificing quality for speedy discovery. In the applied social sciences there was a widespread concern that articles with a clear policy agenda might be “written to order” and thus less trustworthy.

A wider range of sources of information were recognised as having become available to researchers. Sometimes they were trusted more than they had been. A number of researchers admitted to using Wikipedia as a starting point and some argued that it had become a more reliable recently. The word “admitted” is used deliberately: there was always a defensive aspect to the assertion. Most social scientists used grey literature, for example reports, and/or historical sources and in this connection in these disciplines Google was recognised as particularly useful, at least as a starting point for finding this information. Resources of all sorts were subject to the same scrutiny that journals received. The fact that a particular journal or another resource was available through the library was not proposed by anyone interviewed as a reason for trusting it.

In some fields, pre-eminently computer science, personal websites were used to host all of a researcher's publications. In these fields researchers look there for publications. Several researchers outside these fields were wondering whether to adopt this approach. In one university where four researchers were interviewed there was pressure from administration to populate personal websites with publications. The researchers were not very interested. There was very little interest in (or knowledge about) institutional repositories.

Established researchers in most disciplines were keen to avoid what they characterised as 'noise' which could mean blogs as well as alerts. They were very confident that their strategies of keeping themselves informed worked well. This did not mean that such individuals had to spend time looking for new knowledge. They did not. The knowledge came to them. Why were mature researchers so confident? They were in touch with their equivalents in different groups at conferences and by e-mail. However if one person in the group followed other relevant researchers on Twitter or using the other mechanisms mentioned, he or she would diffuse what had been learnt or pointed to throughout the group and through other related or trusted groups. How could such sources be trusted? They were trusted because the person sending the message was trusted – this was the main way in which trust came in to the picture. Some established researchers suggested that they might soon start tweeting or reading blogs – if they had the time.

Researchers had to be prompted to bring the role of data into the discussion. The changing role of data which has been a feature of recent literature was not usually raised as a feature of the digital transition. A mature crop scientist from the UK was an exception in raising data during the questions on citation practices:

                              There is no access to raw data in this paper. (However) our funders encourage (such access). Some of the open access journals have very good facilities for providing access to background data. Data access is a good idea. Access to tools used for analysis…is also a good idea. (When) you were critiquing a paper you would normally run a different analysis on the data to show why your analysis was better
                           
                        

However, although access to full content was much more important than access to data when trust was involved, increased access to data was seen as one of those aspects of change that was positive. There was some discussion of where data should be located. One older scientist liked to find all the data he needed within the paper and not have to go to another location. He also was worried that the persistence of links could not be relied upon. This did not represent a widespread concern though one young scientist did raise a similar point and said that he had been trained to make every paper a ‘complete’ record. Another mature scientist pointed out that the way in which the data was created was as important as the data itself. One change, commented upon particularly by a few older scientists was that a lot of different sources of data could be searched whereas in the past researchers only understood how a small number of databanks worked. This does raise trust issues in terms of assessing work in which researchers are not themselves experts. Researchers varied in whether they found this an issue or not and this often depended on their relationship with co-authors and the extent of their research network. Some scientists explained that older hypotheses based on theory only can now be tested on data because there are new techniques for collecting, measuring and viewing data. Thus technology enables increased confidence in testing hypotheses.

As has already been mentioned another output from the overall project (Thornley et al., 2015) provides a quantitative analysis of the responses to the critical incident interview which focussed on 5 references in a recently published paper by the interviewee. The section on citation behaviour in this paper concentrates on those responses which shed light on digital transition and trust behaviour. The reason for focusing especially on citation behaviour in the overall scheme was: (a) it provided a fix on a specific and documented part of researchers’ scholarly communication activities about which people could be questioned; (b) citing behaviour requires the author to choose and select and we could learn a lot about the role trustworthiness played in this.

The top five reasons for choosing/trusting a citation were: (1) the author was known to the researcher; (2) the journal or conference proceedings were known to the researcher; (3) the reference was a classic/seminal work in the field; (4) the reference supported their methodology; (5) the research group/institution was known to the researcher. This clearly shows that personal knowledge is the main way to establish the authority of the source except in cases when it is accepted by the communities concerned as a seminal work. The other reasons for trusting, such as checking methods or citation counts could be seen as things to do when researchers did not know enough about the source; this is indeed what the interviewers were told. We found no evidence of change in citation behaviour as a result of technological change though technology has made available such tools as citation counts in a way which was not possible before.

Almost all papers chosen by interviewees began with a number of older references. This could be to an earlier paper sometimes over fifty years old and sometimes a book or a chapter in the book. One reference would be to the seminal work that started the field (defined it as a separate area of research) and later references would point to where there was a breakthrough creating a new subdivision of a research field. These references were characteristically held in a database and not thought about much. They were not classified in the way they have been classified here by the researchers themselves. They were a given. These contributions were highly trusted even in fast moving fields though in some cases the field was so new that there were no relevant papers older than ten years. Seminal papers were trusted because the results recorded and the theories derived from them had become knowledge on which new and successful research had been built.

Another group of references regularly cited by the researchers interviewed were regularly cited in papers published by the author because they were part of the literature related to the work being discussed in the paper which was known to be trustworthy and part of the stock of knowledge relied upon. Interviewees, especially senior researchers, often considered that they knew all the important people in their field and those associated with them (see Section 3.2.1 above).

A mature legal scholar from the UK stated when questioned about citations:

                              As a general point most of the references in this paper are (to) literature I already knew about and have done for a long time
                           
                        

Works from such groups and individuals were trusted for citation purposes where it related to the research under discussion in the paper. It was not just a matter of recognising an author. The author was in a context – the people he or she worked with. There were also negative views of authors or groups who were distrusted even though to those outside the field they might be regarded as “big names”. Some interviewees felt indeed that “big names” may have their work less rigorously reviewed than other researchers which led to their association with some lower quality publications.

Journals were more heavily cited than other publications even in disciplines where books were also commonly used to deliver research findings. None of those interviewed saw this as a change in behaviour in spite of the fact that in many disciplines books have become less regarded over the decades. The journal name could add credibility to the author. Journals known to have rigorous peer review processes were especially seen as objects of trust. When citing an article it was the reputation of the journal in the field that was more important than ranking by impact factor. Younger academics had learnt the relative reputations of journals in the field from their supervisor but they were sometimes more swayed by the impact factor especially as there was so much pressure to publish in journals with a high impact factor. Mature researchers, who had established their position, could afford to be less concerned about impact factors. A little over half of the researchers interviewed claimed that they always checked, with specific exceptions, a potential citation using their own criteria to establish that it could be trusted. No-one actually said that they did not do this but we might assume that in some cases where there was no explicit assertion of checking there might have been no checking of the reference. Open access journals were less trusted on the whole than other journals but, where the question of open access was explored further, researchers agreed that they would cite peer reviewed open access journals. They came into the category of newer and therefore less established journals articles, which needed special scrutiny.

Conference papers were cited often in disciplines where conference proceedings are an important information source, with engineering and computer science being the most important. Such papers often provided a clearer statement of the concepts and results than in the more rigid and concise form of the paper. Conference proceedings were almost always seen as less authoritative than full academic papers but they can be more useful in some cases. There is recognition by researchers that some conference proceedings are more trustworthy than others: there is a consensus here within the discipline or field. In many disciplines the separate publishing of conference proceedings as books has become less common – they are not seen as an economic proposition by publishers – but this was not mentioned as an example of change by those who cited them.

Even in the sciences books were frequently cited. In the sciences such citations were classic or seminal references. They were trusted for the reasons mentioned in an earlier section (Section 3.2.1). When challenged, the researchers admitted that they had not been looked at never mind read for some time. As one would have expected in the social sciences citing of books was more common. It is of course surprising that in the social sciences the publication invariably offered for consideration to the interviewer was a journal article (short form) rather than the long form of even a book chapter. None of the researchers mentioned why this choice was made.

In the sciences social media were never cited. There have been some widely publicised initiatives to use the blog format in mathematical research (Gowers, 2013). Researchers in relevant fields asked about such initiatives were adamant that this was not the way scholarly communication would or should work in the future: one major source of concern was that the open mode meant that some of those commenting were not recognised specialists in the field. In the social sciences blogs were very rarely cited in the sense that they represented authority like book or journal citations. Such citations only occurred when there was no appropriate reference from a formal channel such as a journal. Blogs or websites were usually seen as “sources” in the sense used by historians and cited in that sense.

It would seem logical that the literature searched at the start of the project is re-used as references to be cited when the time came to write a paper, however, this does not seem to be always the case. As a paper was being written authors searched the literature again not just to find new references to work in the field which may have been published recently but also for references from related fields. These references help “back up” the results being described in that they are reporting similar (maybe the methodology) work in other fields. There is a similar process at work when the researcher feels the need to defend against academic attack. Sometimes the scrutiny of literature being cited is not as careful as would usually be the case. In these circumstances the searching method did not seem to be a big factor in establishing credibility. Most participants agreed that they could find reliable sources anywhere, but a few participants of different ages and disciplines thought the top “hits” in a search were more trustworthy. They were more likely to trust the top “hits” in a database than a search engine. Many researchers started with Google or Google Scholar then switched to a more specialised database such as PubMed Central when they had a more defined search query. In some cases researchers found appropriate databases in an unfamiliar field starting with Google. This was particularly true of social scientists and much less so among scientists. Researchers also used Google to check the credibility of an author.

Researchers have long preferred to cite peer reviewed sources: peer review was regarded as a main indicator of authority, quality and reliability. There was no evidence of any change in the status given to peer review among those interviewed or indeed during the whole project. Researchers were aware of problems such as the time peer review might take, the bias of reviewers and the involvement of academic politics. When such problems were perceived as affecting a particular journal there might be extra scrutiny of the journal concerned. However researchers interviewed did not mention different types of peer review and different ways of conducting it. In the earlier stage in this project (the focus groups) a number of journal editors were involved. Very few journal editors were interviewed and none of them discussed their own practices. There was also no reference to the processes which PLOS One has adopted, where the methodology only and not the usefulness of the article are considered, even though this journal was the only open access publication mentioned by name and in a positive way. Trust in peer review did not depend on scrutiny but, as we have seen, scrutiny was normal in most cases where no other evidence of trustworthiness was involved.

Researchers were adamant that in most cases they “read” any paper than was cited by them. The term “read” was used by researcher but it is likely on the evidence of practices given the term “scanned” would have been more appropriate. With time and experience the researchers claimed that they had become more confident in their judgement and citing decisions. They became familiar with the top journals and authors, and learned who was known for producing quality work. Interviewees thought even if source was unfamiliar, they should still be able to understand the methodology to judge its reliability. If they saw a mistake in the mathematics they would not trust the material. When the research was cross disciplinary, they would often ask colleagues at their university or from within their circle of trust. People often started with abstract, then the methods, then the major figures, and then they would read the entire article. Abstracts were very important tools to determine the article's reliability. If the abstract was poor in the sense that it did not clearly explain the contents of the article, the researcher would be likely not to cite the article unless they had other good reasons for doing so. Researchers rarely looked at the dataset (if available). Researchers did however look at a source's bibliography to establish trust. They looked for the citations they expect to see (e.g. top authors, seminal works, and key journals). If key references were missing, they would be on-guard. If the participant was not familiar with an author, they would “Google” the author for previous work, university affiliations, and other credentials.

This section deals with the relationship of trust to the decision on where to publish and whether there have been any changes in practice.

All researchers knew where they had to get published in order to obtain tenure or, in the case of UK, do well in the Research Excellence Framework (http://www.ref.ac.uk/). It was necessary to publish in top peer reviewed journals ranked by impact factor. Attitudes were much the same on both sides of the Atlantic, but at the time when the interviews were conducted the pressure on UK researchers to do well in the REF was much more evident and specific. There was overwhelming agreement that external pressure to publish in highly ranked journals (again ranked by impact factors) had grown in recent years and implicitly that this pressure interfered with the free exercise of their deployment of trust criteria. The perception among those interviewed in the UK was that the importance of the journal was defined by impact factor not, as they preferred, the prestige of the journal in the discipline or the nature of the audience. This belief among researchers has existed during the various assessment schemes devised by the UK government and no amount of denials has dented it – or so it would appear in these interviews when the question came up. The fact that there was strong external pressure on scholars to publish in journals which were understood to reflect external judgements was understood as part of a system which had to be accepted. Social scientists on the whole were less happy with the system than scientists. Happiness with the system did not depend on the age of the researcher. A small number of researchers (all social scientists) hoped that the system might become more flexible with greater recognition of monographs or blog postings.

The top publishers were all known and in a way respected, but (especially in the social sciences) there was significant hostility or at best ambivalence to the role of publishers, when their role came into the discussion. There were in this situation doubts expressed both about the added value they were providing and the cost of the system they ran. One publisher was perceived as pricing their journals too high – “ripping off” was the word used by one social scientist – but that same publisher was name-checked as the publisher of a lot of very important and well-run journals in a number of fields. Two researchers preferred to publish in journals owned by “their” learned society but sometimes another journal was more appropriate. Researchers did not however have any vision of how the system might change and very few had decided to eschew publishing in commercial journals even those who were closely involved in open access journals.

There were very mixed views about the academic value of open access. In general it was regarded with suspicion as a vehicle for communication. Some were worried about a trade-off between increased speed to publication and a lowering of quality. This concern existed in spite of the fact that in general open access was seen as a good thing for the developing world and for outreach. There was a resistance to the idea of paying to publish. However, on those occasions (3) where previously hostile researchers had opted for publication in an open access (in one case because of a co-author's choice) they were pleased with quality of the peer review process and much more positive about open access journals in general. It was surprising that more academics did not appreciate the high impact factors of some open access journals – perhaps because these journals were mainly in biomedicine and certainly not common in the social sciences. Lots of researchers however did know PLOS One. They knew about its impact factor. They knew that it processed papers quickly. Very few researchers saw traditional journals being supplanted by other modes of dissemination. Likewise very few expected a totally open access environment to emerge where all research outputs would be available free to all. Where the issue was discussed researchers seemed to assume a mixed economy, where open access journals co-existed with subscription or toll access journals.

The impact factor did not come up as much as might be expected except in the context already mentioned (see Section 3.3.1). Where external pressure was not invoked their choice was governed by other considerations. Younger researchers did in some cases put forward the view that high impact factor journals are likely to contain papers that had been properly peer reviewed but mature researchers were less impressed. It was the quality of peer review that counted. As we have seen there was a tacit hierarchy of journals in a discipline which governed a lot of decisions depending on what level of journal was deemed appropriate (by an experienced researcher) for the level of likely interest in the paper. The relationship of the interviewee to journals either as an editorial board member or indirectly because trusted colleagues were editors did not get mentioned as much as might have been expected: one suspects that it was assumed as a positive when choices had to be made.

Researchers were sure there would be changes. Two researchers mentioned the possibilities created by linked data but there was in general not much speculation or offerings of exciting new models.

There was however interest in outreach and in the use of social media to reach out to the public and to practitioners. In the UK this was partly due to the 20% of REF scores allocated to “impact” but in the US too there was a lot of interest in an engagement agenda. It was not entirely due to policy pressure. Researchers in applied areas were committed to “translation”, to making their work comprehensible and relevant to the practitioner community they worked with. Even researchers who had very little interest in the social media did see an increased role for social media in outreach but complementary to journals rather than replacing them.

Much of the widespread discussion in some scholarly circles about open science (open laboratory notebooks for example) did not seem to have impacted on any of the scientists interviewed. Some (7) researchers however expressed concern about sharing data – an important part of the open agenda – it was as much among younger researchers as older ones. Even fewer (all social scientists) were not happy about sharing ideas too soon: the instant exposure possible with social media worried some. Where these questions were raised, it was sometimes linked to a generalised dislike (more often mentioned) about being told what to do by administration or funders: this was resented. The general principle when such matters were discussed was that ideas should only be shared pre-publication with trusted individuals as there had been cases when ideas had been used and not attributed in their publications by other people outside their circles.

@&#DISCUSSION@&#

So how has the digital transition and some of its important manifestations, notably open access and social media, impacted on trustworthiness in scholarly communications and how do our findings relate to those of other researchers?

The question asked at the very outset was whether the digital transition had resulted in changes in researcher behaviour. The overwhelming response was that yes it had. The word most commonly used was “easier”: it was easier to discover and it was easier to disseminate. Usually mention of discovery came first and there were almost always positive references to the move from print to online in all disciplines. There was an equation of dissemination (scholarly communication) with publishing but at the same time there was a distinction between scholarly communication and outreach in its broadest sense. The fact that there were now more publishing outlets was rarely mentioned except in general terms. In spite of these changes there was no change in how trust was exercised. Journals read and journals submitted to were selected in the same way as they always had been. Metrics were less important than experience and personal recommendation from those known to the respondent. The quality of peer review was always mentioned as of the greatest importance.

Nevertheless there was a greater emphasis on the need to make up one's own mind, to use one's own judgement about what could be trusted. There was much emphasis on experience enabling a good choice of journals to submit to but if the journal was not known or recommended there was a need to look at the website for assurance about mechanisms of peer review and what academics were associated with it. However the most significant example of a lack of trust in external indicators came in responses to questions about citations. Most researchers insisted that they almost always felt it necessary to read any source they cited. One can assume that reading can be understood as scanning. Nevertheless this was an example of researchers relying on their own judgement and it is not surprising that older researchers were more confident in their judgement. Some of those interviewed expressly stated that they had changed their behaviour and that they were less trusting than they had been. An implication of this way of working was the researchers' need to have access to full text (this was mentioned) but this need did not demand a spread of open access: most but not all found it easy to supplement library access by other means (see Section 3.1 above) of acquiring what they wanted.

Established researchers, whose experience enabled them to compare the present situation to the first research experience they had as a research student, sometimes characterised the change in practice by reference to visits to the library in those early days.

With the exception of a tiny minority of interviewees (2), this was the only time the library was mentioned and it was not seen as responsible for the easy access which was regularly mentioned.

Finally, there was hardly any talk about transforming scholarly communication, so often mentioned in the discussions among intermediaries such as publishers and librarians. Social scientists were in some cases less accepting of the continuance of the traditional structure of journal publishing than scientists.

Publishing open access is a model which has become possible because of the digital revolution. In principle open access was always welcomed by interviewees because it enabled greater access but researchers rarely mentioned the model or the philosophy behind the model even when they complained about the costs of the subscription model and about the behaviour of traditional publishers “ripping off” the research community. A small number of researchers preferred to publish in open access journals, for example those involved in developmental studies (2) or parasitology (1) but each of these three admitted to also publishing in top journals financed by subscription.

What was surprising was the number of researchers, the majority, who were suspicious of open access journals. They were often distrusted. This was almost entirely due to the experience of the so-called “predatory” journals. A small number (3) considered that paying to publish inevitably led to a distortion of the peer review process: because you were paying there was a presumption of a lower quality. When it came to a question of a specific article from such a journal (was it worth reading or might one cite it?) researchers adopted the same criteria that they used when confronting any journal they did not know. Quality of peer review was a trust touchstone. It has to be remembered that most of the interviewees worked in disciplines where there were few if any open access journals and certainly not established ones with impact factors. As has already been mentioned, the one open access journal which was often mentioned was PLOS One. Interviewees knew papers in that journal were well cited. It can be generalised that experience of publishing in or reading articles in open access journals will overcome the instinctive prejudice against new journals and the tendency to be suspicious of any publication not well known to trusted colleagues.

Our finding that, despite doubts about open access journals, they were seen as a good thing for the developing world and for outreach has much support in the literature (Björk, 2012; Nicholas & Rowlands, 2005; Schroter & Tite, 2006; Schroter, Tite, & Smith, 2005).

Almost all researchers made a clear distinction between formal and informal methods of communication with social media in the latter grouping and journals very much in the former. This was implicit. The terms were not used. Once again trustworthiness depended on whether or not the form of communication was peer reviewed. However in many disciplines it was accepted that ideas (always in short supply) could well be gained from the social media. They could be expressed in blogs. They could be conveyed in tweets or communicated on networking sites such as ResearchGate.

Again informal communication was trusted depending on who was doing the communicating. Were they part of the circle of trust within which researchers worked? Were they part of the wider circle of researchers known about? The medium did not matter much if they were. As a generalisation however scientists did not cite knowledge transmitted informally as building bricks for their research conclusions and in many disciplines blogs were not cited. As a generalisation social scientists were often looking for a different sort of information expressed in terms of new ideas and also “sources” of evidence. For social scientists ideas could come from anywhere and then tested. They used social media more in this way but they varied among themselves as to whether they would cite social media.

Social media did come into its own when the interviewees discussed outreach. Almost all those responding recognised the utility of social media as a way of reaching a wider audience. Some also did see it as a way of getting information out to a wider group of your own peers. They were almost all social scientists. The reasons could be promotional.

The finding that social media was used by increasingly more researchers to keep in touch are consistent with the results of two major studies, one commissioned by the Research Information Network (RIN, 2010) to investigate how researchers perceived and used web 2.0, and the other undertaken by CIBER (Nicholas & Rowlands, 2011; Rowlands, Nicholas, Russell, Canty, & Watkinson, 2011) to explore how and where social media supported or hindered the research lifecycle.

Even if many established researchers do not tweet or read blogs most of them believe there is a widespread belief that social media have a future in scholarly communications (Maynard & O'Brien, 2010; Ponte & Simon, 2011; 
                        RIN, 2010).

Many researchers made the point that they themselves did not engage with social media. There was too much “noise”. They did not have time to engage in activities where those outside their expert fields might present ill-informed views and get in the way of proper scholarly interaction. Nevertheless there was frequently mention of a member of the research group (usually categorised as younger) who used social media to highlight the work of the group not only to the wider audience but also to their peers.

Few younger researchers (9) presented themselves for interview, but among established researchers there were frequently mentions of the way in which younger researchers worked. In their attitudes to trust there was no single common way of thinking among this group except insofar as they all recognised that they were learners. Established researchers, particularly older ones, mentored younger doctoral candidates and post-doctoral students. A few interviewees mentioned that they had to teach their juniors what resources to trust and what potential vehicles for dissemination to trust. It could be argued that younger researchers needed more help than they might have need in the past because there were more sources of information and not all were trustworthy – but this seemed to be an implicit rather than an explicit assumption. One of the problems presented by “predatory” open access journals was that younger researchers did not always recognise these journals for what they were. Younger researchers were also perceived by their elders as more knowledgeable about changes brought about by the digital revolution especially the opportunities present by social media.

With less than one-hundred people interviewed it is only possible to suggest places to look for diversity. Young researchers concurred with the perception of their strengths and weaknesses as digital natives or at least those who had always lived within a digital environment as researchers. However when they discussed what they cited and where they might publish they tended if anything to be more conservative than their elders. They drew a distinction between formal and informal communication. They recognised that they had to publish in journals known for their quality if they were to get recognition and tenure. On the whole however, with a few exceptions in the social sciences, they accepted the traditional order of things as the way you do research. They did not yearn for a more open environment. Because they did not have as much knowledge about, for example, people in their field who were trustworthy they tended lean more on external criteria such as impact factors than their elders. They saw themselves as apprentices not as transforming scholarship.

Several researchers, although presenting as social scientists, had an academic background in the humanities which influenced their practices; and there were significant differences in what sources they used and what they cited. There were some outlying disciplines in the sciences, specifically computer science and mathematical physics, where some trust behaviours were different. In general however there was a remarkable similarity in attitudes and behaviours across both the sciences and the social sciences

In respect to country and subject there were, surprisingly, very few differences. The attitudes of the researchers in the USA seemed to be remarkably similar to those in the UK. There also does seem now to be a standard way of scholarly communication with the journal even more important channel than it was in the past. The widespread adoption of a particular format of the journal and the journal article (derived from the sciences) appears to be associated with standardised behaviour. What is so surprising also is the fact that most researchers interviewed (see Section 3.1) saw change as coming though they were uncertain what form it would take.

@&#CONCLUSIONS@&#

We sought to determine whether the way academic researchers assign and judge trustworthiness to the sources/channels they use, cite and where they publish has changed as a result of the digital revolution. Our main findings can be summarised as:

                        
                           (a)
                           There were no substantial changes. ‘Publications’ were always perceived as journal articles by the researchers interviewed. The main impact of the digital revolution can be summarised as making life easier for researchers. Online content can now be accessed from the desktop and libraries were rarely visited. The only significant change in decisions about use was a greater confidence among researchers in the US and the UK in using personal judgement to decide on whether a particular journal or other information source could be trusted rather than using a proxy such as impact factors. Reliance on the judgement of other researchers known to the interviewees remained a central litmus test when using, citing and deciding where to publish.

Open access journals have been made possible by the digital revolution. Almost all researchers considered the concept of open access as a positive development but they had doubts about the quality of open access journals. This was partly because the journals were relatively new and not well known to the circles of trust within which researchers worked. These new journals had yet to establish themselves as part of the list of trusted journals all established researchers had in mind. However the so-called “predatory” open access journals had led many researchers to a general suspicion which led to careful inspection in using, citing and seeking vehicles for publication. There was evidence that experience of open access journals, particularly publishing in open access journals, led to acceptance. There was a surprising lack of use of social media as a means of communication between researchers and as a channel for research information. Established researchers claimed they did not have time to tweet.

During the research it became clear that it was important to ascertain not only how a researcher decided what to trust, but also to understand the context in which the researcher worked – the researcher's own research group and the other groups trusted by that group either as collaborators or as sources of trusted information. Rather surprisingly established researchers tended to come forward to be interviewed rather more than early career researchers or those with a foot on the ladder, but not yet reaching the top rung. This did enable interviewers to get a wider picture of such features of academic life as attendance at conferences (not being reduced in spite of the growth in digital communication) where initial trust decisions about people were made.

This study was funded by the Alfred P. Sloan Foundation (grant no. 2012-6-21).

Critical Incident Interviews

                        
                           1
                           Over the course of your career have you observed any changes in citing and publishing behaviour, either personally or which you have observed?


                     Questions 2–4 focus on the sources in the participant's last published publication (Critical incident).
                     
                        
                           2
                           We have randomly selected five of your citations, and we would like to go through each of them and discuss the reasons for choosing it.

Prompting questions include:

                        
                           •
                           What made you include this citation?

We are not questioning the quality or trustworthiness of your sources, and we believe your sources are “good”, but we would like to know what made you consider this a reliable (authoritativeness, expert/competent nature, trustworthiness, reputation/brand) source?

Do you remember how you found it?

Was the source you searched (e.g. Web of Science, Google) a factor of trustworthiness?

Now we would like you to select two of your citations, and discuss the reasons for choosing it.

Prompting questions include:

                        
                           •
                           What made you include this citation?

We are not questioning the quality or trustworthiness of your sources, and we believe your sources are “good”, but we would like to know what made you consider this a reliable (authoritativeness, expert/competent nature, trustworthiness, reputation/brand) source?
                           

Do you remember how you found it?

Was the source you searched (e.g. Web of Science, Google) a factor of trustworthiness?

Did you share or consider sharing any of this content in other ways (e.g., blog, tweet, conference presentation, pre-print)?

Why/why not?

If so, why did you choose that particular format?

If so, did you have a choice (e.g. mandatory request by funders, pressure from project leader)?

Did you include links to data in your publication? Why?

How do you view the use of social media in scholarly communication?

Is there a link between formal and informal publishing (e.g. tweets referring to papers)

@&#REFERENCES@&#

