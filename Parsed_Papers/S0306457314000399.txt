@&#MAIN-TITLE@&#Supervised sentiment analysis in Czech social media

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We explore state-of-the-art supervised machine learning methods for sentiment analysis of Czech social media.


                        
                        
                           
                           We provide a large human-annotated Czech social media corpus.


                        
                        
                           
                           We explore different pre-processing techniques and employ various features and classifiers.


                        
                        
                           
                           We experiment with five different feature selection algorithms.


                        
                        
                           
                           Results are also reported on other widely popular domains, such as movie and product reviews.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sentiment analysis

Czech language

Social media

Machine learning

Feature selection

@&#ABSTRACT@&#


               
               
                  This article describes in-depth research on machine learning methods for sentiment analysis of Czech social media. Whereas in English, Chinese, or Spanish this field has a long history and evaluation datasets for various domains are widely available, in the case of the Czech language no systematic research has yet been conducted. We tackle this issue and establish a common ground for further research by providing a large human-annotated Czech social media corpus. Furthermore, we evaluate state-of-the-art supervised machine learning methods for sentiment analysis. We explore different pre-processing techniques and employ various features and classifiers. We also experiment with five different feature selection algorithms and investigate the influence of named entity recognition and preprocessing on sentiment classification performance. Moreover, in addition to our newly created social media dataset, we also report results for other popular domains, such as movie and product reviews. We believe that this article will not only extend the current sentiment analysis research to another family of languages, but will also encourage competition, potentially leading to the production of high-end commercial solutions.
               
            

@&#INTRODUCTION@&#

Sentiment analysis has become a mainstream research field since the early 2000s. Its impact can be seen in many practical applications, ranging from analyzing product reviews (Stepanov & Riccardi, 2011) to predicting sales and stock markets using social media monitoring (Yu, Wu, Chang, & Chu, 2013). The users’ opinions are mostly extracted either on a certain polarity scale, or binary (positive, negative); various levels of granularity are also taken into account, e.g., document-level, sentence-level, or aspect-based sentiment (Hajmohammadi, Ibrahim, & Othman, 2012).

Most of the research in automatic sentiment analysis of social media has been performed in English and Chinese, as shown by several recent surveys, i.e., (Liu & Zhang, 2012; Tsytsarau & Palpanas, 2012). In Czech, there have been very few attempts, although the importance of sentiment analysis of social media became apparent, for example, during the recent presidential elections.
                        1
                        
                           http://www.mediaguru.cz/2013/01/analyza-facebook-rozhodne-o-volbe-prezidenta/ [in Czech].
                     
                     
                        1
                      Many Czech companies also discovered a huge potential in social media marketing and started launching campaigns, contests, and even customer support on Facebook—the dominant social network of the Czech online community with approximately 3.6 million users.
                        2
                        
                           http://www.m-journal.cz/cs/jaky-je-skutecny-pocet-ceskych-uzivatelu-facebooku__s288x9161.html [in Czech].
                     
                     
                        2
                      One aspect still eludes many of them: automatic analysis of customer sentiment of products, services, or even a brand or a company name. In many cases, sentiment is still labeled manually, according to one of the leading Czech companies for social media monitoring.

Automatic sentiment analysis in the Czech environment has not yet been thoroughly targeted by the research community. Therefore it is necessary to create a publicly available labeled dataset as well as to evaluate the current state of the art for two reasons. First, many NLP methods must deal with high flection and rich syntax when processing the Czech language. Dealing with these issues may lead to novel approaches to sentiment analysis as well. Second, freely accessible and well-documented datasets, as known from many shared NLP tasks, may stimulate competition, which usually leads to the production of cutting-edge solutions.
                        3
                        E.g., named entity recognition based on Conditional Random Fields emerged from CoNLL-2003 named entity recognition shared task.
                     
                     
                        3
                     
                  

This article focuses on document-level
                        4
                        Or post-level, as documents correspond to posts in social media.
                     
                     
                        4
                      sentiment analysis performed on three different Czech datasets using supervised machine learning. For the first dataset, we created a Facebook corpus consisting of 10,000 posts. The dataset was manually labeled by two annotators. The other two datasets come from online databases of movie and product reviews, whose sentiment labels were derived from the accompanying star ratings from users of the databases. We provide all these labeled datasets under Creative Commons BY-NC-SA licence
                        5
                        
                           http://creativecommons.org/licenses/by-nc-sa/3.0/.
                     
                     
                        5
                      at http://liks.fav.zcu.cz/sentiment.

The rest of this article is organized as follows. Section 2 examines the related work with a focus on Czech research and social media. Section 3 thoroughly describes the datasets and the annotation process. In Section 4, we list the employed features and describe our approach to classification. Section 5 contains the results and provides a thorough discussion. Finally, Section 5.3 explores the influence of feature selection methods.

@&#RELATED WORK@&#

There are two basic approaches to sentiment analysis: dictionary-based and machine learning-based. Whereas dictionary-based methods usually depend on a sentiment dictionary (or a polarity lexicon) and a set of handcrafted rules (Taboada, Brooke, Tofiloski, Voll, & Stede, 2011), machine learning-based methods require labeled training data that are later represented as features and fed into a classifier. Recent attempts have also investigated semi-supervised methods that incorporate auxiliary unlabeled data (Zhang, Si, & Rego, 2012).

The key point of using machine learning for sentiment analysis lies in engineering a representative set of features. Pang, Lee, and Vaithyanathan (2002) experimented with unigrams (presence of a certain word, frequencies of words), bigrams, part-of-speech (POS) tags, and adjectives on a movie review dataset. Martineau and Finin (2009) tested various weighting schemes for unigrams based on the TFIDF model (Manning, Raghavan, & Schütze, 2008) and proposed delta weighting for a binary scenario (positive, negative). Their approach was later extended by Paltoglou and Thelwall (2010) who proposed further improvements in delta TFIDF weighting.

The focus of current sentiment analysis research is shifting towards social media, mainly targeting Twitter (Kouloumpis, Wilson, & Moore, 2011; Pak & Paroubek, 2010) and Facebook (Go et al., 2009; Ahkter & Soria, 2010; Zhang et al., 2011; López, Tejada, & Thelwall, 2012). Analyzing media with a very informal language benefits from involving novel features, such as emoticons (Pak & Paroubek, 2010; Montejo-Ráez, Martínez-Cámara, Martín-Valdivia, & Ureña López, 2012), character n-grams (Blamey, Crick, & Oatley, 2012), POS and POS ratio (Ahkter & Soria, 2010; Kouloumpis et al., 2011), or word shape (Go et al., 2009; Agarwal, Xie, Vovsha, Rambow, & Passonneau, 2011).

In many cases, the gold data for training and testing the classifiers are created semi-automatically (Kouloumpis et al., 2011; Go et al., 2009; Pak & Paroubek, 2010). In the first step, random samples from a large dataset are drawn according to the presence of emoticons (usually positive and negative) and are then filtered manually. Although large high-quality collections can be created very quickly with this approach, it makes a strong assumption that every positive or negative post must contain an emoticon.


                        Balahur and Tanev (2012) performed experiments with Twitter posts as part of the CLEF 2012 RepLab.
                           6
                           
                              http://www.limosine-project.eu/events/replab2012.
                        
                        
                           6
                         They classified English and Spanish tweets with a small but precise lexicon, which also contained slang, combined with a set of rules that captured the manner in which sentiment is expressed in social media.

Finally, we would like to direct the reader to an in-depth survey by Tsytsarau and Palpanas (2012) for actual results obtained from the above-mentioned methods.

The basic reason for using feature selection (or reduction) methods for supervised sentiment analysis is twofold: first, the reduced feature set decreases the computing demands for the classifier, and, second, removing irrelevant features can lead to better classification accuracy. Furthermore, noise and redundancy in the feature space increase the likelihood of overfitting (Abbasi, France, Zhang, & Chen, 2011).

A study by Sharma and Dey (2012) compares five methods for feature selection, namely Information Gain, Chi Square, Gain Ratio, Relief-F, and Document Frequency, together with seven different classifiers. Results are reported on the widely-used movie review database from Pang et al. (2002). The best performance was achieved by using the SVM classifier and the Gain Ratio selector with the number of features ranging from 2000 to 8000 and employing only unigrams as features sorted by their frequency.


                        Abbasi, Chen, and Salem (2008) proposed an entropy-weighted genetic algorithm that combines Information Gain with a genetic algorithm for selecting features in a bootstrapping manner, tuned on held-out data. They performed document-level binary sentiment of English and Arabic and used SVM as the main classifier. Their results were superior to other approaches, such as plain SVM or Information Gain selection. In their later work, Abbasi et al. (2011) proposed another feature selection method called the Feature Relation Network. This manually constructed network of feature dependencies (e.g., subsumption
                           7
                           
                              ‘is-a’ hierarchical relation.
                        
                        
                           7
                         or parallel relations of various n-grams) relies on SentiWordNet in order to assign the final feature weights.

One of the classical papers on feature selection for text classification by Forman (2003) proposes a metric called Bi-Normal Separation and provides an extensive comparison with another twelve existing feature selection methods. Using SVM as the underlying classifier, the proposed method yields the best results and is suitable for skewed (imbalanced) classes. Other examples of feature selection methods for sentiment analysis or text classification can be found in, e.g., (Chen, Huang, Tian, & Qu, 2009; Aghdam, Ghasem-Aghaee, & Basiri, 2009).

Since feature selection is also important outside the domain of text classification, Wasikowski and Chen (2010) conducted a systematic study, focusing on dealing with class imbalance on small samples. They compare seven selection methods on 11 small datasets with highly skewed classes and conclude by recommending two best-performing algorithms, especially for scenarios that require a small number of features. Another approach based on dynamic mutual information is presented in (Liu, Sun, Liu, & Zhang, 2009). Again, the experiments are conducted on 16 benchmark datasets with a rather small size (up to 8124 instances only) and a small number of features (from 18 to 279), which is a fundamentally different scenario from machine learning-based sentiment analysis.

Feature selection, however, does not have to lead to a better performance in all cases, as reported e.g. by Boiy and Moens (2009), who report Chi-square selection results in their preliminary tests without any success.


                        Veselovská and Šindlerová (2012) presented an initial research on Czech sentiment analysis. They created a corpus which contains polarity categories of 410 news sentences. They used the Naive Bayes classifier and a classifier based on a lexicon generated from annotated data. The corpus is not publicly available, and because of its small size no strong conclusions can be drawn.


                        Steinberger et al. (2012) proposed a semi-automatic “triangulation” approach to creating sentiment dictionaries in many languages, including Czech. They first produced high-level gold-standard sentiment dictionaries for two languages and then translated them automatically into a third language by means of a state-of-the-art machine translation service. Finally, the resulting sentiment dictionaries were merged using the overlap of the two automatic translations.

A multilingual parallel news corpus annotated with opinions on entities was presented in (Steinberger, Lenkova, Kabadjov, Steinberger, & der Goot, 2011). Sentiment annotations were projected from one language to several others, which saved annotation time and guaranteed comparability of opinion mining evaluation results across languages. The corpus contains 1274 news sentences where an entity (the target of the sentiment analysis) occurs. It contains seven languages including Czech. The research targets fundamentally different objectives from our research as it focuses on news media and aspect-based sentiment analysis.

Recent experiments with incorporating word clusters as additional features to tackle the issue of the high flection of Czech were successful, especially in the Czech movie review domain (Habernal & Brychcín, 2013). The clusters were obtained from semantic spaces created on unlabeled external data. Further improvement of the overall classification performance was achieved by exploiting the global target of the analyzed reviews using Gibbs sampling (Brychcín & Habernal, 2013).

The initial selection of Facebook brand pages for our dataset was based on the ‘top’ Czech pages, according to the statistics from SocialBakers.
                           8
                           
                              http://www.socialbakers.com/facebook-pages/brands/czech-republic/.
                        
                        
                           8
                         We focused on pages with a large Czech fan base and a sufficient number of Czech posts. Using Facebook Graph API and Java Language Detector
                           9
                           
                              http://code.google.com/p/jlangdetect/.
                        
                        
                           9
                         we acquired 10,000 random posts in the Czech language from nine different Facebook pages. The posts were then completely anonymized as we kept only their textual contents.

Sentiment analysis of posts at Facebook brand pages usually serves as marketing feedback on user opinions about brands, services, products, or current campaigns. Thus we consider the sentiment target to be the given product, brand, etc. Typically, users’ complaints constitute negative sentiment, whereas joy or happiness about the brand is treated as positive. We also added another class called bipolar which represents both positive and negative sentiment in one post.
                           10
                           For example “to bylo moc dobry ,fakt jsem se nadlabla :-D skoda ze uz neni v nabidce”—“It was very tasty, I really stuffed myself :-D sad it’s not on the menu anymore”.
                        
                        
                           10
                         In some cases, the user’s opinion, although positive, does not relate to the given page.
                           11
                           Certain campaigns ask the fans to, e.g., write a poem—these posts are mostly positive (or funny, at least) but are irrelevant in terms of the desired task.
                        
                        
                           11
                         Therefore the sentiment is treated as neutral in these cases, in accordance with our above-mentioned assumption.

The complete 10k dataset was independently annotated by two annotators. The inter-annotator agreement (Cohen’s 
                           
                              κ
                           
                        ) reached 0.66 which represents a substantial agreement level (Pustejovsky & Stubbs, 2013), and therefore the task can be considered as well-defined.

The gold data were based on the agreement of the two annotators. They disagreed in 2216 cases. To solve these conflicts, we involved a third super-annotator to assign the final sentiment label. Even after the third annotator’s labeling, however, there was still no agreement for 308 labels. These cases were later solved by a fourth annotator. We discovered that most of these conflicting cases were classified as either neutral or bipolar. These posts were often difficult to label because the author used irony, sarcasm or the context of previous posts. These issues remain open.

The Facebook dataset contains 2587 positive, 5174 neutral, 1991 negative, and 248 bipolar posts, respectively. We ignore the bipolar class later in all experiments. The sentiment distribution among the source pages is shown in Fig. 1
                        . The statistics reveal negative opinions towards cell phone operators (www.facebook.com/o2cz, www.facebook.com/TmobileCz, and www.facebook.com/vodafoneCZ) and positive opinions towards, e.g., perfume sellers (www.facebook.com/Xparfemy.cz) and Prague Zoo (www.facebook.com/zoopraha).

Movie reviews as a corpus for sentiment analysis have been used in research since the pioneering research conducted by Pang et al. (2002). Therefore we covered the same domain in our experiments as well. We downloaded 91,381 movie reviews from the Czech Movie Database
                           12
                           
                              http://www.csfd.cz/.
                        
                        
                           12
                         and split them into three categories according to their star rating (0–2 stars as negative, 3–4 stars as neutral, 5–6 stars as positive). The dataset contains 30,897 positive, 30,768 neutral, and 29,716 negative reviews, respectively.

Another very popular domain for sentiment analysis deals with product reviews (Hu & Liu, 2004). We scraped all user reviews from a large Czech e-shop Mall.cz
                           13
                           
                              http://www.mall.cz.
                        
                        
                           13
                         which offers a wide range of products. The product reviews are accompanied by star ratings on a scale of zero to five. We took a different strategy for assigning sentiment labels. Whereas in the movie dataset the distribution of stars was rather uniform, in the product review domain the ratings were skewed towards the higher values. After a manual inspection we discovered that four-star ratings mostly correspond to neutral opinions and three or fewer stars denote mostly negative comments. Thus we split the dataset into three categories according to this observation. The final dataset consists of 145,307 posts (102,977 positive, 31,943 neutral, and 10,387 negative).

As pointed out by Laboreiro, Sarmento, Teixeira, and Oliveira (2010), tokenization significantly affects sentiment analysis, especially in the case of social media. Although Ark-tweet-nlp tool (Gimpel et al., 2011) was developed and tested in English, it yields satisfactory results in Czech as well, according to our initial experiments on the Facebook corpus. Its significant feature is proper handling of emoticons and other special character sequences that are typical of social media. Furthermore, we remove stopwords using the stopword list from Apache Lucene project.
                           14
                           
                              http://lucene.apache.org/core/.
                        
                        
                           14
                        
                     

In many NLP applications, a very popular preprocessing technique is stemming. We tested the Czech light stemmer (Dolamic & Savoy, 2009) and High Precision Stemmer.
                           15
                           Publication pending; please visit http://liks.fav.zcu.cz/HPS/.
                        
                        
                           15
                         Another widely-used method for reducing the vocabulary size, and thus the feature space, is lemmatization. For the Czech language the only currently available lemmatizer is shipped with the Prague Dependency Treebank (PDT) toolkit (Hajic et al., 2006). We, however, used our in-house Java HMM-based implementation with the PDT training data as we needed better control over each preprocessing step. Kanis and Skorkovská (2010) experimented with a lemmatizer based on Ispell. Following their work, we developed an in-house lemmatizer using rules and dictionaries from the OpenOffice suite.

Part-of-speech tagging was done with our in-house Java solution that utilizes Prague Dependency Treebank (PDT) data as well. Since, however, PDT is trained on news corpora, we doubt it is suitable for tagging social media that are written in very informal language (see, e.g., (Gimpel et al., 2011) where similar issues were tackled in English).

Since the Facebook dataset contains a huge number of grammar mistakes and misspellings (typically ’i/y’, ‘
                           
                              
                                 
                                    e
                                 
                                 
                                    ˇ
                                 
                              
                           
                        /je/ie’, and others), we incorporated phonetic transcription to the International Phonetic Alphabet (IPA) in order to reduce the effect of these mistakes. We relied on eSpeak
                           16
                           
                              http://espeak.sourceforge.net.
                        
                        
                           16
                         implementation. Another preprocessing step might involve removing diacritics, as many Czech users type only unaccented characters. Posts without diacritics, however, represent only about 8% of our datasets, and thus we decided to keep diacritics intact.

We were also interested in whether named entities (e.g., product names, brands, places, etc.) carry sentiment and how their presence influences classification accuracy. For these experiments, we employ a CRF-based named entity recognizer (Konkol & Konopík, 2013) and replace the words identified as entities with their respective entity type (e.g., McDonald’s becomes company). This preprocessing has not been widely discussed in the literature devoted to document-level sentiment analysis, but Boiy and Moens (2009), for example, remove the ‘entity of interest’ in their approach.

The complete preprocessing diagram and its variants is depicted in Table 1
                        . Overall, there are 16 possible preprocessing ‘pipe’ configurations.

We use presence of unigrams and bigrams as binary features. The feature space is pruned by minimum n-gram occurrence empirically set to five. Note that this is the baseline feature in most of the related work.

Similarly to the word n-gram features, we added character n-gram features, as proposed by, e.g., (Blamey et al., 2012). We set the minimum occurrence of a particular character n-gram to five, in order to prune the feature space. Our feature set contains 3-grams to 6-grams.

Direct usage of part-of-speech n-grams that cover sentiment patterns has not shown any significant improvement in the related work. Still, POS tags provide certain characteristics of a particular post. We implemented various POS features that include, e.g., the number of nouns, verbs, and adjectives (Ahkter & Soria, 2010), the ratio of nouns to adjectives and verbs to adverbs (Kouloumpis et al., 2011), and the number of negative verbs obtained from POS tags.

We adapted the two lists of emoticons that were considered as positive and negative from (Montejo-Ráez et al., 2012). The feature captures the number of occurrences of each class of emoticons within the text.

Although simple binary word features (presence of a certain word) achieve a surprisingly good performance, they have been surpassed by various TFIDF-based weightings, such as Delta TFIDF (Martineau & Finin, 2009), and Delta BM25 TFIDF (Paltoglou & Thelwall, 2010). Delta-TFIDF still uses traditional TFIDF word weighting but treats positive and negative documents differently. All the existing related works which use this kind of feature, however, deal only with binary decisions (positive/negative), and thus we filtered out neutral documents from the datasets.
                              17
                              Opposite of leave-one-out cross-validation in (Paltoglou & Thelwall, 2010), we still use 10-fold cross-validation in all experiments.
                           
                           
                              17
                            We implemented the most promising weighting schemes from (Paltoglou & Thelwall, 2010), namely Augmented TF, LogAve TF, BM25 TF, Delta Smoothed IDF, Delta Prob. IDF, Delta Smoothed Prob. IDF, and Delta BM25 IDF.

Feature selection methods assign a certain weight to each feature, depending on its significance (discriminative power) for each class. After the weights are obtained, the top k features can be kept for the classifier, or the features with low weight can be cut off at a certain threshold.

Let 
                           
                              
                                 
                                    t
                                 
                                 
                                    k
                                 
                              
                           
                         and 
                           
                              
                                 
                                    
                                       
                                          t
                                       
                                       
                                          k
                                       
                                    
                                 
                                 
                                    ¯
                                 
                              
                           
                         denote the presence and absence, respectively, of a particular feature in a certain class (e.g., 
                           
                              
                                 
                                    c
                                 
                                 
                                    1
                                 
                              
                           
                        , and 
                           
                              
                                 
                                    c
                                 
                                 
                                    2
                                 
                              
                           
                        ). To estimate the joint probability of a feature in a given class, we will use the following table with appropriate feature counts:
                           
                              
                                 
                                 
                                    
                                       
                                          
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Then N denotes the total number of features in all classes, 
                           
                              N
                              =
                              a
                              +
                              b
                              +
                              c
                              +
                              d
                           
                        . The joint probability 
                           
                              p
                              (
                              
                                 
                                    t
                                 
                                 
                                    k
                                 
                              
                              ,
                              
                                 
                                    c
                                 
                                 
                                    1
                                 
                              
                              )
                           
                         can then be estimated as
                           
                              (1)
                              
                                 p
                                 (
                                 
                                    
                                       t
                                    
                                    
                                       k
                                    
                                 
                                 ,
                                 
                                    
                                       c
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       a
                                    
                                    
                                       N
                                    
                                 
                                 ,
                              
                           
                        and similarly for 
                           
                              p
                              (
                              
                                 
                                    t
                                 
                                 
                                    k
                                 
                              
                              ,
                              
                                 
                                    c
                                 
                                 
                                    2
                                 
                              
                              )
                           
                        . The probability of a particular feature in all classes 
                           
                              p
                              (
                              
                                 
                                    t
                                 
                                 
                                    k
                                 
                              
                              )
                           
                         is given by
                           
                              (2)
                              
                                 p
                                 (
                                 
                                    
                                       t
                                    
                                    
                                       k
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       a
                                       +
                                       b
                                    
                                    
                                       N
                                    
                                 
                                 .
                              
                           
                        
                     

Furthermore, 
                           
                              
                                 
                                    c
                                 
                                 
                                    1
                                 
                              
                           
                         can be estimated as
                           
                              (3)
                              
                                 p
                                 (
                                 
                                    
                                       c
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       a
                                       +
                                       c
                                    
                                    
                                       N
                                    
                                 
                                 .
                              
                           
                        
                     

The conditional probability of 
                           
                              
                                 
                                    t
                                 
                                 
                                    k
                                 
                              
                           
                         given 
                           
                              
                                 
                                    c
                                 
                                 
                                    1
                                 
                              
                           
                         is given by
                           
                              (4)
                              
                                 p
                                 (
                                 
                                    
                                       t
                                    
                                    
                                       k
                                    
                                 
                                 |
                                 
                                    
                                       c
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       a
                                    
                                    
                                       a
                                       +
                                       c
                                    
                                 
                                 .
                              
                           
                        
                     

Henceforth, let n denote the number of classes, 
                           
                              m
                              =
                              {
                              
                                 
                                    t
                                 
                                 
                                    k
                                 
                              
                              ,
                              
                                 
                                    
                                       
                                          t
                                       
                                       
                                          k
                                       
                                    
                                 
                                 
                                    ¯
                                 
                              
                              }
                           
                        , and all logarithms are to the base 2.

We follow with the formulas for the particular feature selection methods. For a more detailed discussion of these methods, please refer to, e.g., (Forman, 2003; Zheng, Wu, & Srihari, 2004; Uchyigit, 2012; Patočka, 2013).

Mutual Information is always non-negative and symmetrical, 
                              
                                 MI
                                 (
                                 X
                                 ,
                                 Y
                                 )
                                 =
                                 MI
                                 (
                                 Y
                                 ,
                                 X
                                 )
                              
                           . 
                              
                                 (5)
                                 
                                    MI
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             0
                                          
                                          
                                             n
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             0
                                          
                                          
                                             m
                                          
                                       
                                    
                                    log
                                    
                                       
                                          p
                                          (
                                          
                                             
                                                t
                                             
                                             
                                                k
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                       
                                          p
                                          (
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          p
                                          (
                                          
                                             
                                                t
                                             
                                             
                                                k
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        

Also known as Kullback–Leibler divergence or relative entropy. It is a non-negative and asymmetrical metric.
                              
                                 (6)
                                 
                                    IG
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             0
                                          
                                          
                                             n
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             0
                                          
                                          
                                             m
                                          
                                       
                                    
                                    p
                                    (
                                    
                                       
                                          t
                                       
                                       
                                          k
                                       
                                    
                                    ,
                                    
                                       
                                          c
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    log
                                    
                                       
                                          p
                                          (
                                          
                                             
                                                t
                                             
                                             
                                                k
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                       
                                          p
                                          (
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          p
                                          (
                                          
                                             
                                                t
                                             
                                             
                                                k
                                             
                                          
                                          )
                                       
                                    
                                    +
                                    p
                                    (
                                    
                                       
                                          
                                             
                                                t
                                             
                                             
                                                k
                                             
                                          
                                       
                                       
                                          ¯
                                       
                                    
                                    ,
                                    
                                       
                                          c
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    log
                                    
                                       
                                          p
                                          (
                                          
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                             
                                                ¯
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                       
                                          p
                                          (
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          p
                                          (
                                          
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                             
                                                ¯
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        

Chi Square (
                              
                                 
                                    
                                       χ
                                    
                                    
                                       2
                                    
                                 
                              
                           ) can be derived as follows:
                              
                                 (7)
                                 
                                    GSS
                                    (
                                    
                                       
                                          t
                                       
                                       
                                          k
                                       
                                    
                                    ,
                                    
                                       
                                          c
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    =
                                    p
                                    (
                                    
                                       
                                          t
                                       
                                       
                                          k
                                       
                                    
                                    ,
                                    
                                       
                                          c
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    p
                                    (
                                    
                                       
                                          
                                             
                                                t
                                             
                                             
                                                k
                                             
                                          
                                       
                                       
                                          ¯
                                       
                                    
                                    ,
                                    
                                       
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                       
                                       
                                          ¯
                                       
                                    
                                    )
                                    -
                                    p
                                    (
                                    
                                       
                                          t
                                       
                                       
                                          k
                                       
                                    
                                    ,
                                    
                                       
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                       
                                       
                                          ¯
                                       
                                    
                                    )
                                    p
                                    (
                                    
                                       
                                          t
                                       
                                       
                                          k
                                       
                                    
                                    ,
                                    
                                       
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                       
                                       
                                          ¯
                                       
                                    
                                    )
                                    ,
                                 
                              
                           
                           
                              
                                 (8)
                                 
                                    NGL
                                    (
                                    
                                       
                                          t
                                       
                                       
                                          k
                                       
                                    
                                    ,
                                    
                                       
                                          c
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                N
                                             
                                          
                                          ·
                                          GSS
                                          (
                                          
                                             
                                                t
                                             
                                             
                                                k
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                       
                                          
                                             
                                                p
                                                (
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      k
                                                   
                                                
                                                )
                                                p
                                                (
                                                
                                                   
                                                      
                                                         
                                                            t
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                   
                                                   
                                                      ¯
                                                   
                                                
                                                )
                                                p
                                                (
                                                
                                                   
                                                      c
                                                   
                                                   
                                                      i
                                                   
                                                
                                                )
                                                p
                                                (
                                                
                                                   
                                                      
                                                         
                                                            c
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                   
                                                   
                                                      ¯
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                           
                              
                                 (9)
                                 
                                    
                                       
                                          χ
                                       
                                       
                                          2
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             0
                                          
                                          
                                             n
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             0
                                          
                                          
                                             m
                                          
                                       
                                    
                                    NGL
                                    
                                       
                                          (
                                          
                                             
                                                t
                                             
                                             
                                                k
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           
                        


                           
                              
                                 (10)
                                 
                                    OR
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             0
                                          
                                          
                                             n
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             0
                                          
                                          
                                             m
                                          
                                       
                                    
                                    log
                                    
                                       
                                          p
                                          (
                                          
                                             
                                                t
                                             
                                             
                                                k
                                             
                                          
                                          |
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          p
                                          (
                                          
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                             
                                                ¯
                                             
                                          
                                          |
                                          
                                             
                                                
                                                   
                                                      c
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                             
                                                ¯
                                             
                                          
                                          )
                                       
                                       
                                          p
                                          (
                                          
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                             
                                                ¯
                                             
                                          
                                          |
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          p
                                          (
                                          
                                             
                                                t
                                             
                                             
                                                k
                                             
                                          
                                          |
                                          
                                             
                                                
                                                   
                                                      c
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                             
                                                ¯
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        


                           
                              
                                 (11)
                                 
                                    RS
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             0
                                          
                                          
                                             n
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             0
                                          
                                          
                                             m
                                          
                                       
                                    
                                    log
                                    
                                       
                                          p
                                          (
                                          
                                             
                                                t
                                             
                                             
                                                k
                                             
                                          
                                          |
                                          
                                             
                                                c
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                       
                                          p
                                          (
                                          
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                             
                                                ¯
                                             
                                          
                                          |
                                          
                                             
                                                
                                                   
                                                      c
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                             
                                                ¯
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        

All evaluation tests were performed with two classifiers, Maximum Entropy (MaxEnt) and Support Vector Machines (SVM). Although the Naive Bayes classifier is also widely used in related work, we did not include it as it usually performs worse than SVM or MaxEnt. We used a pure Java framework for machine learning
                           18
                           
                              http://liks.fav.zcu.cz/ml.
                        
                        
                           18
                         with default settings (the linear kernel for SVM).

@&#RESULTS@&#

For each combination from the preprocessing pipeline (refer to Table 1) we assembled various sets of features and employed two classifiers. In the first scenario, we classified into all three classes (positive, negative, and neutral).
                        19
                        We ignore the bipolar posts in the current research.
                     
                     
                        19
                      In the second scenario, we followed a strand of related research, e.g., (Martineau & Finin, 2009; Celikyilmaz, Hakkani-Tür, & Feng, 2010), that deals only with positive and negative classes. For these purposes we filtered out all the neutral documents from the datasets. Furthermore, in this scenario we evaluate only features based on weighted delta-TFIDF, as, e.g., in (Paltoglou & Thelwall, 2010). We also involved only the MaxEnt classifier into the second scenario.

All tests were conducted in the 10-fold cross validation manner. We report the macro F-measure, as it allows comparison of classifier results on different datasets. Moreover, we do not report the micro F-measure (accuracy) as it tends to prefer performance on dominant classes in highly unbalanced datasets (Manning et al., 2008), which is, e.g., the case of our Product Review dataset where most of the labels are positive.


                        Table 2
                         shows the results for the three-class classification scenario on the Facebook dataset. The row labels denote the preprocessing configuration according to Table 1. In most cases, the maximum entropy classifier significantly outperforms SVM. The combination of all features (the last column) yields the best results regardless of the preprocessing steps. The reason might be that the character n-gram feature captures subtle sequences which represent subjective punctuation or emoticons, that were not covered by the emoticon feature. On average, the best results were obtained when HPS stemmer and lowercasing or phonetic transcription were involved (lines ShCl and ShPe). This configuration significantly outperforms other preprocessing techniques for token-based features (see column FS4: Unigr
                        
                        +
                        
                        bigr
                        
                        +
                        
                        POS
                        
                        +
                        
                        emot.).

In the second scenario we evaluated various TFIDF weighting schemes for binary sentiment classification. The results are shown in Table 3
                        . The three-character notation consists of term frequency, inverse document frequency, and normalization. Because of the large number of possible combinations, we report only the most successful ones, namely Augmented—a and LogAve—L term frequency, followed by Delta Smoothed—
                           
                              Δ
                              (
                              
                                 
                                    t
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                        , Delta Smoothed Prob.—
                           
                              Δ
                              (
                              
                                 
                                    p
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                        , and Delta BM25—
                           
                              Δ
                              (
                              k
                              )
                           
                         inverse document frequency; normalization was not involved. We can see that the baseline (the first column bnn) is usually outperformed by any weighted TFIDF technique. Moreover, using any kind of stemming (the row entitled various*) significantly improves the results. For the exact formulas of the delta TFIDF variants please refer to (Paltoglou & Thelwall, 2010).

We also tested the impact of TFIDF word features when added to other features from the first scenario (refer to Table 2). Column FS1 in Table 3 displays results for a feature set with the simple binary presence-of-the-word feature (binary unigrams). In the last column FS2 we replaced this binary feature with the TFIDF-weighted feature 
                           
                              a
                              Δ
                              (
                              
                                 
                                    t
                                 
                                 
                                    ′
                                 
                              
                              )
                              n
                           
                        . It turned out that the weighted form of the word feature does not improve the performance, compared with the simple binary unigram feature. Furthermore, a set of different features (words, bigrams, POS, emoticons, character n-grams) significantly outperforms a single TFIDF-weighted feature.

Furthermore, we report the effect of the dataset size on the performance. We randomly sampled 10 subsets from the dataset (1k, 2k, etc.) and tested the performance, still using 10-fold cross-validation. We took the most promising preprocessing configuration (ShCl) and MaxEnt classifier. As can be seen in Fig. 2
                        , while the dataset grows to approx 6–7k items, the performance rises for most combinations of features. With a 7k-item dataset, the performance begins to reach its limits for most combinations of features and hence adding more data does not lead to a significant improvement.

The influence of named entity filtering is shown in Table 4
                        . In most cases, removing named entities leads to a significant drop in classification. Thus we can conclude that in our corpus, the named entities themselves represent an important opinion-holder. This also corresponds to the sentiment distribution as shown in Fig. 1 (e.g., sentiment towards mobile phone operators is rather negative) and thus by removing the brand name from the data the classifier loses useful information.

To see the upper limits of the task itself, we also evaluate the annotator’s judgments. Although the gold labels were chosen after a consensus of at least two people, there were many conflicting cases that had to be solved by a third or even a fourth person. Thus even the original annotators do not achieve a 1.00 F-measure on the gold data.

We present ‘performance’ results of both annotators and of the best system as well (MaxEnt classifier, all features, ShCl preprocessing). Table 5
                            shows the results as confusion matrices. For each class (p—positive, n—negative, 0—neutral) we also report precision, recall, and F-measure. The row headings denote gold labels; the column headings represent values assigned by the annotators or the system.
                              20
                              Even though the task has three classes, the annotators also used ‘b’ for ‘bipolar and ‘?’ for ‘cannot decide’.
                           
                           
                              20
                            The annotators’ results show what can be expected from a ‘perfect’ system that solves the task the way a human would.

In general, both annotators judge all three classes with very similar F-measures. By contrast, the system’s F-measure is very low for negative posts (0.54 vs. 
                              
                                 ≈
                              
                            0.75 for neutral and positive). We offer the following explanation. First, many of the negative posts surprisingly contain happy emoticons, which could be a misleading feature for the classifier. Second, the language of the negative posts in not as explicit as for the positive ones in many cases; the negativity is ‘hidden’ in irony, or in a larger context (i.e., “Now I’m sooo satisfied with your competitor :)”)). This remains an open issue for future research.

For the other two datasets, the product reviews and movie reviews, we slightly changed the configuration. First, we removed the character n-grams from the feature sets, otherwise the feature space would become too large for feasible computing. Second, we abandoned SVM as it became computationally infeasible for such large datasets.


                        Table 6
                         (left-hand side) presents the results of the product reviews. The combination of unigrams and bigrams works best, almost regardless of the preprocessing. By contrast, POS features rapidly decrease the performance. We suspect that POS-related features do not carry any useful information in this case and also bring too much ‘noise’ to the classifier.

In the right-hand side of Table 6 we can see the results of the movie reviews. Again, the bigram feature performs best, paired with a combination of HPS stemmer and phonetic transcription (ShPe). Adding POS-related features causes a large drop in performance. We can conclude that for larger texts, the bigram-based feature outperforms unigram features and, in some cases, a proper preprocessing may further significantly improve the results.


                        Table 7
                         shows the effect of replacing named entities by their types. Again, named entities (e.g., actors, directors, products, brands) are very strong opinion-holders and thus their filtering significantly decreases classification performance.

Using the two most promising preprocessing pipelines (ShCl, ShPe), we conducted experiments with feature selection methods as introduced in Section 4.3. We classify into three classes using both MaxEnt and SVM classifiers on the Facebook dataset and using only MaxEnt on the other datasets (because of computational feasibility, as mentioned previously in Section 5.2).

Feature selection methods assign a certain weight to each feature and cut off those features whose weight is under a certain threshold. To estimate an optimal parameter automatically, we measured how the feature weight threshold influences the performance. For this purposes we used held-out data (10% of the training data). In each fold of the 10-fold cross validation, the optimum threshold for feature cut-off was set such that the performance on the held-out data was maximized.

In the previous experiments (Section 5), the feature space was pruned by a minimum occurrence which was empirically set to five. This prior pruning is not necessary for automatic feature selection. Therefore, we removed this prior filtering for the experiments on the Facebook data.
                           21
                           For the other two datasets, the product reviews and movie reviews, we still kept the minimum occurrence set to five, as otherwise the feature space would become too large for feasible computing.
                        
                        
                           21
                        
                     


                        Figs. 3–6
                        
                        
                        
                         show dependency graphs of the macro F-measure given a feature weight threshold. Note that these figures depict parameter estimation for only one fold from the 10-fold cross-validation and thus serve only as an illustration of the feature selection behavior. It is apparent that Information Gain and Mutual Information are able to filter out noisy features to a large extent yet keep the performance almost unchanged. The worst selector is Chi Square as it drastically lowers the performance even with a very small filtering threshold.

Overall, a significant improvement from 73.38% (baseline) to 73.85% was achieved for the product reviews, by means of the Mutual Information feature selector and ShPe preprocessing pipeline (see Table 8
                        ). Yet very similar results were obtained with a different preprocessing pipeline (ShCl). For the movie reviews dataset (Table 9
                        ) and the Facebook dataset with and without feature space pruning (Tables 10, 11
                        
                        , respectively) no significant improvement was achieved.

We can conclude that, in our settings, feature selection does not lead to a better overall performance, however, it can speed up the classification by filtering out noisy features.

Given the results achieved on the Facebook dataset, the following strategies for sentiment analysis of social media in Czech can be considered. First, the preprocessing pipeline should take into account text properties typical of social media, such as proper tokenization (with respect to emoticons, URLs, etc.), stemming, and lower-casing. Additional normalization, such as phonetic transcription, can also increase performance because of the many grammatical errors present in such texts (the case of, e.g., i/y; ie/
                           
                              
                                 
                                    e
                                 
                                 
                                    ˇ
                                 
                              
                           
                         in Czech). Second, the Maximum Entropy classifier yields better results than the linear kernel SVM; moreover, training is significantly shorter. The feature set consisting of unigrams, bigrams, emoticons, and various POS features gives the best overall results. Third, filtering named entities or feature selection did not improve the overall performance for our dataset.

It should be noted that the number of examined domains was limited because we restricted our dataset only to the top nine most popular Czech Facebook brand pages. It is thus worth investigating how the system would tackle the issues of domain-dependent features and domain portability. This remains an open question for future work.

@&#CONCLUSION@&#

This article presented in-depth research on supervised machine learning methods for sentiment analysis of Czech social media. We created a large Facebook dataset containing 10,000 posts, accompanied by human annotation with substantial agreement (Cohen’s 
                        
                           κ
                        
                      0.66). The dataset is freely available for non-commercial purposes.
                        22
                        We encourage other researchers to download our dataset for their research in the sentiment analysis field.
                     
                     
                        22
                      We thoroughly evaluated various state-of-the-art features and classifiers as well as different language-specific preprocessing techniques and feature selection algorithms. We significantly outperformed the baseline (unigram feature without preprocessing) in three-class classification and achieved an F-measure of 0.69 using a combination of features (unigrams, bigrams, POS features, emoticons, character n-grams) and preprocessing techniques (unsupervised stemming and phonetic transcription). In addition, we reported results in two other domains (movie and product reviews) with a significant improvement over the baseline.

To the best of our knowledge, this article is the first one of its kind that deals with sentiment analysis in Czech social media in such a thorough manner. Not only does it use a dataset that is magnitudes larger than any in the related work but also incorporates state-of-the-art features and classifiers. We believe that the outcomes of this article will not only help to set the common ground for sentiment analysis for the Czech language but also help to extend the research beyond the mainstream languages and may be applied to sentiment analysis in other Slavic languages, such as Slovak or Polish.

@&#ACKNOWLEDGMENTS@&#

This work was supported by grant no. SGS-2013-029 Advanced computing and information systems, by a POSTDOC grant from the University of West Bohemia, and by the European Regional Development Fund (ERDF), project “NTIS – New Technologies for Information Society”, European Center of Excellence, CZ.1.05/1.1.00/02.0090. Access to computing and storage facilities owned by parties and projects contributing to the National Grid Infrastructure MetaCentrum, provided under the programme “Projects of Large Infrastructure for Research, Development, and Innovations” (LM2010005), is gratefully acknowledged. Access to the CERIT-SC computing and storage facilities provided under the programme Center CERIT Scientific Cloud, part of the Operational Program Research and Development for Innovations, Reg. No. CZ. 1.05/3.2.00/08.0144, is greatly appreciated. We thank Tomáš Brychcín for his High-Precision Stemmer implementation, Michal Konkol for his machine learning library, and Michal Patočka for his implementation of feature selection algorithms.

@&#REFERENCES@&#

