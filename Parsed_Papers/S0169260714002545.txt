@&#MAIN-TITLE@&#A low-cost real time virtual system for postural stability assessment at home

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Equilibrium conditions control is very important in the elderly and in many pathologies.


                        
                        
                           
                           Equilibrium control is commonly evaluated by using a stabilometric platform.


                        
                        
                           
                           A low-cost virtual stabilometric platform is presented.


                        
                        
                           
                           Results demonstrate that the virtual stabilometric platform is equivalent to a physical one.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Virtual stability assessment

Stabilometric force platform

Body model representation

Recognition and tracking algorithms

Home use medical device

@&#ABSTRACT@&#


               
               
                  Background and objective
                  The degeneration of the balance control system in the elderly and in many pathologies requires measuring the equilibrium conditions very often. In clinical practice, equilibrium control is commonly evaluated by using a force platform (stabilometric platform) in a clinical environment.
                  In this paper, we demonstrate how a simple movement analysis system, based on a 3D video camera and a 3D real time model reconstruction of the human body, can be used to collect information usually recorded by a physical stabilometric platform.
               
               
                  Methods
                  The algorithm used to reconstruct the human body model as a set of spheres is described and discussed. Moreover, experimental measurements and comparisons with data collected by a physical stabilometric platform are also reported. The measurements were collected on a set of 6 healthy subjects to whom a change in equilibrium condition was stimulated by performing an equilibrium task.
               
               
                  Results
                  The experimental results showed that more than 95% of data collected by the proposed method were not significantly different from those collected by the classic platform, thus confirming the usefulness of the proposed system.
               
               
                  Conclusions
                  The proposed virtual balance assessment system can be implemented at low cost (about 500$) and, for this reason, can be considered a home use medical device. On the contrary, astabilometric platform has a cost of about 10,000$ and requires periodical calibration. The proposed system does not require periodical calibration, as is necessary for stabilometric force platforms, and it is easy to use. In future, the proposed system with little integration can be used, besides being an emulator of a stabilometric platform, also to recognize and track, in real time, head, legs, arms and trunk, that is to collect information actually obtained by sophisticated optoelectronic systems.
               
            

@&#INTRODUCTION@&#

Human postural stability is a multidisciplinary topic of great interest since it has a huge impact on several aspects of everyday life. How individuals maintain balance, have correct and efficient postural position, prevent falls and the occurrence of severe injuries are aspects that relate to medicine and neurophysiology, biomechanics and robotics, as well as rehabilitation, occupational safety, ergonomics and sport science applications. One-third to one half of the elderly people (over 65) has some difficulty with balance control [1]. Evidence has been collected suggesting that falls in the elderly are due to difficulties in adapting one's balance in response to changes in sensory information [2], as well as increased sway in the medio-lateral (ML) and anterior–posterior (AP) directions compared to young adults [3]. Body sway is defined as the slight postural movements made by an individual in order to maintain or to recover a balanced position.

Body stability analysis serves to evaluate the body sway of an individual and is useful to estimate the status of the equilibrium and the risk of a fall connected with the current situation.

In body stability analysis, the following parameters are evaluated: posture, balance, centre of mass (and, consequently, centre of gravity), centre of pressure and time to contact [4,5].

Posture describes the orientation of any body's segment relative to the gravitational vector. It is an angular measure from the vertical. Balance describes the dynamics of body posture to avoid falls. It is related to the inertial forces acting on the body and the inertial characteristics of body segments. Typically, body sway is evaluated by measuring some parameters: the excursions of the centre point of pressure (COP) or the centre of gravity (COG) derived by the centre of mass (COM).

COM is a point equivalent of the total body mass, corresponding to the weighted average of the COM of each body segment in the space. It is a passive variable controlled by the balance control system and it is mainly affected by the mass distribution and the skeletal structure. COG represents the vertical projection of the COM onto the ground. COP is the point location of the vertical ground reaction force vector. It represents a weighted average of all the pressures over the surface of the area in contact with the ground. The COP reflects both the horizontal location of the COG and the reaction forces due to muscular activity.

The manuscript is structured as follows: Section 2 provides related works on postural stability analysis; Section 3 describes the system assumptions; Section 4 contains the system design and the reconstruction algorithm description; Section 5 defines the testing protocol; Section 6 presents the results and discussion, and Section 7 contains conclusions and future work.

@&#RELATED WORKS@&#

Systems for sway measurements are based on simple technology, such as the “swaymeter” [6] and the Wright's ataxiameter [7]. The swaymeter measures the displacements of the body at the waist level; the ataxiameter measures the angular movement of the body around the ankle joint. An inclinometer-based method has also been developed to provide information about body sway [8].

The most popular system used for evaluating human stability of a subject is the stabilometric force platform (an example is [9]). A stabilometric, or force platform, can be used to measure spontaneous body sway with the subject standing on it (static posturography) or the subject's response to an applied/voluntary postural perturbation (dynamic posturography) [10,11].

Body sway can be measured under variable visual and surface conditions, and measures of postural sway have been reported to capture sensorimotor deficits rather than to differentiate between functional performance abilities [12]. Typically, sway is assessed during stance on a stable platform both with the eyes open and with the eyes closed. It can also be measured by using visual control while interacting with a virtual environment [13,14].

The basic principle of the force platform test is to measure the movements of the COP. The aim of data processing is to compute selected parameters of total body sway from the time series of COP positions. Typical parameters in platform measurements are the mean COP position (as a reference point), AP and ML sway, the length of the sway path, as well as sway time (that, along with the sway path, allows the calculation of the sway velocity) and sway area [9]. In general, a subject can keep his own balance if COP trajectory remains inside apolygon designed by considering the external points of the contact surface of his body (for a standing person, the polygon is drawn by joining the extreme points of his feet). For this reason balance is the capability to keep the COP inside the polygon. If such projection goes out of the polygon, the subject's body is in a state of disequilibrium. Obviously, the stabilometric platforms cannot be used to collect information regarding how the subject moved his body to maintain/recover the equilibrium, being their measurement based on the mean COP of the body. Moreover, stabilometric platforms cannot be used to collect information regarding walking subjects. For these reason, a force platform is able to calculate only a small fraction of the useful kinematic variables.

An interesting approach to analyze postural control is presented in Ref. [15]; the purpose of the study was to determine the influence of ankle strategy to maintain the balance in standing; the marker-based method was compared with the method based on the assumption of the inverted pendulum model.

In Ref. [16], postural stability algorithms were studied to provide stability features to classify correctly the type of stance on the force platform; k-nearest neighbour algorithms, support vector machine and Hidden Markov Models (HMMs) were compared.

Posture and balance, also during walking, can be well collected by wireless electro-magnetic sensors or optoelectronic systems [17]. The most accurate are the optoelectronic systems, characterized by using new technologies, mainly based on video cameras. These systems could be classified, for example, into ones with active or passive cameras or active or passive markers.

In balance studies, optical systems have been utilized, first, to track the position of the body segments [18] and, then, to calculate the COG position, through the measurements of the positions of some light-emitting markers placed on chosen body points. Then, by knowing the positions of the centres of the body segments, COGs referred to all body segments and, then, the mean COG value can be calculated. Moreover, the movements of body segments and their directions on all cardinal axes with optoelectric imaging systems can be also recorded. Motion occurs in space and time, and typically, human movement analysis is accomplished by the determination of position, velocity and acceleration, usually at a temporal resolution of about 0.034s (corresponding to the frame rate of 30 frames per second, normally obtainable with current video cameras). In the same way, motion analysis makes it possible to analyze single joint angles, angle velocities and angle accelerations in balance performances. Although optoelectronic systems can provide accurate body sway information, they are very expensive, require to wear markers and, hence, they can be used only in controlled environments.

Markerless vision based motion capture provides a potential alternative for affordable capture of human motion in a wide range of settings. This technique includes a markerless image processing algorithm to estimate the COG trajectory from video sequences obtained from a single, commercially available, RGB camera, positioned laterally to the subject [19]. Also, it has been demonstrated that using a single, frontally placed, not calibrated RGB camera, it can be possible to measure clinically meaningful statistics of standing postural sway among an elderly balance-impaired cohort [20]. These methods, by using a single 2D video camera, allow predicting AP component of COG trajectory, the first, and ML component of COG trajectory, the second.

To improve accuracy and allow both COG components measurements, a two camera system, using inexpensive RGB web cameras, was presented in Ref. [21]. Though this method is more effective than the others, two cameras require to be calibrated and are not really suitable for daily assessment in a domestic environment because they require space. Moreover, two cameras are not sufficient to keep 3D volumetric information regarding the subject body. In fact, to calculate the position of a point in a 3D space, the point has to be “viewed” by at least three different cameras. This would introduce further complexity to the system (the images collected by all the cameras have to be analyzed together), without solving the problems of the obscured directions (a big portion of the body would remain obscured to at least one camera).

We overcome these difficulties by using a single, not calibrated, 3D depth camera and a 3D simplified spheres-based body reconstruction algorithm, to emulate a stabilometric platform. The system has been implemented as part of a framework for fast prototyping of customized human–computer interfaces for rehabilitation [22].

In what follows, the system assumptions are discussed starting from the relationship between COG and COP, following with the justification for choosing COG in our calculations, and finally with the introduction of the body model representation.

In stationary conditions, COG and COP are coincident. In dynamic conditions, COP has greater dynamic range and greater acceleration than COG [4,23]. For this reason, COP trajectory is greater than COG trajectory and it contains also higher frequencies and amplitudes, that is COP is more sensitive than COG in indicating suddenly variations of oscillation direction.

The model used to describe COP–COG relationship is the single inverted pendulum, in case of rigid body approximation, or the double inverted pendulum, if the body is considered to be divided in two parts (upper and lower), each represented by an inverted pendulum [4,24,25].

Generally the COP trajectories allow the calculus of COG and COP–COG relationship, and vice versa. In fact, by assuming as F the frequency of oscillation (Hz), 
                        
                           
                              F
                              0
                           
                           =
                           
                              
                                 (
                                 m
                                 g
                                 h
                                 /
                                 
                                    I
                                    G
                                 
                                 +
                                 m
                                 
                                    h
                                    2
                                 
                                 )
                              
                              
                                 1
                                 /
                                 2
                              
                           
                        
                      the natural body frequency, and m, g, h, I
                     
                        G
                      respectively the mass of the subject, the gravity acceleration, the distance of COM from the ground, and the moment of body inertia around the medio-lateral (ML) or antero-posterior (AP) axis with respect to the COM, the following relation was proposed [26]:
                        
                           (1)
                           
                              
                                 
                                    
                                       COG
                                    
                                    
                                       COP
                                    
                                 
                                 =
                                 
                                    
                                       
                                          F
                                          0
                                          2
                                       
                                    
                                    
                                       (
                                       
                                          F
                                          0
                                          2
                                       
                                       +
                                       
                                          F
                                          2
                                       
                                       )
                                    
                                 
                              
                           
                        
                     
                  

Being the moments of inertia slightly different if calculated for ML or AP oscillations, usually two different relationships have to be evaluated. The observed loss in amplitude in COG with respect in COP, as the sway increases, may be explained by assuming that the body constitutes a low-pass filter.

In Ref. [27], it was demonstrated that the COP–COG relationship is proportional to the horizontal COG acceleration, confirming that the inverted pendulum assumption is reasonable for quite standing.

However, the inverted pendulum model revealed its limits in the case of spinal deformities [28]. Moreover, the inverted pendulum model can be really inaccurate when the subject spreads his arms or legs to maintain the equilibrium, that can be often the case of an old person.

In summary, being the inverted pendulum a rough approximation of the body movements, at least when the model represents situations that are far from reality (in case of infirmities), and being COG and COP really similar trajectories when representing small oscillations, that is often the case of maintaining the equilibrium without external perturbations, from now on we consider COP and COG indistinguishable. For this reason, we will calculate the components ML and AP of the COG of a volumetric model of the body of the examined subject. The volumetric information of the subject will be extracted from the depth map collected by a depth sensing video camera. By positioning a set of spheres of uniform dimensions with their centres on a selected subset of these subject's body depth data captured by the camera, the volumetric model of the examined body will be created as the ensemble of these spheres. The reconstructed model will be a simplified, raw, representation of the body of the subject. The reason is threefold: (a) it will be represented by using a reduced set of spheres; (b) the spheres will have equal mass density (head, trunk, legs, and arms it will be supposed to have the same density); (c) the model will be referred just to the portion of the body “seen” by the camera and it will represent just a shell of the body.

The proposed system is composed of a digital 3D depth sensing video camera, connected to a computer, placed in front of the subject at a distance of about 2.5m from the subject (Fig. 1
                        ), and a rendering programme used both to represent a numerical model of the subject and to perform the COG calculations. Fundamental, for our purposes, is to use a single 3D depth sensing camera frontally placed with respect to the analyzed subject to avoid camera calibration, difficulty of installation and computational overhead.

The last point is particularly important because the video-cameras management, the whole set in the same time, and the 3D scene reconstruction are completely assigned to the application software. In contrast, by using a depth sensing camera, part of the job is run by the camera (the camera provides to the computer the depth map). The camera used in the proposed system is the DepthSense™ 311 [29]. It is a time of flight (TOF) camera [30], using a near infrared wave modulated at 20MHz, continuously transmitted by 8 LEDs to illuminate the scene. A 2D infrared electro-optical sensor, synchronized with the transmitters, collects the infrared map in four points per period of the modulating wave. In this way, from each pixel of the scene the phase difference between the received and transmitted modulated signal is calculated, i.e. the distance of the pixel from the camera is evaluated. This allows calculating the depth map for each frame collected by the camera (maximum temporal resolution: 1/30s). The depth map is used for numerical calculations. First, a background removal is performed before the subject examination: it is a very fast operation (about 1s) performed by collecting and memorizing the scene that is subtracted from each video frame. After that, the subject enters into the scene and the system eliminates those parts of the scene that are unchanged with respect to the memorized scene while leaving unchanged what has been modified: in this case, the resulting video will show only the subject (also if the subject will remain still).

The resulting depth map is used to calculate volumetric information of the subject. What is identified as non-zero in the depth map is identified as part of the body of the subject. By placing a set of spheres of uniform dimensions centred on a selected subset of these non-zero samples, it is possible to obtain a volumetric rendering of the body of the examined subject based on these spheres. The distribution of the spheres, to ensure a uniform sampling, is discussed below.

Spherical sampling has to distribute uniform spherical samples on a sub-domain of a 3D space based on a minimum distance criterion between samples. We propose an efficient sampling method that generates samples on the domain D
                        =[01]3, consisting of a unit cube in the 3D space (the minimum cube in which the subject is inscribed, a sort of cubic 3-dimensional bounding box). The outcome of the method is a set S
                        ={
                           x
                        
                        
                           i
                        
                        ∈
                        D
                        andallowingtothesubject'ssilhouette;
                        i
                        =1, 2, …, N} of N samples for which the following sampling conditions must hold:
                           
                              (2)
                              
                                 
                                    ∀
                                    
                                       
                                          x
                                       
                                       i
                                    
                                    ∈
                                    S
                                    ,
                                    ∀
                                    
                                       S
                                       s
                                    
                                    ⊆
                                    D
                                    ⇒
                                    p
                                    (
                                    
                                       
                                          x
                                       
                                       i
                                    
                                    ∈
                                    
                                       S
                                       s
                                    
                                    )
                                    =
                                    
                                       ∫
                                       
                                          
                                             S
                                             s
                                          
                                       
                                    
                                    d
                                    
                                       x
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    ∀
                                    
                                       
                                          x
                                       
                                       i
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       j
                                    
                                    ∈
                                    S
                                    ⇒
                                    |
                                    |
                                    
                                       
                                          x
                                       
                                       i
                                    
                                    −
                                    
                                       
                                          x
                                       
                                       j
                                    
                                    |
                                    |
                                    ≥
                                    2
                                    r
                                 
                              
                           
                        
                     

Chosen a 
                           x
                        
                        ∈
                        D and allowing to the subject's silhouette, ∃
                           x
                        
                        
                           i
                        
                        ∈
                        S:
                           
                              (4)
                              
                                 
                                    |
                                    |
                                    
                                       x
                                    
                                    −
                                    
                                       
                                          x
                                       
                                       i
                                    
                                    |
                                    |
                                    <
                                    2
                                    r
                                 
                              
                           
                        where r
                        ≤
                        R is a parameter and R represents the radius of each sphere (if r
                        =
                        R the spheres are touching each other). r
                        <
                        R implies that the spheres are partially overlapping and it is useful to avoid holes between contiguous spheres. In what follows we set r
                        =0.85·R unless differently stated.

Condition (2) ensures that a uniformly distributed sample 
                           x
                        
                        
                           i
                        
                        ∈
                        S has a probability of falling inside a subset S
                        
                           s
                        
                        ⊆
                        D equal to the volume of S
                        
                           s
                         (it is supposed that the volume of D equals to (1)). Condition (3) enforces that the minimum distance between any pair of samples is 2r. Condition (4) ensures that the obtained sampling distribution is maximal: it is not possible to insert any further samples without violating the minimum distance constraint (3).

Besides these conditions, some parameters are useful to drive the filling process. One of these is the weighted surface occupied by the subject, defined as 
                           
                              
                                 W
                                 S
                              
                              =
                              
                                 N
                                 p
                              
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    
                                       N
                                       t
                                    
                                 
                              
                              
                                 
                                    p
                                    i
                                 
                                 
                                    w
                                    i
                                 
                              
                           
                        , where N
                        
                           p
                         is the number of pixels allowing to the subject (foreground pixels), N
                        
                           t
                         is the total number of pixels “seen” by the camera (foreground+background pixels), p
                        
                           i
                         is 0 if the pixel belongs to the background and 1 if it belongs to the subject, 
                           
                              
                                 w
                                 i
                              
                              =
                              
                                 d
                                 i
                                 2
                              
                           
                         is the weighting factor defined as the square of its distance 3d from the camera. The weight is necessary to ensure that a pixel at a distance 3d from the camera is nine times greater than one whose distance is d from the camera (the farther pixel covers a surface which is 9 times greater than the closer).

The space surrounding the subject is supposed to be a cube C (its edge equals the maximum dimension of the subject 3D bounding box and it is laterally centred on the subject ML component of the COG) which is recursively divided to assign the spheres in those zones where foreground pixels are present, as reported in Fig. 2
                        . The portion of weighted surface inside each sub-cube is calculated by using the parameter 
                           
                              
                                 W
                                 
                                    S
                                    _
                                    c
                                    u
                                    b
                                    e
                                 
                              
                              =
                              
                                 N
                                 
                                    p
                                    _
                                    c
                                    u
                                    b
                                    e
                                 
                              
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    
                                       N
                                       
                                          t
                                          _
                                          c
                                          u
                                          b
                                          e
                                       
                                    
                                 
                              
                              
                                 
                                    p
                                    i
                                 
                                 
                                    w
                                    i
                                 
                              
                              /
                              
                                 W
                                 S
                              
                              ,
                           
                         where N
                        
                           p_cube
                         is the number of pixels allowing to the subject inside the cube, N
                        
                           t_cube
                         is the total number of pixels “viewed” by the camera (foreground+background pixels) inside the cube. The data structure supporting the cube division is supposed to be an eight-tree (see Fig. 2). The method to distribute the spheres on the region occupied by the body of the subject, that is to construct the sphere-based volumetric avatar, uses the following recursive algorithm:
                           Algorithm
                           AV-SPHERES (C,N,A,L)


                           Input parameters definition:

C is the currently explored cube;

N is the total number of spheres to be assigned;

A is used to contain the number of assigned spheres (at the first iteration, A
                              =0);

L contains the list of cubes not completely filled by useful pixels.
                                 
                                    
                                       
                                       
                                          
                                             k=
                                                W
                                                
                                                   S_cube
                                                (C)
                                          
                                          
                                             If k=0
                                          
                                          
                                             
                                                C=null
                                          
                                          
                                             
                                                Return
                                          
                                          
                                             Else
                                          
                                          
                                             
                                                Q=floor(N*k)
                                          
                                          
                                             
                                                If Q=0,
                                          
                                          
                                             
                                                
                                                Place C in the ordered list L (in decreasing order of N*k values)
                                          
                                          
                                             
                                                
                                                Return
                                          
                                          
                                             
                                                End;
                                          
                                          
                                             
                                                If Q=1
                                          
                                          
                                             
                                                
                                                Assign one sphere in the centre of mass of the foreground pixels in cube C
                                          
                                          
                                             
                                                
                                                A=A+1
                                          
                                          
                                             
                                                
                                                Return
                                          
                                          
                                             
                                                Else
                                          
                                          
                                             
                                                
                                                For i=1:8
                                          
                                          
                                             
                                                
                                                
                                                AV-SPHERES (C_child(i),N,A,L)
                                          
                                          
                                             
                                                
                                                End
                                          
                                          
                                             
                                                End
                                          
                                          
                                             End
                                          
                                       
                                    
                                 
                              
                           

Of the previously defined input parameters, L (initially a void list) contains the list of cubes (including their spatial location and their weighted content, N·k) whose content was greater than zero but to whom no sphere has been assigned, in descending order of N·k. The input parameters are used by the following method to distribute, beside the A spheres, the residual N-A (if present) unassigned spheres (in fact due to the floor integer operation made in the previous algorithm some of the N spheres could remain unassigned):
                           Algorithm
                           AV-SPHERES-COMPLETE (C,N,L)


                           
                              
                                 
                                    
                                       
                                       
                                          
                                             A=0
                                          
                                          
                                             L=[]
                                          
                                          
                                             AV-SPHERES (C, N, A, L)
                                          
                                          
                                             R=N-A
                                          
                                          
                                             If R>0,
                                          
                                          
                                             
                                                For i=1:R
                                          
                                          
                                             
                                                
                                                If L(i)<>NULL
                                          
                                          
                                             
                                                
                                                
                                                Assign one sphere in the centre of mass of the foreground pixels in the cube indicated by L(i)
                                          
                                          
                                             
                                                
                                                Else
                                          
                                          
                                             
                                                
                                                
                                                Assign one sphere in the centre of mass of the spheres previously assigned
                                          
                                          
                                             
                                                
                                                End
                                          
                                          
                                             
                                                End
                                          
                                          
                                             End
                                          
                                       
                                    
                                 
                              
                           

It is important to note that, after the application of the aforementioned algorithm, all the spheres are assigned (the remaining spheres were inserted into the centre of mass of those previously collocated). In the considered application the last step of the algorithm could also be omitted to leave some of the N spheres unassigned (though very improbable, this would occur if the number of the elements of L was lower than N-A). In this case, some sphere would not find allocation (that is, the surface occupied by the subject is lower than that allowed by the whole set of N spheres). In our case, this situation would not constitute a problem because the used spheres are sufficient to calculate the COM of the subject and, as a consequence, the corresponding COG (the only required condition is that the body of the subject is uniformly covered by spheres). On the contrary, if the allocation of the remaining N-A-L spheres were attempted in a position that was different from the COM of those previously placed, it could result in giving more importance (that is more weight) to some zone of the subject body with respect to others, thus producing an error in the COG calculation due to the violation of the assumption that all the parts of the body have the same mass density (as will be clarified below, when a recognition algorithm will be used the uniform mass density condition can be eliminated).

We use a cloud of 150 uniform spheres, to obtain the 3D model of the subject body (Fig. 3
                        ). For each frame, the spatial position of the centres of these spheres is calculated. We have 3 spatial coordinates for each point. For each frame, we use the spatial coordinates of the 150 centroids to calculate the average centroid: the mean value of the x, y, and z coordinates are collected separately. By assuming x be the ML direction, y the AP direction and z the vertical direction, with respect to the subject position (Fig. 1), we obtain the COG value as the projection of the average point in the x–y plane (corresponding in taking just its x and y coordinates). Note that the proposed virtual platform calculated both the ML and AP displacements under the same condition: the calculation, in both cases, is based on the COG of the numerical 3D model composed by 150 spheres.

We register into a file the temporal evolution of the x–y projection of this point by storing a variable consisting of a series of rows (one for each time instant) and 3 columns (time, x and y, respectively). These data and their representation into a 3D virtual environment are used by a framework, developed by using the open source graphical Irrlicht Engine [31], also used to design virtual reality assistive rehabilitation exercises [22].

In order to test the effectiveness of the proposed virtual balance board, a comparison between data calculated by using it and data measured by a force platform [9] was performed. To measure just the equilibrium point and its little oscillations, in stationary conditions, the comparison could have been restricted to the measured data from different subjects (gender, height, weight, and age) while they were still on the stabilometric platform, keeping their arms along the body, for a predetermined, fixed, time period. However, data regarding temporal resolution, maximum/minimum oscillation measurement, the presence of calculation noise due to model flickering, and indirect inter subjects comparison on the same movements, are very useful to define the accuracy of the proposed system. All these aspects were well evaluated by using a given set of movements, the same for all analyzed subjects, produced following a specifically designed equilibrium task. To this aim, we implemented the task by assuming that the subject had to maintain his/her equilibrium standing barefoot on a virtual circular board (diameter 50cm) sustained on its centre by a virtual, invisible, rigid, and inextensible cable while the board received external solicitations, as reported in Fig. 4
                      (a similar equilibrium exercise, an arm swing for forward-backward movements, was described and used in Ref. [13]). The cable was considered to be fixed to an invisible node at a height of 2.5m from the board plane and was able to oscillate in any direction (AP, ML and also laterally at an intermediate direction). The board was subjected to external destabilizing perturbations of randomized force, direction and duration.

The virtual task was implemented with the framework described in Ref. [22] by using the same 3D depth sensing camera [29] used to reconstruct the 3D model of the body of the subject, the iisu™ middleware [32] which ensured the communication between the camera and a suitably implemented end-user application, and a high performance real time engine [31] for rendering the subject's body 3D data in the virtual environment.

The hardware used for the implementation of the system was composed of a PC equipped with an Intel™ Pentium™ D (3.20GHz), 2GB RAM, 500GB HD, Nvidia GeForce 6600 256MB (all supported by a Windows™ XP Professional). Note that, though the used hardware represented an entry level PC, the described model reconstruction algorithm and task were executed in real time (that is, being 25 the number of points per second given by the force platform, the proposed method used a frame rate of 25fps and each frame was processed and visualized faster than 1/25s).

It is important to note that to implement the aforementioned equilibrium task, besides the resources used to implement the virtual stabilometric platform, a projection screen (1.0m×1.2m, placed at a distance of 1.3m) was necessary to allow the interaction of the user with the virtual environment of the task (the virtual stabilometric platform does not require any screen or graphic interaction). Moreover, to avoid spatial interferences between the screen and the camera, the screen was positioned in front of the subject while the camera was positioned on the side of the subject. This represented a difference with the setup of the virtual force platform where the camera was installed in front of the subject (see Fig. 1): possible implications of this particular setup are discussed below.

The reconstructed 3D model of the body was projected on a screen at 30fps. The model was located over the virtual board and the COG was calculated in real time. The invisible cable supporting the oscillating virtual board was used to construct the physical model of the task. The subject maintained the equilibrium by tilting the upper part of his body in the same direction of the perturbation received by the board. At each frame, the COG of the model was calculated, memorized and, in the same time, its current position compared with the centre of the virtual board. In order to keep the subject informed about the movement direction and amplitude he had to perform to recover the equilibrium, an arrow was draw on the screen indicating the versus toward which the upper part of the body had to be moved (the shown versus is the same of the perturbation received by the board).

Considering that some directions would be obscured by the presence of the body model on the screen, the arrow was always shown in front of the model: its orientation would serve to indicate the direction of motion. The task was designed to be continuous, with a random interval between perturbations. The parameters used were the following: (1) subject body weight; (2) static and dynamic friction of the board with respect to the pivot (5kgm/s2 and 0.5kgm/s2, respectively); (3) task duration (120s, corresponding to 3000 points); (4) random interval sequence between perturbations (chosen once and maintained fixed for all subjects); (5) random sets of force amplitudes, directions, and duration. Each impressed pulse force was calculated as follows: F
                     =
                     mΔv/Δt, where m was the body mass, Δv was the velocity variation after the perturbation and Δt was the time lag in which the pulse force was applied. All the directions of Δv, that is the directions of the applied forces, their amplitude, and duration were chosen randomly (before the task started), and were considered to be the same for all subjects, but unknown to the subjects.

@&#RESULTS AND DISCUSSION@&#

We collected a series of experimental data by using the virtual platform from a set of 6 young healthy subjects volunteers (2 women and 4 men; mean age 30.5 years, σ
                     =3.39 years; mean height 177.3cm, σ
                     =6.25cm; mean weight 73.8kg, σ
                     =11.58kg), for the execution of the virtual equilibrium task described above.

During the execution of the task, subjects were required to stand barefoot on a force platform [9]. In this way, data calculated by the virtual platform (calculated virtual COG) and those measured by the physical platform (real measured COP) were collected in a synchronized way.

It is important to note that the joint use of the virtual platform and the physical platform was done to determine the accuracy of our method, for comparison: in operative conditions, the virtual platform is intended to substitute the physical platform.

Data were analyzed in Matlab® 
                     [33] and, for each subject, the following parameters were calculated, by using the physical board as reference: maximum amplitude of the ML component of the oscillation, maximum amplitude of the AP component of the oscillation, maximum error of the ML component, maximum error of the AP component, average error both along ML and AP directions, and standard deviation error both along ML and AP directions.

The results are summarized in Table 1
                     . The ML and AP paths, both for the calculated COG and for the measured COP, are shown in Fig. 5
                     , while the resulting trajectories are reported in Fig. 6
                     .

The experimental results demonstrate that the proposed system has a lower dynamic range than the physical platform, particularly evident in correspondence of abrupt changes of direction. This is mainly due to the fact that our system calculates COG while the physical platform measures COP (the relationship between COG and COP has been discussed above). Besides that, differences can be also due to the approximations used in our model: the model represented just a shell of the body; the body was represented by using a reduced set of spheres; the spheres had all the same mass density; in the implemented task, the body showed to the camera its lower surface, the lateral side (in the normal usage, the subject would show its frontal side, thus improving the reconstructed model).

Moreover, some trajectory drift, though very low, is present on data collected by our model, as confirmed by observing the trajectories reported in Figs. 5 and 6. This effect, being unsystematic, was probably due to the variation of exposition of the body with respect to the camera in different frames, or group of frames, as the subject moved his body during the exercise. In particular, the different assignment of the spheres to the portion of body viewed by the depth camera in different frames can be the responsible for little, locally positioned, oscillations (flickering noise). At the same time, variations of the body orientation while responding to different forces applications during the exercise, involving groups of frames, produced a low frequency trajectory drift effect.

By analyzing the data in Table 1, it is evident that, though the maximum errors were high (for Subject #6 they were very high), the average errors and the standard errors deviations were acceptable for most part of the trajectories (also for Subject #6), as confirmed also by looking at Figs. 5 and 6, and at Table 1. In fact, excluding peaks, the error levels both in ML and AP components were lower than or equal to (≤10%) the maximum oscillations (see Table 1); this makes them acceptable by considering the assumptions made for the virtual force platform. The reason why the maximum errors were worse for Subject #6 than in other Subjects is that this subject was the most reactive to the proposed task. In fact, Subject #6 performed the largest amount of oscillations and, in the same time, with the largest rapidity (largest acceleration): this created bigger differences between COG and COP.

To compare a new measurement technique with a well-established measurement method, the Bland–Altman method was adopted because neither the correlation coefficient nor techniques such as regression analysis are appropriate [34]. We compare the virtual platform calculations with the physical force platform measurements by using Bland–Altman plots.

The results (Fig. 7
                     ) confirmed that the great part of the 3000 data points calculated by our method (above 95%), for each direction and subject, were not significantly different from those measured by a force platform. In particular, the portion of the data points (in percentage) contained inside the confidence interval M
                     ±1.96 σ (where M is the mean of the differences, and σ is the standard deviation of the differences) were the following (from left to right and from top to bottom of Fig. 7): 95.6, 96.7, 96.3, 97.2, 95.7, 97.8, 96.3, 97.8, 96.14, 97.1, 95.6, 95.9.

Residual differences of the proposed method with respect to the force platform were attributable to the use of a different methodological approach and not to physiological factors.

Moreover, Fig. 7 highlights that data calculated by our method had a lower dynamic range than those measured by a stabilometric platform (the negative linear trends occurred because data from our method were always used as left member of the calculated differences) and that greater differences were almost entirely localized at the extremes of the measuring interval.

The previous results allow to conclude that, in a rectangular region of about 8cm in the ML direction and 10cm in the AP direction, located at the centre of the system, the virtual system yields results that are in line with those measured by a force platform. The presence of some significant differences occurring in the central region was due to rapid orientation changes in the motion direction, thus making evident the differences between COG and COP.

@&#CONCLUSION@&#

We presented a low-cost virtual balance assessment system based on a 3D video camera and a 3D real time model reconstruction of the human body and its comparison with data collected by a stabilometric platform. The proposed system was effective in calculating the COG oscillations of a subject while the force platform measured the COP oscillations.

Though we calculated COG and compared it with the COP measured by a force platform, the proposed system represents quite well, and in real time (with high temporal resolution), the oscillations made by the analyzed subjects. Lower spatial resolution of the proposed method resulted where the direction of motion rapidly changed (mainly due to COG–COP differences).

The proposed system demonstrated good sensitivity in representing low displacements, as confirmed both by experimental data. In the same time, low displacements of the body were well represented also very far from the central equilibrium position (about 14cm away from the centre).

The virtual balance assessment system can be implemented at low cost (about 500$) and also used at home. In fact, a stabilometric platform, having a cost of about 10,000$ and being a professional equipment, has to be used in a clinical environment by an expert.

The proposed system can be easily installed and used: the user has just to push a button and to remain still on a given, fixed, position of the floor. Moreover, it requires a reduced space: the video-camera is screwed to the ceiling close to a little personal computer (without the screen) placed on a shelf. The analyzed subject has to stay still in front of the camera at 2–3m from it, when the system is in use (when using the system, this space has to remain free).

With the proposed system a complete screening of the equilibrium state of a subject can be repeated as frequently as needed and data can be easily remote on a web server and monitored through Internet. Moreover, the system does not require periodical recalibration, as is necessary for stabilometric force platforms.

The proposed simplified model can be used as a first step of a real time tracking algorithm to recognize and track, in real time, head, legs, arms and trunk (and related COGs): the proposed system could be used to emulate, besides a stabilometric platform, also optoelectronic equipment. In this case, it would be useless to apply any correction formulas (a simplified version, for the inverted pendulum model is reported in Eq. (1)) to the calculated data to obtain COP if we would accept to measure “free body” movement parameters (the COG–COP relationships would become very complex and the inverted pendulum model would be ineffective). The development of an efficient recognition and tracking algorithm for the single body parts is currently in progress.

No author has any conflict of interest with respect to this article.

@&#ACKNOWLEDGMENTS@&#

The authors wish to thank Mrs. Carmelita Marinelli for the help in the figures preparation and Prof. Stephane Perrey (Montpellier 1 University, Montpellier, France) for the loan of the stabilometric platform.

@&#REFERENCES@&#

