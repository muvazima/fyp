@&#MAIN-TITLE@&#Coverage-based resampling: Building robust consolidated decision trees

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Coverage-based resampling determines the number of samples to be used based on dataset’s class distribution.


                        
                        
                           
                           Consolidated trees achieve better results for most performance measures when using higher coverage values.


                        
                        
                           
                           CTC ranks first against multiple genetics-based and classical algorithms for rule induction.


                        
                        
                           
                           CTC combined with SMOTE tops state of the art techniques designed to tackle class imbalance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Comprehensibility

Consolidated decision trees

Class imbalance

Resampling

Inner ensembles

@&#ABSTRACT@&#


               
               
                  The class imbalance problem has attracted a lot of attention from the data mining community recently, becoming a current trend in machine learning research. The Consolidated Tree Construction (CTC) algorithm was proposed as an algorithm to solve a classification problem involving a high degree of class imbalance without losing the explaining capacity, a desirable characteristic of single decision trees and rule sets. CTC works by resampling the training sample and building a tree from each subsample, in a similar manner to ensemble classifiers, but applying the ensemble process during the tree construction phase, resulting in a unique final tree. In the ECML/PKDD 2013 conference the term “Inner Ensembles” was coined to refer to such methodologies. In this paper we propose a resampling strategy for classification algorithms that use multiple subsamples. This strategy is based on the class distribution of the training sample to ensure a minimum representation of all classes when resampling. This strategy has been applied to CTC over different classification contexts. A robust classification algorithm should not just be able to rank in the top positions for certain classification problems but should be able to excel when faced with a broad range of problems. In this paper we establish the robustness of the CTC algorithm against a wide set of classification algorithms with explaining capacity.
               
            

@&#INTRODUCTION@&#

In data mining, a classification problem occurs when an object needs to be assigned to a predefined group or class based on a number of observed attributes related to that object [1].

Class imbalance has been considered one of the main problems in data mining in recent years [2–6]. The class imbalance problem occurs when at least one of the classes (minority class/es) is underrepresented in the original training sample compared to the remaining classes. The imbalance can be either intrinsic (directly related to the nature of the data, such as the diagnosis of rare diseases) or extrinsic. Extrinsic imbalance can be caused by limitations in the data collection process [7]. Class imbalance is present in several real problems, such as medical diagnosis [8], insurance fraud detection [9], customer churn prevention [10], traffic incident detection [11] and DNA sequencing [12].

Class imbalance has a detrimental effect on classification algorithms that maximize overall accuracy [3]. In the presence of class imbalance, such algorithms might build a trivial classifier that classifies all examples as majority class, obtaining a high overall accuracy but misclassifying all minority class examples (which is usually the class of interest). This is the case with the well known C4.5 decision tree algorithm [13] and its pruning mechanism. This mechanism iteratively deletes leaf nodes by looking for the deletion that maximizes accuracy gain until no deletion increases accuracy. In the presence of class imbalance, the deleted branches are usually responsible for correctly classifying the minority class examples [14]. Class imbalance can also amplify the effects of other classification problems such as concept complexity [15], high dimensionality combined with small sample size [3] or small disjuncts [16].

The CTC (Consolidated Tree Construction) algorithm [17] was proposed for an insurance fraud detection problem where class imbalance was present [9]. CTC creates a set of subsamples from a training sample and builds a decision tree from each subsample in a similar manner to Bagging [18] but applying the ensemble process when building the tree by voting on the split on each of the tree’s nodes. Abbasian et al. [19] recently coined the term “Inner Ensembles” for similar procedures and suggested extending it to other algorithms, such as Bayesian networks and K-means. Unlike ensemble algorithms, the final model of the CTC algorithm is a simple decision tree understandable by humans. The mining of understandable patterns is a current trend in data mining, as highlighted in a recent special issue of a high-ranking journal in the field of artificial intelligence [6]. Consolidated trees are more stable and less complex than the C4.5 trees they are based on. Consolidated trees change far less when induced from different training samples and are thus more stable [20]. The complexity of the trees, represented as the amount of internal nodes, is smaller in consolidated trees. These features are important because, as Turney [21] and Domingos [22] pointed out separately “engineers are disturbed when different batches of data from the same process result in radically different decision trees. The engineers lose confidence in the decision trees, even when we can demonstrate that the trees have high predictive accuracy.” and “a single decision tree can easily be understood by a human as long as it is not too large”.

In the work presented in this paper a novel resampling methodology is proposed and applied to the CTC algorithm. This methodology uses the notion of coverage, the minimum percentage of instances from any class of the training sample present in the subsample set with a different class distribution, to determine the amount of subsamples needed. Thus, instead of setting a fixed amount, the number of subsamples is determined by the data set’s class distribution, the subsample type and the chosen coverage value. The greater the class imbalance present in the training set, the more subsamples are necessary to achieve the same coverage. The results achieved by CTC using this new resampling strategy are compared to those published by Fernández et al. [23]. They proposed a taxonomy of sixteen rule-based evolutionary algorithms, dividing them into 3 main categories and 5 families. The discriminating ability of the algorithms was tested in three different contexts: a set of 30 standard (mostly multi-class) data sets, 33 two-class imbalanced data sets and the same two-class data sets preprocessed with SMOTE to balance the class distribution. All the data sets were taken from the KEEL repository.
                        2
                        
                           http://sci2s.ugr.es/keel/datasets.php.
                     
                     
                        2
                      For each of the three contexts an intra-family comparison was performed and the best ranking algorithms of each family of the taxonomy were compared, along with a fixed set of six classical non-evolutionary classification algorithms. All twenty-two algorithms used in their work (whether rule-based or not) are explanatory, which makes them natural rivals to CTC. This makes that experiment an ideal environment to test CTC with the coverage-based resampling strategy.

The main contribution of this paper is the use of the notion of coverage. Depending on the difficulty of the problem (defined by the class distribution in the data set) and the characteristics of the subsamples to be created, the coverage determines the adequate number of samples to build consolidated trees. In previous works, CTC has never been used with data sets with such high degree of class imbalance and such small size. Coverage-based resampling ensures that the number of samples does not fall short of representing all classes to a minimum degree, independently of the class distribution. Furthermore, we have generalized this strategy in the context of multi-class data sets, where class imbalance is also present but usually not studied. In the analysis performed in this work in three classification contexts, a coverage value of 99% has been determined to be the most adequate for the CTC algorithm. Also, although applying SMOTE had previously never improved CTC’s performance in a significant manner, the combination of coverage-based resampling with the use of SMOTE has been able to do so.

In this work we want to establish CTC’s robustness by showing that it ranks in the top positions for different classification contexts compared against a wide range of algorithms, all with explaining capability. The significance of CTC’s performance compared to its competitors is backed up by performing rigorous statistical testing following the guidelines established in the field of machine learning research [24–26].

The rest of the paper is organized as follows. Section 2 gives an overview of the related work in the fields of class imbalance, tree and rule induction algorithms and the CTC algorithm. Section 3 presents the coverage-based resampling and states the hypothesis of this work. Sections 4 and 5 respectively describe the experimental setup and the analysis of results. Finally, Section 6 gives this work’s conclusions and details future work.

@&#RELATED WORK@&#

This section reviews the latest developments in decision tree and rule induction methods and techniques to solve the class imbalance problem. The last subsection reviews the research on the CTC algorithm that has led to the experiments presented in this paper.

In machine learning, sometimes the reason why a classifier makes a decision is as important as the accuracy of the decision itself. This is especially true for domains where the classifier works as a decision support system for humans, such as medical diagnosis and fraud detection. Some classification algorithms have a white box nature, where the decision of the classifier can easily be interpreted by a human. Decision trees and rule sets are classifiers of this type.

A classification tree, also known as a decision tree, is a set of conditions organized in a hierarchical structure. Instances are classified by navigating them from the root node down to a leaf, according to the outcome of the tests along the path [27].

Some of the earliest decision tree algorithms were CHAID [28] and CART [29]. Later Quinlan’s ID3 [30] and its successor C4.5 [13] were published. Decades later, C4.5 is still considered one of the top algorithms in machine learning [31]. These algorithms differ from each other in a number of ways, such as: the type of attributes they can handle, their split criteria when making decisions, the type of split made and the presence of a pruning mechanism. Oblique decision trees [32] build decision trees where each split is made based on more than one variable.

Rule induction algorithms formulate rules that aim to describe the concept of interest as a set of conditions for the attributes that describe the examples. Some rule induction methods such as C4.5-Rules [13] and PART [33] derive rules from decision trees. In other rule induction algorithms rule sets are formed from scratch by sequentially building rules using a separate-and-conquer strategy: Each rule covers a portion of the training sample, the examples covered by that rule are removed from the training sample and the next rule is built [34]. A rule set is able to separate examples belonging to the class of interest from the rest. Algorithms such as IREP [35] build rules from scratch. Cohen [36] proposed Ripper, which shows an improvement over IREP’s discriminating capacity without sacrificing much computational efficiency.

Rule-induction algorithms can use several techniques to build rules. One of these techniques is evolutionary algorithms. Rule-based techniques that make use of evolutionary algorithms are genetics-based machine learning (GBML) algorithms. In the past, GBML systems were classified into the Michigan and Pittsburgh categories. However not all evolutionary rule-based algorithms fall into those categories and recently a new taxonomy was proposed based on the representation of the chromosome of the associated evolutionary algorithm [23]. This taxonomy defines five categories, divided into three families. The first family encompasses those algorithms where the chromosome is a rule and has three subcategories that differ in their approach: the classic Michigan approach, the iterative rule learning approach and the genetic cooperative-competitive learning approach. In the next family, rule sets are encoded as chromosomes and this category is composed of those algorithms that follow the Pittsburgh approach. In the third and last family, chromosomes represent either decision trees or a rule extracted from the branch of the tree. Table 1
                         shows which GBML algorithms fall into which category.

Decision trees are very sensitive to their training sample. A small change in the training sample can lead to the construction of a completely different tree using the same algorithm. This property is called instability [37].

The most common way of improving the results of decision trees while reducing their instability is the construction of ensembles of classifiers. Ensembles consist of several classifiers whose individual decisions are combined. Ensemble classifiers usually give better results than the classifiers they are composed of [38], as long as the individual classifiers that compose the ensemble disagree with each other [39]. Unfortunately ensemble classifiers do not have an explaining capacity; they are of black box nature, since the deciding factors are lost in the process of combining the decisions made by individual classifiers, which can have completely different internal structures. The most widely known ensembles are Bagging [18] and Boosting [40].

There have been approaches to give explaining capacities to ensemble classifiers. Combined Multiple Models or CMM [22] is one such example. CMM first creates a Bagging ensemble. The original training sample and several artificial examples are classified with the ensemble and then those same examples with the label assigned by the ensemble are used as the training sample to build a simple classifier, in this case a C4.5-Rules rule set.

The consolidation of decision tree algorithms [17] also improves the discriminating capacity and stability of simple decision trees while keeping their explaining capacity by applying the ensemble methodology during the tree building process. The term “Inner Ensembles” has recently been used to refer to this kind of methodology [19].

As mentioned in the introduction, the class imbalance problem manifests when the number of examples in at least one of the classes represents a small fraction of the training set. Traditionally, approaches to solve the class imbalance problem have been divided into three main groups [41]. The first group of approaches involves data resampling approaches that modify the class distribution existing in the available sample by either undersampling or oversampling one or more classes, because studies show that the class distribution present in the original training sample is not always the best for the learning process [16,47,42]. The first resampling techniques randomly deleted majority class examples or randomly replicated minority class examples in order to balance the classes. These computationally economical techniques were followed by a set of intelligent resampling techniques that greatly improved classifier results, at a greater computational cost. The most widely used intelligent resampling technique is the Synthetic Minority Over-sampling Technique or SMOTE [43]. SMOTE is an oversampling technique where new synthetic minority class examples are created based on already existing minority class examples. To achieve this, a minority class example and some of its nearest minority class neighbors are selected. The difference between the feature vector of the minority class example and one of its neighbors is multiplied by a random number between 0 and 1 and added to the feature vector value of the minority class example, thus creating a new example between the minority class example and its neighbor. The number of times this process is repeated depends on the amount of oversampling needed. One of the identified drawbacks of SMOTE is that replication of examples close to examples from a different class can lead to an overlap between classes [44]. Since the appearance of SMOTE, several variations have been proposed. Some of these variations, such as Borderline-SMOTE [45], attempt to minimize the potential overlapping SMOTE can generate. In a similar fashion, SMOTE is sometimes combined with data cleaning techniques to also avoid overfitting. One such combination is SMOTE+ENN, where SMOTE is combined with Wilson’s Edited Nearest Neighbor. ENN removes examples whose class differs from two of its three closest neighbors. Regarding intelligent undersampling, EUSCHC (an undersampling method that performs evolutionary undersampling) offers an alternative to traditional techniques focused on data cleaning and achieves competitive results [46]. The wrapper paradigm proposed by Chawla [47] finds the optimum amount of undersampling or oversampling needed for a particular data set before applying the resampling that creates the training set. The main advantage of data resampling approaches is that they are independent of the learning algorithm.

The second group of approaches involves algorithmic approaches that develop or modify existing algorithms to make them sensitive to the class distribution. Some examples of algorithmic approaches are modifications to already existing algorithms [48–51] or systems based on one-class learning [52–54]. These approaches require the identification of why the original algorithm fails to correctly classify examples from a determined class. This reason can be highly dependent on the specific domain at hand.

The last group of approaches includes the cost-sensitive approaches that take the cost of different types of misclassification into account and try to minimize the total cost of the errors. The misclassification costs can either be directly introduced by domain experts or learned automatically. In data sets with class imbalance, the minority class is usually the class of interest and thus the misclassification costs for the minority class are higher. Cost sensitive approaches can be divided into two main groups, direct approaches and meta-learning. Direct approaches create or modify an algorithm so that the misclassification costs are used when building the classifier, such as tree building algorithm using these costs to decide which attribute to use for a split. Meta-learning preprocesses the training data to introduce costs or modifies the output of the classifier to take the costs into account. A widely known example of cost-sensitive learning is Domingos’ Metacost [55].

Lately, ensemble-based methods have proven to be strong competitors to solve the class imbalance problem. These methods usually combine a data resampling method or a cost-sensitive approach and an ensemble classifier. When data resampling is used in combination with ensembles, the resampling of the data is usually performed within each iteration of building the individual classifiers that make up the ensemble. Cost-sensitive ensembles work in a similar manner by introducing the cost minimization in the ensemble process. In both of these cases, the base algorithm of the ensembles remains unchanged. The ensemble classifiers are usually Bagging-based or Boosting-based, or a combination of both, building an ensemble of ensembles. A recent review on ensemble classifiers for data sets with class imbalance [56] concluded that SMOTEBagging [57] (SMOTE combined with Bagging) performs the best; with statistically significant differences compared to the rest of the algorithms considered except for RUSBoost [58] (random undersampling combined with Boosting).

As mentioned in the introduction, the Consolidated Tree Construction (CTC) algorithm was designed for a car insurance fraud detection problem with a high degree of class imbalance where an explanation was required. CTC creates a set of subsamples from a training sample and builds a decision tree from each subsample. Instead of building each tree independently, the decision on each split is voted on by all trees. All trees comply with the majority vote and make the same split regardless of their individual vote. The process is repeated until the trees agree to stop growing. Fig. 1
                         shows the process of building a consolidated tree. In consequence all trees are identical and, thus, unlike the output of traditional ensemble algorithms, the final model of the CTC algorithm is a simple decision tree understandable by humans. An in depth explanation of the CTC algorithm can be found in [17] and the algorithm is also available
                           3
                           
                              http://www.sc.ehu.es/aldapa/weka-ctc/.
                        
                        
                           3
                         as an official WEKA package (J48Consolidated).

The first resampling strategies used with consolidated trees consisted of building stratified subsamples proportional in size to the training sample (
                           
                              S
                              =
                              75
                              %
                           
                        ) and also using bootstrap samples. Although for some databases CTC performed better with bootstrap samples, stratified subsamples generally garnered better results [59]. Comparing CTC with C4.5, Bagging and CMM (Explanation extracted from Bagging) [22], CTC comes between Bagging and CMM with C4.5 coming last and with no statistically significant differences between Bagging and CTC [60].

Weiss and Provost [16] suggested the class distribution present in the training sample might not be the optimal one for the problem and proposed an optimal class distribution for several data sets for use with the C4.5 algorithm. Perez et al. [17] tested these data sets with CTC and their proposed optimal distribution and concluded that the optimal distribution for C4.5 and CTC are not the same. A later extension work [61] aimed to obtain the optimal class distribution for CTC in 30 data sets. The experiment combined class distributions ranging from 2% to 98% and multiple values for the number of samples (
                           
                              N
                              _
                              S
                           
                        ) parameter between 5 and 50. Fig. 2
                         belongs to that work and represents the average AUC value for CTC for any class distribution and value for 
                           
                              N
                              _
                              S
                           
                         and shows that for these data sets the best results are obtained using balanced or almost balanced class distributions. From that for point on, research focused on the use of balanced subsamples.

The experiments mentioned in this section required the number of subsamples for CTC (and Bagging and CMM) to be tuned for each data set or set of data sets. This tuning consisted of analyzing the performance of the algorithms with different subsample amounts, sometimes using a range as wide as twelve values between 3 and 200 samples. The work presented in this paper bases the number of subsamples used for a data set on the amount needed to represent a certain percentage of the training sample or coverage, removing the need for tuning of the number of subsamples. A recent paper on CTC [62] explored this idea and achieved competitive results. However, the methodology used in that paper only used the ratio between sample and subsample sizes and did not take the data set’s class distribution into account and only tested it in one of the three classification contexts analyzed in this paper.

Creating balanced subsamples without oversampling the minority class requires undersampling the majority class. As a consequence, resampling imbalanced data sets results in subsamples where the majority class is only represented in a small proportion. Previously used 
                        
                           N
                           _
                           S
                        
                      values, ranging from 3 to 200, might not be enough to cover a significant portion of the majority class in imbalanced data sets, while the highest values are completely unnecessary and time consuming for the most balanced problems. Coverage-based resampling is a strategy that adapts to the class distribution present in the data set and computes a particular number of samples (
                        
                           N
                           _
                           S
                        
                     ) for a data set and coverage value. In previous works it has been observed that CTC works best when most of the information of the original sample is covered by the set of subsamples. We therefore hypothesize that CTC will work best with high coverage values that minimize the loss of information when resampling.

Coverage represents the probability of any example in the training sample being in at least one subsample; in other words, the smallest percentage of examples from any given class of the original training sample covered in the set of subsamples generated. When subsamples have a different class distribution from the original training sample each class will be represented to a different degree in a subsample. This degree depends on the use of replacement when creating subsamples and the ratio between the number of examples from a class in the subsamples and the number of examples from the same class in the original sample. Depending on this ratio, the subsampling process will favor some classes less than others. We define the least favored class as the one with the lowest ratio of instances in a subsample to instances in the training sample. Since the goal of coverage is to ensure a minimum representation for all classes, the probability of an example from the least favored class being present in a given subsample will be used as a basis. The probability for examples from any other class will be higher than that.

For example, let us consider the experiment presented in this article, specifically the part regarding imbalanced two-class data sets. In this experiment the subsamples are fully balanced by randomly undersampling the majority class without replacement. One of the subsample sizes we use is the double of the number of minority class examples in the original training sample. This means that each of these subsamples will fully cover the minority class. However, each subsample is not able to cover the majority class entirely, and the greater the class imbalance present in the original training sample the greater number of subsamples that will be needed to cover a specific percentage of examples. Fig. 3
                      shows how different subsample numbers translate into different coverage values. In this Figure balanced subsamples are created by randomly undersampling the majority class from a training sample with 33% of minority class examples.

Let 
                        
                           
                              
                                 t
                              
                              
                                 lfc
                              
                           
                        
                      be the probability of an example of the least favored class (lfc) being present in a subsample. As mentioned, this probability is influenced by this type of subsampling. Bauer and Kohavi [63] already published the probability for bootstrap samples. In the work presented in this paper we modify the class distribution, do not use replacement, and the probability is calculated by dividing the number of examples of the least favored class in a subsample by the number of examples of the least favored class in the training sample.
                        
                           
                              
                                 
                                    t
                                 
                                 
                                    lfc
                                 
                              
                              =
                              
                                 
                                    lfClassExamplesOnSubsample
                                 
                                 
                                    lfClassExamplesOnOriginalSample
                                 
                              
                           
                        
                     
                  

Then, the probability of an example of the lfc not being present in a subsample is:
                        
                           
                              1
                              -
                              
                                 
                                    t
                                 
                                 
                                    lfc
                                 
                              
                           
                        
                     
                  

The generation of each subsample is an independent event, so the probability of an example not being in any of the 
                        
                           N
                           _
                           S
                        
                      subsamples is:
                        
                           
                              
                                 
                                    (
                                    1
                                    -
                                    
                                       
                                          t
                                       
                                       
                                          lfc
                                       
                                    
                                    )
                                 
                                 
                                    N
                                    _
                                    S
                                 
                              
                           
                        
                     
                  

Thus, the probability of an example being in at least one of 
                        
                           N
                           _
                           S
                        
                      subsamples or the coverage is:
                        
                           
                              coverage
                              =
                              1
                              -
                              
                                 
                                    (
                                    1
                                    -
                                    
                                       
                                          t
                                       
                                       
                                          lfc
                                       
                                    
                                    )
                                 
                                 
                                    N
                                    _
                                    S
                                 
                              
                           
                        
                     
                  

Since we seek to work with fixed coverage values and want to compute the amount of subsamples 
                        
                           N
                           _
                           S
                        
                      for a specific coverage, we have to solve:
                        
                           
                              N
                              _
                              S
                              =
                              ⌈
                              
                                 
                                    log
                                 
                                 
                                    1
                                    -
                                    
                                       
                                          t
                                       
                                       
                                          lfc
                                       
                                    
                                 
                              
                              (
                              1
                              -
                              coverage
                              )
                              ⌉
                           
                        
                     
                  

As an example, the abalone19 imbalanced two-class data set has 4174 examples. The experiment uses a 5-fold cross validation, meaning that 4/5 of the sample go into the training sample and the other 1/5 of the sample is used as the test sample each time. For this data set the training sample has 3340 examples. The class distribution for the minority class is 0.77% in this case, meaning that 26 examples in the training sample belong to the minority class while the other 3314 belong to the majority class. A subsample type used in this paper has the same number of examples for each class, which is the size of the minority class in the training sample. Thus, in this case the subsamples have 52 examples (26 from each class) so each that subsample has all the minority class examples but only a few of the majority class examples. That makes the majority class the least favored class. Thus, in this case lfClassExamplesOnSubsample is 26 while lfClassExamplesOnOriginalSample is 3314. From this we calculate 
                        
                           
                              
                                 t
                              
                              
                                 lfc
                              
                           
                        
                      to be 0.0078. Thus, if we wanted to obtain a coverage of 99% (i.e. 0.99), for the abalone19 data set we calculate that 585 subsamples would be required.

The use of coverage brings a clear improvement to the computational cost of building consolidated trees. Instead of testing a range of 
                           
                              N
                              _
                              S
                           
                         values, the coverage allows the direct use of a specific number. This subsection analyzes the performance improvement brought by the use of coverage.


                        Fig. 4
                         shows the relationship between the number of subsamples used and the time required to build a consolidated tree in the context of standard data set classification by averaging the subsample numbers and times for the 30 standard data sets. The behavior is almost identical in the other classification contexts and figures for the subsample/time relationship for those contexts are provided in the website, with the additional material for this article.
                           4
                           
                              http://www.sc.ehu.es/aldapa/2014/coverage-ctc.
                        
                        
                           4
                         Each point in the chart relates to a coverage value. The position of the point on the X axis represents the average number of subsamples used for that coverage while the position on the Y axis represents the average time to build trees with that coverage value. The figure shows that tree construction time increases linearly with the number of subsamples used.

In previous works, the number of subsamples to be used with CTC was found by scanning though a set of predefined 
                           
                              N
                              _
                              S
                           
                         values. Thus, if x were the average time taken to build a consolidated tree for each subsample the tree uses and we considered a range of values 
                           
                              N
                              _
                              S
                              =
                              {
                              3
                              ,
                              5
                              ,
                              10
                              ,
                              20
                              ,
                              30
                              ,
                              40
                              ,
                              50
                              ,
                              75
                              ,
                              100
                              ,
                              125
                              ,
                              150
                              ,
                              200
                              }
                           
                        , the execution time for this search would amount to:
                           
                              
                                 
                                    
                                       T
                                    
                                    
                                       exec
                                    
                                 
                                 =
                                 (
                                 3
                                 +
                                 5
                                 +
                                 10
                                 +
                                 20
                                 +
                                 30
                                 +
                                 40
                                 +
                                 50
                                 +
                                 75
                                 +
                                 100
                                 +
                                 125
                                 +
                                 150
                                 +
                                 200
                                 )
                                 x
                              
                           
                        
                     

Using a predefined coverage value, a single 
                           
                              N
                              _
                              S
                           
                         value is computed and the execution time to build a consolidated tree can be formulated as follows:
                           
                              
                                 
                                    
                                       T
                                    
                                    
                                       exec
                                    
                                 
                                 =
                                 Kx
                                 ;
                                 K
                                 =
                                 
                                    max
                                 
                                 (
                                 3
                                 ,
                                 N
                                 _
                                 S
                                 (
                                 coverage
                                 )
                                 )
                              
                           
                        
                     

In order to make this clearer we will show the numbers for one of the most imbalanced and the most balanced standard data sets used in this paper (nursery and iris respectively). From our empirical tests we know x to be 26.02ms for the nursery data set. As Table 2
                         shows, scanning through all 
                           
                              N
                              _
                              S
                           
                         values would require about 21024.16ms while directly using the 
                           
                              N
                              _
                              S
                           
                         value computed from a specific coverage value (99% for example) would only need 1925.48ms (an improvement of 1092%). In the case of balanced data sets the improvement is greater. If the data set is balanced, each subsample covers a greater portion of the whole training sample and thus higher coverage values are reached with much less subsamples than those in the predefined range of subsample numbers. For the iris data set x is 5.49ms and scanning through the predefined set of subsample numbers would require 4435.92ms while using the fixed coverage value only takes 32.94ms to build the consolidated tree (an improvement of 13467%).

This section will describe the structure of the study, introduce the analyzed data sets and present the experimental methodology.

This paper presents the results of the CTC algorithm compared to those published by Fernández et al. [23], and so the experiment follows the same structure. Their work was divided into two main contexts: analysis of standard (mostly multi-class) data sets and analysis of very imbalanced two-class data sets. The part on two-class data sets was further divided into two parts, analysis of the original data sets and analysis of the data sets preprocessed with SMOTE. The performance metrics used for the standard data sets were kappa and accuracy, and GM (the geometric mean of 
                           
                              
                                 
                                    TP
                                 
                                 
                                    rate
                                 
                              
                           
                         and 
                           
                              
                                 
                                    TN
                                 
                                 
                                    rate
                                 
                              
                           
                        ) for the two-class imbalanced data sets.

Our work, and thus the presentation of our results and comparison with theirs, follows the same structure in this paper.

For our work we have used the exact same training/test splits used and published by Fernández et al. in the KEEL repository.
                           5
                           
                              http://sci2s.ugr.es/gbml/.
                        
                        
                           5
                        
                     

The standard data sets are 30 databases from the UCI repository. Most of these data sets are multi-class and they present a broad range of class distributions.

The imbalanced data sets consist of 33 data sets from the UCI repository, also with a broad range of class distributions, with minority class presence ranging from 0.77% to 35.51%.


                        Table 3–5
                        
                        
                         summarize the features of standard, imbalanced and imbalanced data sets preprocessed with SMOTE respectively. Individual numbers for each data set are in the additional material for this paper.

@&#METHODOLOGY@&#

The study presented in this paper uses a 5-run 5-fold cross-validation methodology, the same as that used by Fernández et al., even using the same training/test partitions provided at the KEEL repository. Two different subsample sizes were used for CTC, all subsamples having a balanced class distribution. The first, referred to as sizeOfMinClass, was proposed by Weiss and Provost [16]. These subsamples have a size equal to the number of minority class examples in the original training sample. The other subsample size, called maxSize multiplies the size of the previous subsamples by the number of classes. This subsample size was chosen because it is the biggest size possible for balanced class distribution subsamples without oversampling the minority class and it is also the most widely used strategy when random undersampling is applied.

For the CTC algorithm, some of the trees in our experiments were unpruned. As mentioned in the introduction, in the presence of class imbalance there is the risk of decision trees ending up with just one leaf when pruning (a root leaf). It makes no sense to use this kind of tree when the performance metric is GM, one of the performance metrics used in this study. Trees with just one node will classify all test examples as either positive (minority class) or negative (majority class). Thus, either 
                           
                              
                                 
                                    TP
                                 
                                 
                                    rate
                                 
                              
                           
                         or 
                           
                              
                                 
                                    TN
                                 
                                 
                                    rate
                                 
                              
                           
                         will be zero, meaning that GM will also be zero. Table 6
                         shows an example of this issue. For the first fold of the glass5 data set the percentage of examples of the minority class for the test set is 0.0465. The pruning process deletes nodes until the error cannot be reduced anymore, in this case until the tree is just a single root node and this tree classifies all examples as negatives. In consequence the error is reduced from 0.1395 to 0.0465 (the minority class’ a priori probability) and GM is reduced from 0.9239 to 0 because all positive examples are also classified as negative, which results in the true positive rate (one of the elements of GM) being zero. Because of this, whenever pruning resulted in a tree with just the root node the unpruned tree was used. For raw imbalanced classification 27 data sets produced at least some unpruned trees. However, the presence of unpruned trees is minimal in most cases. The data set with the most is abalone, with 25% unpruned trees. For imbalanced classification preprocessed with SMOTE, unpruned trees were present in 22 problems; 3 producing more unpruned than pruned trees.

For each of the three contexts we first analyzed the effect the changes in coverage value and subsample size have on CTC. Then we continued by comparing the performance of CTC against the algorithms used by Fernández et al. in their study. These algorithms were divided into two main groups: 16 evolutionary algorithms (XCS, UCS, SIA, HIDER, CORE, OCEC, COGIN, GIL, Pittsburgh genetic interval rule learning algorithm or Pitts-GIRLA, DMEL, GAssist, OIGA, ILGA, hybrid decision tree-genetic algorithm or DT-GA, oblique decision trees or Oblique-DT, and TARGET) and 6 classical algorithms (CART, AQ, CN2, C4.5, C4.5-Rules and Ripper). Table 1 shows the evolutionary algorithm taxonomy. The performance measures used are those used by Fernández et al.: kappa and accuracy for standard data sets and the geometric mean for imbalanced data sets.

The experimentation in this paper applies the coverage based methodology to the CTC algorithm. The coverage values used in this study consist of eleven fixed values: 10%, 20%, 30%, 40%, 50%, 75%, 90%, 95%, 99% and 99.9%. CTC requires a minimum of three subsamples, so a fixed number of three subsamples is also used. Furthermore, whenever the number of subsamples for a given coverage, subsample size and data set falls below three, three subsamples are used. For our experiment, where subsamples have a balanced class distribution, the least favored class used to compute the number of subsamples for a coverage is the majority class, while the subsample size is that of the minority class in the training sample or double its size, depending on which subsample size we want to create. There are two exceptions to this rule:
                           
                              •
                              The standard data sets are mostly multi-class. Sometimes the least populous class (the minority class) has as little as one example. This makes sizeOfMinClass subsamples impossible and maxSize subsamples too small. Because of this, a rule has been put in place to ensure that the number of minority class examples found in a subsample is greater than or equal the 1% of the total number of examples in the data set when creating sizeOfMinClass subsamples and 2% when creating maxSize subsamples. In these cases random oversampling is applied to the minority class until this minimum is reached. Only three data sets out of thirty need this exception to the general rule: abalone, car and ecoli.

The iris standard data set is unique because it is fully balanced. It is composed of 3 classes, each with 50 examples. Using the general rule to build maxSize subsamples, a single subsample would already contain all of the examples of the original training sample, resulting in a coverage of 100%. In this case, in order to obtain a set of different subsamples to build the consolidated trees, we reduce the size of the subsamples produced in order to guarantee that the subsamples will be stratified: i.e. the subsamples will also be balanced. Thus, in the case of the iris standard data set maxSize the subsample size will be 55% that of the training sample. This specific ratio was chosen because it is the average ratio between subsamples and the original training sample for all data sets.

The following tables summarize the subsample numbers used for some coverage values. Table 7
                         refers to sizeOfMinClass and maxSize subsamples of standard data sets while Table 8
                         refers to sizeOfMinClass and maxSize subsamples of imbalanced data sets. These are the same as the numbers for the imbalanced data sets preprocessed with SMOTE, since the ratio of number of examples of the least favored class in a subsample to number of examples of the least favored class in the original training sample is the same. For the detailed tables with all subsample numbers for all coverage values and data sets, refer to the website with the additional material for this article.

The total number of consolidated trees in this study is 52,800 ((30 standard data sets+33 imbalanced data sets+33 SMOTE-preprocessed imbalanced data sets)
                        
                           
                              ×
                           
                        
                        5 runs
                        
                           
                              ×
                           
                        
                        5 folds
                        
                           
                              ×
                           
                        
                        11 
                           
                              N
                              _
                              S
                           
                         values (10 coverage values plus a fixed value of 3 subsamples)
                        
                           
                              ×
                           
                        
                        2 subsample sizes) with subsample numbers ranging between 605 and 144,980 per data set giving a total of 1,028,750 subsamples.

@&#RESULTS@&#

In this section of the article we will analyze the performance of CTC using the coverage based resampling and compare its results to several classification algorithms.

As Demšar pointed out [24], the machine learning research community has become aware of the need to statistically validate their results. The tests proposed by Demšar were later improved by García et al. [25,26]. The Friedman Aligned Ranks test (an improvement introduced by García et al. [26]) includes a weighting system, giving more weight to the data sets that are harder to classify. This statistical test is used to prove the significance of the results achieved by CTC in comparison to its competitors in this study. In the work we are comparing to, Fernández et al. [23] compared 
                        
                           n
                           ×
                           n
                        
                      algorithms and used the Shaffer post hoc test to find statistically significant differences between pairs of algorithms. In our work we are comparing our algorithm (CTC) to the competitors and thus the 
                        
                           1
                           ×
                           n
                        
                      test (Holm’s test) we have used is better suited. When 
                        
                           1
                           ×
                           1
                        
                      comparisons were needed, the Wilcoxon Signed Ranks test [24] was used.

Before comparing the discriminating capacity of CTC against the evolutionary and classical algorithms we perform multiple intra-CTC comparisons for all subsample size and coverage value combinations. For purposes of simplicity the tables regarding the intra-CTC comparisons have been moved to the additional material for this article.

In summary, when comparing subsample sizes, sizeOfMinClass versus maxSize, for any of the three classification contexts, whenever there are statistically significant differences, according to the Wilcoxon test, between the subsample sizes for the same coverage it is in favor of the bigger (maxSize) subsamples. Using the maxSize subsample size we continue by comparing the performance of CTC with different values for the coverage. We decided to base the decision on which coverage to use on the average performance of CTC for each value. Fig. 5
                      represents the performance for kappa and accuracy for standard data set classification. In both cases there is an upward trend with both performance measures peaking at a coverage of 99%. Figs. 6 and 7
                     
                      display the average values of the geometric mean (GM) and both the true positive (
                        
                           
                              
                                 TP
                              
                              
                                 rate
                              
                           
                        
                     ) and true negative (
                        
                           
                              
                                 TN
                              
                              
                                 rate
                              
                           
                        
                     ) rates for imbalanced data set classification with and without SMOTE. In both cases the performance follows a similar trend, with the true positive rate increasing with coverage, while the true negative rate decreases at a greater rate, causing the geometric mean to also decrease. It should be noted, however, that in the presence of class imbalance, the positive class is usually of most interest. Depending on the context, the true positive rate is much more important than the true negative rate. These performance measures are the ones that are used later in this section to compare CTC to the rest of the algorithms. There are however other performance measures, such as the F1-Score and MCC (Matthews Correlation Coefficient [64]) that are widely used in the literature in the presence of class imbalance. Figs. 8 and 9
                     
                      show the performance of CTC for those metrics and different coverage values. It is clear from these figures that CTC’s performance is better with higher coverage values. In light of this we have chosen a coverage of 99% with maxSize subsamples as representative of the CTC algorithm.

The following subsections compare the performance of CTC against a set of algorithms already compared by Fernández et al. [23] using the Friedman Aligned Ranks test to look for statistically significant differences among the algorithms compared and the Holm post hoc test to find between which pairs of algorithms those differences exist. For each of the classification contexts we compare CTC against the best algorithm of each of the 5 genetics-based algorithm families and 6 classical algorithms. Some of the evolutionary algorithms for each family differ from context to context while the classical algorithms stay the same: CART, AQ, CN2, C4.5, C4.5-Rules and Ripper. In order to show the robustness of the CTC algorithm, we also tested it globally across all data sets of the three classification contexts. The final subsection compares CTC’s performance to state-of-the art algorithms proposed to fight class imbalance.

In this subsection we compare the performance of CTC against a set of evolutionary and classical algorithms on standard classification data sets. The average results of the five evolutionary algorithms, six classical algorithms and CTC over 5-runs of a 5-fold cross validation are presented in Tables 9 and 10
                        
                         for the performance measures kappa and accuracy respectively. The results achieved by CTC for each data set can be found in the additional material for this paper.

The evolutionary algorithms ranking best for each family in the taxonomy are XCS, SIA, OCEC, GAssist and Oblique-DT. Using the Friedman Aligned Ranks test, CTC ranks 1st for kappa and 4th for accuracy with GAssist ranking first. The average ranks for this test and adjusted p-values computed using the Holm post hoc test are shown in Tables 9 and 10. The results of this test indicate the presence of statistically significant differences in both cases with p-values of 0.0035 (test statistic value 27.80) and 0.0036 (test statistic value 27.70). The algorithms in the tables are ordered by their rank according to the Friedman Alligned Ranks test and the dashed line marks the point where statistically significant differences with the control (best ranking) algorithm begin to appear. Lower ranks indicate better performance. These tests have been performed using the KEEL platform. For the kappa measure, the Holm test finds statistically significant differences between CTC and SIA, CART, OCEC, CN2, C4.5-Rules and AQ. Unfortunately, when executing the Friedman Aligned Ranks test followed by the Holm post hoc test, the KEEL platform automatically uses the best ranking algorithm as the control algorithm for the Holm test. As CTC does not rank first for accuracy in this case it cannot be used as the control algorithm. However, using GAssist as the control algorithm, the Holm test cannot find statistically significant differences with CTC (p-value=1). Following the steps of other authors [56], a Wilcoxon test was performed between GAssist and CTC to further compare them. The results of the Wilcoxon tests are provided in Table 11
                        , with no statistically significant differences found between CTC and GAssist.


                        Fig. 10
                         gives a visual representation of the ranks computed by the Friedman Aligned Ranks test for standard data sets and the relative distance between algorithms. The algorithms above the black line are those for which the statistical test has been unable to find significant differences with the best ranking algorithm. The distance between the best and worst performing algorithms is wide, indicating a substantial difference between the algorithms, which is confirmed by the Friedman Aligned Ranks tests. For both of the performance measures the ranking positions are similar for most algorithms. The first four ranking algorithms are the same in both cases. For these four algorithms the same order is kept for both measures, except for CTC moving to fourth position for accuracy. The relative distances among these algorithms are similar in both cases. The biggest rank difference between metrics is for C4.5-Rules, which performs much worse for kappa than for accuracy. Ripper and OCEC also increase their distances from the best algorithms, but to a lesser degree. In summary, the two metrics do not give a similar result for all algorithms. It should be noted that in some of the standard data sets class imbalance is present. For example in 12 of the 30 standard data sets the ratio of examples in the class with least examples to the class with most examples is less than 0.2. With such an imbalance an algorithm could build a trivial classifier and still obtain a good accuracy measure. We feel that kappa is better suited for this context.

In summary, for the standard classification context using multi-class data sets, CTC places 1st for kappa and 4th for accuracy when compared against several evolutionary and classical algorithms, with no statistically significant differences for accuracy with GAssist (the best ranking algorithm).

For the imbalanced data set classification, the performance measure used by Fernández et al. [23] is the Geometric Mean (GM for short) of the true positive and true negative rates because it gives the same weight to the accuracy of both classes.

In this case, the evolutionary algorithms ranking best for each family in the taxonomy are UCS, SIA, OCEC, GAssist and Oblique-DT. Table 12
                         shows the average results for all algorithms and their rank computed using the Friedman Aligned Ranks test. CTC obtains both the best average GM and best ranking for this classification context, with statistically significant differences (according to the Holm test) with GAssist, CART, SIA, OCEC, UCS, AQ and CN2. This is significant since GAssist was the best ranking algorithm using accuracy for standard data sets, without statistically significant differences with CTC. This shows how robust CTC is for the standard and imbalanced classification contexts as opposed to other algorithms only ranking in top positions for certain problems. The Friedman Aligned Ranks test computes a p-value of 0.0015 (test statistic value 30.21).

The average ranks for imbalanced classification are also represented in Fig. 11
                        . CTC ranks first for the classification of imbalanced data, closely followed by Ripper. Most of the algorithms fall in the first half of the interval between the best and worst algorithm, which suggests that the remaining algorithms get most of the worst positions for each data set. In fact the worst possible average rank for an algorithm in this experiment is 380 and CN2 gets an average ranking of 341.1818. This is the only context where C4.5-Rules ranks above C4.5, the algorithm it is based on. Statistical significance between the results for CTC and the other algorithms appears at a point in the gap between C4.5 and GAssist. These considerable gaps also translate into big jumps in the p-values in Table 12.

This subsection deals with the result of the analysis of CTC in the context of imbalanced two-class data sets preprocessed with SMOTE until the class distribution is balanced. The performance measure used for this classification context is also the geometric mean of the true positive and true negative rates.

Before proceeding with the rest of the experiment we wanted to test if there are differences between CTC with and without the use of SMOTE. Previous studies failed to find such significant differences but this is the first time this type of experiment has been performed with data sets of such a high degree of class imbalance and balanced subsamples. In this case, a Wilcoxon test does show statistically significant differences in favor of CTC on data sets preprocessed with SMOTE as seen in Table 13
                         with a p-value of 0.013.

We follow with the comparison between CTC and the evolutionary and classical algorithms. The evolutionary algorithms chosen by Fernández et al. for this context are XCS, SIA, CORE, DT-GA and GAssist. In this case, CTC does not place first for the average geometric mean nor the rank computed by the Friedman Aligned Ranks test, with XCS coming top for both while CTC places third. The Friedman Aligned Ranks test computes a p-value of 0.0018 (test statistic value 29.59). The Holm post hoc test uses XCS as the control algorithm and it is unable to find statistically significant differences between XCS and CTC, as seen in Table 14
                        . We complete this analysis with a Wilcoxon test between XCS and CTC and, as seen in Table 15
                        , this test is also unable to find statistically significant differences.

The average ranks of Table 14 are visually represented in Fig. 12
                        . In this case, XCS ranks first and is closely followed by the next six algorithms. Statistically significant differences between XCS and the other algorithms begin to appear in the gap between DT-GA and Ripper. In this context AQ ranks in a lower position than CN2 did without SMOTE, getting an average rank of 350.4394 from a maximum of 380.

In order to compare the performance of algorithms without and with SMOTE we can refer to Figs. 11 and 12 and their counterparts Tables 12 and 14. Only 9 of the 12 algorithms coincide between the lines representing the rankings of the two contexts, as 3 of the GBML categories have different winners for different contexts. The best ranking algorithm when SMOTE is used is XCS, an algorithm that was not present in the rank where SMOTE was not used. XCS and GAssist seem to be the two algorithms benefiting most from the preprocessing while the two best ranking algorithms from the previous subsection, CTC and Ripper to a greater extent, fall into lower positions. This loss in rank should, however, not be understood as performance diminishing. In fact, looking at the average performance values in Tables 12 and 14 it is clear that the performance of these two algorithms improves with the use of SMOTE, with statistically significant differences in the case of CTC. The true reason for CTC losing ranking positions is that other algorithms benefit more from the use of SMOTE.

In summary, for the context of classifying imbalanced two-class data sets preprocessed with SMOTE, CTC ranks 3rd behind XCS and GAssist with no statistically significant differences between XCS and CTC according to the Holm and Wilcoxon tests. Also, applying SMOTE before using CTC does show a statistically significant improvement.

Finally, we proceed to compare CTC with the evolutionary and classical algorithms across the three contexts, a total of 96 data sets, in order to analyze the robustness of the CTC algorithm. The classical algorithms are always the same six. However, the evolutionary algorithms chosen from each of the families proposed by Fernández et al. vary from context to context. Some algorithms never stand out for any of the three problems while others only classify as best of the family in some contexts. For this final comparison all evolutionary algorithms winning at least one intra family comparison and all classical algorithms will be used. Thus the list of 15 algorithms compared (8 genetics-based, 6 classical and CTC) is composed of UCS, XCS, SIA, CORE, OCEC, GAssist, DT-GA, Oblique-DT, C4.5, C4.5-Rules, CART, RIPPER, AQ, CN2 and CTC.

For the standard data set comparison two performance measures were used, kappa and accuracy. However, when combining the results of the three experiments, the two metrics for the same experiments should not be used since that would give the standard classification experiment twice the weight of the other contexts. Although standard data sets are not labeled as such, some of them have a very skewed class distribution (in other words, a great class imbalance). As the literature shows us [65,14,3], accuracy is not a suitable metric when class imbalance is present. Thus, this global comparison among algorithms across classification contexts is performed using kappa for standard data sets and the geometric mean for imbalanced data sets, both preprocessed with SMOTE and not preprocessed.

A Friedman Aligned Ranks test is used to rank the performance of the 15 algorithms. Table 16
                         lists the computed ranks. This table also shows the adjusted p-values of the Holm post hoc test since the Friedman Aligned Ranks tests indicates the presence of statistically significant results with a p-value of 0.000000000074 (test statistic value 89.04). CTC ranks first and is the control algorithm, with the Holm test finding statistically significant differences with half of its competitors: DT-GA, SIA, UCS, CART, OCEC, CORE, AQ and CN2. It is interesting to note that some algorithms rank in the top positions for a specific context but fall to the mid-lower sections of the list when comparing them across all three classification problems. XCS is the most notable example, ranking first for classifying imbalanced data sets preprocessed with SMOTE and 2nd or 3rd for standard data sets (depending on the measure), not winning the intra-family comparison for imbalanced data sets, while ranking 6th overall. CTC, which ranks 1st globally for imbalanced classification and standard classification measured using kappa and is in the top positions in the rest of comparisons proves to be the most robust across a wide range of problems.

Using accuracy, instead of kappa, as the performance measure for standard classification makes a quantitative difference but no qualitative differences. The average rank computed by the Friedman Aligned Ranks test is not as low, but CTC still ranks first globally.

In Fig. 13
                         the ranks for global classification and the distances between them are graphically displayed. CTC ranks first overall, followed by C4.5 (its base classifier), which is closely followed by the next 5 algorithms. From that point on there are statistically significant differences between CTC and algorithms ranking worse than Oblique-DT. With a maximum rank of 1392.5, AQ and CN2 get an average rank of 1074.1094 and 1110.0156 respectively, indicating these algorithms are not appropriate for the classification contexts used in this paper.

Analyzing CTC’s global performance, it is noticeable that balanced subsamples boost the algorithm’s performance. We hypothesize that using balanced subsamples hides the class imbalance from each individual tree and helps the decision making mechanism (C4.5’s entropy) create better discriminating splits, and that CTC, with its inner ensemble approach can combine the individual decisions from each subsample in an effective manner. This is because a high coverage value ensures that the information loss when subsampling is minimal. This behavior extends to each of the three contexts analyzed where CTC ranks in top positions. It is also observed that the oversampling of the training sample with artificial examples until it is balanced is also beneficial for CTC. However, even with statistically significant differences between the absence or use of SMOTE, CTC still gains less than most other algorithms from its use.

Considering that as shown in the coverage section, with an increase in coverage, and thus an increase in the number of subsamples, the construction of consolidated trees requires a greater computational effort, we decided to also compare the discriminating capacity of CTC against the rest of the algorithms with a coverage of just 50%. While the differences in ranks are smaller, CTC covering half of the least favored class’ examples in the training sample still ranks first globally while maintaining statistically significant differences with almost the same number of rivals and almost no ranking positions change, with only SIA and DT-GA swapping positions.

In view of the good results CTC obtains compared to GBML and classical algorithms in the context of class imbalance, we decided to compare the results of CTC with several techniques proposed recently to specifically tackle the class imbalance problem. We looked in the recent literature for papers whose results were publicly available and where the same 33 imbalanced data sets were used. We identified three papers that met these criteria.

One of the papers offered a global ranking of all the algorithms compared in that paper. From this paper we selected the two best ranking algorithms, since there were no statistically significant differences between them. The other two papers offered multiple independent comparisons but did not compare the winners of each. From these papers the best technique for each individual comparison was taken.

In the first paper [66], three preprocessing techniques were used with the C4.5 and PART algorithms. The preprocessing techniques were SMOTE, SMOTE+ENN and EUSCHC. In both cases EUSCHC obtained the best results, so EUSCHC+C4.5 and EUSCHC+PART were chosen to be compared with CTC. In the second paper [56], a taxonomy of ensemble methods was proposed and later compared hierarchically. The final comparison in this paper compared the best algorithms for each ensemble category. In this case the best ranking algorithm was SMOTEBagging, with statistically significant differences with the other competitors except for RUSBoost. Therefore, both of them were chosen to be compared with CTC. In the third paper [67] a comparison similar to the first paper was performed, testing four preprocessing methods on the C4.5, SVM, kNN and FH-GBML algorithms. The preprocessing methods in this paper were SMOTE, SMOTE+ENN, CS (cost sensitive variant), and Wrapper-based variants: Wr_SMOTE, Wr_US (Wrapper Undersampling) and Wr_SMOTE+ENN. From this paper the best preprocessing method for each algorithm was chosen, these being SMOTE+ENN+C4.5, SMOTE+ENN+SVM, CS+FH-GBML and Wr_SMOTE+kNN.

All three papers published the results for the simplified definition of the area under the ROC curve (AUC) presented in the following equation:
                           
                              
                                 AUC
                                 =
                                 
                                    
                                       1
                                       +
                                       
                                          
                                             TP
                                          
                                          
                                             Rate
                                          
                                       
                                       -
                                       
                                          
                                             FP
                                          
                                          
                                             Rate
                                          
                                       
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        For a more in-depth analysis of the AUC metric refer to [68].

Since all of these techniques consist of a preprocessing method designed to tackle the class imbalance problem and a classification algorithm and we also know CTC performs better with SMOTE preprocessing, we decided to add both CTC and SMOTE+CTC to the comparison. The performance values of this comparison can be found in the additional material for this article. The tests used for this comparison were the same used in the rest of this paper: the Friedman Aligned Ranks test to detect statistically significant differences and Holm’s test as the post hoc test to calculate p-values between algorithm pairs. The average AUC values, average ranks, and adjusted p-values are displayed in Table 17
                        .

The Friedman Aligned Ranks test obtains a test statistic of 30.26 (p-value 0.0004) indicating the presence of statistically significant differences. The results in Table 17 are ordered by rank, with the best ranking algorithms on top and the dashed line represents the point where statistically significant differences between the best ranking algorithm and the rest begin to appear. Fig. 14
                         is a visual representation of the average ranks. These elements show that SMOTE+CTC is both the best performing and best ranking technique, with significant differences with all competitors except for those preprocessed with EUSCHC. CTC without preprocessing with SMOTE has the fourth best average performance but ranks seventh according to the statistical test. This leads to the same conclusion as in the class imbalance subsection; that is, there are statistically significant differences in favor of preprocessing with SMOTE before building consolidated trees.

As seen in Fig. 14, CTC with SMOTE ranks first and is some distance ahead of its closest competitors, having statistically significant differences with SVM with SMOTE and any method ranking lower. Using the AUC metric the gap between CTC with and without SMOTE is greater than it was using the geometric mean. Statistically significant differences also appear between CTC using SMOTE and C4.5 using SMOTE+ENN if the AUC is used. C4.5 using SMOTE+ENN was chosen for this comparison because it performed better than C4.5 with just SMOTE [67].

In view of the competitive results obtained by C4.5 and PART preceded by EUSCHC, this preprocessing method could be used in the future in conjunction with CTC as an alternative for SMOTE.

@&#CONCLUSIONS@&#

This paper presents a new resampling strategy that is applicable to any algorithm that requires the use of multiple subsamples, such as bagging, boosting or CTC. Instead of using fixed numbers of subsamples without taking the data sets’ aspects into account, this methodology uses the subsample’s change in class distribution from the training sample and the subsample’s size and type (with or without replacement) to compute the number of subsamples to ensure that the combination of subsamples covers a certain percentage of the training sample. We refer to this notion as the coverage value.

In this paper we also applied the coverage methodology to CTC, an algorithm that requires the use of several subsamples of the original training sample (as a lot of multiple classifier systems), but with a resulting single decision tree. We used two subsample types with a balanced class distribution. The subsample types differ in size with one subsample type being the same size as the minority class in the original training sample (sizeofMinClass) and the other one being the same amount multiplied by the number of classes (maxSize). The discriminating capacity of CTC with a wide range of coverage values was tested for three supervised classification contexts, these being standard data set classification, imbalanced two-class classification and imbalanced two-class classification preprocessed with SMOTE. We compared our results to those published by Fernández et al. [23] where they compared evolutionary and classical algorithms, all with explaining capacity.

On the one hand, comparing the performance of CTC using different subsample sizes, statistically significant differences always favor the use of bigger, maxSize subsamples so for following comparisons this subsamples were used. We decided to use a coverage value of 99%, based on the average values of the performance measures.

Comparing the performance of CTC with the original imbalanced and preprocessed data sets, there are statistically significant differences in favor of using SMOTE prior to CTC.

In summary, CTC ranks first for imbalanced two-class classification and standard data sets if kappa is used as the performance measure, while it ranks 4th if accuracy is used as the measure and it ranks 3rd for imbalanced classification if SMOTE is used to balance the class distribution prior to generating the subsamples. When CTC does not rank first, no statistically significant differences are found between the best ranking algorithm and CTC. The Consolidated Tree Construction algorithm proves to be the most robust of the 22 algorithms compared (16 evolutionary and 6 classical) for the three classification contexts comprising 96 data sets. It is the only one of the algorithms to place in the top third for all classification problems and it also places first globally. Depending on the application context, a lower coverage could be used if computational cost were an issue. CTC with a coverage of 50%, using a lower proportion of examples from the training set, still ranks first against the evolutionary and classical algorithms.

Additional experiments compare the performance of CTC in the context of imbalanced data set classification with several state of the art methods for tackling class imbalance. CTC ranks seventh behind several combinations of resampling and classifier algorithms. However, CTC with SMOTE resampling ranks first, with statistically significant differences with most of the competing algorithms.

For future work, we propose extending the application of the coverage-based resampling to other algorithms that use multiple subsamples. We would also like to study the performance of those algorithms with coverage values closer to 100%. We are also interested in extending the research presented in this paper by using subsamples with a class distribution other than a fully balanced distribution, with the aim of improving the true negative rate and, thus, the geometric mean. We would also like to apply the consolidation methodology (also known as inner ensemble methodology) to other decision tree or rule induction algorithms. Finally, in view of the good results that some of the algorithms tackling class imbalance obtain using resampling strategies other than SMOTE, we would like to analyze the performance of CTC with some of these preprocessing methods; such as EUSCHC or SMOTE+ENN.

@&#ACKNOWLEDGEMENT@&#

This work was funded by the Department of Education, Universities and Research of the Basque Government (Eusko Jaurlaritza/Gobierno Vasco) through Grant IT-395-10 and Grant PRE-2013-1-887 (BOPV/2013/128/3067), by the Science and Education Department of the Spanish Government (ModelAccess project, TIN2010-15549), and by the Basque Government’s SAIOTEK program (Datacc2 project, SPE12UN064) and the University of the Basque Country under Grant UFI11/45. The authoring team visited Professor Bao-Liang Lu’s research group at the Shanghai Jiao Tong University under the European Commission project Marie-Curie project NICaiA (PIRSES-247619) during the composition of this manuscript. We would like to thank the reviewers of this paper for their helpful comments. Also thanks to Javier Infante who carried out the previous experiments that sparked the idea for the work presented in this article. Finally we would like to thank the authors whose works we have used for comparison for publishing their full results for other researchers to use, and also Francisco Herrera’s research group (SCI2S) for providing the tools to test the statistical significance of the results.

@&#REFERENCES@&#

