@&#MAIN-TITLE@&#Bridging the vocabulary gap between questions and answer sentences

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We introduce two novel LM-based models to relax the exact matching assumption in IR.


                        
                        
                           
                           The class-based model clusters words to provide a coarse-grained word representation.


                        
                        
                           
                           The trigger model captures pairs of trigger and target words to find word relationships.


                        
                        
                           
                           Different types of word co-occurrence and triggering are studied within the models.


                        
                        
                           
                           We further studied the combination of both models to achieve the best result.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sentence retrieval

Language modeling

Word Clustering

Triggering

Question answering

@&#ABSTRACT@&#


               
               
                  We propose two novel language models to improve the performance of sentence retrieval in Question Answering (QA): class-based language model and trained trigger language model. As the search in sentence retrieval is conducted over smaller segments of text than in document retrieval, the problems of data sparsity and exact matching become more critical. Different techniques such as the translation model are also proposed to overcome the word mismatch problem. Our class-based and trained trigger language models, however, use different approaches to this aim and are shown to outperform the exiting models. The class model uses word clustering algorithm to capture term relationships. In this model, we assume a relation between the terms that belong to the same clusters; as a result, they can be substituted when searching for relevant sentences. The trigger model captures pairs of trigger and target words while training on a large corpus. The model considers a relation between a question and a sentence, if a trigger word appears in the question and the sentence contains the corresponding target word. For both proposed models, we introduce different notions of co-occurrence to find word relations. In addition, we study the impact of corpus size and domain on the models. Our experiments on TREC QA collection verify that the proposed model significantly improves the sentence retrieval performance compared to the state-of-the-art translation model. While the translation model based on mutual information (Karimzadehgan and Zhai, 2010) has 0.3927 Mean Average Precision (MAP), the class model achieves 0.4174 MAP and the trigger model enhances the performance to 0.4381.
               
            

@&#INTRODUCTION@&#

Sentence retrieval plays an important role in QA systems. It aims at finding small segments of text that contain an exact answer to the users’ questions rather than overwhelm them with a large number of retrieved documents, which they must sort through to find the desired answer. The retrieved sentences are then further processed using a variety of techniques to extract the final answers.

Different techniques used for document retrieval have been also applied to sentence-level retrieval. Among them, language model-based information retrieval proposed by Ponte and Croft (1998) has been proven to outperform traditional methods like TF-IDF and Okapi (Merkel & Klakow, 2007). However, the performance of this model in sentence retrieval is worse than the task of retrieving documents, which indicates that current document retrieval techniques are not sufficient for finding relevant sentences. Because there are major differences between document retrieval and sentence retrieval in terms of their size. As a result, many of the assumptions made for document retrieval do not hold for sentence retrieval. One important reason is the brevity of sentences. The brevity of sentences exacerbates the usual term mismatch problem which should be taken into consideration while designing a sentence retrieval engine. This problem is not pronounce in document level retrieval, since documents are large and they normally contain many terms which express the same concept in multiple ways. Such a property leads to there being different terms which talk about the same concept. As a result, even having one of these terms in the query is enough to find the relevant document. In addition, the frequency of individual words in a document is normally greater than one, especially for words that talk about the main topic of the document. Thus the frequency of words in documents helps to find the documents which talk about the user’s information need. In sentence retrieval, contrary to document retrieval, we face a very short text which briefly talks about a topic, and it is very unlikely to find sentences which contain the same terms as query terms. Even if the query words appear in a sentence, the frequency of those words is not higher than one or two.

The word unigram model is the most common approach used in the majority of information retrieval literature. When estimating word unigrams, the model only considers the exact literal words present in the query. Since no relationship between words is considered by this model, it will fail to retrieve other relevant information. Even well-known approaches for overcoming term mismatch problem like automatic query expansion and pseudo-relevance feedback, which have been great success stories for document retrieval, had rather mixed success when they were applied to sentence retrieval (Murdock, 2006). This problem motivated us to find a more sophisticated model to search for query words rather than just their distributions in the sentences to be retrieved.

One of the useful approaches is capturing word relationship to reduce the negative effects of the term mismatch problem in sentence retrieval. This goal can be achieved using different approaches such as the class-based language model and the trained trigger language model. The class-based model addresses the sentence brevity problem and bridges the semantic vocabulary gap between the question and the sentences. In this method, we apply a word clustering algorithm to capture word relationships. In other words, instead of building a fine-grained model based on vocabulary terms, a coarse-grained model based on word clusters is built. As a result, the class-based model is able to retrieve sentences which have no or few shared terms with input question, but their words belong to the same clusters as the question terms. The trained trigger model captures pairs of trigger and target words based on their co-occurrences in the training corpus. The model uses unsupervised approaches when training on a large corpus of raw text and apply supervised approaches when training a corpus of question and answer sentence pairs. Having pairs of trigger and target words; in the retrieval step, if the trigger word appears in an input question and the target word appears in a sentence, then we can say there is a relation between the question and the sentence even though they share no or few terms.

The structure of the paper is as follows. In Section 2, we review related work for both general language modeling approaches and term relationship approaches in information retrieval. In Section 3, the class-based model and the word clustering algorithm are introduced. Different notions of word co-occurrence applied in our class-based model are described in Section 4. Section 5 introduces the proposed trained trigger language model. In this section, we also show how the proposed model can be integrated with the exact matching methods to improve the system performance. Notions of word co-occurrence for triggering model are described in Section 6. In Section 7, the experimental results are presented. Finally, Section 8 concludes the paper.

@&#RELATED WORK@&#

Statistical language modeling has been successfully used in speech recognition (Jelinek, 1998) and many natural language processing tasks, including part of speech tagging, syntactic parsing (Charniak, 1993), and machine translation (Brown et al., 1990). Language modeling for information retrieval has received researchers’ attention in recent years. The efficiency of this approach, its simplicity, the state-of-the-art performance, and clear probabilistic meaning are the most important factors which contribute to its popularity (Ponte & Croft, 1998; Zhai & Lafferty, 2001).

The idea of using language modeling techniques for information retrieval was first introduced by Ponte and Croft (1998). In the proposed method, called query likelihood, a language model is inferred for each document and then the likelihood of the query according to the estimated language model is calculated. Thereafter, documents are ranked based on their query likelihood scores. Using the query likelihood model, the similarity between a document and the input query is defined as the probability of generating the query Q given the document model D which can be estimated by the multiple Bernoulli model or the multinomial model.

In the original method proposed by Ponte and Croft (1998), the multiple Bernoulli model was used for estimating document model D. In this model, each query is considered a set of unique terms, and two different probabilities are calculated: the probability of producing the query terms, and the probability of not producing other terms. Then, the product of these two factors is used as the model. The multiple Bernoulli model is defined as follows:
                           
                              (1)
                              
                                 P
                                 (
                                 Q
                                 |
                                 D
                                 )
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          w
                                          ∈
                                          Q
                                       
                                    
                                 
                                 P
                                 (
                                 w
                                 |
                                 D
                                 )
                                 ×
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          w
                                          ∉
                                          Q
                                       
                                    
                                 
                                 [
                                 1
                                 -
                                 P
                                 (
                                 w
                                 |
                                 D
                                 )
                                 ]
                              
                           
                        In further research by Hiemstra (1998), the multinomial model was used for this purpose. In this model, each query is considered a sequence of terms, and the query probability is computed by multiplying the probability of each individual term. The multinomial model is defined as follows:
                           
                              (2)
                              
                                 P
                                 (
                                 Q
                                 |
                                 D
                                 )
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 P
                                 (
                                 
                                    
                                       q
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 D
                                 )
                              
                           
                        where M is the number of query terms and 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                           
                         denotes the ith term in the query Q. After Hiemstra, Song and Croft (1999) and Miller, Leek, and Schwartz (1999) also used the same method, and this model has subsequently been used in most of the work in this area.

As mentioned in the previous section, by estimating the basic unigram model, the ranking algorithms only try to match the literal words that present in queries and texts; but they will fail to retrieve other relevant information. To avoid this problem, researchers have attempted to model term relationships in different ways.

Using hand-crafted thesauri such as WordNet is one of the prevalent techniques (Mandala, Tokunaga, & Tanaka, 1998; Schütze & Pedersen, 1997). Thesaurus is incorporated with different retrieval models to find dependencies among words. Robertson, van Rijsbergen, and Porter (1981) used a thesaurus for the probabilistic retrieval model; Cao, Nie, and Bai (2005, 2006), Croft and Wei (2005) applied it to the language model-based retrieval. Voorhees (1994) used WordNet for query expansion; and Liu, Liu, Yu, and Meng (2004) used it to disambiguate word senses when expanding the query. Manually produced resources contain useful information and as such their use might seem like a good idea. On the other hand, manual processing causes many problems such as inconsistency and ambiguity (Jones, 1971). The absence of proper nouns causes another problem in this way; since most of the thesauri do not consider proper nouns, we cannot find relationship between these nouns and other terms. In addition, we cannot find measure of dependency between terms which have different parts of speech. Besides these problems, building a thesaurus is labor intensive and it is more desirable to use an automatic method.

Using statistical methods is another well-known approach in this way to model term relationships. Cao et al. (2005) used co-occurrence relationships and combined them with the relationships extracted from WordNet. Wei and Croft (2007) introduced a probabilistic term association measure and utilized this measure in document language models. Burgess, Livesay, and Lund (1998) utilized words co-occurrence in window scaling. Qui and Frei (1993) applied another similar method in order to expand queries. In their proposed method, the weight of each new query term is taken from its similarity to the original query term. In all above models, they limited their methods to a window-based approach which is not able to model certain types of word relations. In addition, finding a suitable window size automatically is not easy (Wei & Croft, 2007).

Another approach for overcoming term mismatch problem is the statistical translation language model proposed by Berger and Lafferty (1999). The model computes the probability of generating a query as a translation of a document while using synthetic queries to estimate translation probability. This probability is then used as a measure of relevance of a document to a query to rank documents. The translation model has been applied to different information retrieval tasks. Jeon, Croft, and Lee (2005) used the translation model for finding questions that are similar to a user’s question in the community QA archive. Unlike QA systems which use user’s question as input and return a short answer to the question, in Jeon’s task both input and output of the system are questions; i.e., the system receives user’s question and returns similar questions that have already been asked in a community QA forum. They used the similarity between answers in the archive to estimate the translation probability. The translation model has also been modified and used for ad hoc retrieval. Karimzadehgan and Zhai (2010) modified this model by using mutual information for estimating the translation probability. Although mutual information is the best estimation for translation model so far and it is more efficient and has more coverage than the original estimation, calculating normalized mutual informational between all word pairs is computationally expensive. In addition, the model is limited to the word co-occurrence within documents which is very general contextual information.

For applying term relationships, researchers also tried to use document reformulation. Cluster-based document model (Liu & Croft, 2004; Tao, Wang, Mei, & Zhai, 2006) and LDA-based document models (Wei & Croft, 2006) are two important methods in this area which use document clusters instead of the whole collection to smooth the language model. Both models, however, are computationally expensive especially for large collections.

Besides various research on document retrieval, sentence/passage retrieval has also been studied separately for different applications that require short text retrieval. To the best knowledge of the authors, the proposed models for language model-based sentence retrieval do not differ from the state-of-the-art methods for document retrieval. Murdock (2006) applied the translation model to sentence retrieval for question answering and novelty detection. She used different approaches such as, mutual information, bilingual dictionary, and WordNet to estimate the translation probability. Balasubramanian, Allan, and Croft (2007) worked on sentence retrieval to find redundant information in a text. They applied topic models, dependence models, relevance models, and translation models, which have already been used for document retrieval (Berger & Lafferty, 1999; Lavrenko & Croft, 2001; Metzler & Croft., 2005; Wei & Croft, 2006). Cui, Kan, Chua, and Xiao (2004) used WordNet to improve the sentence retrieval performance for definition question answering.

In addition to the language model-based retrieval, other retrieval models have also been studied to improve sentence retrieval performance. Pasca and Harabagiu (2001) used WordNet to improve passage retrieval for question answering within a boolean retrieval model. Their proposed model works based on three feedback loops from WordNet to find morphological, lexical, and semantic alternations to expand the queries. Yang, Chua, Wang, and Koh (2003) also developed a method based on boolean passage retrieval for event-based question answering. They improved their model by expanding input queries using pseudo-relevance feedback. Additionally, they used WordNet gloss words and synset words to enhance pseudo-relevance feedback. Losada and Fernández (2007) improved vector space sentence retrieval by proposing a method which works based on extracting highly frequent terms from top retrieved documents. The model was applied to the novelty detection task. In addition to the similarity of the sentences to the input query, in this model, the number of highly frequent terms in the sentences were also taken into consideration. In another study, the authors also proposed using different kinds of opinion-based features to improve sentence retrieval performance (Fernández & Losada, 2012). As mentioned, these four methods were used within the boolean or vector space frameworks which have been proven to underperform language model-based retrieval.

In language model-based sentence retrieval, the probability 
                           
                              P
                              (
                              Q
                              ∣
                              S
                              )
                           
                         of generating the query Q conditioned on the observation of the sentence model S is first calculated, and thereafter sentences are ranked in descending order of this probability. For word-based unigrams, the probability of the query Q given the sentence model S is estimated based on the query terms.
                           
                              (3)
                              
                                 
                                    
                                       P
                                    
                                    
                                       word
                                    
                                 
                                 (
                                 Q
                                 |
                                 S
                                 )
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 P
                                 (
                                 
                                    
                                       q
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 S
                                 )
                              
                           
                        where M is the number of query terms, 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                           
                         denotes the ith term in the query Q, and S is the sentence model (Song & Croft, 1999). Using the maximum likelihood estimation, 
                           
                              P
                              (
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                              ∣
                              S
                              )
                           
                         is calculated based on the frequency of the query term 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                           
                         in the sentence S:
                           
                              (4)
                              
                                 
                                    
                                       P
                                    
                                    
                                       word
                                    
                                 
                                 (
                                 
                                    
                                       q
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 S
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             f
                                          
                                          
                                             S
                                          
                                       
                                       (
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             w
                                          
                                       
                                       
                                          
                                             f
                                          
                                          
                                             S
                                          
                                       
                                       (
                                       w
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    f
                                 
                                 
                                    S
                                 
                              
                              (
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         is the number of times the query term 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                           
                         appears in the sentence S and w denotes vocabulary words.

In our proposed class-based unigrams, instead of calculating the probability of each query term given the sentence model, the probability of the cluster that contains the query term is calculated. As a result, 
                           
                              P
                              (
                              Q
                              ∣
                              S
                              )
                           
                         is estimated as follows:
                           
                              (5)
                              
                                 
                                    
                                       P
                                    
                                    
                                       class
                                    
                                 
                                 (
                                 Q
                                 |
                                 S
                                 )
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 P
                                 (
                                 
                                    
                                       q
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 
                                    
                                       C
                                    
                                    
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                                 ,
                                 S
                                 )
                                 P
                                 (
                                 
                                    
                                       C
                                    
                                    
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                                 |
                                 S
                                 )
                              
                           
                        where 
                           
                              
                                 
                                    C
                                 
                                 
                                    
                                       
                                          q
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                           
                         is the cluster that contains 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              P
                              (
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                              ∣
                              
                                 
                                    C
                                 
                                 
                                    
                                       
                                          q
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                              ,
                              S
                              )
                           
                         is the emission probability of the ith query term given its cluster and the sentence model. 
                           
                              P
                              (
                              
                                 
                                    C
                                 
                                 
                                    
                                       
                                          q
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                              ∣
                              S
                              )
                           
                         is the probability of generating the cluster that contains 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                           
                         having the observation of the sentence model S. To calculate this probability, we adapted the maximum likelihood estimation as follows:
                           
                              (6)
                              
                                 
                                    
                                       P
                                    
                                    
                                       class
                                    
                                 
                                 (
                                 
                                    
                                       C
                                    
                                    
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                                 |
                                 S
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             f
                                          
                                          
                                             S
                                          
                                       
                                       (
                                       
                                          
                                             C
                                          
                                          
                                             
                                                
                                                   q
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             w
                                          
                                       
                                       
                                          
                                             f
                                          
                                          
                                             S
                                          
                                       
                                       (
                                       w
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    f
                                 
                                 
                                    S
                                 
                              
                              (
                              
                                 
                                    C
                                 
                                 
                                    
                                       
                                          q
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                              )
                           
                         is the number of times that all of the words which are in the same cluster as 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                           
                         appear in the sentence S.

To calculate the above probability, the sentence model is created based on clusters instead of terms. To this aim, each cluster is considered a single entity and these new entities are used to create the model. To be more specific, the following steps are required for the word-based model:
                           
                              1.
                              For each sentence in the word-based model, the sentence terms are extracted.

The sentence unigram model is created based on the sentence terms.


                                 
                                    
                                       P
                                       (
                                       Q
                                       ∣
                                       S
                                       )
                                    
                                  is estimated based on the sentence model.

In the class-based model, contrary to the word-based model, the word clusters are used instead of the words. Therefore, the required steps are as follows:
                           
                              1.
                              Using a term clustering technique, a map between the terms and the clusters is created.

Instead of extracting the terms, the clusters in which the terms belong to are extracted.

The sentence unigram model is created from the clusters.


                                 
                                    
                                       P
                                       (
                                       Q
                                       ∣
                                       S
                                       )
                                    
                                  is estimated based on the new class-based sentence model.

There are many clustering algorithms that have been widely used in different applications Manning, Raghavan, and Schütze (2008). In addition, there are various algorithms that are especially developed for clustering lexical words, such as those proposed by Brown, Pietra, Souza, Lai, and Mercer (1992), Pereira and Tishby (1992), Ney, Essen, and Kneser (1994), and McMahon and Smith (1996). Even though the Brown word clustering Brown et al. (1992) is the oldest proposed algorithm, research on various natural language processing applications showed the superiority of this algorithm to other word clustering algorithms, and even the superiority of this model to recent word embedding methods Collobert and Weston (2008), Mnih and Hinton (2009) when representing words in a vector space model Turian, Ratinov, Bengio, and Roth (2009), Turian, Ratinov, and Bengio (2010). As a result, we also use the Brown word clustering technique to cluster lexical terms.

Brown clustering is a hierarchical bottom-up algorithm which uses average mutual information between adjacent clusters to merge cluster pairs. Using average mutual information, the algorithm consider contextual information to find the similar words and put them in the same cluster. To this aim, an input corpus statistics, such as 
                           
                              f
                              (
                              w
                              ,
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                        , is required, where 
                           
                              f
                              (
                              w
                              ,
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                         is the number of times the word 
                           
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                           
                         is seen in the context w. Both w and 
                           
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                           
                         are assumed to come from a common vocabulary. Beginning with each vocabulary item in a separate cluster, the bottom-up approach merges the pair of clusters that minimizes the loss in average mutual information between the word cluster 
                           
                              
                                 
                                    C
                                 
                                 
                                    
                                       
                                          w
                                       
                                       
                                          ′
                                       
                                    
                                 
                              
                           
                         and its context cluster 
                           
                              
                                 
                                    C
                                 
                                 
                                    w
                                 
                              
                           
                        . Different words seen in the same contexts are good candidates for merging, as are different contexts in which the same words are seen; because appearing in the same context shows that those words can be replaced by each other and assigned to the same cluster as a result. In other words, since the context of a word is the best hint to guess the meaning of that word, we can assume that other words that mutually co-occur with the context of the target word also have similar meaning Morita et al. (2004).

One of the advantages of the Brown algorithm is using mutual information as similarity measure, which is suitable for the retrieval task. Considering language model-based sentence retrieval, the goal is maximizing query likelihood. The log likelihood function of clusters using maximum likelihood estimation is calculated as follows (Martin, Liermann, & Ney, 1998):
                           
                              (7)
                              
                                 
                                    
                                       F
                                    
                                    
                                       likelihood
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          w
                                          ,
                                          
                                             
                                                w
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                                 f
                                 (
                                 w
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       ′
                                    
                                 
                                 )
                                 log
                                 p
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       ′
                                    
                                 
                                 |
                                 w
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                w
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                                 f
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       ′
                                    
                                 
                                 )
                                 log
                                 p
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       ′
                                    
                                 
                                 |
                                 
                                    
                                       C
                                    
                                    
                                       
                                          
                                             w
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                                 )
                                 +
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                C
                                             
                                             
                                                w
                                             
                                          
                                          ,
                                          
                                             
                                                C
                                             
                                             
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      ′
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 f
                                 (
                                 
                                    
                                       C
                                    
                                    
                                       w
                                    
                                 
                                 ,
                                 
                                    
                                       C
                                    
                                    
                                       
                                          
                                             w
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                                 )
                                 log
                                 p
                                 (
                                 
                                    
                                       C
                                    
                                    
                                       
                                          
                                             w
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                                 |
                                 
                                    
                                       C
                                    
                                    
                                       w
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                w
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                                 f
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       ′
                                    
                                 
                                 )
                                 log
                                 
                                    
                                       f
                                       (
                                       
                                          
                                             w
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                    
                                    
                                       f
                                       (
                                       
                                          
                                             C
                                          
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                       )
                                    
                                 
                                 +
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                C
                                             
                                             
                                                w
                                             
                                          
                                          ,
                                          
                                             
                                                C
                                             
                                             
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      ′
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 f
                                 (
                                 
                                    
                                       C
                                    
                                    
                                       w
                                    
                                 
                                 ,
                                 
                                    
                                       C
                                    
                                    
                                       
                                          
                                             w
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                                 )
                                 log
                                 
                                    
                                       f
                                       (
                                       
                                          
                                             C
                                          
                                          
                                             w
                                          
                                       
                                       ,
                                       
                                          
                                             C
                                          
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                       )
                                    
                                    
                                       f
                                       (
                                       
                                          
                                             C
                                          
                                          
                                             w
                                          
                                       
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                w
                                             
                                             
                                                ′
                                             
                                          
                                       
                                    
                                 
                                 f
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       ′
                                    
                                 
                                 )
                                 log
                                 f
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       ′
                                    
                                 
                                 )
                                 +
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                C
                                             
                                             
                                                w
                                             
                                          
                                          ,
                                          
                                             
                                                C
                                             
                                             
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      ′
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 f
                                 (
                                 
                                    
                                       C
                                    
                                    
                                       w
                                    
                                 
                                 ,
                                 
                                    
                                       C
                                    
                                    
                                       
                                          
                                             w
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                                 )
                                 log
                                 
                                    
                                       f
                                       (
                                       
                                          
                                             C
                                          
                                          
                                             w
                                          
                                       
                                       ,
                                       
                                          
                                             C
                                          
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                       )
                                    
                                    
                                       f
                                       (
                                       
                                          
                                             C
                                          
                                          
                                             w
                                          
                                       
                                       )
                                       f
                                       (
                                       
                                          
                                             C
                                          
                                          
                                             w
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        The above formula shows that the mutual information of the clusters of adjacent words, which is used in the Brown algorithm (Brown et al., 1992), can be derived from the log likelihood function. Algorithm 1 shows the Brown word clustering algorithm in more detail.
                           Algorithm 1
                           The brown word clustering algorithm (AMI stands for average mutual information) 
                                 
                                    
                                       
                                       
                                          
                                             
                                                
                                                Initial Mapping: Put a single word in each cluster
                                          
                                          
                                             
                                                Compute the initial AMI of the collection
                                          
                                          
                                             
                                                
                                                repeat
                                             
                                          
                                          
                                             
                                                
                                                for each pairs of clusters do
                                             
                                          
                                          
                                             
                                                
                                                Merge the pair of clusters temporarily
                                          
                                          
                                             
                                                
                                                Compute AMI of the collection
                                          
                                          
                                             
                                                
                                                end
                                                
                                                for
                                             
                                          
                                          
                                             
                                                Merge the pair of clusters which has the minimum decrement in AMI
                                          
                                          
                                             
                                                Compute AMI of the new collection
                                          
                                          
                                             
                                                
                                                until reach the predefined number of clusters
                                          
                                          
                                             
                                                
                                                repeat
                                             
                                          
                                          
                                             
                                                Move each term to the cluster for which the resulting partition has the greatest AMI
                                          
                                          
                                             
                                                
                                                until no more increment in AMI
                                          
                                       
                                    
                                 
                              
                           

As shown in the algorithm, clusters are initialized with a single term. Then, in each iteration, all pairs of clusters are temporarily combined and the average mutual information between adjacent clusters is computed. Thereafter, the best cluster pair, which offers the minimum decrement in average mutual information, is permanently combined together. The process continues for 
                           
                              V
                              -
                              K
                           
                         iterations, where V is the number of terms and K is the predefined number of clusters. After finishing the iterations, a final step is performed to increase the average mutual information. In this step, we check whether moving words to other clusters can increase the average mutual information or not; whereby each term is moved to that cluster for which the resulting partition has the greatest average mutual information. The algorithm terminates when average mutual information ceases to increase. The time complexity of the Brown algorithm is 
                           
                              O
                              (
                              
                                 
                                    V
                                 
                                 
                                    3
                                 
                              
                              )
                           
                        . However, an incremental version of this algorithm which starts with one cluster for each of the K most frequent words, and then adds one word at each iteration is 
                           
                              O
                              (
                              V
                              ∗
                              
                                 
                                    K
                                 
                                 
                                    2
                                 
                              
                              )
                           
                         (Brown et al., 1992).

While originally proposed with bigram statistics, the algorithm is agnostic to the definition of co-occurrence; e.g., if 
                           
                              〈
                              w
                              ,
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                              〉
                           
                         are verb-object pairs, the algorithm clusters verbs based on their selectional preferences; if 
                           
                              f
                              (
                              w
                              ,
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                         is the number of times w and 
                           
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                           
                         appear in the same documents, it will produce semantically (or topically) related word-clusters, and so on. In the next section, we describe different notions of word co-occurrence used in our study to cluster lexical terms.

If two content words w and 
                           
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                           
                         are seen in the same document, they are usually topically related. In this notion of co-occurrence, how near or far away from each other they are in the document is irrelevant, as is their order of appearance in the document. Document-wise co-occurrence has been successfully used in information retrieval and many natural language processing applications, such as automatic thesaurus generation (Manning et al., 2008). The idea of latent semantic analysis proposed by Landauer and Dumais (1997) is also from this notion of co-occurrence.

The document-wise word co-occurrence frequency is symmetric; i.e., 
                           
                              f
                              (
                              w
                              ,
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                              )
                              =
                              f
                              (
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                              ,
                              w
                              )
                           
                        . Statistics of this type of co-occurrence may be collected in two different ways. In the first case, 
                           
                              f
                              (
                              w
                              ,
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                         is simply the number of documents that contain both w and 
                           
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                           
                        . This is usually the notion used in ad hoc retrieval. Alternatively, we may want to treat each instance of 
                           
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                           
                         in a document that contains an instance of w to be a co-occurrence event. Therefore, if 
                           
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                           
                         appears three times in a document that contains two instances of w, the former method counts it as one co-occurrence, while the latter as six co-occurrences. We use the latter statistic, since we are concerned with retrieving sentence sized documents, wherein a repeated word is more significant.

Since topic changes sometimes happen within a single document and our end task is sentence retrieval, we also investigate the notion of the word co-occurrence in smaller segments of text, such as sentences. In contrast to the document-wise model, the sentence-wise co-occurrence does not consider whole documents, and it only concerns itself with the number of times that two words co-occur in the same sentences. Word sense disambiguation in machine translation and data recovery (Dagan, Marcus, & Markovitch, 1993) is one of the natural language processing applications that this notion of word co-ocurrence is used for.

The window-wise co-occurrence statistic is an even narrower notion of context, considering only the terms in a window surrounding 
                           
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                           
                        . Specifically, a window of a fixed size is moved along the text, and 
                           
                              f
                              (
                              w
                              ,
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                         is set as the number of times both w and 
                           
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                           
                         appear in the window. This notion of co-occurrence is a well-known approach widely used in speech recognition (Farhat, Isabelle, & O’Shaughnessy, 1996) and various natural language processing applications including word sense disambiguation (Hassel, 2005), semantic categorization (Bullinaria, 2008; Bullinaria & Levy, 2007), syntactic categorization (Finch & Chater, 1992), and hyperspace approximation to language (Lund & Burgess, 1996).

Since the window size is a free parameter, different sizes may be applied. As shown by Huckle (1995), using large window of text cannot improve the performance over those obtained from a narrower context. Even Farhat et al. (1996) reported that a window of 
                           
                              ±
                              1
                           
                         which refers to word bigram is the optimum size for the window-wise co-occurrence. As a result, in our experiments, we used word bigram for the window-wise co-occurrence.

As an example for bigram co-occurrence, consider the sentences in Fig. 1
                        . Based on these sample sentences and using the bigram co-occurrence, the words “Saturday” and “Tuesday” are assigned to the same cluster; because both words appear in the same context “on” and “morning”.

Another notion of word co-occurrence derives from having a syntactic relationship with the context w. The syntax-wise co-occurrence statistic is similar to the sentence-wise co-occurrence, in that co-occurrence is defined at the sentence level. However, in contrast to the sentence-wise model, w and 
                           
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                           
                         are said to co-occur only if there is a syntactic relation between them in that sentence; e.g., in this type of co-occurrence, the nouns that are used as objects of the same verb are clustered into the same class, such as “tea” and “water” which are used with the verb “drink” as represented in the sample sentences of Fig. 2
                        .

The syntax-wise co-ocuurrence is also widely used in various natural language processing applications, such as automatic word retrieval (Lin, 1998a), distributional soft clustering (Pereira, Tishby, & Lee, 1993), and word disambiguation (Li & Abe, 1998). In some of the mentioned work, only one type of syntactic relations is taken into account. For example, Pereira et al. (1993) only used the verb-direct object relation for their soft clustering task.

To prepare such statistics, all sentences in the corpus must be syntactically parsed. We found that a dependency parser is an appropriate tool for our goal, because it directly captures the dependencies between the words without the mediation of any virtual (non-terminal) nodes. Having all sentences in the parsed format, 
                           
                              f
                              (
                              w
                              ,
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                         is defined as the number of times that the words w and 
                           
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                           
                         have a parent–child relationship of any syntactic type in the dependency parse tree. For our experiments we use MINIPAR (Lin, 1998b) to parse the whole corpus due to its robustness and speed. Fig. 3
                         shows a sample sentence that is parsed by MINIPAR.

As described, the proposed class-based model aims at finding the relation between words to overcome the term mismatch problem in sentence retrieval. The trigger model is also proposed to solve the same problem but from another perspective. This new approach uses the contextual information to extract the pair of terms that trigger each others. In this model, we assume that by training a language model on a corpus, we can find pairs of trigger and target words; so that in the retrieval step, if the trigger word appears in the question and the target word in a sentence, then we can hypothesize that there is a relation between the question and the sentence even thought they share no or few terms.

As mentioned in Section 3, in the word unigram model, the probability of generating the query Q given the observation of the sentence model S is calculated by Eq. (3). In this model, for each sentence in the search space a separate language model is trained
                           1
                           The word train that is used in this paper means parameter estimation and it is not the similar concept used in supervised machine learning methods which need labeled data.
                        
                        
                           1
                        ; and then using the maximum likelihood estimation, 
                           
                              P
                              (
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                              ∣
                              S
                              )
                           
                         is calculated based on the frequency of the query term 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                           
                         in the sentence S, as presented in Eq. (4). As a result, based on the maximum likelihood estimation, 
                           
                              P
                              (
                              Q
                              ∣
                              S
                              )
                           
                         for word unigram is defined as follows:
                           
                              (8)
                              
                                 
                                    
                                       P
                                    
                                    
                                       word
                                    
                                 
                                 (
                                 Q
                                 |
                                 S
                                 )
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             f
                                          
                                          
                                             S
                                          
                                       
                                       (
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             w
                                          
                                       
                                       
                                          
                                             f
                                          
                                          
                                             S
                                          
                                       
                                       (
                                       w
                                       )
                                    
                                 
                              
                           
                        In the trained trigger language model, contrary to the word unigram which trains a separate model for each sentence, a single model is trained on a large corpus first. Then, the trained model is being used for all of the sentences to be retrieved. The trained model is represented by 
                           
                              f
                              (
                              w
                              ,
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                        , where 
                           
                              f
                              (
                              w
                              ,
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                         is the number of times the word w triggers the target 
                           
                              
                                 
                                    w
                                 
                                 
                                    ′
                                 
                              
                           
                        . There are different notions of triggering that can be used to find pairs of trigger and target words. The notions of triggering used in our experiments are discussed in Section 6. Having a trained model based on trigger and target words, for each word in the query and each word in the sentence, we propose the following equation to calculate the probability of generating the query term given the sentence term:
                           
                              (9)
                              
                                 
                                    
                                       P
                                    
                                    
                                       trigger
                                    
                                 
                                 (
                                 
                                    
                                       q
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 
                                    
                                       s
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             f
                                          
                                          
                                             C
                                          
                                       
                                       (
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             s
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   q
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          
                                             f
                                          
                                          
                                             C
                                          
                                       
                                       (
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             s
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                           
                         is the jth term in the sentence and 
                           
                              
                                 
                                    f
                                 
                                 
                                    C
                                 
                              
                              (
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         is the number of times the query term 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                           
                         triggers the sentence word 
                           
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                           
                         based on the training corpus C.

Having the proposed trigger model for calculating 
                           
                              P
                              (
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                              ∣
                              
                                 
                                    s
                                 
                                 
                                    j
                                 
                              
                              )
                           
                        , the likelihood of generating query term 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                           
                         given the sentence S is adapted as:
                           
                              (10)
                              
                                 
                                    
                                       P
                                    
                                    
                                       trigger
                                    
                                 
                                 (
                                 
                                    
                                       q
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 S
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       N
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          N
                                       
                                    
                                 
                                 
                                    
                                       P
                                    
                                    
                                       trigger
                                    
                                 
                                 (
                                 
                                    
                                       q
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 
                                    
                                       s
                                    
                                    
                                       j
                                    
                                 
                                 )
                              
                           
                        where N is the sentence length. As a result, in our proposed trigger model the probability of generating the query Q having the observation of the sentence S is defined as follows:
                           
                              (11)
                              
                                 
                                    
                                       P
                                    
                                    
                                       trigger
                                    
                                 
                                 (
                                 Q
                                 |
                                 S
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   N
                                                
                                             
                                          
                                       
                                    
                                    
                                       M
                                    
                                 
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          N
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             f
                                          
                                          
                                             C
                                          
                                       
                                       (
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             s
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   q
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          
                                             f
                                          
                                          
                                             C
                                          
                                       
                                       (
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             s
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        Similar to the basic word unigram model, a smoothing technique is required to deal with the zero probability of unseen words. Any of the smoothing methods proposed for the word unigram model (Zhai & Lafferty, 2001) can also be applied to this model.

As we can see in Eq. (10), we assume that consecutive words in a sentence are independent. Although this assumption is not realistic, we do not expect that it degrades system performance. Because in real application, a large portion of sentence terms are not related to the query term and the probability of the query word given those sentence words is very low; only a limited number of sentence terms are relevant to the query word and have a high probability of which having two consecutive relevant sentence words is very unlikely. As a result, this assumption does not decrease performance significantly.

Comparing the time complexity of the proposed model with the normal unigram model. For each input query, the word unigram model is running in 
                           
                              O
                              (
                              KM
                              )
                           
                         time, where K is the number of sentences in the search space and M is the query length. In the trained trigger language model, we have an additional factor which is the number of words in the sentence to be retrieved. As a result, the running time of the trigger model is 
                           
                              O
                              (
                              KMN
                              )
                           
                        . Although the time complexity of our model is higher than the word unigram model, it is still identical to the translation model.

As mentioned, the proposed trained trigger language model aims to capture relationships between words and use these relations in the retrieval step. As a result, the model is able to find more relevant sentences and increase system recall reasonably. This method may decrease the system precision, however, by retrieving more non-relevant sentences. To avoid this problem, we use the linear interpolation (Jelinek & Mercer, 1989) of our proposed triggering models and the baseline exact matching models, thereby benefiting from the advantages of both models and avoiding their disadvantages.

To use the interpolation of word unigram and the triggering models, the probability of generating 
                           
                              
                                 
                                    q
                                 
                                 
                                    i
                                 
                              
                           
                         given both models are computed from Eq. (4) and (10) and interpolated by weighting parameter λ which should be tuned using the held-out data. This interpolation model can be extended by using more than one notion of triggering in addition to the exact matching model. The following formula shows the linear interpolation model when different notions of triggering are interpolated by word unigram model.
                           
                              (12)
                              
                                 P
                                 (
                                 Q
                                 |
                                 S
                                 )
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   t
                                                
                                             
                                          
                                          (
                                          
                                             
                                                λ
                                             
                                             
                                                t
                                             
                                          
                                          
                                             
                                                P
                                             
                                             
                                                
                                                   
                                                      trigger
                                                   
                                                   
                                                      t
                                                   
                                                
                                             
                                          
                                          (
                                          
                                             
                                                q
                                             
                                             
                                                i
                                             
                                          
                                          |
                                          S
                                          )
                                          )
                                          +
                                          
                                             
                                                
                                                   1
                                                   -
                                                   
                                                      
                                                         ∑
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   
                                                      
                                                         λ
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                P
                                             
                                             
                                                word
                                             
                                          
                                          (
                                          
                                             
                                                q
                                             
                                             
                                                i
                                             
                                          
                                          |
                                          S
                                          )
                                       
                                    
                                 
                              
                           
                        where trigger
                           t
                         can be any of the triggering models defined in the next section.

A similar interpolation model can be used for combining different notions of triggering with self triggering
                           2
                           The self triggering model is described in Section 6.1.
                        
                        
                           2
                         instead of word unigram.


                        Self triggering is the simplest type of the trained trigger language model in which each word only triggers itself. The idea of self triggering is the same as the basic unigram model, since this model only retrieve sentences whose terms match the query terms. Although this model does not use any contextual information, it is still considered an essential part of a retrieval engine and has to be used in combination with any other notions of triggering; because without using such a basic part, the retrieval model is not able to give a priority to the words appeared in both the query and the sentence.

This model assumes that there is a relation between the words which appear in the same sentences. To use inside sentence triggering, the model should be trained on a large unannotated corpus while considering that each word in a sentence triggers all other words in the same sentence.

Using this model, we can retrieve sentences which do not share many terms with the query, but their terms frequently co-occurr with the query terms in the same sentences of the training corpus. To be more precise, consider the following question and the answer sentence:
                           
                              Q: “Who invented the automobile?”
                              

A: “Nicolas Joseph Cugnot invented the first self propelled mechanical vehicle.”
                              

Using the question as a query in word unigram or self triggering, we only have one common term “invent” between the query and the sentence which is not enough to put this correct sentence at the top of the ranked list, since there are many sentences in the search space which contain the word “invent”, such as:
                           
                              –
                              
                                 “Thomas Edison invented the first commercially practical light.”
                              


                                 “Alexander Graham Bell of Scotland is the inventor of the first practical telephone.”
                              

Even though many sentences in the search space contain the word “invent”, the inside sentence triggering model gives a higher score to the correct sentence; because the model knows the word “automobile” triggers the target word “vehicle”, since they frequently co-occur in the same sentences of the training corpus, as shown
                           3
                           These samples are only simple examples to illustrate our goal. Of course, more pairs of words can be extracted when using triggering models.
                        
                        
                           3
                         in Fig. 4
                        .


                        Across sentence triggering uses a wider context than inside sentence triggering: it considers that each word in a sentence triggers all words in the next sentence of the training corpus. Because two adjacent sentences mostly talk about the same topic and it is very likely that different words with the same concept and meaning are used in two consecutive sentences.

As an example, consider a corpus which contains the adjacent sentences presented in Fig. 5
                        . Using such a corpus for training an across sentence triggering model, we can find that the word “build” triggers the word “open” which is in the next sentence. As a result, this trained trigger model can retrieve the correct answer for the following question:
                           
                              Q: “Where was the first McDonald’s built?”
                              

A: “Two brothers from Manchester, Dick and Mac McDonald, open the first McDonald’s in California.”
                              

It is clear that neither word unigram nor self triggering is able to rank the correct sentence highly; because there are many sentences in the search space talking about “first McDonald’s”, but non-relevant to the question, such as:
                           
                              –
                              
                                 “The site of the first McDonald’s to be franchised by Ray Kroc is now a museum in Des Plaines, Illinois.”
                              


                                 “The first McDonald’s TV commercial was a pretty low-budget affair.”
                              

Another notion of triggering derives from using more supervision when training the model. This notion of triggering requires a set of question and answer pairs as a training corpus. Having such a model helps us to find another type of word pairs that trigger each other. In question and answer pair triggering, each word in the question triggers all words which appear in the answer sentence.

Again this model can retrieve sentences which share no or few words with the question but their terms frequently co-occur with question terms in question and answer pairs that are used to train the model.

As an example, consider the following question and its correct answer sentence.
                           
                              Q: “How high is Everest?”
                              

A: “Everest is 29,029 feet.”
                              

Similar to the previous examples, the above question and answer sentence share a very limited number of terms, in this case only the term “Everest”. As a result, it is very unlikely that the word unigram model or the self triggering model ranks the correct answer highly. Because there are many non-relevant sentences in the search space which contain the word “Everest”, such as:
                           
                              –
                              
                                 “Everest is located in Nepal.”
                              


                                 “Everest has two main climbing routes.”
                              

However, the question and answer pair triggering model gives a higher score to the correct sentence because the model knows that in a large portion of questions that contain the word “high”, the term “feet” appears in the answer. As a result, in the trained model, the word “high” triggers the target word “feet”. Fig. 6
                         shows some sample pairs of question and answer sentence such that the question contains the term “high” and the answer sentence consists of the word “feet”.


                        Named entity triggering is a generalized form of the question and answer pair model. Similar to the question and answer pair triggering, we need a supervised training in which a corpus of questions and answers is required. In contrast to the previous model, named entity triggering does not use all of the words appear in the answer sentence; i.e., in this model, the question words only trigger the named entity label of the answer string and the other terms of the answer sentence are ignored.

Such a triggering model could be very useful to give a higher probability to the sentences which contain the target named entity label. For example, consider the following question and answer sentence:
                           
                              Q: “When was Einstein born?”
                              

A: “Albert Einstein was born on 14 March 1879.”
                              

A normal word unigram model can retrieve the correct answer as the question and the answer sentence share two words: “Einstein” and “born”. However, if some sentences like the following ones are available in the search space, then the model is not able to give a higher priority to the correct sentence because all of the sentences share the same words with the question:
                           
                              –
                              
                                 “Albert Einstein was born in Germany.”
                              


                                 “Albert Einstein was born in a Jewish family.”
                              

Using the named entity triggering model for sentence retrieval, the model knows that the word “when” triggers all words which has DATE as their named entity label, because in the training corpus it is very likely to have dates as an answer to the questions start with “when” as presented in Fig. 7
                        . As a result, the correct answer which contain a DATE is ranked highly.

@&#EXPERIMENTAL RESULTS@&#

To evaluate our methods for sentence-level retrieval, we used the set of questions from TREC
                           4
                           
                              http://trec.nist.gov.
                        
                        
                           4
                         2005 and TREC 2006 QA track. The set of questions from TREC 2006 was used as test data to evaluate our models, while the TREC 2005 question set was used as held-out data to study the smoothing parameters and interpolation weights.

The TREC 2006 QA task contains 75 question-series that each of them focuses on a target. Among the focused targets, 19 targets are PERSONs, 19 are ORGANIZATIONs, 19 are EVENTs, and 18 are THINGs (Dang, Lin, & Kelly, 2006). The series contained a total of 403 factoid questions, 89 list questions, and 75 other questions. For our experiments, the factoid questions have been used, while the sentence retrieval models described in this paper are suitable for different types of questions. Table 1
                         shows an example of a question-series in TREC 2006 QA data.

Since the relevance judgments provided by NIST are only annotated at document level, we need a sentence-level relevance judgment. As a result, the Question Answer Sentence Pair (QASP) corpus (Kaisser & Lowe, 2008) that is described in Section 7.2.3 was used for sentence-level relevance judgments; while the questions of TREC QA 2002–2004 of this corpus is used for training, the questions of 2005 and 2006 are used as held-out and test data respectively; so that there is no overlap between the train and evaluation sets.

QA systems typically employ sentence retrieval after initial document retrieval. To simulate this and evaluate the sentence retrieval component of our QA system independently of the document retrieval component, we adopted the following experimental setup. A separate sentence collection was first created for each question-series which consists of a series of related questions. On average, there are 17 documents/270 sentences that are relevant to each question topic (documents which are relevant to any of 5–6 different questions in a question-series) while the number of relevant sentences to each individual question is only 4 sentences (on average). Moreover, the non-relevant sentences in each collection exemplify exactly the sort of typical QA system false alarms we want our sentence retrieval system to avoid. Non-relevant sentences in our collection are from (1) documents relevant to similar yet different questions and (2) non-relevant sentences found in the relevant documents.

While our experimental setup does not evaluate sentence retrieval in the context of a full-blown QA system with answer extraction or noisier document retrieval on difficult queries, as articulated above, our intent by design was to focus on evaluating the sentence retrieval subsystem in isolation.

Another advantage of this search collection is that in addition to availability of many non-relevant sentences in the collection, we know that all relevant sentences are also available in the input collection. So that, the upper bound of sentence retrieval performance is 100%; if we use the document retrieval output, it is very likely that for a portion of questions no correct answer appears in the retrieved documents. As a result, even a perfect sentence retrieval method cannot improve system performance, as the correct sentences are not available in the input data at all. Overall, we believe this sentence search collection is realistic, even if somewhat optimistic.

In this paper we introduced two different approaches to improve sentence retrieval performance. For each of the models, we need to estimate the corresponding parameters within a training phase; to this aim, a sample of text is required which we call it training corpus. As a result, for sentence retrieval experiments, in addition to the held-out and the test data, training corpora are required.

For all experiments on the class-based model, a large unannotated corpus is required to train the models in an unsupervised fashion. For this purpose, we used the AQUAINT1 corpus.
                           5
                           See catalog number LDC2002T31 at http://www.ldc.upenn.edu/Catalog.
                        
                        
                           5
                         As an alternative for a large unannotated corpus for the class-based model, the Google n-gram corpus
                           6
                           See catalog number LDC2006T13 at http://www.ldc.upenn.edu/Catalog.
                        
                        
                           6
                         is used which only covers n-grams up to 5-grams without including the raw texts at the sentence or document level. For a part of experiments on trained trigger model which applies inside or across sentence triggering, we still require a raw text corpus, so that the AQUAINT1 corpus is utilized. For the other part of experiments, which uses question and answer pair triggering or named entity triggering, we need to train our models in a supervised fashion in which a corpus of question and answer sentence pairs is required. As for the supervised model, we used the Question Answer Sentence Pair (QASP) corpus
                           7
                           
                              http://homepages.inf.ed.ac.uk/s0570760/data.
                        
                        
                           7
                         and the Yahoo! Answers Comprehensive Questions and Answers corpus
                           8
                           
                              http://webscope.sandbox.yahoo.com.
                        
                        
                           8
                         (version 1.0). In this section, we describe all four corpora in more detail.

The AQUAINT1 corpus is released by the Linguistic Data Consortium (LDC) and consists of newswire text data in English, extracted from three sources: the Xinhua News Service, the New York Times News Service, and the Associated Press Worldstream News Service. The sampling for this corpus covers the period from January 1996 to September 2000, inclusive, for the Xinhua text collection, and from June 1998 to September 2000, inclusive, for New York Times and Associated Press. The corpus contains almost 450 million word token appeared in 23 million sentences and 1.3 million documents.

The Google n-gram corpus, also known as Web 1T corpus, is released by Google through LDC (Brants & Franz, 2006). The n-gram counts of this dataset were generated from Web pages that are publicly accessible in English. It consists of unigram, bigram, trigram, 4-gram, and 5-gram counts calculated over 1 trillion words of the Web page’s text. Data collection took place in January 2006 and all of the data are created on or before January 2006. The text were tokenized following the Penn Treebank tokenization, except that words are usually split on hyphens, dates, email addresses, and URLs, which are kept as single tokens. The individual terms in the n-grams occurred at least 200 times, otherwise they were replaced with the special token <UNK>. The n-grams themselves must appear at least 40 times to be included in the corpus.

Although the Google n-gram corpus is only available at the n-gram level, the size of this dataset motivated us to use this dataset for part of our experiment which only low order n-grams are required. For example, the Google n-gram corpus includes 328,431,792 bigrams, while AQUAINT1 consists of only 14,093,661 bigrams.

To train our models for the supervised approaches which need a set of question and answer sentence pairs, we used the Question Answer Sentence Pair corpus of Kaisser and Lowe (2008). This corpus contains pairs of TREC QA questions and their relevant answer sentences. As the relevance judgment at document level is provided by NIST, Kaisser and Lowe used the original document-level relevance judgment and produced the answer sentences by refining theses documents via Amazon’s Mechanical Turk
                              9
                              
                                 http://mturk.amazon.com.
                           
                           
                              9
                            crowdsourcing platform. All the documents that contain the relevant sentences are from the AQUAINT1 corpus. The QASP corpus is available for five years of TREC QA from 2002 to 2006.

As mentioned, since the original TREC relevance judgments were only made at the coarser document level, this is the only dataset that we can also use for evaluating our model. Therefor, we only used 2002–2004 to train our models, while 2005 and 2006 are kept for held-out and test data. Table 2
                            shows the number of questions for each year of TREC QA which is available in this corpus and the average number of answer sentences for each question.

As another alternative for training our model with question and answer sentence pairs, we used Yahoo! Answers Comprehensive Questions and Answers corpus (version 1.0) (Webscope, 2009). The motivation of using this dataset is to have a larger corpus for training the model. As mentioned, the QASP corpus of Kaisser and Lowe is limited to five years TREC QA of which only three years, that includes less than 1000 questions, are used to train our model, which is a very small train set.

The Yahoo! Answers corpus is derived from the Yahoo! community QA forum.
                              10
                              
                                 http://answers.yahoo.com.
                           
                           
                              10
                            This website is public to any Web user willing to browse or download them (Webscope, 2009). This dataset was collected in October 2007. It includes all the questions and their corresponding answers. The size of the corpus is 3.45GB while containing 4,483,032 questions and their answers. Each question in the dataset marked as subject. More description about the question is also available as content. In our experiments, both the subject and the content are considered to be a question and all answers including the best one are used as answer sentences.

To evaluate our proposed models, three different evaluation metrics, namely Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and Precision at n (P@n), have been used.


                        Precision at n is a useful metric for evaluating information retrieval systems. This measure is mostly interesting for people who want to receive good results at the first n items of the retrieved documents. For example, Web-search engine users are interested in receiving good documents on the first page, and they do not care about the rest of the pages. As it is clear from the name, for calculating precision at n, only the top n sentences are taken into account, and the number of correct items within these top n sentences are counted as precision at n.


                        Mean average precision is another common metric in the information retrieval community. Average precision calculates precision of the system after retrieving each relevant document/sentence and then averages these precision values. Since in information retrieval we have a set of queries, the mean of the average precisions over all queries is reported. For each input query, the average precision is calculated as follows:
                           
                              (13)
                              
                                 Average precision
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       K
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          k
                                          =
                                          1
                                       
                                       
                                          K
                                       
                                    
                                 
                                 P
                                 @
                                 k
                              
                           
                        where k is the rank of relevant documents in the retrieved list.


                        Reciprocal rank is mostly used for applications which are interested in only one correct answer; i.e., no matter how many relevant items are available in the retrieved list, as soon as the first correct item appears, the user is satisfied with the results. This measure is more useful for the question answering community than it is for the information retrieval community, because in question answering, normally one correct item is enough to answer a question. For each input query, the score is the inverse of the rank at which the first correct sentence is returned. In case no correct sentence appears in the output list, reciprocal rank is zero. Similarly to mean average precision, which calculates the mean over all queries, mean reciprocal rank is also the mean over the reciprocal rank of all queries. For each input query, the reciprocal rank is calculated as follows:
                           
                              (14)
                              
                                 Reciprocal rank
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       R
                                    
                                 
                              
                           
                        where R is the position of the first correct item in the ranked list.

To evaluate the class-based model for sentence retrieval, we used the sentence retrieval evaluation set, as described in Section 7.1. Before evaluation, we had to cluster lexical items by the Brown word clustering algorithm. To this aim, we used the SRILM toolkit (Stolcke, 2002), as it contains the implementation of the Brown algorithm. SRILM is a toolkit that is under development in the SRI Speech Technology and Research Laboratory
                           11
                           
                              http://www-speech.sri.com/.
                        
                        
                           11
                         since 1995. The toolkit was developed to build and apply statistical language models. The preliminary goal of this toolkit was to be used in speech recognition, statistical tagging and segmentation, and machine translation. However, this toolkit is also useful for other applications, such as language model-based information retrieval.

Based on the SRILM implementation, the vocabulary and the co-occurrence statistics of the vocabulary terms are used as the input and then the vocabulary terms are clustered into a predefined number of clusters based on the co-occurrence statistics. Since the algorithm requires specifying the number of clusters, tests were conducted for 50, 100, 200, 500, and 1000 clusters of the vocabulary terms. So that, we can find out how many clusters are suitable for our sentence retrieval task. In all of our experiments, we used a 35,000-word vocabulary.

As mentioned, four different types of word co-occurrence have been applied in our experiments: document-wise, sentence-wise, syntactic relation, and bigram. All these notions of word co-occurrence are extracted from AQUAINT1. In addition, we also used the Google n-gram corpus to study the impact of the corpus size on sentence retrieval performance. Since there is no possibility of extracting document-wise, sentence-wise, or syntax-wise co-occurrence statistics from the Google n-gram corpus, we only collected bigram statistics which are available in this corpus.


                        Table 3
                         shows the number of word pairs extracted from the two corpora with different definitions of co-occurrence. The statistics only include word pairs for which both constituent words are present in the 35,000-word vocabulary of our search collection.

Mean average precision of class-based sentence retrieval for term clustering using different definitions of word co-occurrence from AQUAINT1 is shown in Fig. 8
                        . For all experiments Bayesian smoothing with Dirichlet prior (Mackay & Peto, 1995) was applied.

The first observation from the results is that the class-based model significantly outperform the word-based baseline model. In addition to this observation, by comparing different notions of word co-occurrence for word clustering, we can see that:
                           
                              •
                              The best result achieved by the sentence-wise co-occurence is similar to the best result of the document-wise co-occurrence.

All the results achieved by the syntax-wise co-occurrence are superior to the sentence-wise co-occurrence. This observation indicates that merely co-occurring in a sentence is not very indicative of word similarity, while relations extracted from syntactic structure improve system performance.

Bigram co-occurrence significantly outperforms all other notions of co-occurrence; i.e., using a very narrow context which captures completely local information achieves the best clustering results.

In the next step of our experiments, we evaluated the performance of sentence retrieval based on the clusters trained on the Google n-gram corpus. Fig. 9
                         shows the mean average precision for class-based sentence retrieval when the bigram co-occurrence statistics from the Google n-grams are used. For better visualization, we repeated the mean average precision results using AQUAINT1 bigram statistics from Fig. 8 in Fig. 9.

Based on the results, the following observations are notable:
                           
                              •
                              The bigram co-occurrence statistics derived from Google n-gram consistently result in better mean average precision than AQUAINT1 bigram. The result indicates that even though the Google n-gram corpus is from a different and much broader domain than the test data, it outperforms the AQUAINT1 corpus due to sheer size.

The mean average precision curve of the bigram statistics from Google n-gram is flatter than AQUAINT1 bigram; i.e., the performance is not very sensitive to the number of clusters. As a result, this model is more reliable when the optimum number of clusters is not clear.

Finally, Table 4
                         shows mean average precision, mean reciprocal rank, and precision at 5 for each notion of co-occurrence with the best number of clusters. From the table we can see that overall, the best result is from Google bigram co-occurrence statistics with 100 clusters, achieving 0.4210 mean average precision; while the best model derived from AQUAINT1 has 0.4174 mean average precision for bigram co-occurrence with 100 clusters, and the mean average precision of the word-based model (baseline) is 0.3701.

To evaluate the proposed trained trigger language model, we used the sentence retrieval evaluation set, as described in Section 7.1, while the following corpora are used to train different notions of triggering:
                           
                              •
                              Inside Sentence Triggering: AQUAINT1

Across Sentence Triggering: AQUAINT1

QA Pair Triggering: QASP and Yahoo! Answers

Named Entity Triggering: QASP

As mentioned, any of the available smoothing techniques that normally used for word unigram can also be applied to the trained trigger language model. However, as showed by Murdock (2006), Momtazi and Klakow (2009), unlike document retrieval, sentence retrieval performance is not sensitive to the effect of smoothing and three well-known smoothing methods (Zhai & Lafferty, 2001), namely absolute discounting, Bayesian smoothing with Dirichlet prior, and Jelinek-Mercer linear interpolation achieve very similar results in sentence-level retrieval. Following this observation, we only used Bayesian smoothing with Dirichlet prior to smooth word unigram and trained trigger models.

To evaluate different types of triggering we interpolated them with a baseline model which consider the exact matching between query terms and sentence terms. As we mentioned in Section 5.2, the proposed triggering types only consider contextual information and they do not give a priority to the sentences that contain the query words. As a result, each of the triggering types, including inside sentence triggering, across sentence triggering, question and answer pair triggering, and named entity triggering are interpolated with either word unigram or self triggering models as baseline. Table 5
                         presents the results of our experiments, interpolating different notions of triggering with either self triggering or word unigram.

The results show that using any of the contextual information in addition to the exact matching models significantly improves sentence retrieval performance in which all the improvements are highly statistically significant 
                           
                              (
                              p
                              <
                              0.01
                              )
                           
                         according to the 2-tailed paired t-test.

The results verify that searching for both the exact literal match and the trigger and target words between question and sentences play important roles. When the exact matching models avoid returning non-relevant sentences, they fail to find most of the relevant sentences which share few words with the question. The triggering models, on the other hand, are able to capture more relevant sentences and enhance retrieval performance when they are integrated with the exact matching models.

In addition to the above observation, comparing different notions of triggering, the following points should be highlighted:
                           
                              •
                              Comparing the results of the baseline word unigram model and our proposed self triggering model, we investigated the correctness of our hypothesis about self triggering. We hypothesized that self triggering should perform similarly to word unigram. As presented in the table, the difference between word unigram and self triggering performance is not statistically significant, which shows that both models perform similar to each other. The minor difference between self triggering and word unigram comes from the role of smoothing in calculating the query likelihood probability. In word unigram, we smooth the sentence model first and then calculate the probability of query word given smoothed sentence model. In self triggering, however, we first calculate the probability of query word given each sentence word and smooth the probability afterward.

The results of interpolating different triggering models with self triggering is similar to the results of interpolating those models with the word unigram model; i.g., there is no significant difference between the performance of sentence retrieval when either self triggering or word unigram is used as baseline with the inside sentence triggering. This similarity indicates that it is not necessary to use a particular model as the baseline for the proposed trained trigger models. As a result, this model can easily be integrated with any of the available language modeling techniques.

Comparing the results of the unsupervised notions of triggering, we can see that inside sentence triggering and across sentence triggering perform close to each other. It might be due to the similar word relationship that they capture.

Comparing the results of question and answer triggering based on different corpora, we can see that the model trained on the Yahoo! Answers corpus outperforms the model trained on the QASP corpus. This observation indicates that even though the Yahoo! Answers corpus is from a different and much broader domain than the QASP corpus and it includes more noisier data than QASP, it significantly improves the system performance due to sheer size. In addition, this result shows that although automatic QA systems are considered a different topic from community QA forums, the later ones can be a great help to improve the performance of the former ones.

Comparing the results of named entity triggering with other notions of triggering, we can see that this model underperforms other types of triggering perhaps due to the small training corpus. The model, however, is still significantly superior to the baseline exact matching models.

Overall, the question and answer pair triggering based on the Yahoo! Answers corpus, which achieved the best performance among models trained in a supervised fashion, performs similar to the unsupervised notions of triggering. This observation shows that even without using an annotated corpus we can achieve significant improvements that are comparable with the supervised methods.

As another step of our experiments, using Eq. (12), we interpolated more than one notion of triggering with the baseline to see whether we can achieve more improvement by benefiting from different approaches simultaneously. Fig. 10
                         shows sentence retrieval MAP when interpolating two different notions of triggering with word unigram as baseline. To have a better visual comparison, parts of the results from Table 5 were repeated here.

As shown in the figure, interpolating inside sentence triggering and across sentence triggering with the baseline did not beat the performance compared to using only one of these models with the baseline. Similarly, the results shows that interpolating question and answer pair triggering using both the QASP corpus and the Yahoo! Answers corpus did not improve the performance of the system compared to using the Yahoo! Answers corpus alone. On the other hand, using inside sentence triggering together with question and answer pair triggering based on the QASP corpus improves the system performance. Interpolating the baseline with across sentence triggering and question and answer pair triggering based on the Yahoo! Answers corpus also shows an improvement compared to interpolating the baseline with either of these methods. We have the same observation when interpolating inside sentence triggering and named entity triggering with the baseline. One reason for such different results in these combinations is that both inside and across sentence triggering models capture similar word relationships as they both train the model in an unsupervised fashion while using the same training corpus. As a result, achieving more improvement by combining similar approaches is not easy. Similarly, both models which use question and answer pair triggering train the model in a supervised fashion and based on the same idea. As a result, it is difficult to achieve more improvement when combining them together. Using inside sentence triggering with either question and answer triggering or named entity triggering, or using across sentence triggering with question and answer triggering, however, interpolates two different approaches which were trained on different types of corpora. As a result, it is easier to achieve an improvement on system performance when using totally different notions of triggering.

Overall, we can see the interpolation of named entity triggering and inside sentence triggering with word unigram baseline achieved the best performance. This result indicates that although named entity triggering could not beat the other models, it can significantly improve sentence retrieval performance when combining with another notion of triggering. The main reason for such result is the different behavior of named entity triggering compared to other types of triggering. While other notions capture the relationship between words, named entity triggering only focuses on the named entity label of words. This information might not be enough to achieve a high performance, but when it comes along other models its positive impact becomes more pronounce.

As we saw, the proposed class-based and trained trigger language models, which both address the same problem in sentence retrieval, significantly improve sentence retrieval performance compared to the standard word unigram model which does not consider such contextual information.

As mentioned, language model-based information retrieval provides a comprehensive framework to retrieve relevant sentences. The original word unigram model, however, suffers from the exact matching assumption; i.e., this model only retrieves sentences which share many words with the input query, and cannot retrieve sentences which share no or few words with the query, but contain other words that closely relate to the query. Our proposed models aim at relaxing this assumption by taking words relationships into consideration. However, the algorithms used for considering word relationships and also the types of word relationships captured with the two proposed models are different. The trained trigger model captures the possible relations between a query term and each of the sentence terms using the pairs of trigger and target words extracted from a training corpus and calculates the probability of the query term given the sentence term based on these relations which result in retrieving more relevant sentences. In the class-based model, however, instead of the fine-grained literal representation of words, a coarse-grained word representation, which is word clusters, is defined. Thus, the input query and the sentences become more general and the model can retrieve more relevant sentences as a result. In addition to the algorithms, the ways that the word relationships are captured in these two models are different. Using the Brown clustering algorithm in class-based model, the words that have similar context are assigned to the same cluster; i.e., two words that are in the same cluster, are not necessary co-occurred with each other, but they are mutually co-occurred with the same other words. The trained trigger model, however, consider the direct co-occurrence of the words with each other. This difference also answers the question what is the difference between the sentence-wise co-occurrence in the class-based model and inside sentence triggering in the trigger model. In sentence-wise co-occurrence used within the clustering algorithm, two words are considered relevant if they frequently co-occur with the same words within the scope of sentences, while in inside sentence triggering two words are considered relevant if they frequently co-occur with each other in sentences.

Comparing the results of the class-based model and the trained trigger model, as presented in Fig. 11
                        , we can see that the trigger language model improves sentence retrieval performance more than the class-based model. This result indicates the superiority of trigger model to the class-based model in terms of effectiveness. In terms of efficiency of the models, however, the class-based model is better than the trigger model and it has a lower time complexity compared to the trigger model. While the trigger model is running in 
                           
                              O
                              (
                              KMN
                              )
                           
                         time, the class-based model is running in 
                           
                              O
                              (
                              KM
                              )
                           
                        .

As another step of our experiments, we combined our both proposed models using a linear interpolation between the baseline exact matching model, the class-based model, and the trained trigger model while the following methods were used for each of the models:
                           
                              •
                              baseline model: word-based unigram

class-based model: 100 clustered trained of bigram statistics extracted from the Google n-gram corpus

trained trigger model: across sentence triggering trained on the AQUAINT1 corpus

We can see that the combination of class-based, triggering, and unigram models beats each of the individual techniques. However, the improvement is not statistically significant.

Additionally, we compared our proposed models with the state-of-the-art language modeling technique which addresses the same problem of information retrieval. To this aim, we implemented the translation model (Berger & Lafferty, 1999) while using mutual information for estimating the translation probability (Karimzadehgan & Zhai, 2010). Moreover, we compared our models with the pseudo-relevance feedback (Rocchio, 1971) as one of the promising query expansion approaches. The results are presented in Fig. 11.

Although various researchers have shown that pseudo-relevance feedback improves document retrieval performance significantly (Lease, 2009), it is clear from the results that this technique degrades sentence retrieval performance. One of the reasons for the mixed success of this method in sentence retrieval is the problem of sentence brevity, which leads to very low term frequency at the sentence level. In the query expansion tasks, when we expand the query based on the available words in the top ranked documents, it is nearly unavoidable to have a few spurious terms in the list of expanded terms. Since different words in the documents have different frequencies, the problem of spurious terms does not affect the document retrieval performance, because the frequency of the correct expanded terms in the relevant documents is normally much higher than the frequency of the spurious words. As a result, the score of the documents that are retrieved due to the spurious words is lower than the score of the correctly retrieved documents. In sentence-level retrieval, contrary to document retrieval, since the sentences are normally very short and it is unlikely to have multiple mentions of the same word in a sentence, only a few words from the expanded terms are matched with the sentences. As a result, the scores of the sentences that are matched to the correct expanded terms are similar to the scores of the sentences that are matched to the spurious words. Therefore, many non-relevant sentences are ranked highly when using query expansion techniques.

For the translation model, we used mutual information for estimating the translation probability (Karimzadehgan & Zhai, 2010). As shown by Karimzadehgan and Zhai (2010), normalized mutual information between word pairs is the best estimation of the statistical translation model and it outperforms the original translation model (Berger & Lafferty, 1999) which is estimated based on synthetic queries. Following Karimzadehgan and Zhai (2010), we estimated translation models using normalized mutual information between word pairs and regularized the models by self-translation probabilities. To have a reasonable comparison between the translation model and our models, we used the AQUAINT1 corpus for calculating mutual information in the translation model. The presented results for both class-based and trained trigger models are also based on the same training corpus.

As shown in the results, both class-based and trained trigger language models significantly outperform the translation model. Although the translation model also addresses the word mismatch problem and outperforms the standard word unigram model, it underperforms our proposed models. The reported results verify the superiority of the class-based and trained trigger language models in capturing word relationships compared to the translation model.

Comparing our proposed models with the translation model, we can see that the idea of the trigger model is close to the translation model in which both models find a relationship between the query term and each of the sentence terms. The trained trigger language model, however, exploits additional information to relax the exact matching assumption. To find pairs of trigger and target words in an unsupervised fashion, we used different kinds of context that a word appears in. In addition, we used a set of question and answer sentence pairs to capture the trigger and the target terms in a supervised fashion and also employed the named entity information to further extend this model. Moreover, in the trigger model we provided a framework to benefit from different notions of triggering at the same time which results in further improvement as represented in Fig. 10. Overall, we defined five different notions of triggering to improve sentence retrieval performance while using both unsupervised and supervised approaches.

In this paper, we proposed two new language modeling approaches for sentence-level retrieval to capture term relationships and relax the exact matching assumption.

By applying the class-based model, we found out that this model which uses term clustering is an effective approach to this end. In addition, we compared different notions of word co-occurrence for clustering terms, using document-wise, sentence-wise, window-wise, and syntax-wise co-occurrence statistics derived from AQUAINT1 and Google n-gram data sources. The result showed that although the domain of the Google n-grams is dissimilar to the test set, it outperforms the models derived from AQUAINT1 due to sheer size. Using bigram statistics from the Google n-gram corpus improves the mean average precision of sentence retrieval from 0.3701 to 0.4210.

The trained trigger model finds pairs of trigger and target words based on their co-occurrence. We used various notions of triggering in both unsupervised and supervised fashions. The results showed that both unsupervised and supervised approaches significantly improve sentence retrieval performance when they are used in combination with the baseline models. The results verify that using both the term relationship and the exact matching methods is important to build an accurate sentence retrieval engine. Our results also showed that we can achieve more improvement when exploiting more than one notion of triggering: using word unigram as the baseline with inside sentence triggering and named entity triggering is the best combination of our proposed methods which enhances the system mean average precision to 0.4437.

The results indicated that our class-based and trained trigger language models outperform the state-of-the-art translation model. Using the same training corpus, the class-based model achieved 2.47% absolute improvement in MAP compared to the translation model based on mutual information (Karimzadehgan & Zhai, 2010). Moreover, the trained trigger model achieved 4.54% absolute improvement in MAP. Both improvements are statistically significant according to the 2-tailed paired t-test, 
                        
                           p
                           <
                           0.05
                        
                      and 
                        
                           p
                           <
                           0.01
                        
                      respectively.

@&#ACKNOWLEDGEMENTS@&#

The authors would like to thank Yahoo! Labs Webscope for their Yahoo! Answers Comprehensive Questions and Answers corpus. Saeedeh Momtazi was funded by the German research foundation DFG through the International Research Training Group (IRTG 715) Language Technology and Cognitive Systems.

@&#REFERENCES@&#

