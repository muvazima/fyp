@&#MAIN-TITLE@&#Word segmentation and pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Human translations guided language discovery for speech processing.


                        
                        
                           
                           Pronunciation extraction for non-written languages using cross-lingual information.


                        
                        
                           
                           Alignment model Model 3P for cross-lingual word-to-phoneme alignment.


                        
                        
                           
                           Algorithm to deduce phonetic transcriptions of words from Model 3P alignments.


                        
                        
                           
                           Analysis of appropriate source languages based on efficient evaluation measures.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Pronunciation dictionary

Non-written languages

Lexical language discovery

Under-resourced languages

Speech-to-speech translation

Word segmentation

@&#ABSTRACT@&#


               
               
                  In this paper, we study methods to discover words and extract their pronunciations from audio data for non-written and under-resourced languages. We examine the potential and the challenges of pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment. In our scenario a human translator produces utterances in the (non-written) target language from prompts in a resource-rich source language. We add the resource-rich source language prompts to help the word discovery and pronunciation extraction process. By aligning the source language words to the target language phonemes, we segment the phoneme sequences into word-like chunks. The resulting chunks are interpreted as putative word pronunciations but are very prone to alignment and phoneme recognition errors. Thus we suggest our alignment model Model 3P that is particularly designed for cross-lingual word-to-phoneme alignment. We present two different methods (source word dependent and independent clustering) that extract word pronunciations from word-to-phoneme alignments and compare them. We show that both methods compensate for phoneme recognition and alignment errors. We also extract a parallel corpus consisting of 15 different translations in 10 languages from the Christian Bible to evaluate our alignment model and error recovery methods. For example, based on noisy target language phoneme sequences with 45.1% errors, we build a dictionary for an English Bible with a Spanish Bible translation with 4.5% OOV rate, where 64% of the extracted pronunciations contain no more than one wrong phoneme. Finally, we use the extracted pronunciations in an automatic speech recognition system for the target language and report promising word error rates – given that pronunciation dictionary and language model are learned completely unsupervised and no written form for the target language is required for our approach.
               
            

@&#INTRODUCTION@&#

@&#MOTIVATION@&#

We explore cross-lingual information to discover the vocabulary of an non-written target language. We align source language words to target language phoneme sequences across languages, i.e. cross-lingually. Based on this alignment, we induce phoneme sequences forming words of the target language. The resulting pronunciations are used in automatic speech recognition (ASR) systems. Our research is inspired by the following challenges that language technology faces nowadays:
                           
                              •
                              There are over 7100 living languages and dialects in the world (Gordon and Grimes, 2014). ASR and Machine Translation (MT) systems exist only for few of them due to the large amount of monolingual or respectively bilingual data which is necessary to train such systems. For example, transcribed speech resources, large amounts of text for language modeling, pronunciation dictionaries, and parallel sentence-aligned text corpora are of great importance to create speech processing systems. Schultz and Kirchhoff (2006, p. 37) estimate that thorough transcription of 1h conversational speech data normally takes about 20h of effort. Undoubtedly, creating sentence-aligned parallel corpora required by MT tools is very time consuming as well. Therefore, a major challenge of language technologies in our globalized world is to overcome this dependence on large amounts of training data or to significantly reduce the costs of data collection efforts. Our approach only requires written sentences in a source language, spoken translations of them in the target language and a phoneme recognizer which can recognize the target language phonemes.

A lot of the world's languages and dialects do not have an acknowledged written form (Schultz and Kirchhoff, 2006; Nettle and Romaine, 2000) despite their widespread use for oral communication (for example reported for Arabic dialects by Chiang et al. (2005)). Nowadays, language technology generally requires a written script. Our approach bypasses the written form, since it operates on the phoneme level on the target language side.

Rapid language adaptation (for instance investigated by Vu et al. (2010)) deals with the challenge of rapidly porting ASR systems to new languages and domains. These techniques are suitable in situations where languages with few linguistic resources suddenly appear in the focus of interest due to various reasons – for example Haitian Creole in the scope of international relief operations for victims of the Haiti earthquake in 2010. Results of this work might be applicable to such a situation for developing ASR and MT systems in the future since the only resources required in the target language are spoken translations which can be generated in near real-time.

Human simultaneous translations are common in the real world, for instance in the international press or multilingual parliaments. If we have access to a speech recognition system that transcribes the source language and generates the required written translations, such parallel speech (Paulik and Waibel, 2013) can be used for our approach.


                           Fig. 1
                           
                           
                              1
                           
                           
                              1
                              Worf's head is taken from http://www.flickr.com/photos/datarknz/3442646145/ (accessed 05.10.14).
                            illustrates the scenario that we investigate in this work and shows which resources are assumed to be available. English serves in this example as the resource-rich source language. We have chosen the constructed language Klingon (Okrand, 2008) spoken by the fictional Klingons in the Star Trek universe to represent an under-resourced target language. We assume that we are able to peacefully convince a Klingon understanding English to speak Klingon translations of English sentences – in this case “I am sick” and “I am healthy”. These written English sentences may have been generated by an English ASR system. Based only on this limited knowledge base, an English speaker may hypothesize that /ji/, /piv/ and /rop/ are phoneme sequences corresponding to Klingon words, although he or she has no knowledge of Klingon at all.

No more than the described resources are required for our approach, i.e. we assume to have only the following items available:
                              
                                 •
                                 Source language sentences in written form.

Their spoken translations in the target language.

Phoneme recognition for the target language.

We develop methods that
                              
                                 •
                                 extract the vocabulary of the target language and

reconstruct target language word sequences uttered by the speaker.

Our goal is to support the development of language technologies for under-resourced and non-written languages:
                              
                                 •
                                 Since, we operate only on the phoneme level, no written form is required in the target language. Therefore, our approach is particularly relevant for non-written languages and dialects.

We extract the vocabulary and the corresponding word pronunciations, which can be used in a pronunciation dictionary for the target language. The pronunciation dictionary is an important part of ASR systems.

Reconstruction of the word sequence from the phoneme sequence in the target language allows us to build language models that are essential in both ASR and MT systems. Moreover, we obtain a parallel sentence-aligned corpus on the word level through the hypothesized system very cost-efficiently which can be used for training a statistical MT system.

In this work, we study the influence of phoneme recognition errors to our methods but do not focus on the phoneme recognition itself: In Section 2.3, we simulate recognition errors by inserting errors according to the smoothed confusion matrix of a real phoneme recognizer. In Section 3.3, in addition to error-free phonetic transcriptions, we compare phoneme recognizers with 13.1% and 45.1% recognition error rate that have both been trained on target language (English) transcriptions (oracle). As we argue in Section 3.3.2 we can expect the quality of speaker-dependent phoneme recognition for other target languages in the future without transcriptions to be in this magnitude.

In order to present our algorithms, we reproduce how an unbiased English speaker would approach the task of extracting Klingon words from fluent speech as in Fig. 1: First, he or she would subconsciously consider the speech as a sequence of articulated sounds (phonemes) rather than pure acoustics. Second, he or she would try to find a cross-lingual mapping between words and phonemes based on different assumptions – for example that a given English word(-sequence) (like I am) is represented by the same Klingon phoneme(-sequence) (like /ji/) wherever it occurs. If this particular mapping occurred frequently, it would strongly suggest that I am and /ji/ are translations of one another, and that /ji/ is a Klingon word. This fundamental idea is also incorporated into IBM Models 1–5 (Brown et al., 1993) in Statistical Machine Translation (Koehn, 2010) in the form of lexical translation probabilities. Furthermore, by the process of elimination, the content words /rop/ and /piv/ could be extracted although they occur in the corpus only once.

As shown in Fig. 2
                           , we transfer this paradigm to machines as follows: we start with the written word sequence in the source language on one side and the audio file of the spoken translation in the target language on the other side. The first step is to run a phoneme recognizer and transform the audio file into a phoneme sequence. However, this sequence does not contain any clues of where a word ends and the next word starts. In order to find such word boundaries, we have to consult other knowledge sources (such as other occurrences of the source language words in the corpus) and find a mapping between the source language words and the target language phonemes. In other words, we have to find a word-to-phoneme alignment. Section 2.1 provides a brief introduction to the theory of finding such alignments and Section 2.2 presents our solution. Given such an alignment, it is straightforward to extract word boundary markers: We insert a word boundary marker into the phoneme sequence wherever two neighbouring phonemes are aligned to different source language words (i.e. wherever a black alignment line in Fig. 2 ends).

Then, we deduce word pronunciations from these units, introduce the vocabulary in terms of word labels, and extract a pronunciation dictionary. As dictionaries are so fundamental to speech processing systems, much care has to be taken to select a dictionary that is as free of errors as possible. Therefore, our proposed methods compensate for alignment and phoneme recognition errors. The following paragraph sketches the complete scenario.

(1) We recognize the spoken translations with a phoneme recognizer. (2) We build an alignment between the source language words and the phonemes in the corresponding recognized phoneme sequence in the target language. (3) Using this cross-lingual alignment, we segment the phoneme sequence into word units. (4a) The word segmentation induces phonetic transcriptions of target language words, which are used in a pronunciation dictionary for an ASR system. (4b) The segmented phoneme sequence is replaced by a sequence of word labels. This results in a parallel training corpus on the word level for a Statistical Machine Translation (SMT) system as described by Besacier et al. (2006) which is also used for language modeling. Our final goal is to bootstrap a speech-to-speech translation (S2S) system without any linguistic knowledge of the target language. In this paper we focus on step 2, step 3 and step 4a.

We have to face two major challenges:
                              
                                 •
                                 
                                    High phoneme error rates. One great advantage of our approach is that no a priori knowledge about the target language is necessary. At the same time, this is a severe drawback since no acoustic models and therefore no highly tuned language-dependent phoneme recognizer is available for the target language – even the phoneme set might be unknown. This introduces frequent phoneme recognition errors. In our later experiments, we investigate the influence of recognition errors but refrain from training a phoneme recognizer in a completely unsupervised way and refer to Jansen et al. (2013).


                                    Inaccurate alignments. Another major challenge is to compensate for frequent alignment errors. We use alignment models derived from SMT to find word-to-phoneme mappings. However, as in general the solution space for word-to-phoneme alignments is significantly larger than for word-to-word alignments, finding such alignments automatically is even harder. Furthermore, high phoneme error rates interfere with statistical alignment models.

@&#RELATED WORK@&#

Dictionaries are used to train speech processing systems by describing the pronunciation of words in manageable units such as phonemes (Martirosian and Davel, 2007). The production of pronunciation dictionaries can be time-consuming and expensive if they are manually written by language experts. Therefore, several data-driven approaches to automatic dictionary generation (Bisani and Ney, 2008; Novak et al., 2012), and to leverage off pronunciations from the World Wide Web have been introduced (Ghoshal et al., 2009; Can et al., 2009; Schlippe et al., 2010, 2012, 2014). However, those approaches do not help for dialects or languages without a written form. If the target language does not have a written form, it has been proposed that one be defined, though training people to use it consistently is in itself very hard and prone to inconsistencies (e.g. Iraqi Arabic transcription techniques in the Transtac Speech-to-Speech Translation Project Besacier et al., 2006). As we operate only on the phoneme level on the target side, we implicitly introduce an artificial writing system, where the words are represented by their pronunciations (word labels). This enables to bypass the written form in S2S. Thus our approach is highly relevant for S2S of under-resourced languages (Besacier et al., 2014), and those which are not written.

In this work, we use cross-lingual word-to-phoneme alignments to segment a phoneme sequence into word-like chunks and therefore require written translations in a source language. In addition to bilingual word segmentation approaches as ours, the following monolingual methods for word segmentation have been used in the past that do not need written translations. First, Minimal Description Length analysis (Kit, 2000; Goldsmith, 2006) approximates the optimal compression of a (phoneme-)string (corresponding to its Kolmogorov complexity). Assuming that a word sequence of a language is the optimal compression of the corresponding phoneme sequence, the data segmentation induced by such compression methods is taken as the word segmentation. The second approach uses adaptor grammars (Johnson, 2008), which are context free grammars that learn new rules from the training data. Since recent studies underline the feasibility of applying adaptor grammars to the word segmentation problem (Johnson and Goldwater, 2009; Jansen et al., 2013), we use them in Section 2.3 representatively for all monolingual word segmentation methods.

In our approach, we use the information of a parallel segment-aligned corpus between word sequences in the source language and phoneme sequences in the target language similar to Besacier et al. (2006). In an oracle experiment, they replace the words in the target language with their pronunciations and remove word boundary markers. For segmenting these phoneme sequences into words, however, they run a monolingual unsupervised algorithm in contrast to us using cross-lingual word-to-phoneme alignment. After applying the monolingual segmentation to an Iraqi phoneme sequence, they use the resulting word sequences in the training process of an MT system which translates Iraqi phoneme segments to English words. Their results show that even with low word accuracy of their word segmentation algorithm (55.2%), vocabulary extraction efforts are applicable to MT.

To the best of our knowledge, Stüker and Waibel (2008) are the first who align words in one language to phonemes in another language, which is the basic idea of our approach. They use the word-to-word aligner GIZA++ (Och and Ney, 2003) to align English word sequences to Spanish phoneme sequences on the BTEC corpus (Kikui et al., 2003). Stüker et al. (2009) combine the monolingual word segmentation method proposed by Besacier et al. (2006) and the GIZA++-based approach by taking the most frequent word segments from the monolingual segmentation output, replacing them and then using GIZA++ for segmenting the remaining phoneme sequences. Both works report acceptable alignment quality on perfect phonetic transcriptions. However, Stahlberg et al. (2012) show that the word segmentation precision of GIZA++ word-to-phoneme alignments is not significantly higher than a monolingual approach when phoneme recognition errors are more common. Therefore, they propose a new alignment model for word-to-phoneme alignment (see Section 2) and achieve significantly higher word segmentation and alignment quality in F-score, precision, and accuracy. Stahlberg et al. (2013) conduct first experiments to extract pronunciations from the segmented phoneme sequences. Stahlberg et al. (2014) study the influence of phoneme recognition errors to the pronunciation extraction quality from word-to-phoneme alignments using a new extraction method (source word independent clustering). Stahlberg et al. (2014) also bootstrap an ASR system for the target language based on the extracted pronunciations. In this work, we extend the investigation of phoneme recognition errors and compare both pronunciation extraction methods by applying them to new language pairs from the Christian Bible. Additionally, we also report the performance of target language speech recognizers using the extracted pronunciations.

Our methods are based on cross-lingual word-to-phoneme alignments. Therefore, it is required to obtain the phoneme sequence in the target language. Bootstrapping a phoneme recognizer without information of the target language is not a trivial task. It may be bootstrapped using recognizers from other languages and adaptation techniques as presented by Vu et al. (2011) and Sitaram et al. (2013). Phonetic language discovery in zero-resource speech technologies (Jansen et al., 2013) (i.e. identifying phoneme like subword units for acoustic modeling in an unknown target language) is addressed among others by Lee and Glass (2012), Varadarajan et al. (2008), Chaudhuri et al. (2011). Since we focus on word segmentation and pronunciation extraction, we use oracle phoneme recognizers with different error rates as described in Section 3.3.2 for simplification.

We tackle the following challenges for the “human translations guided language discovery for speech processing” (Stüker and Waibel, 2008; Besacier et al., 2014). First, we present our alignment model Model 3P to cross-lingually segment phoneme sequences in a target language into word units with the help of written translations in a source language (Section 2). Stahlberg et al. (2012) show that using Model 3P outperforms a state-of-the-art monolingual word segmentation approach (Johnson, 2008) and a GIZA++ alignment on the English-Spanish portion of the BTEC (Kikui et al., 2003) corpus. In this work, we additionally switch the roles of English and Spanish and report results with Spanish being the target language. Furthermore, we extend our experiments to a wide variety of new language pairs with data from the Christian Bible.

Second, we present two algorithms to deduce phonetic transcriptions of target language words from Model 3P alignments (source word dependent (Stahlberg et al., 2013) and independent clustering (Stahlberg et al., 2014)), introduce the target language vocabulary in terms of word labels, and extract a pronunciation dictionary. Stahlberg et al. (2013) explore 14 translations in 9 languages in an oracle experiment based on error-free phonetic transcriptions to build a dictionary for English and investigate the criteria of the translations to achieve high quality in the resulting dictionary. They test our pronunciation extraction algorithm (source word dependent clustering) on parallel data from the Christian Bible since it is available in many languages in written form and in some languages also as audio recordings. A variety of linguistic approaches to Bible translation (Thomas, 1990) allows to compare different translations within the same source language. In this work, we additionally investigate the influence of phoneme recognition errors. Stahlberg et al. (2014) evaluate source word independent clustering on a small corpus consisting of 200 sentences but also take recognition errors from a language-mismatched phoneme recognizer into consideration. In the present paper we apply source word independent clustering to the much larger Bible corpus and compare it to source word dependent clustering. We learn that source word dependent clustering avoids multiple entries for the same word in the dictionary but source word independent clustering produces phonetically better pronunciations and works better on noisy input.

Finally, we use the extracted pronunciations in an ASR system for the target language English in Section 4. For example, when we have access to a speaker-dependent acoustic model, we achieve a word error rate (WER) of 33.4%.

Cross-lingual word-to-phoneme alignments introduced by Besacier et al. (2006), Stüker and Waibel (2008), Stüker et al. (2009) and tackled by us with the alignment model Model 3P (Stahlberg et al., 2012) are the basis for our pronunciation extraction algorithm in Section 3. In the following sections, we describe our approach for finding word-to-phoneme alignments automatically with English taking the role as the resource-rich source language and Spanish as the “under-resourced” target language. Afterwards, we demonstrate the performance of Model 3P on a English-Spanish portion of the BTEC corpus (Kikui et al., 2003) and a variety of translations of the Christian Bible.

As our approach is highly inspired by ideas originating from statistical machine translation (SMT) (Koehn, 2010), we briefly discuss those in this section. The central data structure in SMT is the word alignment, which identifies word position pairs in parallel sentences that belong to words that are translations of one another. For instance, the alignment in Fig. 3
                         indicates that the Spanish word esto is a possible translation of the English word this. Various statistical models for estimating the probability of such alignments exist in literature, such as the HMM Model (Vogel et al., 1996), the IBM Model hierarchy 1–5 (Brown et al., 1993) and their variations (Och and Ney, 2000, 2003). GIZA++ (Och and Ney, 2003) is an implementation of the IBM Models and the HMM Model widely used in SMT for automatically finding word-to-word alignments. In our experiments we use GIZA++ to find initial experiments which are then further optimized according our proposed alignment model Model 3P.


                        Model 3P is an extension of the IBM Model 3 (Brown et al., 1993). The parameters of the latter model are composed of a set of fertility probabilities n(·|·), p
                        1, a set of translation probabilities t(·|·), and a set of distortion probabilities d(·|·). The fertility models how many target words can be produced by a single source word. The translation probabilities indicate the possible lexical translations for a source word. The distortion model deals with word reorderings like moving the word for from the last to the first position in the alignment in Fig. 3. According to IBM Model 3, the following generative process produces the target language sentence f from a source language sentence e with length l (Knight, 1999).
                           
                              1
                              For each source word e
                                 
                                    i
                                  indexed by i
                                 =1, 2, …, l, choose the fertility (how many target words e
                                 
                                    i
                                  produces) ϕ
                                 
                                    i
                                  with probability n(ϕ
                                 
                                    i
                                 |e
                                 
                                    i
                                 ) (fertility step).

Choose the number ϕ
                                 0 of “spurious” target words that are not to be generated from a word in the source sentence using probability p
                                 1 and the sum of fertilities from step 1. Sometimes it is more appropriate to generate target words “spuriously” because they have no direct origin in the source sentence. This is modeled by inserting an abstract source token e
                                 0
                                 =
                                 NULL at zero-th position that generates the ϕ
                                 0 “spurious” target words (null insertion step).

Let 
                                    m
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          0
                                       
                                       l
                                    
                                    
                                       ϕ
                                       i
                                    
                                 .

For each i
                                 =0, 1, 2, …, l, and each k
                                 =1, 2, …, ϕ
                                 
                                    i
                                 , choose a target word τ
                                 
                                    ik
                                  with probability t(τ
                                 
                                    ik
                                 |e
                                 
                                    i
                                 ) (lexical translation step).

For each i
                                 =1, 2, …, l, and each k
                                 =1, 2, …, ϕ
                                 
                                    i
                                 , choose target position π
                                 
                                    ik
                                  with probability d(π
                                 
                                    ik
                                 |i, l, m) (distortion step).

For each k
                                 =1, 2, …, ϕ
                                 0, choose a position π
                                 0k
                                  from the ϕ
                                 0
                                 −
                                 k
                                 +1 remaining vacant positions in 1, 2, …, m, for a total probability of 1/ϕ
                                 0
                                 !.

Output the target sentence with words τ
                                 
                                    ik
                                  in positions π
                                 
                                    ik
                                  (0≤
                                 i
                                 ≤
                                 l, 1≤
                                 k
                                 ≤
                                 ϕ
                                 
                                    i
                                 ).


                        Fig. 4
                         illustrates the generation of the Spanish sentence Para qué se usa esto from the English sentence What's this used for. Equation 1 states the process as a general formula:


                        
                           
                              (1)
                              
                                 P
                                 (
                                 a
                                 ,
                                 f
                                 |
                                 e
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      m
                                                      −
                                                      
                                                         ϕ
                                                         0
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         ϕ
                                                         0
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 ·
                                 
                                    
                                       (
                                       1
                                       −
                                       
                                          p
                                          1
                                       
                                       )
                                    
                                    
                                       m
                                       −
                                       2
                                       
                                          ϕ
                                          0
                                       
                                    
                                 
                                 ·
                                 
                                    
                                       
                                          p
                                          1
                                       
                                    
                                    
                                       
                                          ϕ
                                          0
                                       
                                    
                                 
                                 ·
                                 
                                    ∏
                                    
                                       i
                                       =
                                       1
                                    
                                    l
                                 
                                 n
                                 (
                                 
                                    ϕ
                                    i
                                 
                                 |
                                 
                                    e
                                    i
                                 
                                 )
                                 ·
                                 
                                    ∏
                                    
                                       j
                                       =
                                       1
                                    
                                    m
                                 
                                 t
                                 (
                                 
                                    f
                                    j
                                 
                                 |
                                 
                                    e
                                    
                                       
                                          a
                                          j
                                       
                                    
                                 
                                 )
                                 ·
                                 
                                    ∏
                                    
                                       j
                                       :
                                       
                                          a
                                          j
                                       
                                       ≠
                                       0
                                    
                                    m
                                 
                                 d
                                 (
                                 j
                                 |
                                 
                                    a
                                    j
                                 
                                 ,
                                 l
                                 ,
                                 m
                                 )
                                 
                                    ∏
                                    
                                       i
                                       =
                                       1
                                    
                                    l
                                 
                                 
                                    ϕ
                                    i
                                 
                                 !
                              
                           
                        
                     

The alignment a is represented as a vector of integers, in which a
                        
                           i
                         stores the position of the source word connected to the target word f
                        
                           i
                        . For instance, a
                        =(4, 1, 1, 3, 2) for the alignment in Fig. 3. This representation implies that a single target word f
                        
                           i
                         cannot be linked with more than one source word – only 1:n mappings are possible. In SMT this restriction is usually addressed by switching the roles of source and target language and symmetrizing the alignments obtained from both translation directions (Koehn, 2010). When we use IBM Model 3 for aligning source language words to target language phonemes, however, the opposite translation direction performs very poorly: Many parameters of IBM Model 3 (for example the translation probabilities) are contingent on the source token itself. So the alignment model has to find the correct word only given a single phoneme. In our experiments, this results in all phonemes being aligned to a single (frequent) word in the sentence and thus contains no valuable information.

Statistical alignment models for word-to-word alignment are well-studied in SMT literature. However, for the word segmentation, aligning the source language words to the target language phonemes is required. One method for automatically obtaining such word-to-phoneme alignments is to use word-to-word alignment models from SMT. Stüker and Waibel (2008) use a perfect phoneme transcription on the target side, and run the word-to-word aligner GIZA++ to align English words to Spanish phonemes. The quality of the found alignments was comparable to similar experiments with words instead of phonemes on the target side. Our experiments in Section 2.3 suggest that significantly better results are achieved by using our new alignment model Model 3P for word-to-phoneme alignment, in particular with respect to word segmentation quality. Therefore, we first run GIZA++ and generate preliminary word-to-phoneme alignments. These alignment serve as initial values for EM optimization according Model 3P. Our new alignment model extends the IBM Model 3 by additional dependencies for the translation probabilities t(·|·) and a set of word length probabilities o(·|·). The generative process upon which it is based can be described as follows:
                           
                              1
                              For each source word e
                                 
                                    i
                                  indexed by i
                                 =1, 2, …, l, choose the fertility ϕ
                                 
                                    i
                                  with probability n(ϕ
                                 
                                    i
                                 |e
                                 
                                    i
                                 ) (fertility step).

Choose the number ϕ
                                 0 of “spurious” target words to be generated from e
                                 0
                                 =
                                 NULL, using probability p
                                 1 and the sum of fertilities from step 1 (null insertion step).

Let 
                                    m
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          0
                                       
                                       l
                                    
                                    
                                       ϕ
                                       i
                                    
                                 .

For each i
                                 =1, 2, …, l, and each k
                                 =1, 2, …, ϕ
                                 
                                    i
                                 , chose a target word position π
                                 
                                    ik
                                  with probability d(π
                                 
                                    ik
                                 |i, l, m) (distortion step).

For each k
                                 =1, 2, …, ϕ
                                 0, choose a word position π
                                 0k
                                  from the ϕ
                                 0
                                 −
                                 k
                                 +1 remaining vacant positions in 1, 2, …, m, for a total probability of 1/ϕ
                                 0
                                 !.

For each i
                                 =0, 1, …, l, and each k
                                 =1, 2, …, ϕ
                                 
                                    i
                                 , choose the word length ψ
                                 
                                    ik
                                  with probability o(ψ
                                 
                                    ik
                                 |e
                                 
                                    i
                                 ) (word length step).

For each i
                                 =0, 1, …, l, and each k
                                 =1, 2, …, ϕ
                                 
                                    i
                                 , and each j
                                 =1, 2, …, ψ
                                 
                                    ik
                                 , choose a target phoneme τ
                                 
                                    ikj
                                  with probability t(τ
                                 
                                    ikj
                                 |e
                                 
                                    i
                                 , j) (lexical translation step).

Output the target phoneme sequence with phonemes τ
                                 
                                    ikj
                                  in positions π
                                 
                                    ik
                                  (0≤
                                 i
                                 ≤
                                 l, 1≤
                                 k
                                 ≤
                                 ϕ
                                 
                                    i
                                 , 1≤
                                 j
                                 ≤
                                 ψ
                                 
                                    ik
                                 ).

Besides the fact that Model 3P skips step 4 of IBM Model 3 (lexical translation), both models are identical until applying the distortion model (step 4 in Model 3P or step 5 in IBM Model 3). At this point, we can regard the target sequence in Model 3P as a sequence of anonymous tokens, each is a placeholder for a target word. In step 6, we choose the number of phonemes which each of these tokens will take in the final phoneme sequence according to the word length probabilities o(·|·). The next step fills in the phonemes, depending on the source word e
                        
                           i
                         and their phoneme position j in the target word. Fig. 5
                         illustrates an instance of the generative process of Model 3P.

In Model 3P, an alignment 
                           A
                           ∈
                           
                              
                                 {
                                 
                                    
                                       ℕ
                                    
                                 
                                 ∪
                                 {
                                 −
                                 }
                                 }
                              
                              
                                 4
                                 ×
                                 m
                              
                           
                         is a matrix rather than an integer vector like in IBM Model 3. It captures additional model decisions made in the fertility and word length step, which would be hidden in an integer vector representation:
                           
                              •
                              
                                 A
                                 0j
                                 : English word position connected to the j-th target phoneme.


                                 A
                                 1j
                                 : Position of the target word belonging to the j-th target phoneme.


                                 A
                                 2j
                                 : Word length in phonemes of the target word A
                                 1j
                                 .


                                 A
                                 3j
                                 : Phoneme position of the j-th target phoneme in the corresponding target word.

An example alignment is shown in Fig. 6
                        . The word boundary between the 6th and 7th phoneme (between /ke/ and /se/) would not be reconstructible in a simple integer vector representation. Note that a single target language phoneme cannot be aligned to more than one source language word as Model 3P inherits the 1:n restriction from IBM Model 3 mentioned at the end of Section 2.1. Eq. (2) expresses Model 3P as a general formula:


                        
                           
                              (2)
                              
                                 P
                                 (
                                 A
                                 ,
                                 f
                                 |
                                 e
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      k
                                                      −
                                                      
                                                         ϕ
                                                         0
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         ϕ
                                                         0
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 ·
                                 
                                    
                                       (
                                       1
                                       −
                                       
                                          p
                                          1
                                       
                                       )
                                    
                                    
                                       k
                                       −
                                       2
                                       
                                          ϕ
                                          0
                                       
                                    
                                 
                                 ·
                                 
                                    
                                       
                                          p
                                          1
                                       
                                    
                                    
                                       
                                          ϕ
                                          0
                                       
                                    
                                 
                                 ·
                                 
                                    ∏
                                    
                                       i
                                       =
                                       1
                                    
                                    l
                                 
                                 (
                                 n
                                 (
                                 
                                    ϕ
                                    i
                                 
                                 |
                                 
                                    e
                                    i
                                 
                                 )
                                 ·
                                 
                                    ϕ
                                    i
                                 
                                 !
                                 )
                                 ·
                                 
                                    ∏
                                    
                                       j
                                       =
                                       1
                                    
                                    m
                                 
                                 t
                                 (
                                 
                                    f
                                    j
                                 
                                 |
                                 
                                    e
                                    
                                       
                                          A
                                          
                                             0
                                             j
                                          
                                       
                                    
                                 
                                 ,
                                 
                                    A
                                    
                                       3
                                       j
                                    
                                 
                                 )
                                 ·
                                 
                                    ∏
                                    
                                       j
                                       :
                                       
                                          A
                                          
                                             0
                                             j
                                          
                                       
                                       ≠
                                       0
                                       ,
                                       
                                          A
                                          
                                             1
                                             j
                                          
                                       
                                       ≠
                                       −
                                    
                                    m
                                 
                                 (
                                 d
                                 (
                                 
                                    A
                                    
                                       1
                                       j
                                    
                                 
                                 |
                                 
                                    A
                                    
                                       0
                                       j
                                    
                                 
                                 ,
                                 l
                                 ,
                                 k
                                 )
                                 ·
                                 o
                                 (
                                 
                                    A
                                    
                                       2
                                       j
                                    
                                 
                                 |
                                 
                                    e
                                    
                                       
                                          A
                                          
                                             0
                                             j
                                          
                                       
                                    
                                 
                                 )
                                 )
                              
                           
                        
                     

Extending IBM Model 4 (Brown et al., 1993) in a similar way turned out to be not useful in our case. IBM Model 4 replaces the distortion model based on absolute positions in IBM Model 3 by a model based on displacement (relative positioning). The distortion probabilities depend on word classes of the source and target words (not directly on the words) in order to reduce the size of the parameter table. Unfortunately, finding the word classes automatically for the target language in a word-to-phoneme alignment scenario (for example with the mkcls tool (Och, 1999)) is not possible: The target language vocabulary is unknown and by itself part of the learning problem. Therefore, in first experiments we removed the dependency on the target language word from the distortion model and adapted it for word-to-phoneme alignment. However, we could not detect improvements compared to Model 3P alignments in our experiments.

The word segmentation problem describes the task of segmenting phoneme sequences into word units. Stahlberg et al. (2012) show that unsupervised learning of word segmentation is more accurate when information of another language is used – for example by using Model 3P word-to-phoneme alignments. Given such an alignment, we insert a word boundary marker into the phoneme sequence wherever two neighbouring phonemes are aligned to different source language words (i.e. wherever a black alignment line in Fig. 2 ends).

We intersperse the perfect phoneme sequences with phoneme errors to investigate the performance of different word segmentation approaches depending on the underlying phoneme error rate (PER). In order to imitate recognition errors realistically, we train a phoneme recognizer on the Spanish GlobalPhone corpus (Schultz et al., 2013) and use the NIST sclite scoring and evaluation tool (Fiscus, 2007) to create its confusion matrix 
                              R
                              ∈
                              
                                 
                                    
                                       ℝ
                                    
                                 
                                 
                                    36
                                    ×
                                    36
                                 
                              
                           . The matrix entry (R)
                              so
                            contains the probability P
                           
                              R
                           (o|s) that the phoneme recognizer confuses the stimulus phoneme s with the observed phoneme o (substitution). An additional row and a column model insertions and deletions, so that all elements in a row sum up to 1 and induce a probability distribution. R is smoothed with λ
                           ∈[0, 1] to control the PER. We insert phoneme errors according to the resulting distribution: We replace each phoneme s in the perfect phoneme transcription with a phoneme o with probability P
                           
                              λ
                           (o|s). The Spanish phoneme recognizer has a PER of 25.3%. Fig. 7
                            shows that the resulting PER is linear in λ.

The PISA Alignment Tool
                              2
                           
                           
                              2
                              A multi-threaded implementation is available at http://pisa.googlecode.com/.
                            is an implementation of our alignment model Model 3P. First, we report our results for the alignment between English words and Spanish phoneme sequences on a subset of the English-Spanish Basic Travel Expression Corpus (BTEC) (Kikui et al., 2003). This parallel corpus consists of conversations considered to be useful for people traveling in another country. We remove sentences longer than 50 words or sentence pairs exceeding a word ratio of 9:1 and end up with 123k sentence pairs and vocabulary sizes of 12k for English and 20k for Spanish. The quality of the found word segmentations for both cross-lingual approaches and a monolingual approach is summarized in Fig. 8
                           . We use the Adaptor Grammar implementation from Johnson (2008) with the colloc-syllable grammar representatively for unsupervised monolingual word segmentation methods. Using PISA for the alignment between English words and correct Spanish phoneme sequences results in 90.0% segmentation accuracy (VIM, 2004) (76.5% in F-score) and thus outperforms Adaptor Grammars by 24.72% relative in accuracy (124.11% in F-score). Furthermore, we report still 83.9% segmentation accuracy on a phoneme sequence containing 25.3% errors produced by our simulated phoneme recognizer (see Tables 1 and 2
                           
                           ).

The monolingual approach seems to be more robust against recognition errors since its curve in Fig. 8 is flat. One reason is that the adaptor grammar requires a mapping of the phonemes to a vowel and a consonant set. Given the definition of vowel and consonant sets, the monolingual approach can better compensate for vowel-vowel or consonant-consonant confusions by the phoneme recognizer. The second reason is that the monolingual method is just above chance for noisy phoneme sequences: There are n
                           
                              phoneme
                           
                           =2,823,030 phonemes in n
                           
                              sentence
                           
                           =113,099 sentences. Each sentence contains a trivial sure boundary before its first phoneme (true positive). The adaptor grammar inserts n
                           
                              AG
                           
                           =404,627 boundaries at the n
                           Ω
                           =
                           n
                           
                              phoneme
                           
                           −
                           n
                           
                              sentence
                           
                           =2,709,931 remaining positions, whereas the reference contains n
                           
                              ref
                           
                           =587,948 boundaries. Assuming uniform distribution and independence for simplification, the probability for a word boundary in the reference is p
                           
                              ref
                           
                           =
                           n
                           
                              ref
                           /n
                           Ω
                           ≈0.217. When we distribute the n
                           
                              AG
                            boundaries completely at random instead of applying adaptor grammars, the boundary probability is p
                           
                              AG
                           
                           =
                           n
                           
                              AG
                           /n
                           Ω
                           ≈0.149. The expected F-score of this trivial segmentation is:
                              
                                 (3)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   E
                                                
                                             
                                             (
                                             
                                                true
                                                 
                                                positive
                                             
                                             )
                                             =
                                             
                                                n
                                                sentence
                                             
                                             +
                                             
                                                p
                                                ref
                                             
                                             ·
                                             
                                                p
                                                AG
                                             
                                             ·
                                             
                                                n
                                                Ω
                                             
                                             ≈
                                             200
                                             ,
                                             887
                                          
                                       
                                       
                                          
                                             
                                                
                                                   E
                                                
                                             
                                             (
                                             
                                                false
                                                 
                                                positive
                                             
                                             )
                                             =
                                             (
                                             1
                                             −
                                             
                                                p
                                                ref
                                             
                                             )
                                             ·
                                             
                                                p
                                                AG
                                             
                                             ·
                                             
                                                n
                                                Ω
                                             
                                             ≈
                                             316
                                             ,
                                             839
                                          
                                       
                                       
                                          
                                             
                                                
                                                   E
                                                
                                             
                                             (
                                             
                                                false
                                                 
                                                negative
                                             
                                             )
                                             =
                                             
                                                p
                                                ref
                                             
                                             ·
                                             (
                                             1
                                             −
                                             
                                                p
                                                AG
                                             
                                             )
                                             ·
                                             
                                                n
                                                Ω
                                             
                                             ≈
                                             500
                                             ,
                                             160
                                          
                                       
                                       
                                          
                                             
                                                
                                                   E
                                                
                                             
                                             (
                                             precision
                                             )
                                             =
                                             
                                                
                                                   
                                                      
                                                         E
                                                      
                                                   
                                                   (
                                                   
                                                      true
                                                       
                                                      positive
                                                   
                                                   )
                                                
                                                
                                                   
                                                      
                                                         E
                                                      
                                                   
                                                   (
                                                   
                                                      true
                                                       
                                                      positive
                                                   
                                                   )
                                                   +
                                                   
                                                      
                                                         E
                                                      
                                                   
                                                   (
                                                   
                                                      false
                                                       
                                                      positive
                                                   
                                                   )
                                                
                                             
                                             ≈
                                             38.8
                                             %
                                          
                                       
                                       
                                          
                                             
                                                
                                                   E
                                                
                                             
                                             (
                                             recall
                                             )
                                             =
                                             
                                                
                                                   
                                                      
                                                         E
                                                      
                                                   
                                                   (
                                                   
                                                      true
                                                       
                                                      positive
                                                   
                                                   )
                                                
                                                
                                                   
                                                      
                                                         E
                                                      
                                                   
                                                   (
                                                   
                                                      true
                                                       
                                                      positive
                                                   
                                                   )
                                                   +
                                                   
                                                      
                                                         E
                                                      
                                                   
                                                   (
                                                   
                                                      false
                                                       
                                                      negative
                                                   
                                                   )
                                                
                                             
                                             ≈
                                             28.7
                                             %
                                          
                                       
                                       
                                          
                                             
                                                
                                                   E
                                                
                                             
                                             (
                                             
                                                 F
                                                −
                                                score
                                             
                                             )
                                             =
                                             2
                                             ·
                                             
                                                
                                                   
                                                      
                                                         E
                                                      
                                                   
                                                   (
                                                   precision
                                                   )
                                                   ·
                                                   
                                                      
                                                         E
                                                      
                                                   
                                                   (
                                                   recall
                                                   )
                                                
                                                
                                                   
                                                      
                                                         E
                                                      
                                                   
                                                   (
                                                   precision
                                                   )
                                                   +
                                                   
                                                      
                                                         E
                                                      
                                                   
                                                   (
                                                   recall
                                                   )
                                                
                                             
                                             ≈
                                             32.9
                                             %
                                          
                                       
                                    
                                 
                              
                           
                        

Although this is only a rough estimate, it shows that on noisy phoneme sequences the monolingual method is only slightly better than this trivial random segmentation resulting in a flat curve in Fig. 8. Even if the effectiveness of adaptor grammars was demonstrated on short and error-free phoneme sequences (e.g. on child-directed speech (Johnson, 2008; Johnson and Goldwater, 2009)), they failed to segment longer and noisier phoneme sequences reliably in our experiments.

In a second experiment, we switch English and Spanish so that Spanish takes the role as resource-rich source language that is aligned to the “under-resourced” target language English. Phoneme errors are simulated as before – with a smoothed confusion matrix of an English phoneme recognizer with 45.1% PER (39 phonemes). Fig. 9
                            shows that the word segmentation results are similar to the first experiment but the achieved gain by using PISA instead of GIZA++ is smaller than before. Using PISA for the alignment between Spanish words and correct English phoneme sequences results in 88.6% segmentation accuracy (78.9% in F-score) and thus performs better than the monolingual word segmentation approach by 28.1% relative in accuracy (117.0% in F-score). We report still 68.5% segmentation accuracy on a phoneme sequence containing 45.1% errors produced by our simulated phoneme recognizer. We think that the improvements of PISA compared to GIZA++ are smaller on Spanish-English than on English-Spanish since the GIZA++ alignments are already better and PISA has less space for improvements. GIZA++ is better on Spanish-English than on English-Spanish because the Spanish phoneme sequences are longer (Table 3
                           ). Therefore, the discrepancy between the number of source and target tokens is more severe for English-Spanish (7.31:25.53=1:3.49) than for Spanish-English (6.89:20.21=1:2.93) and cannot be modeled by the fertility model of GIZA++ so well.

In addition to the experiments on the BTEC corpus, we apply both cross-lingual word segmentation approaches GIZA++ and PISA to the Christian Bible corpus, which we use in our pronunciation extraction experiments in Section 3.3.4. Thereby English serves as target language, whereas the remaining 14 translations represent the source languages. More information about the Bible corpus is given in Section 3.3.1. Table 4
                            gives an overview of the PISA improvements. As demonstrated with the BTEC corpus, PISA substantially improves the GIZA++ alignments on the Bible corpus for all investigated language pairs. On the Bible corpus we report both the biggest and smallest relative gain over all available source languages on error-free phoneme sequences – we do not have access to manual reference segmentations for recognized phoneme sequences. We observe that similar to the BTEC corpus the lower the GIZA++ segmentation accuracy, the larger the relative gain achieved by PISA.

Our goal is to extract word pronunciations of a target language with the help of cross-lingual word-to-phoneme alignments. In this section, we present two different approaches (source word dependent and independent clustering) and compare them in Section 3.3.

Let V
                     
                        src
                      be the vocabulary of the source language plus the NULL token and PhonemeSet
                     
                        trgt
                      the phoneme set of the target language. The data source we explore in our scenario is a set DB
                     ⊂
                     V
                     
                        src
                     
                     +
                     ×
                     PhonemeSet
                     
                        trgt
                     
                     + of pairs containing a written sentence and its spoken translation. The sentence is represented as a sequence of source language words and the translation is represented as a sequence of target language phonemes. As described in Section 2, we use the PISA Alignment Tool to find word-to-phoneme alignments for each sentence-phoneme sequence pair in DB. An alignment A
                     
                        s,t
                      of a pair (s, t)∈
                     DB consists of a mapping between tokens in s and phoneme sub-sequences in t. We formalize A
                     
                        s,t
                      as a word over an alphabet containing pairs of source language words and target language phoneme sequences:


                     
                        
                           
                              
                                 A
                                 
                                    s
                                    ,
                                    t
                                 
                              
                              ∈
                              
                                 
                                    (
                                    
                                       V
                                       src
                                    
                                    ×
                                    
                                       
                                          
                                             PhonemeSet
                                             trgt
                                          
                                       
                                       +
                                    
                                    )
                                 
                                 +
                              
                           
                        
                     
                  

Each element in A
                     
                        s,t
                      contains a putative target language word represented by its phonemes and the source language word aligned to it. We postulate that the source language words are elements in s, and that concatenating all target language words results in the complete phoneme sequence t.

The difference between source word dependent and independent clustering is discussed in the next sections. Our general algorithm for extracting word pronunciations from word-to-phoneme alignments is illustrated in Fig. 10
                     . It consists of five steps:
                        
                           1
                           
                              Word-to-phoneme alignment: Use the PISA Alignment Tool to generate word-to-phoneme alignments A
                              
                                 s,t
                               for each pair (s, t)∈
                              DB.


                              Alignment pair collection: Collect all cross-lingual mappings in the alignments in the set P:
                                 3
                              
                              
                                 3
                                 For technical reasons, we define the ∈ sign for a symbol x
                                    ∈Σ and a word 
                                       w
                                       ∈
                                       
                                          Σ
                                          +
                                       
                                     as 
                                       x
                                       ∈
                                       w
                                       :
                                       ⇔
                                       ∃
                                       i
                                       ∈
                                       
                                          
                                             ℕ
                                          
                                       
                                       :
                                       x
                                       =
                                       
                                          w
                                          i
                                       
                                    .
                              
                              
                                 
                                    
                                       P
                                       ←
                                       {
                                       (
                                       
                                          w
                                          s
                                       
                                       ,
                                       
                                          w
                                          t
                                       
                                       )
                                       ∈
                                       
                                          V
                                          src
                                       
                                       ×
                                       
                                          
                                             
                                                PhonemeSet
                                                trgt
                                             
                                          
                                          +
                                       
                                       |
                                       ∃
                                       (
                                       s
                                       ,
                                       t
                                       )
                                       ∈
                                       DB
                                       :
                                       (
                                       
                                          w
                                          s
                                       
                                       ,
                                       
                                          w
                                          t
                                       
                                       )
                                       ∈
                                       
                                          A
                                          
                                             s
                                             ,
                                             t
                                          
                                       
                                       }
                                    
                                 
                              
                           


                              Phoneme sequence clustering: Conceptually, P contains lexical translations and pronunciation variants of them. In reality, the set is affected by phoneme recognition and alignment errors. Therefore, we group elements in P into clusters C
                              
                                 i
                              :
                                 
                                    
                                       C
                                       =
                                       {
                                       
                                          C
                                          1
                                       
                                       ,
                                       
                                          C
                                          2
                                       
                                       ,
                                       …
                                       c
                                       ,
                                       
                                          C
                                          n
                                       
                                       }
                                       ⊂
                                       
                                          2
                                          P
                                       
                                       
                                       with
                                       
                                       P
                                       =
                                       
                                          ⊎
                                          
                                             i
                                             ∈
                                             [
                                             1
                                             ,
                                             n
                                             ]
                                          
                                       
                                       
                                          C
                                          i
                                       
                                       .
                                    
                                 
                              
                           


                              Phoneme level combination: At this point, the clusters should contain phoneme sequences representing the same target language word but differing due to alignment and phoneme recognition errors. Thus we try to reconstruct the correct phoneme sequence for each cluster by merging its elements with the nbest-lattice (Stolcke et al., 1997) program of the SRI Language Modeling Toolkit (Stolcke et al., 2002). nbest-lattice aligns the phoneme sequences to each other and applies a voting at each position. Therefore, nbest-lattice can extract the correct pronunciation even if all elements in the cluster contain errors. We obtain a set H
                              ⊂
                              PhonemeSet
                              
                                 trgt
                              
                              + of n phoneme sequences (|H|=
                              n), which are now assumed to correspond to real target language words.


                              Dictionary generation: For each pronunciation h
                              ∈
                              H, we choose a word label 
                                 
                                    id
                                    h
                                 
                                 ∈
                                 
                                    
                                       ℕ
                                    
                                 
                               and add both to the pronunciation dictionary Dict.

Source word dependent and independent clustering differ in step 3. Source word dependent clustering extracts pronunciations based on the assumption that phoneme sequences which are aligned to the same source language word are likely to represent the same target language word. They only differ in phoneme recognition and alignment errors. Source word dependent clustering applies a two-level approach (Fig. 11
                        ): First, we form coarse-grained clusters by grouping all elements in P that have the same source language word. However, this cannot separate different translations of the same source language word from each other: In our example from Fig. 10, the German word Sprache has two different English translations (Speech and Language). Therefore, a fine-grained clustering algorithm post-processes the source word dependent clusters. We apply the density-based clustering algorithm DBSCAN (Ester et al., 1996) (ϵ
                        =1, minPts
                        =3) implemented in the ELKI (Achtert et al., 2012) environment with the edit distance metric. Roughly speaking, DBSCAN finds clusters with mutually density-reachable elements. An element is density-reachable if it is within ϵ-distance and surrounded by sufficiently many other elements (minPts). We use DBSCAN as it does not require the desired number of clusters as input parameter.

Source word dependent clustering in the previous section assumes that phoneme sequences which are aligned to the same source language word are likely to represent the same target language word. Source word independent clustering drops this assumption completely since we observed that the alignments sometimes induce the correct word segmentation but map the wrong source language words to the segments. For example, the phoneme sequence dh eh n (then) in Fig. 12
                         is incorrectly aligned to “Hiob” (Job) but all the word boundaries are set correctly. Therefore, when extracting pronunciations with the source word “Hiob”, dh eh n pollutes the cluster containing Job pronunciations, and it cannot be used to extract a pronunciation for then. Since there is no equivalent for then in the German counterpart, it should be aligned to NULL. However, the NULL cluster in source word dependent clustering is usually very large and widespread: All kinds of words can be modeled as “spurious” and aligned to NULL (see the generative story of Model 3P in Section 2.2). Therefore, it is hard to extract correct pronunciations from the NULL cluster with source word dependent clustering.

Consequently, source word independent clustering (Stahlberg et al., 2014) groups pronunciations in P not regarding the source language word. As illustrated in Fig. 13
                        , it replaces the two-level approach of source word dependent clustering by a single-level clustering solely based on the edit distance. Combining DBSCAN with source word independent clustering performs poorly in our experiments, so we use the k-means (MacKay, 2003) algorithm to provide additional hints for the clustering process like the desired number of clusters. The means are initialized with the k most frequent elements. k should be initialized close to the actual vocabulary size of the target language which can be estimated using the vocabulary sizes of the source languages. In the latter experiments we assume to know the target language vocabulary size. Section 3.3.6 discusses the performance degradations when k is set too low or too high in detail.

@&#EXPERIMENTS@&#

We test our pronunciation extraction algorithms on parallel data from the Christian Bible since it is available in many languages in written form
                              4
                           
                           
                              4
                              Extracted from http://www.biblegateway.com/ (accessed 05.10.14).
                            and in some languages also as audio recordings. A variety of linguistic approaches to Bible translation (Dynamic equivalence, formal equivalence, and idiomatic translation (Thomas, 1990)) enables us to compare different translations within the same source language. In our experiments, English takes the role of the under-resourced target language. English is by no means under-resourced and comprehensive pronunciation dictionaries are readily available (Weide, 2005). However, we feel that understanding the target language gives a deeper insight in the strengths and weaknesses of our algorithms. We select an English Bible translation with predominant use of the formal equivalence translation method (i.e. syntactical equivalent and literal)(Borland, 2003): English Standard Version (ESV) (Crossway, 2001). The diagram in Fig. 14
                            shows that the words in the ESV Bible are distributed approximately according the Zipfian distribution (Manning and Schütze, 1999) (s
                           =0.98, asymptotic standard error of 0.04%): The frequency of any word is inversely proportional to its rank in the frequency table. A large portion of the words has only one occurrence (30.5%), while only 26% occur ten times or more in the text. Some of the function words (in, the, and) are repeated very often but also content words like lord, god, said, people, Israel occur frequently. High word frequencies are suitable for our extraction algorithm since we merge more phoneme sequences which leads to better error correction as shown in Section 3.3.8. Verses in the Christian Bible are identified by unique verse numbers (such as Galatians 5:22), which are consistent with verse numbers across all investigated Bible translations. Based on these numbers, we extract a parallel and verse-aligned corpus consisting of 30.6k verses in 15 different written translations in 10 languages (Table 5
                           ). We refer to the English portion as EN
                           
                              all
                           .

In initial experiments, we replace the words with their canonical pronunciations and remove word boundary markers to generate the target language phoneme sequences. Thereby, we simulate a perfect phoneme recognizer (0% phoneme error rate). The pronunciations are taken directly from the CMUdict pronunciation dictionary (Weide, 2005) (39 phonemes) or generated with a grapheme-to-phoneme model trained on it.

Word-to-phoneme alignments are the basis of our pronunciation extraction. As described by Stahlberg et al. (2012), GIZA++ first calculates initial alignments that are then further refined by the PISA Alignment Tool applying Model 3P. However, GIZA++ has restrictions regarding the maximum number of target tokens (100 in our GIZA++ version) or the maximum ratio between the number of source language words and target language phonemes in a verse pair (maxfertility was set to 12 in our experiments). We remove all verses which violate these restrictions and end up with 23k verses. We refer to this subcorpus as EN
                           
                              filt
                           .

For English, Crossway (the publisher of the ESV Bible) provides high quality verse-level audio recordings of a single male speaker for the entire Bible text.
                              5
                           
                           
                              5
                              Available for purchase at http://www.crossway.org/bibles/esv-hear-the-word-audio-bible-610-dl/ (accessed 05.10.14).
                            This enables us to test our methods using real phoneme recognizers instead of merely error-free phonetic transcriptions. All recordings (EN
                           
                              all
                           ) have a total length of 67:16h from which the filtered subcorpus EN
                           
                              filt
                            consists of 40:28h speech.

We trained context-dependent acoustic models (AMs) for English on two different training sets with different performances on EN
                           
                              filt
                            to investigate the influence of phoneme recognition errors to our methods. AM
                           13.1 was trained on EN
                           
                              all
                            without EN
                           
                              filt
                            (EN
                           
                              all
                           
                           \
                           EN
                           
                              filt
                           ) and represents a speaker-dependent system without overlapping training and test set. AM
                           45.1 was trained on the Wall-Street-Journal (WSJ) corpus (Paul and Baker, 1992) and therefore contains speaker independent, corpus mismatched acoustic models. All experiments use the CMUdict (Weide, 2005) phoneme set consisting of 39 phonemes. The preprocessing consists of feature extraction applying a Hamming window of 16ms length with a window shift of 10ms. Each feature vector has 143 dimensions by stacking 11 adjacent frames of 13 Melscale Frequency Ceptral Coefficients (MFCC) frames. A Linear Discriminant Analysis (LDA) transformation is computed to reduce the feature vector size to 42 dimensions. The AM uses a fully-continuous 3-state left-to-right Hidden Markov Model (HMM) (Rabiner, 1989) with emission probabilities modeled by Gaussian Mixtures with diagonal covariances (64 Gaussians per state). For our context-dependent AMs with different context sizes, we stopped the decision tree splitting process at 2,500 triphones. After context clustering, a merge-and-split training was applied, which selects the number of Gaussians according to the amount of data (50 on average, 125k in total).

For phoneme recognition, a trigram phoneme-level language model was trained on the phonetic transcriptions of the corresponding training sets. Table 6
                            summarizes the phoneme error rates on EN
                           
                              filt
                            of phoneme recognizers with the acoustic models AM
                           13.1 and AM
                           45.1.

Both AM
                           13.1 and AM
                           45.1 represent oracle experiments since they were trained using target language transcriptions that are not available in our scenario. Acoustic modeling in the target language is a research topic for its own and not our present concern. The interested reader can consult the literature (Jansen et al., 2013) for details. We feel, however, that for some possible target languages a PER in the magnitude of 45.1% (AM
                           45.1) is realistic even without or only little training transcriptions. For example, Varadarajan et al. (2008) are able to achieve a PER of under 25% for Japanese speech from only one speaker by unsupervised learning of HMMs and only very light supervision for their HMM label-to-phone transducer (5 minutes of transcribed speech). Recent developments in using Deep Neural Networks in zero acoustic model resource systems (as reported by Gales et al. (2014) in the context of the Babel program (Harper, 2014)) are also promising.

Let I be the set of all word labels in the extracted dictionary Dict
                           :
                           I
                           →
                           PhonemeSet
                           
                              trgt
                           
                           +. We measure the structural quality of Dict by the Out-Of-Vocabulary rate (OOV) on running words. The OOV rate cannot be calculated directly since Dict contains word labels instead of written words consisting of graphemes. Therefore, a mapping between the word labels and the written words is required. Let V
                           
                              trgt
                            be the target language vocabulary (written words) and Dict
                           
                              ref
                           
                           :
                           V
                           
                              trgt
                           
                           →
                           PhonemeSet
                           
                              trgt
                           
                           + the reference dictionary with the correct pronunciations. The mapping m
                           :
                           I
                           →
                           V
                           
                              trgt
                            assigns each word label to the written word with the most similar pronunciation (illustrated in Fig. 15
                           ):


                           
                              
                                 (4)
                                 
                                    m
                                    (
                                    n
                                    )
                                    =
                                    
                                       
                                          arg
                                          
                                          min
                                       
                                       
                                          v
                                          ∈
                                          
                                             V
                                             trgt
                                          
                                       
                                    
                                    
                                       d
                                       edit
                                    
                                    (
                                    Dict
                                    (
                                    n
                                    )
                                    ,
                                    
                                       Dict
                                       ref
                                    
                                    (
                                    v
                                    )
                                    )
                                 
                              
                           where d
                           
                              edit
                            denotes the edit distance. The set m(I) of matched vocabulary entries in Dict
                           
                              ref
                            is then used to calculate the OOV rate on running words (word tokens).

While the OOV rate indicates the coverage of Dict on a Bible text, the dictionary phoneme error rate (dictPER) reflects the quality of the extracted pronunciations on the phoneme level. It is defined as the average edit distance between the entries in Dict and the closest entry in the reference dictionary Dict
                           
                              ref
                           :


                           
                              
                                 (5)
                                 
                                    dictPER
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                n
                                                ∈
                                                I
                                             
                                          
                                          
                                             d
                                             edit
                                          
                                          (
                                          Dict
                                          (
                                          n
                                          )
                                          ,
                                          
                                             Dict
                                             ref
                                          
                                          (
                                          m
                                          (
                                          n
                                          )
                                          )
                                          )
                                       
                                       
                                          |
                                          I
                                          |
                                       
                                    
                                 
                              
                           
                        

The dictPER can be greater than zero even when we operate on error-free phonetic transcriptions since it does not capture only the remaining phoneme recognition errors: The dictPER is also heavily affected by segmentation errors (inserted or dropped blank between two words or morphemes). Finally, the Hypo/Ref ratio indicates how many hypothesis entries in Dict are mapped by m to a single reference entry in Dict
                           
                              ref
                            on average (|I| divided by |m(I)|). Ideally, this ratio is 1 since we do not have pronunciation variants in our corpus. The higher the Hypo/Ref ratio, the more pronunciations are extracted unnecessarily.

The absolute numbers for the target language English in our evaluation of source word dependent clustering differ from the numbers reported by Stahlberg et al. (2013) due to the following reasons: First, we improved our text normalization on the Bible texts. The previous text normalization was not always able to remove artefacts in the original HTML source code fetched from the Internet (as described in Section 3.3.1) such as different whitespace characters, escaped HTML entities, or cross references to other Bible passages. Second, we used a more sophisticated method for finding the mapping m to better handle the case when an extracted vocabulary entry in Dict can be matched to two equidistant entries in the reference dictionary Dict
                           
                              ref
                           : We prefer reference dictionary entries that not have been mapped by m so far. This significantly improves the OOV rate and the Hypo/Ref ratio. Third, we changed the handling of elements identified as noise by the clustering algorithm DBSCAN in the source word dependent method (Section 3.1): If the total number of elements is smaller than minPts all elements are combined in a single cluster. Otherwise, noise elements are not added to the final dictionary. Forth, the OOV rate is now calculated on the entire text instead of a small subset for robust results. Despite those changes, we observe comparable correlations between the evaluation measures and the characteristics of the translations as in the previous setup.

In addition to experiments on error-free phonetic transcriptions (designated as R
                           0 in this section), we also use our phoneme recognizers based on the acoustic models AM
                           13.1 and AM
                           45.1 (introduced in Section 3.3.2) to produce the target language phoneme sequences (designated as R
                           13.1 and R
                           45.1). Tables 7 and 8
                           
                            show the performance of both source word dependent and independent clustering using the evaluation measures introduced in Section 3.3.3 for all investigated language pairs. The best results are highlighted. The source languages are ordered descending by segmentation accuracy.

There are often big differences between translations within the same source language: For example, with source word dependent clustering, es3 has 16.3% lower dictPER and 31.5% lower OOV rate than es1 (relative improvement) since es3 La Biblia de las Américas is a very literal translation (Lockman, 1986). Similar gains can be observed within other source languages (Portuguese and French) and source word independent clustering. This suggests that selecting a translation which is as literal as possible (e.g. following the formal equivalence translation method) is a crucial factor for a high overall quality.

Source word independent clustering outperforms source word dependent clustering in terms of dictPER: The values in the first three columns in Table 8 are significantly lower than the corresponding values in Table 7. On the other hand, the Hypo/Ref ratio is usually better when source word dependent clustering is used (middle three columns). There is no such clear tendency regarding the OOV rate. For example, using es3 (first row), source word dependent clustering does better than source independent clustering with R
                           0 (2.57>2.49) but worse with R
                           13.1 (3.78<4.04). The opposite is true for bg and R
                           0 (1.96<3.39) or es2 and R
                           13.1 (4.52>4.31). On very noisy phoneme sequences (last column), source word independent clustering always leads to significantly better OOV rates than source word dependent clustering. Therefore, whether source word dependent or independent clustering should be used depends on the application: If the dictionary is required to contain a minimum of unnecessary entries (and thus needs to have a low Hypo/Ref ratio), source word dependent clustering is the right choice. If the phonetic correctness of the pronunciations is more important (low dictPER) or the input sequences are very noisy, source word independent clustering should be used. In Section 4 we elaborate on the case when the dictionary is used in an ASR system.

For all source languages and both clustering methods, dictPERs and OOV rates increase on noisy phoneme sequences: The R
                           0 columns contain lower values than the corresponding R
                           13.1 and R
                           45.1 columns in the Tables 7 and 8 for both the dictPER and the OOV rate. This performance drop is less severe with source word independent clustering: The dictPER is usually only slightly affected by recognition errors. For example, es3 achieves a dictPER of 32.55% on phoneme sequences with 45.1% errors (R
                           45.1) which is still better than most other source languages on perfect phoneme sequences (R
                           0). Consequently, the influence of the translation is more important than the influence of phoneme recognition errors. This indicates that we are able to compensate for phoneme recognition errors. The increase of the OOV rates with noisier phoneme sequences is usually larger (about factor 2 on average for source word independent clustering) but stays on a surprisingly low level given that nearly every second phoneme in the input sequences is wrong (R
                           45.1). This again supports the claim that both pronunciation extraction methods and the alignment model Model 3P are robust against recognition errors. There is no clear tendency for the Hypo/Ref ratio. Table 9
                            illustrates the effectiveness of both source word dependent and independent clustering to compensate for alignment and phoneme recognition errors and reduce the noise in the dictionaries. When no clustering is applied, the phoneme sequence segments induced by the word-to-phoneme alignments are directly used as pronunciations. In this case, the dictionaries are huge and thus have a low OOV rate – it is a matter of chance that the mapping m hits a reference entry. Consequently, the OOV rate is only comparable when the dictPER is on the same level. The Hypo/Ref ratio gives a better picture of the improvements due to clustering. Without clustering, 5 of 6 pronunciations are extracted unnecessarily (Hypo/Ref ratio ≥6) on noisy phoneme sequences (R
                           13.1 and R
                           45.1). With clustering, the Hypo/Ref ratio is 3.6 to 5.3 times lower. Furthermore, the dictPERs with both clustering methods are significantly lower than without clustering. This indicates that we are able to compensate for errors even on the phoneme level.

Our source word independent clustering algorithm is based on the k-means algorithm. The parameter k (number of desired clusters) should be initialized with a value as close to the target language vocabulary size as possible. However, the vocabulary size of the target language is unknown in our scenario. Therefore, Stahlberg et al. (2014) derive it from the vocabulary size of a very similar source language. Fig. 16
                            shows the impact to our evaluation measures when k is set too low or too high for the target language English on error-free phoneme sequences. The OOV rate decreases with k since the larger the extracted dictionary the more reference entries can be mapped by m. This drop of the OOV rate comes at the expense of the dictPER and the Hypo/Ref ratio – the dictionary becomes noisier. If k is initialized with the correct value (12k is the vocabulary size of EN
                           
                              filt
                           ), the OOV rate and Hypo/Ref ratio is on the same level as source word dependent clustering but the dictPER is lower. Small variations of k around 12k cause only minor changes of the error rates indicating that the estimated value for the target language vocabulary size does not have to be very accurate. In our other experiments we always set k to the correct value.

We investigate the impact of four factors to our evaluation measures:
                              
                                 •
                                 
                                    Vocabulary size. The vocabulary size of the source language.


                                    Average number of words per verse. The average verse length in the source language.


                                    Average word frequency. The average number of word repetitions in the source language (inverse of the lexical density (Ure, 1971)).


                                    IBM-4 PPL. To measure the general correspondence of the translation to IBM Model based alignment models, we train IBM Model 4 (Brown et al., 1993) on the source and target language transcriptions (written word level) using GIZA++ (Och and Ney, 2003) with default configuration and measure its perplexity.

Tables 10 and 11
                           
                            show Pearson's correlation coefficient |r| (Rodgers and Nicewander, 1988) between those four factors and our evaluation measures from Section 3.3.3. High correlations are highlighted.

As shown in Fig. 17
                           , each cell is subdivided into four cells. Three of these cells (upper row) contain the regression coefficients for error-free phoneme sequences (R
                           0) and noisy phoneme sequences (R
                           13.1 and R
                           45.1). The lower row (bold font) contains the mean of these specific coefficients.

Regardless of how noisy the phoneme sequences are, the vocabulary size and the average word frequency are good indicators to predict the Hypo/Ref ratio for source word dependent clustering (|r|≥.80 in Table 10). The IBM-4 PPL has a high linear correlation with the dictPER on error-free phoneme sequences (|r|=.76) but is less correlated when the phoneme sequences are noisy. For noisy phoneme sequences, the number of word repetitions in the source language becomes more important for the dictPER (|r|≥.77): Our error recovery methods work better when many realizations of the target language word can be merged. In source word dependent clustering, the number of occurrences of a source word is an rough upper bound for the number of elements in its clusters. The verse length also becomes more important with noisier phoneme sequences: With error-free phoneme sequences the alignment model is able to align even long verses. However, only short verses can be aligned reliably when the phoneme sequence is noisy. The dominant factor for the OOV rate is the IBM-4 PPL on error-free phoneme sequences, and the vocabulary size on noisy phoneme sequences. Generally, the vocabulary size of the source language is highly correlated with all evaluation measures due to the dependency on the source word with this clustering method.

In contrast, the linear correlations of the vocabulary size are much smaller for source word independent clustering (Table 11). For this clustering method, the IBM-4 PPL is an important factor for the extraction quality. This suggests that selecting a translation which is as literal as possible (e.g. following the formal equivalence translation method) is crucial.

This section describes the characteristics of words which are likely to be extracted correctly when the source language es3 and the target language English is used and pronunciations are extracted with source word dependent clustering. Experiments with other source languages and source word independent clustering show similar results. Fig. 18
                            indicates that frequently repeated words tend to contain no or only minor errors on the phoneme level (high bar in the interval [0, 0.1)). Other bars are significantly lower. However, they are not necessarily on the exact same level since the words that can be assigned to the intervals are not uniformly distributed: All the words with even number of phonemes can possibly fall into the [0.5, 0.6) interval. In contrast, only words with six phonemes (and one error) or more can have between 10% and 20% errors ([0.1, 0.2) interval). The relation between the word length in terms of phonemes and the phoneme level pronunciation quality is illustrated in Fig. 19
                           . One phoneme words are often extracted incorrectly. This is due to Model 3P's problems with aligning one phoneme words: A single phoneme usually occurs at different positions in the phoneme sequence, and it has to be inferred by the rather weak distortion model and the neighboring alignments which position is the correct one. For longer words, the learned translation probabilities give stronger hints of where the target word is likely to start in the phoneme sequence. Apart from that, very short and very long words are generally extracted more accurately than words with average length.

A look at some extracted pronunciations reveals two major sources of errors for words with only 1–2 phoneme errors:
                              
                                 1
                                 Single phonemes are added or dropped in the beginning or end of a word:
                                       
                                          •
                                          
                                             z f ih s t s instead of f ih s t s (fists)


                                             ih k s t instead of f ih k s t (fixed)


                                             ih z r ey l ah instead of ih z r ey l (israel)

Different words with the same stem are merged together:
                                       
                                          •
                                          
                                             s ih d uw s ih t instead of s ih d uw s t (seduced) or s ih d uw s ih ng (seducing)


                                             ih k n aa l ih jh m instead of ih k n aa l ih jh (acknowledge) or ih k n aa l ih jh m ah n t (acknowledgement)


                                    w er ih n d ih g n ah n t (were indignant)


                                    f ih n ih sh t ih t (finished it)

Our final goal is to integrate the extracted pronunciations in a word recognizer for the target language. The three basic components of our ASR system are acoustic model (AM), pronunciation dictionary, and language model (LM). We use the acoustic models AM
                     13.1 and AM
                     45.1 presented in Section 3.3.2 in our experiments. The pronunciation dictionaries are extracted as described in the previous Section 3. To train a language model, we replace the segments in the segmented phoneme sequences (Section 2) with the closest word labels in the extracted dictionary. Thereby we obtain sequences of word labels which serve as training data for trigram language models. The process is illustrated in Fig. 20
                     . To reduce complexity, we restrict our experimental settings to the best, middle, and worst source language (es3, de2, and se).

The evaluation of the resulting ASR systems is difficult as they output sequences of word labels that are to be compared with the written reference sentences. One conservative way is to modify the standard word error rate (WER): We allow a word label to match with a written word in the reference if its pronunciation is equal to one possible pronunciation variant of the written reference word. However, this measure is highly sensitive to segmentation errors (inserted or missing word boundaries). Which words are written together and which separately is often due to conventions which are impossible to detect for our automatic algorithms. For example, our algorithms often extract the pronunciation n ow h w ah n (“no one”) in our experiments. The phrase “no one” is written separately but “nobody” is written together. As both are grammatically and semantically interchangeable in English, our automatic methods are not able to learn the difference – it would be just as reasonable to treat “no one” as single pronoun and “no-body” as two separate words. To reduce the impact of these artefacts we propose the multi-word error rate (multiWER). The multiWER allows 1:n mappings from words in the reference to a sequence of word labels and vice versa before calculating the WER. Mappings must be applied to all sentences consistently (not only to one sentence) and the concatenated pronunciations on both sides must be equal. Table 12
                      compares both measures with the help of an example. We feel that even the multiWER underestimates the usefulness in a speech-to-speech translation (S2S) scenario: Minor spelling errors are not important as long as the word is always substituted with the same misspelled word label. For example, “against” is misspelled in Table 12 but since there are no similar entries in the dictionary, “against” is always misspelled the same way. Therefore, it would not affect S2S performance. However, we could not find an error measure that tolerates even those artefacts but does not overestimate the real performance.

Table 13
                      summarizes the results with the acoustic model AM
                     13.1. In this setting, the pronunciations are extracted from phoneme sequences with 13.1% PER (R
                     13.1) and the ASR system is based on a speaker dependent acoustic model. The gold standard (correct pronunciation dictionary and a language model trained on the AM
                     13.1 training transcriptions EN
                     
                        all
                     
                     \
                     EN
                     
                        filt
                     ) has a WER of 3.6% and a multiWER of 3.6%: There are no segmentation errors when the correct pronunciation dictionary is applied. In our experiments, the multiWER is usually significantly lower than the WER. This illustrates the effectiveness of the multiWER evaluation measure to reduce artefacts due to segmentation errors. Source word independent clustering consistently outperforms source word dependent clustering for all three source languages. We report a multiWER of 25.3% using the best source language es3.

Table 14
                      shows the results from Table 13 when a worse acoustic model (AM
                     45.1) is used in the ASR system and the phoneme sequences for the pronunciation extraction contain 45.1% errors (R
                     45.1). The gold standard based on AM
                     45.1 has a WER and multiWER of 28.7% and thus is significantly worse than with the AM
                     13.1 acoustic model. This is reflected by the general performance drop compared to Table 13. The differences between the WER and multiWER is smaller than with AM
                     13.1 since spelling errors are more common. The best system (source word independent clustering, source language es3) achieves a multiWER of 83.1%. This indicates that the effectiveness of our methods strongly depends on the underlying acoustics.

In this paper, we have motivated and studied the “human translations guided language discovery for speech processing” (Stüker and Waibel, 2008; Besacier et al., 2014) which is highly relevant to bootstrap dictionaries from audio data for automatic speech recognition and bypass the written form in speech-to-speech translation (S2S), particularly in the context of under-resourced languages and those which are not written at all.

Our focus was the word segmentation and pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment. Initially, we investigated three different unsupervised algorithms for automatically finding word boundaries in phonetic transcriptions. We showed that using information from another language rather than a pure monolingual approach helps to find better segmentations on perfect phoneme sequences. A simple way to incorporate cross-lingual information is to apply word-to-word alignment models from SMT to align words of the other language to the phonemes of the target language. However, when phoneme recognition errors are common, the word segmentation precision is not significantly higher than with the monolingual approach. Therefore, Stahlberg et al. (2012) proposed the new alignment model Model 3P for cross-lingual word-to-phoneme alignment which extends the generative process of IBM Model 3 by a word length step and additional dependencies for the lexical translation probabilities. With this new model, we obtained considerably better word segmentations than with both previous methods. Using Model 3P for the alignment between English words and Spanish phonemes outperformed a state-of-the-art monolingual word segmentation approach on the BTEC corpus by up to 24.72% relative in segmentation accuracy and a GIZA++ alignment by up to 11.95%. We report a word segmentation accuracy of 90.1% on perfect phoneme transcriptions and still 83.9% on a phoneme sequence containing the 25.3% errors produced by our simulated phoneme recognizer. On the Bible corpus, we consistently achieve relative improvements between 20.2% and 59.8% in terms of segmentation accuracy.

Second, we presented two algorithms to deduce phonetic transcriptions of target language words from Model 3P alignments (source word dependent (Stahlberg et al., 2013) and independent clustering (Stahlberg et al., 2014)) and compared them on 15 translations in 10 languages of the Christian Bible. A variety of linguistic approaches to Bible translation (Thomas, 1990) allowed to compare different translations within the same source language. For example, based on noisy target language phoneme sequences with 45.1% errors, we built a dictionary for an English Bible with a Spanish Bible translation with 4.5% OOV rate, in which 64% of the extracted pronunciations contain no more than one wrong phoneme. We learn that source word dependent clustering reduces the number of unnecessary entries in the dictionary but source word independent clustering produces better pronunciations on the phoneme level and works better on noisy input.

Finally, we used the extracted pronunciations in an ASR system for the target language English. Source word independent clustering consistently outperformed source word dependent clustering in this case. When we have access to a speaker dependent acoustic model, we achieve a multi-word error rate (multiWER) of 25.3%.

In the future, we plan to enhance our pronunciation extraction algorithms (source word dependent and independent clustering) based on the results from Section 3.3: Source word dependent clustering needs to be improved to separate pronunciation variants and different words with the same translation more reliably. Enforcing a Zipfian cluster size distribution on source word independent clustering may improve clustering accuracy, e.g. by transforming the clustering problem to a linear programming problem as suggested by Zhu et al. (2010). Errors due to single phonemes dropped or added at the beginning or end of a pronunciation may be reduced by reinforcing the alignments with the extracted pronunciations after each iteration of our algorithm. Monolingual word segmentation methods (Johnson, 2008; Goldsmith, 2010) may give additional hints. In a next step, we will focus on acoustic modeling and apply phonetic discovery methods as suggested by Lee and Glass (2012), Varadarajan et al. (2008), Chaudhuri et al. (2011) on the target language speech rather than use acoustic models trained on target language transcriptions. The acoustic models could be further improved by iteratively recognizing the speech to provide target language transcriptions, and then using the transcriptions to adapt the models. When it comes to evaluating speech recognizers in our setting in the context of speech-to-speech translation (S2S), an automatic measure is to be found that does not penalize partially misspelled words as hard as the word error rate or multi-word error rate but is more meaningful than a phoneme error rate. Furthermore, we intend to use the extracted dictionaries in an ASR system for a truly under-resourced and non-written language. The final goal is to build an S2S system without any linguistic knowledge or writing system of the target language.

@&#REFERENCES@&#

