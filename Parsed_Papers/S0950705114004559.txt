@&#MAIN-TITLE@&#The synergistic combination of particle swarm optimization and fuzzy sets to design granular classifier

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This paper proposes a granular classifier to discover hyperboxes in three phases.


                        
                        
                           
                           The first phase of the proposed model uses the set calculus to build the hyperboxes.


                        
                        
                           
                           The second phase develops the geometry of hyperboxes using PSO algorithm.


                        
                        
                           
                           The PSO is used to optimize the classification rate and expanding the hyperboxes.


                        
                        
                           
                           The third phase identifies the noise points to improve the geometry of classifier.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Hyperbox geometry of classifiers

Granular classifier

Membership functions

Particle swarm optimization

DBSCAN clustering algorithm

@&#ABSTRACT@&#


               
               
                  Granulation extracts a bundle of similar patterns by decomposing universe. Hyperboxes are granular classifiers to confront the uncertainties in granular computing. This paper proposes a granular classifier to discover hyperboxes in three phases. The first phase of the proposed model uses the set calculus to build the hyperboxes; where, the means of the DBSCAN clustering algorithm constructs the structure. The second phase develops the geometry of hyperboxes to improve the classification rate. It uses the Particle Swarm Optimization (PSO) algorithm to optimize the seed_points and expand the hyperboxes. Finally, the third phase identifies the noise points; where, the patterns in the second phase did not belong to any hyperboxes. We have used the capability of membership function of a fuzzy set to improve the geometry of classifier. The performance of a proposed model is carried out in terms of coverage, misclassification error and accuracy. Experimental results reveal that the proposed model can adaptively choose an appropriate granularity.
               
            

@&#INTRODUCTION@&#

Granulation is a well-known computational model used as a significant step in the human cognition process. It is a process that decomposes a universe into parts [8]. Granules are a bundle of similar patterns or objects, which are extracted and collected from datasets based on the characteristic similarity in the granulation process. Characteristic similarity in information granules is defined in terms of indistinguishably, proximity or functionality, where operations and computation are applied on the information granules. For instance, the leveled granular system is created based on the information similarities by the tolerance relation of distance function [1,7]. The information granules are employed to develop and solve the uncertainty, incompleteness or vagueness of the information problem in granular computing [11,28]. Information granules are constructed based on the homogeneity of the patterns in order to establish geometry of the feature space and the decision boundaries. The significant goal of granulation is to find the high homogeneity of the patterns to discover the granules for classification [3,22,24,28]. Development and computation of the information granules are usually the basic issues in granular computing [16]. Granular computing can generate a unified framework from the different problems and technologies by interacting with other granules and gathering essential granular information to prepare suitable outcomes [12,15]. One of the significant capabilities of hyperboxes in classification is that each of them plays a role of classifier for each class under interest. Consequently, unlike the traditional classifiers, the group of classifiers for each class under interest is existed for classification. In addition, the field of cluster analysis suffers from extensive studies to discover certain category of granular classifiers referred as hyperboxes [36]. Therefore, we proposed a model to discover the hyperboxes and noise points for effective classifiers in the geometry of the information granules.


                     Fig. 1
                      shows the schematic view of decomposing the universe into parts with hyperboxes and its refinement from sets of points to fuzzy sets in granular computing. The hyperboxes are built through the homogeneity of the patterns in the granulation process as presented in Fig. 1(a). Twelve hyperboxes build with the granular classifier through the border of classes in Fig. 1(a). Each hyperbox represents only a single class. If pattern x ∈ 
                        
                           
                              
                                 R
                              
                              
                                 n
                              
                           
                        
                      belongs to one of the hyperboxes 
                        
                           
                              
                                 H
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 H
                              
                              
                                 2
                              
                           
                        
                     , …, 
                        
                           
                              
                                 H
                              
                              
                                 c
                              
                           
                        
                      in each class under interest (“class one”, “class two” or “class three”), it assigns and classifies it to that class. Fig. 1(b) considers the patterns that do not belong to any hyperbox namely noise points. Fig. 1 presents the refinement of the hyperboxes through the fuzzy sets in terms of the membership grades to confront the regions of the universe.

In this paper, we have proposed a granular classifier to discover hyperboxes. The model is constructed in three phases. In the first phase, we use DBSCAN clustering algorithm [4] to build the seeds of the hyperboxes defined as the centers of clusters in each pattern class. Then, we have applied an interval analysis to capture the key part of the classifier’s structure whereby the high homogeneity of the patterns is constructed based on the sets of the areas within the feature space. The classifiers are used to establish the geometry of the feature space and the decision boundaries are interpreted as classification processes that will be analyzed according to the evaluation criteria base on coverage, misclassification and accuracy [7,13]. In the second phase, we use the Particle Swarm Optimization (PSO) algorithm to optimize the seed_points of each class of patterns in order to improve the hyperboxes in covering more patterns related to classes. PSO expands the size of hyperboxes by expanding away from the center; showing that optimized_seed_points can cover more points. In the third phase, there are some points remained as noise after optimizing the seed_ points and creating hyperboxes. The noise points do not belong to any clusters and located outside the optimized hyperboxes. We used the membership function in fuzzy sets to allocate the noise points. The fuzzy sets are applied to the regions of the universe where the probability of classification error is high, and the classification result is expressed by membership grades.

The paper is organized as follows. Section 2 explains the related works. Section 3 describes the proposed model to discover the hyperboxes and noise points in the geometry of the information granules. In Section 4, we benchmarked the proposed model with different datasets from the UCI machine learning repository [19], and we evaluate them based on the coverage, misclassification and accuracy measurements. Finally, we conclude the research findings in Section 5.

@&#RELATED WORKS@&#

A review of the existing studies shows a diversity of research in granular computing. Many researchers have used granular computing by different reasoning formalisms to design information granules. They include interval mathematics [11,12,20], fuzzy sets [2,9,10,18], rough sets [17,35], cluster analysis [5,26,35], hybrids models [15,21,23] in various research areas such as image segmentation [28], data analysis [12,16,31], granular data interaction [2,21,25] and the concept of formation of granular using learning algorithms [12,30,31]. Pedrycz and Succi [1] proposed the genetic granular classifier in order to investigate the problem of software maintenance and an immediate interpretation of software data. Their work is applied on the software datasets and divided into two phases. In the first phase, they formed the seeds of hyperboxes through the Fuzzy C-means algorithm. In the second phase, they expanded the hyperboxes by applying genetic algorithm. Result of proposed method represents that the main advantage of the hyperbox classifier is relied in its interpretability. Pedrycz and Succi proposed a method reveals that the geometry of hyperboxes can be developed as a model of software quality. Their proposed method in [1] can also be extended as an object-based suit of software measure. Sanchez [5] proposed the Fuzzy Granular Gravitational Clustering Algorithm (FGGCA) to create fuzzy information granules that incorporates the theory of granular computing. The FGGCA is an unsupervised clustering algorithm finds the information granules on given data. FGGCA is applied on the multivariate data such as Iris, Wine, Seeds and Glass datasets [19]. The result of correctness of the clustering portion is represented the similar performance against other clustering algorithms. The FGGCA can deal with both synthetic and real world datasets in order to generate information granules.

Pedrycz et al. [7] considered the hyperboxes discovery and noise point problems in classification. The fundamental development strategy was orchestrated by set (interval) calculus and fuzzy sets. They formed the hyperboxes based on set calculus. Information granules were built in the unsupervised and supervised version of the DBSCAN clustering algorithm. Furthermore, Hausdorff distance [33,34] has been applied to solve the problem of allocation of degrees of belongingness in Pedrycz et al. [7]. The proposed method is applied in the different datasets in various domains [19]. In particular for classification on iris dataset, their method mostly showed superior in terms of error to other traditional classifiers [47–49] and hyperbox set-based methods [47–51]. Hong Hu et al. [22] proposed the granular system in order to consider the computer vision topic. The proposed method constructed by hybridizing the fuzzy logic and machine learning in order to create the granular system for deep learning. Hong Hu et al. [21] proposed method simulated the hierarchical structure of human brain and applied on the haze-free as an example. The result of proposed method represented some improvement for the task of haze-free compare to the He et al. [27] approach. Hong Hu et al. [21] method considered the deep learning concept in a new way by collaboration of machine learning and fuzzy logic. The proposed method can be extended for pattern recognition and image processing in addition to the brain science and information science under the framework of deep learning.

Pedrycz and Song [29] considered the reduction of the input space as a problem of combinatorial nature. They initially constructed the fuzzy models and enhanced its interpretability with genetic algorithm by reducing the input space dimension. Afterwards, the information granules were formed in the reduced space through Fuzzy C-Means (FCM). Pedrycz and Song [29] method is applied on the synthetic data. The results of accuracy represented that the reduction of input space increases the performance of the model. The experiments in this study showed that the reduction problem could be solved through the different granularity of the model. Pedrycz and Song [29] model considered the preprocessing by genetic algorithm in order to increase the accuracy and interpretability of information granules. Pedrycz et al. [8] presented a concept of granular fuzzy decision in order to consider the distributed modeling, time series characterization and classification problems in fuzzy decision-making. The information granules were presented and processed through the contribution of type-2 and type-3 fuzzy sets with intervals to reconcile differences among existing decisions, quantifying their diversity and associating it with the overall fuzzy decision. However these approaches do not provide better mechanism to recognize noise in classification procedure and also the accuracy evaluation are not reflecting specific consideration in looking to specific experiment using dataset like Ecoli, Glass or Wisconsin. In our experiments we show that our classification algorithm can classify the noise points and calculate the accuracy of proposed method by using granular computing.

We considered the clusters and noise point discovery in a dataset. We used DBSCAN clustering algorithm [4] to obtain information granules for the clusters and noise point discovery. Furthermore, as DBSCAN needs learning and optimizing algorithm to expand the discovery, therefore, DBSCAN is hybridized with PSO algorithm in order to enhance the performance of formalized information granules. Finally, the noise point problem must be considered for covering the universe by fuzzy sets. We have used fuzzy set to represent uncertainty relation in regard to the noise points. Thus, this research is proposing a hybrid recommendation model using DBSCAN clustering algorithm and PSO for optimizing the seed_points and expanding the size of the hyperboxes to discover more points. Furthermore, we apply fuzzy membership grades to solve the problem of noise points belongingness for covering the universe in classification. For this study, we consider the proposed model for several machine learning datasets related to the classification problems from the UCI repository [19].

In this section, we describe the proposed model to discover the hyperboxes and noise points in the geometry of information granules. The general architecture of the proposed model is shown in Fig. 2
                     . Part 1 prepares the datasets from the UCI repository [19] and divides them into 70–30% training and testing. Part 2 shows three phases involved in the training phase. The first phase finds the seeds to create the hyperboxes in each class of patterns. It splits the training datasets based on the class label, and then applies DBSCAN clustering algorithm to build the granular structure of the datasets. The second phase hybridizes the PSO algorithm with DBSCAN clustering algorithm to optimize the seed_points obtained in the first phase. This phase modifies the seed_points using the PSO algorithm to improve the hyperboxes in covering more patterns. The third phase detects the noise points as those patterns not included in any clusters once the seed_points and creating hyperboxes are optimized. This paper applies fuzzy membership grades to solve this problem. Part 3 shows the procedure of testing phase. It tests the seed_points generated and optimized in the first and second phases to build the information granules as classifiers. It applies the fuzzy sets to calculate the membership grade of the noise points; where, it calculates the performance of proposed method in each phase using the evaluation criteria. The details of each phase are described as follows:

The primary structure of the hyperboxes is made of DBSCAN clustering algorithm to create the granular structures. In the first phase, the unsupervised hyperboxes are applied to discover clusters and noise points, and its seed_points are optimized in the next phase via PSO. The underlying concept is that the typical density of patterns in the created cluster is more than outside of that cluster through some predefined probability density requirements. Two global parameters are required to build hyperboxes, namely, radius of the EPS-neighborhood (EPS) [4] and the minimum number of points (MinPoints) in an EPS-neighborhood of that point. It is difficult to create an explicit relationship between MinPoints and EPS. We create a relationship between two significant parameters (EPS and MinPoints) by following the design heuristic presented by Ester et al. [4] in order to create a relationship between two significant parameters. Thus, we describe and introduce some relevant definitions [4] used to construct the primary structure.
                           Definition 1
                           Neighborhood


                           It is the group of similar points, which are placed from a point based on the predefined distance. The figure of the neighborhood is established by the distance functions such as Euclidean for two points ‘
                                 
                                    p
                                 
                              ’ and ‘
                                 
                                    q
                                    .
                                 
                              ’

A point that has more points than MinPoints within a given EPS value is called a seed_point as presented in Fig. 3
                              . Seed_point’s neighborhoods must show the same class.

The EPS-neighborhood of a point ‘
                                 
                                    p
                                    ,
                                 
                              ’ denoted by NEPS (p)
                                 
                                    ,
                                 
                               is shown by {
                                 
                                    q
                                    ∈
                                    D
                                 
                               ∣ 
                                 
                                    d
                                    (
                                    p
                                    ,
                                    q
                                    )
                                    ⩽
                                    EPS
                                 
                              } with respect to MinPoints where D is any dataset. In fact, for each point ‘
                                 
                                    q
                                 
                              ’ placed in a cluster ‘
                                 
                                    C
                                 
                              ’ there is another point ‘
                                 
                                    p
                                 
                              ’ in a cluster ‘
                                 
                                    C
                                 
                              ’ so that ‘
                                 
                                    q
                                 
                              ’ is located inside of the EPS-neighborhood of ‘
                                 
                                    p
                                 
                              ’ as presented in Fig. 3.

A point ‘
                                 
                                    p
                                 
                              ’ is directly density-reachable from point ‘
                                 
                                    q
                                 
                              ’ wrt. EPS, MinPoints if:
                                 
                                    (1)
                                    
                                       
                                          
                                             p
                                             ∊
                                             
                                                
                                                   N
                                                
                                                
                                                   EPS
                                                
                                             
                                             (
                                             q
                                             )
                                          
                                       
                                    

∣NEPS(q)∣ 
                                          
                                             ⩾
                                          
                                        MinPoints (core point condition).

It is clear that the pairs of core points are symmetrical. On the other hand, the core point and border point in a hyperbox are asymmetrical patterns as presented in Fig. 4
                               (a).

A density reachable point in any dataset space is defined as follows:
                                 
                                    •
                                    
                                       
                                          
                                             ∀
                                             p
                                             ,
                                             q
                                          
                                       : If a chain of points 
                                          
                                             
                                                
                                                   p
                                                
                                                
                                                   1
                                                
                                             
                                          
                                       , …, 
                                          
                                             
                                                
                                                   p
                                                
                                                
                                                   n
                                                
                                             
                                          
                                        exist, where 
                                          
                                             
                                                
                                                   p
                                                
                                                
                                                   1
                                                
                                             
                                             =
                                             q
                                          
                                        and 
                                          
                                             
                                                
                                                   p
                                                
                                                
                                                   n
                                                
                                             
                                             =
                                             p
                                          
                                        and there is a point ‘
                                          
                                             
                                                
                                                   p
                                                
                                                
                                                   i
                                                   +
                                                   1
                                                
                                             
                                          
                                       ’ in a chain of points, which is directly density-reachable from ‘
                                          
                                             
                                                
                                                   p
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       ’, then point ‘
                                          
                                             p
                                          
                                       ’ is density reachable from the point ‘
                                          
                                             q
                                          
                                       ’ in regard to EPS and MinPoints.

Density-reachability is the same as direct density-reachability. This relation is transitive, but it is not symmetric as depicted in Fig.4 (b).

A density-connected point in ‘D’ is defined as follows:
                                 
                                    •
                                    
                                       
                                          
                                             ∀
                                             p
                                             ,
                                             q
                                             ,
                                             o
                                             ∈
                                             D
                                          
                                       : If ‘p’ and ‘q’ are density-reachable from ‘
                                          
                                             o
                                          
                                       ’, then a point ‘p’ is density-connected to point ‘q’ with regard to EPS and MinPoints.
                                    

Density-connected points are symmetric for pairs as shown in Fig. 4(c).

A density-based cluster in any dataset space is defined by the following conditions:
                                 
                                    (1)
                                    
                                       
                                          
                                             ∀
                                             p
                                             ,
                                             q
                                          
                                       : if 
                                          
                                             p
                                             ∈
                                             C
                                          
                                        and ‘
                                          
                                             q
                                          
                                       ’ is density-reachable from ‘
                                          
                                             p
                                          
                                       ’ in regard to EPS and MinPoints, then 
                                          
                                             q
                                             ∈
                                             C
                                          
                                       . (Maximality)


                                       
                                          
                                             ∀
                                             p
                                             ,
                                             q
                                             ∈
                                             C
                                          
                                       : ‘
                                          
                                             p
                                          
                                       ’ is density-connected to ‘
                                          
                                             q
                                          
                                       ’ in regard to EPS and MinPoints. (Connectivity)

Each hyperbox includes a seed_point located at the center of a hyperbox and EPS that estimates the border of the hyperbox from the seed_point. A hyperbox is an object with at least MinPoints objects within a radius EPS-neighborhood. In fact, each cluster is constructed by a grouping of hyperboxes in which seed_points of hyperboxes are density-connected.

A point in ‘D’ is named as a noise point if it does not belong to any constructed clusters as shown in Fig. 3.

It is density-reachable from the other seed_point located inside of the hyperbox; however, it is not a seed_point by itself as presented in Fig. 3.

In the following, we describe the algorithm that is used in the primary phase. The aim of this phase is to build hyperboxes for discovering the information granules to create clusters and noisy points in a dataset according to noise points and density-based cluster definitions (refer to Fig. 5
                        ). First, the training datasets are split based on class label. Finding hyperboxes procedure looks at the EPS-neighborhood of each pattern in the training patterns and returns all density-reachable patterns from that pattern with regard to EPS and MinPoints. Thus, this process yields a new hyperbox ‘
                           
                              
                                 
                                    H
                                 
                                 
                                    i
                                 
                              
                           
                        ’ which contains the points in EPS-neighborhood. If there is no a cluster for the specified class label, the new cluster ‘C’ is created for the specified class label. Otherwise, the new hyperbox is added to the existing cluster. Each hyperbox represents only a single class. In the next step, all the unvisited points (border points) in ‘
                           
                              
                                 
                                    H
                                 
                                 
                                    i
                                 
                              
                           
                        ’ are checked by the EPS-neighborhood definition. If EPS-neighborhood of each point in ‘
                           
                              
                                 
                                    H
                                 
                                 
                                    i
                                 
                              
                           
                        ’ contains more than MinPoints among datasets, then the new hyperbox ‘
                           
                              
                                 
                                    H
                                 
                                 
                                    i
                                    +
                                    1
                                 
                              
                           
                        ’ is created and added to cluster ‘C’ in order to build it. This recursive call is performed iteratively to gather all directly density-reachable points until there is no more new point exists for adding to the current cluster ‘C’. DBSCAN algorithm extends its finding hyperboxes procedure for all the points that have not been visited yet to create new hyperboxes for the different classes and create a new cluster. Due to the nature of DBSAN algorithm we have used the Euclidean distance formula [7] given in Eq. (1) as follows:
                           
                              (1)
                              
                                 d
                                 
                                    
                                       
                                          x
                                          ,
                                          y
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                n
                                             
                                          
                                       
                                       
                                          
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                             -
                                             
                                                
                                                   y
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        where ‘x’ and ‘y’ denote the vectors of two datasets and ‘
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                        ’ and ‘
                           
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                           
                        ’ denote the attribute of them. ‘n’ presents the number of attributes.

Optimization of the information granules collection is an important issue in the context of this research and any hybrid system. In the first phase, the hyperboxes that belong to each class are created. In the second phase, seeds of each class are optimized by the PSO method. The aim of this phase is to modify the seed_points by PSO to improve the performance of primary structure to better fit the data. The schematic view of this is presented in Fig. 6
                        . Fig. 6(a) represents the hyperbox created in the primary phase, and it covers five points according to the defined EPS and MinPoints. Moreover, Fig. 6(b) represents the optimized version of the seed_point that expands the dimension of the hyperbox. Thus, the optimized_seed_point covers seven points with respect to EPS and MinPoints. In this situation, the EPS value does not change to cover more points. Thus, the hyperboxes in Fig. 6(a) and (b) have the same size. In fact, the seed point in Fig. 6(a) is modified to cover more points. The expansion of the hyperbox dimension is done via PSO and is described as follows:

In the initialization phase, we separate the seed_points of each class and the training datasets belonging to that class. For example, the “malignant” and “benign” data in training set of Wisconsin (Diagnostic) dataset are separated. Then, the PSO algorithm is applied on “malignant” seed_points by “malignant” datasets as training datasets in order to modify the seed_points for expanding the hyperboxes size. Such action appears in the same way for “benign” seed_points among “benign” datasets as training datasets. Details of seed_points initialization and optimization are described in the following subsections.

We used the Euclidean distance to calculate the distance between seed_points and training datasets (refer to formula 1). The initialization and updating of the velocity, position, ‘
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                              
                           ’ and ‘
                              
                                 
                                    
                                       p
                                    
                                    
                                       g
                                    
                                 
                              
                           ’ parameters must be done in the PSO process. In order to show the initial state of PSO, we initialized the velocity list and position list with zero values and seed_points, respectively. In addition, we defined the ‘
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                              
                           ’ list (i = 1, 2, …, N) as a container of a minimum distance of creating seed_points. The ‘i’ variable is denoted the ith particle (seed_point) in the swarm. In addition, N is the population size (total number of created seed_points in the first phase). Furthermore, ‘
                              
                                 
                                    
                                       p
                                    
                                    
                                       g
                                    
                                 
                              
                           ’ value which indicates the global best particle was obtained as a minimum value of ‘
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                              
                           ’ list. This would make the search space of all possible solutions.

In each iteration, a limited number of attributes were selected according to the token rate to update the position and velocity. We selected 10% of total attributes randomly in each iteration to update the position and velocity. In addition, we run the PSO 500 times for seed_point optimization in the second phase (total rate=40%). ‘
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                              
                           ’ and ‘
                              
                                 
                                    
                                       p
                                    
                                    
                                       g
                                    
                                 
                              
                           ’ parameters were used for updating the position in each iteration (refer to formula 2). Update formula for seed_point’s (particle’s) velocity is as follows:
                              
                                 (2)
                                 
                                    
                                       
                                          V
                                       
                                       
                                          i
                                       
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                    =
                                    
                                       
                                          wV
                                       
                                       
                                          i
                                       
                                       
                                          t
                                       
                                    
                                    +
                                    
                                       
                                          c
                                       
                                       
                                          1
                                       
                                    
                                    
                                       
                                          r
                                       
                                       
                                          1
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   p
                                                
                                                
                                                   i
                                                
                                             
                                             +
                                             
                                                
                                                   X
                                                
                                                
                                                   i
                                                
                                                
                                                   t
                                                
                                             
                                          
                                       
                                    
                                    +
                                    
                                       
                                          c
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          r
                                       
                                       
                                          2
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   p
                                                
                                                
                                                   g
                                                
                                             
                                             +
                                             
                                                
                                                   X
                                                
                                                
                                                   i
                                                
                                                
                                                   t
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where ‘
                              
                                 i
                              
                           ’ variable denotes the number of seed_points in the datasets space (swarm). ‘
                              
                                 
                                    
                                       r
                                    
                                    
                                       1
                                    
                                 
                              
                           ’ and ‘
                              
                                 
                                    
                                       r
                                    
                                    
                                       2
                                    
                                 
                              
                           ’ variables are the random values in velocity updating processes. ‘
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                              
                           ’ and ‘
                              
                                 
                                    
                                       p
                                    
                                    
                                       g
                                    
                                 
                              
                           ’ are denoted the location of the best problem solution vector found by ‘i’ and the location of the best particle (seed_point) among all the particles (seed_points) in the population, respectively. ‘
                              
                                 w
                              
                           ’, ‘
                              
                                 
                                    
                                       c
                                    
                                    
                                       1
                                    
                                 
                              
                           ’ and ‘
                              
                                 
                                    
                                       c
                                    
                                    
                                       2
                                    
                                 
                              
                           ’ (user-supplied coefficients parameters) default values was defined as ‘1.2’ for 
                              
                                 ‘
                                 w
                              
                           ’ and ‘0.5’ for ‘
                              
                                 
                                    
                                       c
                                    
                                    
                                       1
                                    
                                 
                              
                           ’ and ‘
                              
                                 
                                    
                                       c
                                    
                                    
                                       2
                                    
                                 
                              
                           ’ variables. ‘w’ ensures convergence of the PSO algorithm. Then, the updated velocity 
                              
                                 
                                    
                                       
                                          
                                             
                                                V
                                             
                                             
                                                i
                                             
                                             
                                                t
                                                +
                                                1
                                             
                                          
                                       
                                    
                                 
                              
                            is added to the current position 
                              
                                 (
                                 
                                    
                                       X
                                    
                                    
                                       i
                                    
                                    
                                       t
                                    
                                 
                                 )
                              
                            (which assumed as an attribute of seed_point) to achieve the updated position 
                              
                                 
                                    
                                       
                                          
                                             
                                                X
                                             
                                             
                                                i
                                             
                                             
                                                t
                                                +
                                                1
                                             
                                          
                                       
                                    
                                 
                              
                            as follows:
                              
                                 (3)
                                 
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                    =
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                       
                                          t
                                       
                                    
                                    +
                                    
                                       
                                          V
                                       
                                       
                                          i
                                       
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                 
                              
                           
                        

After applying the PSO operation on seed_point in each iteration, the fitness value of changed seed_point (seed’) must be calculated and compared with the last seed_point (seed). Firstly, the distance summation of the seed_point and the existing points in the hyperbox is calculated (refer to the formulas (4) and (5)). The minimization of the 
                              
                                 H
                              
                           ’ leads to maximization of fitness value of optimized_seed_point. The fitness value of last seed_point and changed seed_point is presented in formula (6) and (7), respectively. All these equations are derived from our studies on PSO and its updating in our experiments.
                              
                                 (4)
                                 
                                    H
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                    
                                    d
                                    (
                                    seed
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                              
                           
                           
                              
                                 (5)
                                 
                                    
                                       
                                          H
                                       
                                       
                                          ′
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             n
                                          
                                       
                                    
                                    d
                                    (
                                    
                                       
                                          seed
                                       
                                       
                                          ′
                                       
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                              
                           If the fitness value of changed seed_point was increased, then the changed seed_point substituted by the last one. In fact, it depicted the improvement of changed seed_point performance.
                              
                                 (6)
                                 
                                    F
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   H
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (7)
                                 
                                    
                                       
                                          F
                                       
                                       
                                          ′
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   
                                                      
                                                         H
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

A group of hyperboxes was generated through the primary and secondary phases, and the structure of granular classifier is established in this way. Thus, this structure covers a considerable degree of patterns. The capability of membership function of a fuzzy set can be used to confront the areas of high overlap among the classes where the probability of classification error is high (refer to Fig. 1(b)). After implementing the first and second phases, it was observed that there are still some patterns that do not belong to any cluster and are placed outside the mentioned structure and identified as noise points (refer to Definition 9). Noise points could be positioned very close or far from the number of clusters. In the third phase, we solved this problem by using membership function and improved the geometry of the classifier by fuzzy sets. In fact, we considered all the patterns that are not covered by the primary and secondary phases. In the following, the way of computing the class membership is described. The membership degree of point ‘
                           
                              x
                           
                        ’ known as a noise point is indicated here by ‘
                           
                              
                                 
                                    u
                                 
                                 
                                    i
                                 
                              
                           
                        ’. To calculate the ‘
                           
                              
                                 
                                    u
                                 
                                 
                                    i
                                 
                              
                           
                         ’, the distance of noise point and seed_points of each cluster is calculated by the Euclidean formula. Then, Fig. 7
                         represents that the average distance of noise point and seed_points is calculated as follows (with ‘c’ and ‘n’ being the number of clusters and seed_points, respectively):
                           
                              (8)
                              
                                 avg
                                 (
                                 x
                                 ,
                                 
                                    
                                       C
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       d
                                       (
                                       x
                                       ,
                                       
                                          
                                             S
                                          
                                          
                                             1
                                          
                                       
                                       )
                                       +
                                       d
                                       (
                                       x
                                       ,
                                       
                                          
                                             S
                                          
                                          
                                             2
                                          
                                       
                                       )
                                       +
                                       ⋯
                                       +
                                       d
                                       (
                                       x
                                       ,
                                       
                                          
                                             S
                                          
                                          
                                             n
                                          
                                       
                                       )
                                    
                                    
                                       n
                                    
                                 
                              
                           
                        Thus, the membership value 
                           
                              (
                              
                                 
                                    u
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         of noise point 
                           
                              (
                              x
                              )
                           
                         in each cluster 
                           
                              (
                              
                                 
                                    C
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         among the built clusters 
                           
                              (
                              
                                 
                                    C
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         is calculated by using the Lagrange multipliers technique as follows:
                           
                              (9)
                              
                                 
                                    
                                       u
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             c
                                          
                                       
                                       
                                          
                                             (
                                             avg
                                             (
                                             x
                                             ,
                                             
                                                
                                                   C
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                             ÷
                                             avg
                                             (
                                             x
                                             ,
                                             
                                                
                                                   C
                                                
                                                
                                                   j
                                                
                                             
                                             )
                                             )
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        
                     

In this section, the cooperation of optimized information granules and fuzzy sets is presented and analyzed. The aim is to obtain the best form of information granule to classify the patterns, and define their geometry and belongings to a specific class. In the following subsections, we consider the experimental datasets and the performance evaluation criteria used in this study. Finally, we analyze the performance of the proposed model according to the evaluation criteria. We have analyzed the effect of the proposed model on testing datasets. As it will be shown in experimental result tables in Appendices A–E, two third or more of the datasets included as seed_points in the training datasets for classification. For example, Table 1
                      shows 398 training dataset for Wisconsin dataset. On the other hand, Appendix A, Table 13 represents the number of seed_points in training phase is between 260 and 367 in various MinPts and EPS values.

In this study, we have considered several machine learning datasets related to classification problems from the UCI repository [19]. Table 1 presents the datasets used in this research and represents their area, number of classes (#class), number of attributes (#Att), number of examples (#Ex), number of training datasets (#Train) and number of testing datasets (#Test). The datasets is divided into training and testing datasets, and randomly 70% of the total datasets are assigned for training phase and the remaining for testing phase.

Performances of a proposed classifier for the primary and secondary structure are evaluated by the classification error and coverage percentage for primary and secondary structure. The classification error shows the division of patterns do not belong to any hyperboxes. They are known as misclassified patterns by the whole number of patterns in the primary or secondary structure. ‘
                           
                              α
                           
                        ’ and ‘
                           
                              β
                           
                        ’ denote the classification error in the primary and secondary structure (refer to formula (10) and (11)) 
                        [7]. In addition, formula (12) depicts and quantifies the percentage of patterns which are covered by the primary or secondary structure. We calculate the coverage percentage by dividing the number of patterns that are covered by the primary or secondary structure by the whole number of patterns.
                           
                              (10)
                              
                                 α
                                 (
                                 %
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                number of misclassified patterns in the primary structure
                                             
                                             
                                                number of patterns in the Primary structure
                                             
                                          
                                       
                                    
                                 
                                 ×
                                 100
                              
                           
                        
                        
                           
                              (11)
                              
                                 β
                                 (
                                 %
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                number of misclassified patterns in the secondary structure
                                             
                                             
                                                number of patterns in the secondary structure
                                             
                                          
                                       
                                    
                                 
                                 ×
                                 100
                              
                           
                        
                        
                           
                              (12)
                              
                                 Cov
                                 (
                                 %
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                number of patterns covered by the Primary
                                                /
                                                secondary
                                             
                                             
                                                number of patterns
                                             
                                          
                                       
                                    
                                 
                                 ×
                                 100
                              
                           
                        
                     

In this research, accuracy formula is used to calculate the performance of the proposed technique in classification. Classification accuracy is calculated in dealing with False Negative (FN) and False Positive (FP). FP represents the percentage of legitimate data and FN represents the percentage of non-legitimate data [13].
                           
                              (13)
                              
                                 Accuracy
                                 =
                                 
                                    
                                       
                                          
                                             
                                                TP
                                                +
                                                TN
                                             
                                             
                                                Number of Patterns
                                             
                                          
                                       
                                    
                                 
                                 ×
                                 100
                              
                           
                        
                     

@&#EXPERIMENTAL RESULTS@&#

In this part, we focus on determining whether our proposed model is robust in the classification of the mentioned datasets. The proposed model applied are (i) based on 10 runs, (ii) each run is based on new training–testing datasets, (iii) the compared models are used the same number of runs and (iv) their average result is presented for first, second and third phases in various ‘MinPts’ and ‘EPS’ values. The deviation measure may be decreased by increasing the number of runs. Also, the achieved mean value may be more accurate.

Glass datasets [19] are used in the criminological investigation. The identification of Glass as a “float” type or not is an important issue at the scene of the crime. Thus, we consider the Glass classification in two situations: the Glass is a type of “float” or “not a float”, and the Glass is a type of “non_float” or “not a non_float” as illustrated in Table 2
                           .

In this section, the performance of the proposed model is presented for “float” or “not a float” type in Glass datasets. Table 2 shows that the “float” Glass category consists of two types of Glass and “not a float” Glass category consists of five types of Glass. The following tables represent the performance of the proposed model by various ‘MinPoints’ (2, 3 and 4) and ‘EPS’ (0.3, 0.32) values on training and testing datasets. Appendix A, Table 7 represents the results on training datasets and Appendix A, Tables 8 and 9 represent the results of testing datasets based on the mentioned performance criteria for “float/not a float” categories. Also, the comparison of hyperbox number, which are created in the training phase, coverage percentage and misclassification error percentage in testing phase are represented in Fig. 8
                              . In addition, Fig. 9
                               presents the accuracy comparison in testing phase among the three phases.

The coverage and misclassification error percentages are increased and decreased by increasing the ‘MinPoints’ and ‘EPS’ values, respectively. Appendix A, Tables 8 and 9 shows that the coverage and misclassification error do not increase despite the accuracy, through the seed_point optimization in the second phase. Also, the best coverage and misclassification results belong to the formation in which ‘MinPoint’ and ‘EPS’ values are ‘2’ and ‘0.3’ with less distribution value compared to the formation in which ‘MinPoint’ and ‘EPS’ values are ‘3’ and ‘0.3’. Fig. 9 represents the accuracy comparison results in the first, second and third phases based on the mean and deviation value. The best accuracy results in the first and second phases are 85% and 85.1562%, respectively. These results belong to the formation in which ‘MinPoint’ and ‘EPS’ values are ‘2’ and ‘0.32’, respectively. Also, the formation in which ‘MinPoint’ and ‘EPS’ values are ‘3’ and ‘0.32’ has the best accuracy result in the less distribution of data compare to the formation in which ‘MinPoint’ and ‘EPS’ values are ‘4’ and ‘0.32’.

Similar to the previous section, the performance of the proposed model in three phases is presented for “non_float” or “not a non_float” type in Glass datasets. Appendix B, Table 10 presents the result on training datasets and Appendix B, Tables 11 and 12 depict the results of testing datasets according to the mentioned performance criteria for “non_float, or, not a non_float” categories. Figs. 10 and 11
                              
                               represent the hyperbox number, coverage and misclassification error results in the first and second phases. In addition, Fig. 12
                               presents the accuracy comparison in testing phase among the three phases.


                              Appendix B, Tables 11 and 12 present the coverage and misclassification error results, they are increased and decreased respectively through the seed_point optimizations by the formation in which ‘MinPoint’ value is‘3’ and ‘EPS’ value is ‘0.24’ and ‘0.3’. On the other hand, the accuracy results are increased through the seed_point optimization in all of the formation of ‘MinPoints’ and ‘EPS’ values. Figs. 10 and 11 shows that the maximum value of coverage, minimum value of misclassification error, minimum distribution value of coverage and misclassification error in the first and second phases belong to the formation in which ‘MinPoint’ and ‘EPS’ values are ‘2’ and ‘0.3’ in the testing phase. The maximum values of coverage and misclassification error in the first and second phases are occurred in the formation in which ‘MinPoint’ and ‘EPS’ values are ‘3’ and ‘0.3’ in the testing phase. Also, Fig. 12 shows that the best accuracy results are 88.2812%, 88.4375% and 92.8125% in the first, second and third phases and belong to the formation in which ‘MinPoint’ and ‘EPS’ values are ‘2’ and ‘0.3’ in the testing phase, respectively.

Wisconsin dataset [19] is related to life and medical research. The features of Wisconsin dataset are computed from the breast mass to diagnose the cancer. It has two class types “malignant” and “benign”. Coverage percentage shows that (refer to Appendix C, Tables 13–15) the density of datasets is high and is more than 95% where ‘EPS’ value is more than ‘2.0’. Fig. 14 and Appendix C, Table 15 shows that the coverage and accuracy results are increased and misclassification result is decreased through the seed_point optimization. Fig. 13
                           
                            shows that the maximum value of coverage and minimum value of misclassification error in the first phase belong to the formation in which ‘MinPoint’ and ‘EPS’ values are ‘3’ and ‘2.15’ in the testing phase. In addition, the best coverage and misclassification error results in the second phase are occurred in the formation in which ‘MinPoint’ and ‘EPS’ values are ‘2’ and ‘2.15’ in the testing phase (refer to Fig. 14).

In addition, Appendix C, Tables 13 and 14 show that the maximum distribution value for the coverage and misclassification results in the first and second phases belong to the formation in which ‘MinPts’ and ‘EPS’ are “4” and “2.1” in the testing phase. The minimum distribution value of coverage and misclassification results in the first and second phases belong to the formation in which ‘MinPts’ and ‘EPS’ are “2” and “2.15” in the testing phase. Fig. 15
                            shows that the best accuracy results are 87.6412%, 87.8362% and 88.226% for the first, second and third phases, respectively that belongs to the classifier in which ‘EPS’ value is ‘2.15’ and ‘MinPoints’ is ‘2’.

Ecoli datasets [19] contain protein localization sites and are used in the life area. Table 1 indicates that the number of classes for Ecoli datasets is 8. We used two types of categories for classification as “Cytoplasm, or, Not a Cytoplasm” and “Inner membrane, or, Not an Inner membrane” as presented in Table 3
                           .

The following tables represent the result of the proposed model on Ecoli datasets based on “Cytoplasm” and “not a Cytoplasm” categories according to the mentioned performance criteria as shown in Appendix D, Tables 16–18. Appendix D, Table 17 represents the performance results in the first phase and accuracy results by applying the fuzzy sets. The accuracy of noise points in the first phase is calculated directly by fuzzy sets. Also, Figs. 16 and 17
                              
                               depict the comparison of hyperbox number, coverage and the misclassification error percentage in the first and second phases, respectively. The performance result in Appendix D, Table 17 show that the accuracy and coverage results are decreased and misclassification result is increased through the seeds optimization in the second phase. But, the accuracy and coverage results are increased and misclassification result is decreased by applying the fuzzy sets.

According to Appendix D, Tables 16 and 17, the best result of coverage and misclassification error in the first and second phases belong to the granule formation in which “MinPts” and “EPS” values are ‘4.0’ and ‘0.9’ in the testing phase. Also, the minimum distribution value of coverage and misclassification error in the first phase are occurred in the formation in which ‘MinPoint’ and ‘EPS’ values are ‘4’ and ‘0.9’ in the testing phase. In addition, the minimum distribution value of coverage and misclassification error in the second phase belong to the granule formation in which “MinPts” and “EPS” values are ‘2.0’ and ‘0.9’ in the testing phase. The maximum distribution value of coverage and misclassification in the first and second phases belong to the granule formation in which “MinPts” and “EPS” values are ‘4.0’ and ‘0.8’, respectively. Appendix D, Tables 17 and 18 show that the best accuracy results are 85.3465%, 69.6039% and 85.7425% respectively for the first, second and third phases that belong to the classifier in which ‘EPS’ value is ‘0.9’ and ‘MinPoints’ is ‘3’ (see Figs. 18 and 19
                              
                              ).

This section considers another type of Ecoli datasets as “Inner membrane” or “not an Inner membrane”. The following tables (Appendix E, Tables 19–21) represent the results of the proposed model on Ecoli datasets based on “Inner membrane” or “not an Inner membrane” categories. Appendix E, Table 20 shows the performance results in the first phase and accuracy results after applying the fuzzy sets.


                              Appendix E, Table 21 depicts that the accuracy and coverage is decreased, and the misclassification is increased in the second phase. In addition, applying the fuzzy sets in the third phase does not affect the accuracy. Appendix E, Tables 20 and 21 represents that the minimum distribution values of coverage and misclassification and the best results of coverage and misclassification belong to the granule formation in which “MinPts” and “EPS” are ‘4.0’ and ‘0.9’, respectively. In addition, the maximum distribution values of coverage and misclassification results occurs when “MinPts” and “EPS” values are ‘3.0’ and ‘0.8’, respectively. Appendix E, Tables 20 and 21 show that the best accuracy result is88.6138% for the first and third phases, respectively that belong to the classifier in which ‘EPS’ value is ‘0.9’ and ‘MinPoints’ is ‘2’. In addition, the best accuracy result is 79.9999% for the second phase for the formation in which ‘EPS’ value is ‘0.9’ and ‘MinPoints’ is ‘3’.

We analyze and compare the accuracy and classification error of traditional classifiers and hyperbox set-based models on the Glass, Ecoli and Wisconsin datasets widely used in classification technique. Table 4
                               
                              [6,7,14,32], Table 5
                               
                              [37–42] and Table 6
                               
                              [37,39,43–46] present the comparison result of the reported studies. Tables 4 and 6 represent the proposed classification on Glass and Iris data sets is superior in terms of accuracy to other traditional classifiers and Hyperbox set-based models. On the other hand, Table 5 shows the accuracy result of the proposed method is inferior to traditional classifiers and Hyperbox set-based models.

In this study, we designed a fuzzy granular classifier as hyperboxes. The proposed model represents the cooperation between fuzzy sets and PSO algorithm. The proposed hyperboxes consists of three phases: (i) create the core structure of the proposed model using the set calculus to discover hyperboxes for creating clusters and noisy points, (ii) optimize the seed_points using the PSO algorithm and expand the set calculus to increase the classification rate in the datasets, and (iii) solve the problem of allocation of noise points using the fuzzy sets. The first phase aimed at discovering the hyperboxes to create clusters and noisy points from the existing points in training datasets. In this phase, DBSCAN clustering algorithm was applied to build the granular structure of the datasets. In the second phase, the PSO algorithm was used to hybrid by DBSCAN clustering algorithm to expand the hyperboxes size. This phase aimed at modifying the seed_points in each class under interest by the PSO algorithm to enhance the performance of primary structure. In the first and second phases, a set of hyperboxes was generated to cover patterns; however, there were still some patterns that do not place in any clusters and known as noise points. This problem was solved by using membership function and applying fuzzy sets to improve the geometry of the classifier. In the third phase, we calculate the average distance of a noise point to the centers (seed_points) of the cluster. We applied our model on three datasets of Glass, Ecoli and Wisconsin. The experiments revealed the superiority of cooperative optimization method (PSO) since it enhances the performance of the primary structure.

The main shortcoming of the proposed model is finding the relationship between two global parameters as ‘MinPts’ and ‘EPS’ used to build the hyperboxes for primary phase. Values of ‘MinPts’ and ‘EPS’ variables are assigned based on the patterns density in the universe. Thus, a specific statistical formula requires finding the appropriate values for ‘MinPts’ and ‘EPS’ variables based on the patterns density. For future works, one can examine other clustering algorithms to formalize the information granules, and study other optimization techniques to hybridize with the DBSCAN algorithm to expand the discovery of hyperboxes and noisy points.

@&#ACKNOWLEDGEMENTS@&#

The Universiti Teknologi Malaysia (UTM) and Ministry of Education Malaysia under research university grants 00M19 and 01G72 are hereby acknowledged for some of the facilities that were utilized during the course of this research work.


                     Tables 7–9
                     
                     
                     .


                     Tables 10–12
                     
                     
                     .


                     Tables 13–15
                     
                     
                     .


                     Tables 16–18
                     
                     
                     .


                     Tables 19–21
                     
                     
                     .

@&#REFERENCES@&#

