@&#MAIN-TITLE@&#A pool-based pattern generation algorithm for logical analysis of data with automatic fine-tuning

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a new scheme for classification via Logical Analysis of Data.


                        
                        
                           
                           The approach uses a metaheuristic that produces a pool of diverse patterns.


                        
                        
                           
                           We automatically fine-tuned the algorithm using biased Random Key Genetic Algorithm.


                        
                        
                           
                           We tested the algorithm on benchmark instances from the UCI repository.


                        
                        
                           
                           We carried out a statistical analysis to measure the effectiveness of the algorithm.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Logical Analysis of Data

Data mining

Fine-tuning

bRKGA

Machine learning

@&#ABSTRACT@&#


               
               
                  In this paper, we address the binary classification problem, in which one is given a set of observations, characterized by a number of (binary and non-binary) attributes and wants to determine which class each observation belongs to. The proposed classification algorithm is based on the Logical Analysis of Data (LAD) technique and belongs to the class of supervised learning algorithms. We introduce a novel metaheuristic-based approach for pattern generation within LAD. The key idea relies on the generation of a pool of patterns for each given observation of the training set. Such a pool is built with one or more criteria in mind (e.g., diversity, homogeneity, coverage, etc.), and is paramount in the achievement of high classification accuracy, as shown by the computational results we obtained. In addition, we address one of the major concerns of many data mining algorithms, i.e., the fine-tuning and calibration of parameters. We employ here a novel technique, called biased Random-Key Genetic Algorithm that allows the calibration of all the parameters of the algorithm in an automatic fashion, hence reducing the fine-tuning effort required and enhancing the performance of the algorithm itself. We tested the proposed approach on 10
benchmark instances from the UCI repository and we proved that the algorithm is competitive, both in terms of classification accuracy and running time.
               
            

@&#INTRODUCTION@&#

Let us consider a binary classification problem, in which one is given a dataset composed of observations belonging to one of two classes, e.g., positive or negative, where the class each observation belongs to is known. A typical data mining problem is the classification problem, i.e., finding the class a new observation, not included in the dataset, belongs to. Binary classification finds a large number of applications, spanning from, e.g., medical diagnosis (Alexe, Alexe, Axelrod, Hammer, & Weissmann, 2005; Hammer & Bonates, 2006), to credit risk rating (Hammer, Kogan, & Lejeune, 2006), from maintenance replacement (Ghasemi & Esameili, 2013), to fault diagnosis (Mortada, Yacout, & Lakis, 2013).

Owing to the fact that classification is such a relevant problem in the data mining field, effective techniques to classify data have been developed. The observations to be classified are characterized by a set of attributes, which are believed to affect the class each observation belongs to. However, the relationship between the value of the attributes and the class is unknown and, therefore, needs to be estimated via a training process. The classifier discovers such rules that map an object to a class, based on the values of the attributes.

Logical Analysis of Data (LAD) was introduced in Boros, Hammer, Ibaraki, and Kogan (1997), and Boros, Hammer, Ibaraki, Kogan, Mayoraz, and Muchnik (2000) and is a data analysis methodology that combines ideas from combinatorial optimization and Boolean functions and belongs to the family of supervised learning techniques. The LAD methodology relies on a “rule learning” mechanism and, therefore, is strongly connected with other popular classification rules presented in the machine learning literature. According to Fürnkranz (1999), many rule learning algorithms are based on a sequential covering procedure, in which the steps to follow are: “Learn a rule that covers part of the given training examples, remove the covered examples from the training set, and recursively learn another rule that covers some of the remaining examples until no examples remain.” LAD fits well within this sequential covering framework. For a more extensive discussion, we refer the reader interested in the connection between LAD and other “rule learning” techniques as well as on the “justifiability” of LAD to Boros, Crama, Hammer, Ibaraki, Kogan, and Makino (2011). Along the same line, Dembczynski, Kotlowski, and Slowinski (2010) establish a clear connection between LAD and other methods that fall within the framework of “rule ensamble algorithms,” e.g., algorithms that exploit boosting schemes (Cohen & Singer, 1999; Dembczynski et al., 2010), or based on the set covering machine (Marchand & Shawe-Taylor, 2002), or on the linear programming-boost framework (Kotlowski & Slowinski, 2009), or other linear programming-based approaches (Malioutov & Varshney, 2013).

Along the same line, LAD shares similarities with rough set methods. Rough set theory, proposed by Pawlak (1982, 1991), is a mathematical approach often employed to tackle classification problems (Bazan, Nguyen, Nguyen, Synak, & Wróblewski, 2000; Greco, Matarazzo, & Slowinski, 2001; Pawlak, 1998). In a fashion similar to what is done in LAD, rough set algorithms often rely on the use of discretization techniques, generation of reducts, i.e., patterns in LAD, and the definition of a rough membership function. An extensive overview of rough set methods for data analysis is provided by Chikalov, Lozin, Lozina, Moshkov, Nguyen, Skowron, and Zielosko (2013), where three different methods for data analysis, i.e., test theory, rough set, and LAD are thoroughly presented.

As pointed out by Malioutov and Varshney (2013), often what differentiates rule learning approaches is how the training procedure is managed, since each method might define a specific optimization objective (based, e.g., on statistical theory, mathematical programming, etc.) In addition, these methods might vary in the way in which the rule learning problem is solved (e.g., in a greedy fashion, using an exact method from mathematical programming, with brute-force approaches, via metaheuristics, etc.) In this regard, the LAD approach we propose in this paper can be characterized as: (a) a rule-based method that employs Boolean reasoning; (b) in which rules are built using a sequential covering-type of approach; (c) where the rule learning mechanism is formalized using mathematical programming; and (d) in which the optimization problem of (c) is solved using a pool-based metaheuristic.

LAD is based on four basic steps: (i) data binarization, (ii) support feature selection, (iii) pattern generation, and (iv) theory formation. (For a detailed introduction on methodology and applications of LAD, see also Chikalov et al. (2013).) It is well understood that one of the key features of LAD is concerned with the pattern generation process. As mentioned in Hammer, Kogan, Simeone, and Szedmk (2004), patterns are fundamental blocks in LAD as well as in many other rule induction algorithms, e.g., C4.5 rules (Quinland, 1993), AQ17-HCI (Wnek & Michalski, 1994), RIPPER (Cohen, 1995), SLIPPER (Cohen & Singer, 1999), RuleFit (Friedman & Popescu, 2008), and ENDER (Dembczynski et al., 2010). All these algorithms, thus, place special emphasis on identifying a small subset of patterns. It is interesting to notice, though, that on the one hand, empirical evidence shows that the way in which patterns are built within LAD has a strong bearing on the classification accuracy. On the other hand, due to the large number of patterns that can be constructed from a dataset, the algorithm used to build such patterns strongly determines the practical usability of LAD, especially when it comes to dealing with very large datasets.

It is worth pointing out that LAD and many of the classical rule learning techniques share the same advantage, i.e., knowledge represented by rules is generally easier to interpret by people. In addition, as highlighted by Boros et al. (2011), LAD patterns are “justifiable,” thus enhancing the ability to motivate the reasons behind a certain decision and, consequently, allowing for human insight into what is learned. However, these methods face the same problem, i.e., the search space of literals can become intractable. Consequently, the proposed pool-based approach for pattern generation can be extended to other rule learning methods.

The goal of this paper is to present a novel metaheuristic to generate patterns within LAD, which allows to construct patterns with a predefined criterion in mind, while limiting the computational time required by the pattern generation phase. We introduce the concept of pool of patterns, where each pattern generated by a metaheuristic scheme is added to the pool only if some criteria are satisfied (e.g., diversity, coverage, homogeneity, etc.) The contribution of the paper is twofold: On the one hand, a methodological contribution is made, since it is the first time that a metaheuristic is used for generating a pool of patterns with pre-specified characteristics within LAD and, on the other hand, a further contribution, based on empirical evidence, is made in the direction of accuracy and computational running time, thus allowing to extend the use of LAD to large datasets.

In addition, in this paper we propose a solution to one of the major drawbacks of a number of data mining algorithms, i.e., the calibration and fine-tuning of the algorithmic parameters, via the use of a novel technique to automatically fine-tune such parameters. In the literature, it is quite common to find data mining techniques that require extensive calibration to reach satisfactory classification results. However, the way in which the calibration process should be conducted is very seldom described and, even when guidelines about the fine-tuning phase are provided, such phase still requires a large amount of time and effort on the side of the researcher. In this paper, we propose the use of a novel approach that allows to automatically fine-tune all the parameters required by the algorithm. To the best of the authors’ knowledge, it is the first time that an automatic fine-tuning technique is used in the context of a LAD algorithm.

LAD requires the dataset to be in binary format. Therefore, the first step transforms any non-binary value into a set of binary attributes. Such binarization process is carried out introducing a set of cutpoints for each non-binary attribute, as illustrated in Boros et al. (1997, 2000). Let us consider a non-binary attribute aj
                      and a set of cutpoints cjk
                     , with 
                        
                           k
                           =
                           1
                           ,
                           …
                           ,
                           
                              n
                              j
                           
                        
                     . We binarize attribute aj
                      introducing a set of nj
                      binary values ajk
                      whose value is 1 if aj
                      ≥ cjk
                      and 0 otherwise. It is worth observing, though, that the number of binary values needed to convert a numerical variable into a set of binary values can be quite large. Consequently, the next step of LAD is devoted to finding a minimum size support set that allows to distinguish any two observations belonging to different classes, i.e., a set of cutpoints that is sufficient to preserve the information contained in the original non-binary variables. This phase, the features selection phase, is modeled using a Set Covering problem, which belongs to the class of 
                        NP
                     -hard problems and, therefore, gives rise to the first difficulty in the LAD process. Finding the minimum size support set might prove to be too difficult and, therefore, the associated set covering problem is often solved heuristically (Boros et al., 2000).

Once a minimum size support set is identified, the next step concerns the creation of patterns, i.e., subcubes having a nonempty intersection with one of the two subsets of the data, and an empty intersection with the other subset. Once a pool of positive and negative patterns has been produced, the last step of LAD is concerned with the creation of a theory. The key assumption in this step is that an observation covered by some positive patterns but none (or just a few) of the negative patterns should be classified as positive. Thus, we finally build a discriminant function to compute a weighted score for each new observation. The value of such a score determines how this new observation is classified.

In the LAD literature, special attention has been given to the pattern generation phase. It is well understood that the prevalence, or absence, of patterns of one class provide strong indications about the nature of new, unclassified, observations (Alexe, Alexe, & Hammer, 2006; Alexe & Hammer, 2006; Boros et al., 2000; Hammer et al., 2004). Consequently, special emphasis has been placed on creating patterns that are “good predictors” of the class each new observation belongs to. Broadly speaking, two main approaches for pattern generations emerge from the literature. On the one hand, the traditional approach is enumeration-based or constructive in nature, e.g., bottom-up and top-down approaches (Boros et al., 1997; Boros et al., 2000; Hammer et al., 2004). These types of approaches are computationally quite expensive and pose some limitations in terms of the degree, i.e., the length, of the patterns produced. A second approach for pattern generation relies on mathematical modeling. In their study, Hammer et al. (2004) identify a set of preferences (simplicity, selectivity, and evidence along with their combinations) to introduce the concept of Pareto-optimal patterns with respect to one or more preferences. In Bonates, Hammer, and Kogan (2008), a mixed integer formulation (MIP) for the construction of patterns that satisfy a specific preference was proposed. They proposed an exact algorithm, along with some heuristics, for the generation of maximum patterns, i.e., patterns that cover the maximum number of observations in a class.

In this paper, we propose a third approach to the pattern generation problem. Metaheuristics are intelligent techniques aiming at driving one or more heuristic rules, while adapting their behavior through a set of learning mechanisms. In recent years, metaheuristics have been quite successful in solving combinatorial optimization problems, both from the perspective of solution quality as well as computational time. While the use of metaheuristics for solving combinatorial optimization problems implies that the guarantee of optimality of the final solution must be relinquished, in practice a well-designed metaheuristic can achieve near-optimal solutions in very short computational time. In line with this observation, we present a metaheuristic algorithm for the generation of patterns that are near-optimal with respect to a prespecified preference. More precisely, we will show that the proposed scheme produces near-maximum α patterns, where a maximum α pattern is, among the patterns covering a binary observation α, the one that covers the maximum number of observation of the same class of α, as defined in Chikalov et al. (2013) and Bonates et al. (2008).

The key observation underlying the development of the algorithm presented in this paper is that, as mentioned in Bonates et al. (2008), empirical evidence supports the conclusion that patterns with higher coverage provide a stronger indication about the nature of new observations than those with lower coverage. Consequently, owing to our interest in classification methods, and especially considering the goal of developing a classification algorithm for very large scale datasets, we see fit the development of a metaheuristic for the creation of maximum patterns. It might seem that the major drawback of using a metaheuristic, rather than an exact approach, for pattern generation is related to the lack of guarantee of optimality of the achieved solution. However, as a number of authors pointed out, e.g., Bonates et al. (2008) and Dietterich (1995), in machine learning in general and in LAD in particular, the use of exacts patterns of maximum coverage does not exhibit superior classification performance compared to classification models built using near-optimal patterns. Consequently, the use of a metaheuristic for pattern generation offers the advantage of requiring shorter computational time than an enumerative technique or even an MIP-based approach while, on the other hand, does not necessarily pay a price in terms of accuracy of the final classification model.

The paper is organized as follows: In the next section, we will present some notation and basic definitions. In Section 3 we present the metaheuristic scheme for pattern generation and the strategy used to manage the pool of patterns, while Section 4 presents the overall algorithm; Section 5 describes an in-depth empirical analysis of the performance of the algorithm and, finally, Section 6 concludes with some remarks.

In this section, we will present the basic notation and some definitions in relation with the theory of partially defined Boolean functions (pdBf), borrowing ideas from Chikalov et al. (2013). These concepts are reported here to make the paper self-contained. However, for a detailed description of the concepts and terminology hereby introduced, we direct the interested reader to Chikalov et al. (2013) and Hammer et al. (2004).

Let us define 
                        
                           B
                           n
                        
                      as the Boolean hypercube of dimension n, i.e., the set of all the binary vectors of length n. Thus, any binary vector of length n can be seen as a point of the hypercube 
                        
                           B
                           n
                        
                     . A subcube of 
                        
                           B
                           n
                        
                      is a subset 
                        
                           S
                           ⊆
                           
                              B
                              n
                           
                        
                      for which (i) 
                        
                           
                              |
                              S
                              |
                           
                           =
                           
                              2
                              k
                           
                        
                      for some k ≤ n and (ii) all the points in S have 
                        
                           n
                           −
                           k
                        
                      components with identical value. The dimension of the subcube S is k and is determined by the number of free elements in a space of size n.

A boolean function of n variables is a mapping 
                        
                           f
                           :
                           
                              B
                              n
                           
                           →
                           B
                           ,
                        
                      
                     i.e., a function that defines a 0 or 1 value for each point of the hypercube. We can now call, e.g., a positive point a point 
                        
                           x
                           ∈
                           
                              B
                              n
                           
                        
                      for which 
                        
                           f
                           (
                           x
                           )
                           =
                           1
                           ,
                        
                      while a negative point is a point 
                        
                           x
                           ∈
                           
                              B
                              n
                           
                        
                      for which 
                        
                           f
                           (
                           x
                           )
                           =
                           0
                        
                     .

Considering a binary vector 
                        
                           α
                           =
                           
                              (
                              
                                 α
                                 1
                              
                              ,
                              …
                              ,
                              
                                 α
                                 n
                              
                              )
                           
                           ,
                        
                      a literal li
                      is defined as the component αi
                      of the vector, or its negation 
                        
                           
                              
                                 α
                                 ¯
                              
                              i
                           
                           =
                           1
                           −
                           
                              α
                              i
                           
                        
                     . A term t is a product of literals and the degree of a term is equal to the number of literals. Each term can be viewed as a Boolean function mapping binary vectors to {0, 1}. For example, let us consider the term 
                        
                           t
                           =
                           
                              
                                 x
                                 ¯
                              
                              1
                           
                           
                              x
                              3
                           
                        
                      and the points 
                        
                           
                              α
                              1
                           
                           =
                           
                              (
                              0
                              ,
                              1
                              ,
                              1
                              )
                           
                        
                      and 
                        
                           
                              α
                              2
                           
                           =
                           
                              (
                              0
                              ,
                              1
                              ,
                              0
                              )
                           
                        
                     . We can see that 
                        
                           t
                           (
                           
                              α
                              1
                           
                           )
                           =
                           1
                        
                      and 
                        
                           t
                           (
                           
                              α
                              2
                           
                           )
                           =
                           0
                        
                     . It is worth noting that the value of t(α) is defined for every point in 
                        
                           
                              B
                              n
                           
                           ,
                        
                      since we simply ignore the value of the variables not included in the term. Thus, a subcube 
                        
                           S
                           ⊂
                           
                              B
                              n
                           
                        
                      can be seen as the result of the application of a Boolean function defined by a term t to the points in 
                        
                           B
                           n
                        
                     . For example, considering 
                        
                           
                              B
                              3
                           
                           ,
                        
                      and applying the Boolean function defined by the term 
                        
                           t
                           =
                           
                              
                                 x
                                 ¯
                              
                              1
                           
                           ,
                        
                      we obtain the subcube 
                        
                           S
                           =
                           
                              {
                              
                                 (
                                 0
                                 ,
                                 0
                                 ,
                                 0
                                 )
                              
                              ,
                              
                                 (
                                 0
                                 ,
                                 0
                                 ,
                                 1
                                 )
                              
                              ,
                              
                                 (
                                 0
                                 ,
                                 1
                                 ,
                                 0
                                 )
                              
                              ,
                              
                                 (
                                 0
                                 ,
                                 1
                                 ,
                                 1
                                 )
                              
                              }
                           
                        
                     .

Let us now consider a dataset 
                        
                           Ω
                           ⊂
                           
                              B
                              n
                           
                        
                      partitioned in two subsets 
                        
                           Ω
                           +
                        
                      and 
                        
                           
                              Ω
                              −
                           
                           ,
                        
                      
                     e.g., the sets of positive and negative observations, and composed of binary vectors of length n. In the initial development of the method, we assume that 
                        
                           
                              Ω
                              +
                           
                           ∩
                           
                              Ω
                              −
                           
                           =
                           ∅
                        
                     . However, acknowledging that such an assumption may not hold in some datasets, we will relax it when dealing with fuzzy patterns. We also assume that the dataset Ω does not cover the entire set 
                        
                           
                              B
                              n
                           
                           ,
                        
                      
                     i.e., 
                        
                           
                              Ω
                              +
                           
                           ∪
                           
                              Ω
                              −
                           
                           ⊂
                           
                              B
                              n
                           
                        
                     . A partially defined Boolean function (pdBf) assigns value 1 to every point in 
                        
                           Ω
                           +
                        
                      and 0 to every point in 
                        
                           Ω
                           −
                        
                     . The goal of LAD is to extend a pdBf to a complete one, i.e., a Boolean function that properly maps every point in Ω and, in addition, assigns a value of 0 or 1 to all the other points in 
                        
                           
                              B
                              n
                           
                           ∖
                           Ω
                        
                     . Of course, there are a number of ways in which a pdBf can be extended. LAD attempts to find the “right” way of extending such function. Since it is impossible to know which is the right extension, we implicitly assume that the data in Ω are not just random values but they describe a rational phenomenon. The explanation for this phenomenon can be inferred from the partial explanation provided by Ω. LAD attempts to capture such explanation hidden in the dataset Ω by creating patterns and by forming a theory using those patterns (Chikalov et al., 2013).

A positive pattern is a term t that covers at least a positive point and none of the negative. In other words, a positive pattern defines a subcube of 
                        
                           B
                           n
                        
                      that intersects with 
                        
                           Ω
                           +
                        
                      but is disjoint from 
                        
                           Ω
                           −
                        
                     . A symmetric definition applies to negative patterns.

Once a set of positive and negative patterns is built from Ω, we define a theory, or a LAD model, i.e., an extension of a pdBf. Typically, a subset of the patterns obtained is used to define a discriminant function and such function, in turn, is employed to map any vector in 
                        
                           B
                           n
                        
                      into {0, 1}.

Given a dataset Ω, the number of patterns that can be generated is extremely large and, therefore, the pattern generation process can become computationally expensive. Therefore, we attempt to generate patterns that are more “suitable” for the task at hand, i.e., the definition of an extension of a pdBf for classification. In Hammer et al. (2004), three criteria, i.e., simplicity, selectivity, and preference, have been presented, along with heuristics for generating Pareto-optimal patterns with respect to any of those criteria.

In this paper, we are concerned with a forth criterion, that of maximum α-patterns, as defined in Bonates et al. (2008). Given an observation 
                        
                           α
                           ∈
                           
                              Ω
                              +
                           
                           ,
                        
                      a maximum α-pattern is, among the patterns that cover α, the one that covers the maximum number of observations in 
                        
                           
                              Ω
                              +
                           
                           ,
                        
                      
                     i.e., a pattern t for which its coverage (
                        
                           
                              |
                              t
                              ∩
                           
                           
                              Ω
                              +
                           
                           
                              |
                           
                        
                     ) is maximal. A similar definition applies to maximum patterns for negative observations. It is worth noting that finding patterns with maximal coverage corresponds to, borrowing from the language of machine learning literature, the minimization of 0/1 loss for a single rule/pattern. In Bonates et al. (2008), an integer formulation aimed at finding a maximum α-pattern has been proposed. To make the paper self-contained, we briefly present here the IP formulation proposed in Bonates et al. (2008). Let us consider an observation 
                        
                           α
                           =
                           
                              (
                              
                                 α
                                 1
                              
                              ,
                              …
                              ,
                              
                                 α
                                 j
                              
                              ,
                              …
                              ,
                              
                                 α
                                 n
                              
                              )
                           
                           ∈
                           
                              Ω
                              +
                           
                        
                     . A set of n decision variables yj
                      is used to identify whether literal lj
                      should be included in the pattern (
                        
                           
                              y
                              j
                           
                           =
                           1
                        
                     ) or not (
                        
                           
                              y
                              j
                           
                           =
                           0
                        
                     ). If literal lj
                      is included in the pattern, the value of the corresponding variable is fixed to αj
                     . Therefore, any non-trivial selection of values of the yj
                      variables necessarily defines a pattern that covers observation α. The goal of the formulation is to maximize the number of positive points covered by the pattern defined as the conjunction of literals lj
                      for which the corresponding variable yj
                      is 1. The constraints of the model imposes that none of the negative points is covered by the pattern. The corresponding IP formulation is:

                        
                           (1)
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                (
                                                
                                                   IP
                                                   α
                                                
                                                )
                                             
                                             :
                                             
                                             
                                             max
                                             z
                                             =
                                             
                                                ∑
                                                
                                                   β
                                                   ∈
                                                   
                                                      Ω
                                                      +
                                                   
                                                
                                             
                                             
                                                ∏
                                                
                                                   
                                                      
                                                         
                                                            
                                                               j
                                                               =
                                                               1
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  β
                                                                  j
                                                               
                                                               ≠
                                                               
                                                                  α
                                                                  j
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                                n
                                             
                                             
                                                
                                                   y
                                                   ¯
                                                
                                                j
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (2)
                           
                              
                                 
                                    
                                       
                                          
                                             s.t.
                                             
                                             
                                                ∑
                                                
                                                   
                                                      
                                                         
                                                            
                                                               j
                                                               =
                                                               1
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  γ
                                                                  j
                                                               
                                                               ≠
                                                               
                                                                  α
                                                                  j
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                                n
                                             
                                             
                                                y
                                                j
                                             
                                             ≥
                                             1
                                             ,
                                             
                                             γ
                                             ∈
                                             
                                                Ω
                                                −
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (3)
                           
                              
                                 
                                    
                                       
                                          
                                             y
                                             j
                                          
                                          ∈
                                          
                                             {
                                             0
                                             ,
                                             1
                                             }
                                          
                                          ,
                                          
                                          j
                                          =
                                          1
                                          ,
                                          …
                                          ,
                                          n
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           
                              
                                 y
                                 ¯
                              
                              j
                           
                           =
                           1
                           −
                           
                              y
                              j
                           
                        
                     . It is worth noting that, in order for a positive observation 
                        
                           β
                           ∈
                           
                              Ω
                              +
                           
                        
                      to be covered by a positive pattern, it is required that none of the literals lj
                      for which α and β differ, i.e., such that αj
                      ≠ βj
                     , is selected. In other words, the variable yj
                      should be equal to zero for every literal lj
                      where α and β have different values. Thus, in the objective function, the product takes value one only if a positive point is covered by the pattern, and the sum accounts for the total number of positive points covered by such pattern. The objective function thus states that we want to maximize the number of positive points covered by the pattern. In a similar fashion, for a negative point 
                        
                           γ
                           ∈
                           
                              Ω
                              −
                           
                        
                      not to be covered by a pattern, it suffices to ensure that at least one of the literals lj
                      for which α and γ differ is selected. Constraints (2) ensure that none of the negative points is covered by the positive pattern.

As illustrated in Bonates et al. (2008), the IP program (1)–(3) can be converted into a linear integer program. However, owing to the fact that the program is a generalized set covering problem and, consequently, belongs to the class of 
                        NP
                     -hard problems, the program is still not amenable to an optimal solution in polynomial time and, consequently, might become unpractical when dealing with large datasets. For this reason, the same authors proposed a number of heuristics to solve problem (1)–(3) in short computational time.

In the next section, we will introduce a metaheuristic scheme aimed at generating a population of near-maximal α-patterns. We will first illustrate how to solve problem (1)–(3) for a specific observation α in a metaheuristic fashion and, next, we will introduce the overall scheme used to produce a “suitable” set of near-maximal α-patterns.

Let us now present the details of the Cross Entropy (CE) scheme used to generate maximum α-patterns. (See Rubinstein and Kroese (2004) and De Boer, Kroese, Mannor, and Rubinstein (2005) for a tutorial and a comprehensive overview of the CE, as well as Caserta and Quiñonez (2009) for a detailed presentation of how CE can be adapted to solve combinatorial optimization problems.) The proposed CE can be seen as a population-based stochastic approach for pattern generation. The key idea lies on the identification of structural characteristics and prominent traits in the dataset and on the design of a stochastic mechanisms that creates patterns reflecting those prominent traits with a probability proportional to the level of prominence of the traits themselves. For example, let us imagine that, e.g., p of the observations in 
                        
                           Ω
                           +
                        
                      have a value of 1 for attribute aj
                     . The CE scheme will generate patterns that contain the associated literal lj
                      with probability 
                        
                           
                              p
                              /
                              |
                           
                           
                              Ω
                              +
                           
                           
                              |
                           
                        
                     . Interestingly, though, while the stochastic scheme attaches a high probability to the inclusion of literal lj
                      in a positive pattern, due to its probabilistic nature the scheme still leaves some room to the generation of patterns that do not include such prevalent trait and, apparently, might seem less “suitable” to describe subset 
                        
                           Ω
                           +
                        
                      and to define an extension of the pdBf. However, as we will show in the computational section, this calibrated mix of “suitable” and less “suitable” patterns, which attempts to strike the balance between intensification and diversification, seems to be beneficial in addressing one of the major concerns in data mining. As pointed out in Bonates et al. (2008) as well as in Dietterich (1995), building optimal patterns with respect to a specific criterion, e.g., maximal patterns, does not necessarily guarantee superior classification performance, perhaps due to overfitting. LAD models that make use of a well-designed pool of near-optimal patterns might actually have superior performance. We will provide some empirical evidence to substantiate this argument in the computational section.

Let us now consider the CE scheme we propose to solve problem (1)–(3). In the sequel, we present the pattern generation scheme for positive patterns. The scheme proposed for the creation of negative patterns can easily be inferred by reversing the two subsets. Let us assume we are given a binary vector corresponding to an observation 
                           
                              α
                              =
                              
                                 (
                                 
                                    α
                                    1
                                 
                                 ,
                                 …
                                 ,
                                 
                                    α
                                    j
                                 
                                 ,
                                 …
                                 ,
                                 
                                    α
                                    n
                                 
                                 )
                              
                              ∈
                              
                                 Ω
                                 +
                              
                           
                        . Let us compute a measure of the prevalence of each literal lj
                         in 
                           
                              Ω
                              +
                           
                         and let us indicate such measure with 
                           
                              p
                              j
                              
                                 α
                                 j
                              
                           
                        . In other words, 
                           
                              p
                              j
                              
                                 α
                                 j
                              
                           
                         is a score that measures the “goodness” of selecting literal lj
                         with a value equal to αj
                         in the construction of an α-pattern. Since we want to build a pattern that covers observation α, for every attribute 
                           
                              j
                              ∈
                              A
                              =
                              
                                 {
                                 1
                                 ,
                                 …
                                 ,
                                 n
                                 }
                              
                           
                         we define:

                           
                              (4)
                              
                                 
                                    
                                       l
                                       j
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   
                                                      a
                                                      j
                                                   
                                                   ,
                                                
                                             
                                             
                                                
                                                   if
                                                   
                                                   
                                                      α
                                                      j
                                                   
                                                   =
                                                   1
                                                   ,
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         a
                                                         ¯
                                                      
                                                      j
                                                   
                                                   ,
                                                
                                             
                                             
                                                
                                                   if
                                                   
                                                   
                                                      α
                                                      j
                                                   
                                                   =
                                                   0
                                                   .
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The construction of an α-pattern, i.e., a pattern that covers observation α, can be seen as a sequence of Bernoulli experiments. The decision variable yj
                        , which takes value 1 if literal lj
                         is included in the pattern and 0 if it is not included, can thus be seen as a random variable drawn under the following Bernoulli distribution (In the sequel, for the sake of readability we omit the superscript αj
                         from the parameter 
                           
                              p
                              j
                              
                                 α
                                 j
                              
                           
                        . However, it is worth noting that, whenever we write pj
                         we indicate the probability of success in selecting literal lj
                         following the rules specified in Eq. (4).):

                           
                              (5)
                              
                                 
                                    f
                                    
                                       (
                                       
                                       
                                          y
                                          j
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          (
                                          
                                             p
                                             j
                                          
                                          )
                                       
                                       
                                          y
                                          j
                                       
                                    
                                    
                                       
                                          (
                                          1
                                          −
                                          
                                             p
                                             j
                                          
                                          )
                                       
                                       
                                          (
                                          1
                                          −
                                          
                                             y
                                             j
                                          
                                          )
                                       
                                    
                                    ,
                                    
                                    
                                       y
                                       j
                                    
                                    =
                                    0
                                    ,
                                    1
                                 
                              
                           
                        
                     

To build a term, we run a series of n Bernoulli experiment, where the value of each random variable yj
                         is drawn under the pdf (5). Finally, the term obtained is:

                           
                              (6)
                              
                                 
                                    
                                       
                                          T
                                          α
                                       
                                       =
                                       
                                          ⋀
                                          
                                             
                                                
                                                   
                                                      j
                                                      ∈
                                                      A
                                                      :
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         y
                                                         j
                                                      
                                                      =
                                                      1
                                                   
                                                
                                             
                                          
                                       
                                       
                                          l
                                          j
                                       
                                    
                                 
                              
                           
                        It is worth remembering that the difference between term and pattern is that, while a term is simply a Boolean function that defines a subcube of 
                           
                              
                                 B
                                 n
                              
                              ,
                           
                         a positive pattern requires that such induced subcube intersects 
                           
                              Ω
                              +
                           
                         but is disjoint from 
                           
                              Ω
                              −
                           
                        . The aforementioned set of rules guarantees that the term built under the probability distribution function (5) will cover observation α. Consequently, we know that the intersection of the subcube induced by 
                           
                              T
                              α
                           
                         will be nonempty. However, we cannot guarantee that such subcube is disjoint from 
                           
                              Ω
                              −
                           
                         and, therefore, the term generated using Eq. (5)
might be infeasible with respect to problem (1)–(3), i.e., might not be a pattern.

The concept of patterns presented above relies on the assumption that the information provided by the dataset Ω is correct, i.e., no measurement errors are present, both in the attribute values as well as in the classification values. However, this assumption is violated in many real-life situations. Consequently, we could relax the definition of patterns as done in, e.g., Bonates et al. (2008). The homogeneity of a positive pattern is the percentage of positive observations covered over the total number of observations covered, i.e., positive as well as negative. Thus, a pattern is “pure” or “homogeneous” if its homogeneity is equal to one. A non-homogeneous positive pattern, also called “fuzzy” pattern, has a fuzziness ϕ if the percentage of negative observations covered by it does not exceed ϕ. A positive fuzzy pattern should be such that the percentage of observations covered in 
                           
                              Ω
                              +
                           
                         is larger than the percentage of observations covered in 
                           
                              Ω
                              −
                           
                        . To construct fuzzy patterns, Bonates et al. (2008) rewrite problem (IP
                           α
                        ) as follows:

                           
                              (7)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   (
                                                   
                                                      IP
                                                      α
                                                      ϕ
                                                   
                                                   )
                                                
                                                :
                                                
                                                
                                                max
                                                z
                                                =
                                                
                                                   ∑
                                                   
                                                      β
                                                      ∈
                                                      
                                                         Ω
                                                         +
                                                      
                                                   
                                                
                                                
                                                   ∏
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  j
                                                                  =
                                                                  1
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  
                                                                     β
                                                                     j
                                                                  
                                                                  ≠
                                                                  
                                                                     α
                                                                     j
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   n
                                                
                                                
                                                   
                                                      y
                                                      ¯
                                                   
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  j
                                                                  =
                                                                  1
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  
                                                                     γ
                                                                     j
                                                                  
                                                                  ≠
                                                                  
                                                                     α
                                                                     j
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   n
                                                
                                                
                                                   y
                                                   j
                                                
                                                ≥
                                                1
                                                −
                                                
                                                   s
                                                   γ
                                                
                                                ,
                                                
                                                j
                                                ∈
                                                
                                                   Ω
                                                   −
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (9)
                              
                                 
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   γ
                                                   ∈
                                                   
                                                      Ω
                                                      −
                                                   
                                                
                                             
                                             
                                                s
                                                γ
                                             
                                             ≤
                                             ϕ
                                             
                                                |
                                                
                                                   Ω
                                                   −
                                                
                                                |
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (10)
                              
                                 
                                    
                                       
                                          
                                             
                                                s
                                                γ
                                             
                                             ∈
                                             
                                                {
                                                0
                                                ,
                                                1
                                                }
                                             
                                             ,
                                             
                                             γ
                                             ∈
                                             
                                                Ω
                                                −
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (11)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                y
                                                j
                                             
                                             ∈
                                             
                                                {
                                                0
                                                ,
                                                1
                                                }
                                             
                                             ,
                                             
                                             j
                                             =
                                             1
                                             ,
                                             …
                                             ,
                                             n
                                          
                                       
                                    
                                 
                              
                           
                        where setting 
                           
                              ϕ
                              =
                              0
                           
                         leads back to the pure patterns problem IP
                           α
                        .

Algorithm generate_fuzzy_pattern() describes how the fuzzy patterns are produced. Let us define a predetermined maximum fuzziness level ϕ
                        * and let us indicate with 
                           
                              P
                              +
                           
                         the set of positive patterns built by CE. In addition, let 
                           
                              
                                 σ
                                 +
                              
                              
                                 (
                                 T
                                 )
                              
                           
                         and 
                           
                              
                                 σ
                                 −
                              
                              
                                 (
                                 T
                                 )
                              
                           
                         be the percentage of points in 
                           
                              Ω
                              +
                           
                         and 
                           
                              Ω
                              −
                           
                         covered by term 
                           
                              T
                              ,
                           
                         respectively.

To exploit the stochastic nature of the CE, we attempt to generate a population of N terms. It is worth noting that Algorithm 1
                         does not guarantee that a pattern with fuzziness ϕ
                        * covering observation α will be found. Thus, whenever the procedure generate_fuzzy_pattern() fails, the actual size of the population will be smaller than N. Let us assume, though, that we do have a population of binary vectors 
                           
                              
                                 y
                                 1
                              
                              ,
                              
                                 y
                                 2
                              
                              ,
                              …
                           
                         used to generate fuzzy patterns 
                           
                              
                                 T
                                 1
                              
                              ,
                              
                                 T
                                 2
                              
                              ,
                              …
                           
                         via Eq. (6) and, in addition, that we can compute a fitness measure 
                           
                              f
                              (
                              y
                              )
                              =
                              f
                              (
                              T
                              )
                           
                         for each of those patterns. Once the first population has been drawn, we use the “Maximum Likelihood Estimator” method to review the probabilities 
                           
                              p
                              j
                              
                                 α
                                 j
                              
                           
                         in such a way that the probability of selecting literals that turned out to be “suitable” in the first generation will increase. In other words, we want to increase the probability of reusing literals that appeared in patterns with high fitness value and, conversely, decrease the probability of reusing literals that did not appear in high quality patterns. This learning mechanism allows to obtain a set of revised pdf for the next Bernoulli experiment and, consequently, when we apply Eq. (5) to obtain a new generation of patterns, the chance of obtaining high quality patterns based upon the new pdfs is higher.

This process of “pdf update” and “population generation” can be iterated until some stopping criteria are reached, i.e., either the parameters pj
                         of the pdf converge to a binary vector (and, therefore, the stochastic process converges to a unique solution) or a prespecified maximum number of iterations has been reached.

The “Maximum Likelihood Estimator” method is the key of the learning mechanism that modifies the values of the parameters of the Bernoulli distribution to better reflects “suitable” traits in the dataset. If a trait hidden in 
                           
                              Ω
                              +
                           
                         has been randomly selected and belongs to a pattern with high fitness value, the selection of such trait in future generations is fostered by increasing the value of the corresponding parameter of the associated Bernoulli distribution. Let us assume that, based upon the current pdfs, we have generated a population of size N, i.e., vectors 
                           
                              
                                 y
                                 1
                              
                              ,
                              
                                 y
                                 2
                              
                              ,
                              …
                              ,
                              
                                 y
                                 N
                              
                              ,
                           
                         corresponding to fuzzy patterns 
                           
                              
                                 T
                                 1
                              
                              ,
                              
                                 T
                                 2
                              
                              ,
                              …
                              ,
                              
                                 T
                                 N
                              
                           
                        . Let us now find, within the current population, the fitness value of the 
                           
                              (
                              1
                              −
                              ρ
                              )
                           
                         percent, the value γ for which ρ percent of the population has a better fitness value and 
                           
                              (
                              1
                              −
                              ρ
                              )
                           
                         percent has a worse fitness value. We modify the parameters of the pdfs using the following updating rule (In the sequel, subscript α is omitted. However, all the parameters and values below refer to an α pattern, i.e., a specific pattern that covers observation α.):

                           
                              (12)
                              
                                 
                                    
                                       
                                          p
                                          ^
                                       
                                       j
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                w
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             y
                                             j
                                             w
                                          
                                          ×
                                          
                                             I
                                             
                                                {
                                                f
                                                (
                                                
                                                   y
                                                   w
                                                
                                                )
                                                ≥
                                                γ
                                                }
                                             
                                          
                                       
                                       
                                          ρ
                                          N
                                       
                                    
                                 
                              
                           
                        where 
                           
                              y
                              j
                              w
                           
                         takes value 1 if literal lj
                         is included in pattern w and I
                        {•} is the indicator function, which takes value 1 if condition • is true and 0 otherwise. The rationale behind Eq. (12) is that, whenever a literal, or a combination of literals, appear in a high quality pattern (whether literal lj
                         is taken as aj
                         or its negation 
                           
                              
                                 a
                                 ¯
                              
                              j
                           
                         depends on the value of αj
                        , as given by Eq. (4)), this characteristic of the dataset 
                           
                              Ω
                              +
                           
                         should be taken into account in the next generation of patterns. We capture such trait by readjusting the probability of selection of those literals via Eq. (12).

                           Remark
                           As pointed out by De Boer et al. (2005), in order to prevent the CE from converging too fast to a suboptimal solution, a smoothing factor δ (typically 0.1 ≤ δ ≤ 0.3) could be used in the updating rule. Therefore, to foster a more thorough exploration of the solution space, at each iteration t we use the following updating rule:

                                 
                                    (13)
                                    
                                       
                                          
                                             p
                                             j
                                             
                                                t
                                                +
                                                1
                                             
                                          
                                          =
                                          δ
                                          
                                             
                                                p
                                                ^
                                             
                                             j
                                          
                                          +
                                          
                                             (
                                             1
                                             −
                                             δ
                                             )
                                          
                                          
                                             p
                                             j
                                             t
                                          
                                          .
                                       
                                    
                                 
                              
                           

At each iteration of the CE scheme, we add some of the patterns in the top ρ percent quantile (at most ρN fuzzy α-patterns) to 
                                 
                                    
                                       P
                                       +
                                    
                                    ,
                                 
                               
                              i.e., the set of positive patterns, provided that the added patterns were not already in 
                                 
                                    P
                                    +
                                 
                              . At the end of the CE cycle for an observation α we will have added a near-maximal α-pattern, i.e., the best pattern obtained by CE in terms of coverage, along with a pool of high quality solutions, where the quality is measured according to a set of prespecified criteria, e.g., coverage, diversity, etc. More details about the criteria used for the management of the pool of patterns are provided in Section 3.2.

The fitness function we employ to measure the quality of a pattern can simply be the objective function (1) that accounts for the coverage of the pattern itself. However, we can also define different fitness functions to drive the CE, which might refer to a number of partial preorders defined on a set of patterns, e.g., those presented in Hammer et al. (2004), such as simplicity, selectivity, and evidential preference. One could thus attempt to create prime, strong or spanned patterns, or patterns that are optimal with respect to a combination of preferences by just designing different fitness scores. (For a detailed description of Pareto-optimal patterns in LAD, we direct the interested reader to Chikalov et al. (2013).)

Let us now present the overall CE scheme to generate a pool of positive fuzzy α-patterns with fuzziness ϕ
                        *. We sketch the proposed pattern generation algorithm below. The maximum α-pattern generation scheme is repeated until one of the following termination criterion is reached:

                           
                              •
                              a maximum number of CE iterations has been performed;

the probability vector converged to a binary vector (therefore, the CE scheme converged to a unique solution);

a maximum running time has been reached.

In Algorithm 2
                        , line 6 refers to a local search scheme applied to a selected subset of patterns. The details of such scheme are presented in Section 3.3. On the other hand, line 7 refers to the α-patterns pool management strategy, as presented in Section 3.2.

It is worth noting that, owing to the constraint imposed by the maximum fuzziness level, Algorithm 2 does not guarantee that a pool of patterns covering point α is going to be produced. If this is the case, the maximum fuzziness level is relaxed and the overall generation scheme is repeated, as presented in Section 4.

The pattern generation scheme presented in Section 3 is a population-based approach. Thus, the scheme can be used to collect a set of α-patterns with desirable characteristics, as opposed to simply collecting the best possible solution to problem (1), (8)–(11). We first define a maximum number of α-patterns to be included in the pool. The typical size of the α-patterns pool is, e.g., equal to 10. At each iteration of the CE scheme, a new fuzzy α-pattern 
                           
                              T
                              α
                              k
                           
                         is included in the pool of pattern 
                           
                              
                                 P
                                 α
                                 +
                              
                              ,
                           
                         
                        i.e., the pool of ‘elite’ α-patterns, if at least one of the following criteria is satisfied:

                           
                              (a)
                              the fitness value of the pattern, 
                                    
                                       f
                                       (
                                       
                                          T
                                          α
                                          k
                                       
                                       )
                                       ,
                                    
                                  is better than the value of the best α-pattern found so far; or

the fitness value of the pattern, 
                                    
                                       f
                                       (
                                       
                                          T
                                          α
                                          k
                                       
                                       )
                                       ,
                                    
                                  is better than the value of the worst α-pattern included so far in 
                                    
                                       P
                                       α
                                       +
                                    
                                  and the pattern is sufficiently “diverse” with respect to all the patterns currently in 
                                    
                                       P
                                       α
                                       +
                                    
                                 . Given two patterns 
                                    
                                       T
                                       ′
                                    
                                  and 
                                    
                                       
                                          T
                                          
                                             ′
                                             ′
                                          
                                       
                                       ,
                                    
                                  the diversity measure employed is the edit distance between the two patterns. We compute the edit distance of two patterns as in Sörensen (2007):

                                    
                                       (14)
                                       
                                          
                                             
                                                d
                                                E
                                             
                                             
                                                (
                                                
                                                   T
                                                   ′
                                                
                                                ,
                                                
                                                   T
                                                   
                                                      ′
                                                      ′
                                                   
                                                
                                                )
                                             
                                             =
                                             min
                                             
                                                {
                                                γ
                                                (
                                                E
                                                )
                                                }
                                             
                                             ,
                                          
                                       
                                    
                                 where 
                                    E
                                  is the edit transformation of 
                                    
                                       T
                                       ′
                                    
                                  into 
                                    
                                       
                                          T
                                          
                                             ′
                                             ′
                                          
                                       
                                       ,
                                    
                                  
                                 i.e., then set of edit operations (insertion, deletion, and replacement) required to transform 
                                    
                                       T
                                       ′
                                    
                                  into 
                                    
                                       T
                                       
                                          ′
                                          ′
                                       
                                    
                                 . A new pattern 
                                    
                                       T
                                       α
                                       ′
                                    
                                  is sufficiently diverse with respect to the current pool of patterns if the edit distance of the pool is improving after including such solution into 
                                    
                                       
                                          P
                                          α
                                          +
                                       
                                       ,
                                    
                                  where the edit distance of the pool 
                                    
                                       P
                                       α
                                       +
                                    
                                  is:

                                    
                                       (15)
                                       
                                          
                                             
                                                E
                                                
                                                   (
                                                   
                                                      P
                                                      α
                                                      +
                                                   
                                                   )
                                                
                                                =
                                                
                                                   ∑
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   
                                                      
                                                         |
                                                      
                                                      
                                                         P
                                                         α
                                                         +
                                                      
                                                      
                                                         |
                                                         −
                                                         1
                                                      
                                                   
                                                
                                                
                                                   ∑
                                                   
                                                      l
                                                      =
                                                      k
                                                      +
                                                      1
                                                   
                                                   
                                                      
                                                         |
                                                      
                                                      
                                                         P
                                                         α
                                                         +
                                                      
                                                      
                                                         |
                                                      
                                                   
                                                
                                                
                                                   d
                                                   E
                                                
                                                
                                                   (
                                                   
                                                      T
                                                      α
                                                      k
                                                   
                                                   ,
                                                   
                                                      T
                                                      α
                                                      l
                                                   
                                                   )
                                                
                                                ,
                                             
                                          
                                       
                                    
                                 Consequently, a new pattern 
                                    
                                       T
                                       α
                                       ′
                                    
                                  is added into the pool if there exists at least one pattern 
                                    
                                       
                                          T
                                          α
                                          i
                                       
                                       ∈
                                       
                                          P
                                          α
                                          +
                                       
                                    
                                  such that 
                                    
                                       E
                                       
                                          (
                                          
                                             P
                                             α
                                             +
                                          
                                          ∪
                                          
                                             {
                                             
                                                T
                                                α
                                                ′
                                             
                                             }
                                          
                                          ∖
                                          
                                             {
                                             
                                                T
                                                α
                                                i
                                             
                                             }
                                          
                                          )
                                       
                                       >
                                       E
                                       
                                          (
                                          
                                             P
                                             α
                                             +
                                          
                                          )
                                       
                                    
                                 . In this case, we delete from the pool the pattern 
                                    
                                       T
                                       α
                                       i
                                    
                                  for which the increase in 
                                    
                                       E
                                       (
                                       
                                          P
                                          α
                                          +
                                       
                                       )
                                    
                                  is maximal and, in turn, we add the new pattern 
                                    
                                       T
                                       α
                                       ′
                                    
                                  to the pool.

In an attempt to further improve the quality of the patterns produced by CE, we designed a simple local search scheme. At the end of each CE iteration, i.e., the end of the cycle of lines 3–5 of Algorithm 2, we apply a 1-opt scheme using the steepest-ascent method. Consider a pattern 
                           
                              T
                              α
                           
                         with a given fitness value 
                           
                              f
                              (
                              
                                 T
                                 α
                              
                              )
                           
                        . We attempt all the possible 1-opt exchange respecting the maximum fuzziness level ϕ
                        *, dropping one of the literals in 
                           
                              T
                              α
                           
                         and adding one of the literals not in the pattern but still covering point α. After each swap, we obtain a new pattern 
                           
                              T
                              α
                              ′
                           
                         and compute the fitness value of this new pattern. We replace the initial pattern 
                           
                              T
                              α
                           
                         with the pattern 
                           
                              T
                              α
                              ′
                           
                         that leads to the maximum improvement in the fitness function, if such a pattern is found. The local search scheme stops when a pattern that leads to an improved fitness value cannot be found as a result of a 1-opt exchange.

It is easy to see that the exchange, if occurs, still ensures the coverage of the α point and, in addition, improves the quality of the pattern, computed with respect to a given fitness function.

We now summarize the proposed algorithm, illustrating how the different parts are intertwined to produce near-maximum α-patterns. In the following, let us indicate with S the set of positive points yet not covered by any positive pattern, with 
                        
                           P
                           α
                           +
                        
                      the set of α-patterns generated by the CE scheme applied to a given observation α, while with 
                        
                           P
                           +
                        
                      we indicate the overall set of positive patterns generated by the proposed scheme. In addition, we indicate with 
                        
                           C
                           (
                           T
                           )
                        
                      the set of positive observations covered by the fuzzy pattern 
                        
                           T
                           ,
                        
                      
                     i.e., 
                        
                           C
                           
                              (
                              T
                              )
                           
                           =
                           
                              {
                              
                                 β
                                 i
                              
                              ∈
                              S
                              :
                              T
                              
                                 (
                                 
                                    β
                                    i
                                 
                                 )
                              
                              =
                              1
                              }
                           
                        
                     .

Owing to the elimination of every point α used to generate patterns, Algorithm 3
                      will terminate in at most 
                        
                           
                              |
                           
                           
                              Ω
                              +
                           
                           
                              |
                           
                        
                      iterations. (See line 5 of the algorithm.) However, we cannot guarantee that Algorithm 1 will actually return a pattern. As we can observe from lines 2–10 of Algorithm 1, the CE pattern generation procedure can terminate without returning any pattern and only because the maximum fuzziness level ϕ
                     * was reached. Thus, line 4 of Algorithm 2 might return an empty set, at least for some of the α points used. Whenever this is the case, point α is still eliminated from the set of positive points to be used and we proceed with the pattern generation scheme for another point from S. If, after concluding Algorithm 3, more than 10 percent of the observations in 
                        
                           Ω
                           +
                        
                      is not covered by any pattern in 
                        
                           
                              P
                              +
                           
                           ,
                        
                      we relax ϕ
                     *, e.g., ϕ
                     * ← 1.25ϕ
                     * and we repeat Algorithm patterns_generation_CE().

To evaluate the performance of the proposed algorithm, we designed a set of computational experiments with two goals in mind:

                        
                           •
                           On the one hand, we want to measure the quality of the solutions obtained using Algorithm 2 in solving the pattern generation problem presented in Section 2. Through a thorough computational analysis, we will show that the CE scheme produces near-optimal patterns in a very short computational time.

On the other hand, we will illustrate the effectiveness of the use of a pool of near-optimal α-patterns, as opposed to just the set of maximal α-patterns in binary classification. To assert the effectiveness of the proposed scheme on a data mining task, we will provide measures of classification accuracy and computational running time on a set of well-known benchmark instances.

The computational results presented in this section refers to the proposed algorithm implemented and compiled using the GNU C++ compiler and run on a dual core Pentium 1.8 gigahertz Linux workstation with 4 gigabytes of RAM. Throughout the computational experiment phase, we kept the pool size 
                        
                           
                              |
                           
                           
                              P
                              α
                              +
                           
                           
                              |
                           
                        
                      constant to 10, while the values of the CE parameters, i.e., N, ρ, and δ were determined using a novel fine-tuning technique called biased Random Key Genetic Algorithm (bRKGA). (More details are provided in Section 5.2.1.)

The implementation of LAD requires the definition of a binarization phase, feature selection phase, and theory formation phase. For these steps of the LAD we used a standard implementation from Boros et al. (2000), while the pattern generation phase was implemented using the algorithm presented in Section 4.

Following a consolidated approach in classification, as testbed for the set of computational experiments, we used 10 datasets from the UCI repository (Blake & Merz, 1998). In Section 5.1, we present detailed results for the liver dataset, although similar results have been obtained for all the other datasets from the repository. In Section 5.2 we then present the classification results for each of the datasets and compare the results obtained with the best results reported in the literature.

The objective of this set of computational experiments is to show that the patterns obtained using Algorithm 2, maximum-α-patterns-generation(), are of good quality and that the running time of the proposed algorithm is shorter than the running time required by an MIP solver. The quality of a positive α-pattern is measured using the fitness function defined by Eq. (1), i.e., the number of positive points covered by the pattern. Obviously, the pattern obtained solving problem (IP
                           
                              
                              α
                              ϕ
                           
                        ) to optimality has maximum coverage and, therefore, to measure the quality of a pattern obtained using the proposed metaheuristic we compute the ratio between the number of patterns covered by the latter over the number of patterns covered by the former as:

                           
                              (16)
                              
                                 
                                    γ
                                    =
                                    1
                                    −
                                    
                                       
                                          z
                                          H
                                          α
                                       
                                       
                                          z
                                          
                                             I
                                             P
                                          
                                          α
                                       
                                    
                                 
                              
                           
                        where 
                           
                              z
                              
                                 I
                                 P
                              
                              α
                           
                         is the best solution to problem (IP
                           
                              
                              α
                              ϕ
                           
                        ) found by the MIP solver, and 
                           
                              z
                              H
                              α
                           
                         is the best solution found by proposed metaheuristic algorithm. Due to the exact nature of the IP approach, and given that the same fuzziness level ϕ is kept constant for both the exact approach and the heuristic method, we know that γ ∈ [0, 1]. More precisely, a value of 
                           
                              γ
                              =
                              0
                           
                         implies that the metaheuristic found an optimal solution, while a value of 
                           
                              γ
                              =
                              1
                           
                         indicates that the metaheuristic failed to find a feasible solution.

With respect to the descriptive study of the proposed algorithm, we recorded and plotted the performance of the algorithm when changing the parameters value. The goal of this phase of the computational experiment is to establish the quality of the solutions produced by the metaheuristic, and to determine whether the parameters value affect such performance. We used a plot that shows the empirical distributions of the random variable γ, in a fashion similar to what suggested in, e.g., Resende and Ribeiro (2005). A different plot is created for each instance and each combination of parameters value. More specifically, we tested the algorithm for different values of:

                           
                              •
                              N: the cross entropy population size;


                                 δ: the cross entropy smoothing factor;


                                 ϕ: the fuzziness level.

Given a specific instance, we plot the empirical distribution of the random variable by recording the value of γ over R different runs. Let us indicate with X
                        (i) the ith smallest observation, which is, the ith order statistic of the Xj
                        ’s. The empirical cumulative distribution function is defined as 
                           
                              
                                 F
                                 R
                              
                              
                                 (
                                 
                                    X
                                    
                                       (
                                       i
                                       )
                                    
                                 
                                 )
                              
                              =
                              
                                 (
                                 i
                                 −
                                 0.5
                                 )
                              
                              /
                              R
                              ,
                           
                         with 
                           
                              i
                              =
                              1
                              ,
                              …
                              ,
                              R
                           
                        .

For a given dataset, we solved the same problem using different fuzziness levels ϕ ∈ {0, 0.05, 0.10, 0.15}. With respect to the chart presented in Fig. 1
                        , we fixed the values of N and α to 100 and 0.2, respectively. As typically done in the literature, we used a k-fold cross-validation process, with 
                           
                              k
                              =
                              10
                           
                        . In addition, for each run we apply Algorithm 2 (as illustrated in line 4 of Algorithm 3), until all points in the dataset Ω have been considered. Therefore, each k-fold cross-validation run entails the solution of a number of problems (IP
                           
                              
                              α
                              ϕ
                           
                        ) and, therefore, generates a number of γ values which is not known a priori. Due to the stochastic nature of the algorithm, we run the k-fold cross-validation process five times.

From Fig. 1, we can draw some conclusions about the effectiveness of the metaheuristic proposed in this study. For example, let us consider the case of the metaheuristic when we impose that only “pure” patterns, i.e., non-fuzzy patterns (
                           
                              ϕ
                              =
                              0
                           
                        ) are created. From the picture, we can read, e.g., that 70 percent of the times the solution found by the metaheuristic has a gap value of 0.2 or less. However, if we consider the solutions provided by the metaheuristic when 
                           
                              ϕ
                              =
                              0.05
                              ,
                           
                         we see that the probability of finding a solution whose gap from the optimal is of 0.2 or less is of around 80 percent. Similarly, if we increase the fuzziness level ϕ to 0.15, the probability of finding a solution with gap of 0.2 or less is of around 95 percent. From the picture, it emerges that the quality of the metaheuristic increases with the increase of the fuzziness level. As we will illustrate in the next section, in line with what was already mentioned in the literature, a certain degree of fuzziness seems to improve the robustness of the classification algorithm. Therefore, under the assumption that, in the context of classification, fuzzy patterns are desirable, the metaheuristic approach qualifies as a valid alternative to the exact approach based on the use of an MIP solver. Given the average time required by the MIP solver to solve a single problem (IP
                           
                              
                              α
                              ϕ
                           
                        ), the metaheuristic required, as an average, less than 5 percent of that time. Hence, the metaheuristic is an average of 20 times faster than the MIP approach.

Let us now illustrate how the behavior of the metaheuristic changes with respect to changes in the parameters N, the population size of the CE method, and δ, the smoothing factor of the update rule of the CE method. We would expect parameter N to have an effect on the solution quality (up to a certain extent, the population size should affect the solution quality), while δ determines the convergence rate of CE and, therefore, will definitely affect the running time and, by avoiding fast convergence to suboptimal solution, the quality as well. Fig. 2
                         provides some insight into the behavior of the algorithm. In the picture, we report results with 
                           
                              N
                              =
                              50
                           
                         and 
                           
                              N
                              =
                              100
                           
                         only, since lower values of N always lead to inferior results, while values of N above 100 produce results similar to those reported for 
                           
                              N
                              =
                              100
                           
                        . From the picture, we can appreciate the evolution of the performance of the algorithm in terms of solution quality. Starting from the top-left corner, the fist chart presents the results of the metaheuristic when 
                           
                              N
                              =
                              50
                           
                         and 
                           
                              δ
                              =
                              0.1
                           
                        . While it is true that the algorithm is extremely fast (it requires only 4 percent of the time required by the MIP solver), the probability the heuristic solution generates a gap γ of 0.1 or less is only around 40–50 percent, no matter the fuzziness level. Conversely, if we increase the population size to 
                           
                              N
                              =
                              100
                           
                         and the smoothing factor to 
                           
                              δ
                              =
                              0.9
                           
                         (bottom-right chart), the performance of the algorithm sensibly improves: We observe that the probability of obtaining a heuristic solution with a gap of less than 10 percent is above 95 percent, for any approach using fuzzy patterns. It is also worth to point out that, as an average, the metaheuristic requires 15 percent of the running time required by the MIP solver to solve problem (IPαϕ
                        ). These empirical analysis allows us to draw conclusions about the effectiveness of the metaheuristic approach, both in terms of solution quality as well as computational running time. Throughout the set of experiments presented in Fig. 2, we also observe that the performance of the metaheuristic increases when the fuzziness level increases. For example, the metaheuristic solution presents a decreasing average gap γ with the increase of ϕ from zero to 0.05 and, similarly, from 0.05 to 0.10. However, it is interesting to observe that the behavior of the metaheuristic with 
                           
                              ϕ
                              =
                              0.10
                           
                         and 
                           
                              ϕ
                              =
                              0.15
                           
                         is not significantly different.

Finally, another question that deserves analysis is whether the local search scheme presented in Section 3.3 is actually contributing to enhance the performance of the algorithm. In addition, one might wonder whether a simple random mechanism for pattern generation coupled with local search could produce results similar to those of cross entropy. To provide an answer about the importance of the cross entropy and the local search scheme we resorted again to the empirical cumulative distribution function. In Fig. 3
                        , we present these functions for three versions of the algorithm:

                           
                              •
                              CE : The local search scheme is deactivated and, therefore, the CE alone is used;

CE + LS : The full-feature metaheuristic is used, as presented in Algorithm 3; and

R + LS : A random mechanism is used to produce the initial patterns, i.e., under Eqs. (5)
and (6), which are, then, used by local search.


                        Fig. 3 highlights the importance of both cross entropy and the local search scheme to attain high quality solutions. The three empirical distributions here were obtained from the same dataset, liver, where we run a 10-fold cross-validation experiment with the two aforementioned versions of the algorithm. We fixed the fuzziness level 
                           
                              ϕ
                              =
                              0.1
                           
                         for all the algorithms. We then compared the performance of the three algorithms. From the picture, we can see that it is the intertwined used of CE with local search that allows to attain near-optimal solutions. We can clearly detect that CE + LS has superior performance to the other two versions of the algorithm. For example, the probability that a heuristic solution with a gap γ of 25 percent or less is produced by R + LS is around 50 percent, for CE scheme alone is around 75 percent, while for CE + LS is approximately 98 percent.

Due to the superior performance of the cross entropy over a random mechanism, in the subsequent analysis, we focus on the two versions of the proposed algorithm that makes use of CE. When we compare the running times of the two schemes, the average running time of the CE alone was 0.43 percent of the running time of the exact approach, while the CE + LS required 0.81 percent of the running time consumed by the exact approach. Thus, while the use of a local search scheme does come with a price in terms of computational running time, the increase in running time is clearly justified by the increase in solution quality.

A more rigorous comparison of the two versions of the algorithm allows to determine whether any statistically significant difference emerges. More precisely, we used a Mann–Whitney–Wilcoxon (MWW) rank sum test to determine whether the mean gap 
                           
                              
                                 γ
                                 ¯
                              
                              
                                 C
                                 E
                              
                           
                         is significantly different from the mean gap 
                           
                              
                                 
                                    γ
                                    ¯
                                 
                                 
                                    C
                                    E
                                    +
                                    L
                                    S
                                 
                              
                              ,
                           
                         where γ is computed using Eq. (16). Table 1
                         presents a summary of the relevant statistics. In column one, we identify the two types of algorithm used; columns two to four provide the mean, the standard deviation, and the median of the gap γ. Finally, five and six provide the result of the MWW test, the W statistics and the p-value. At a 95 percent confidence level, we can reject the null hypothesis that the two means are equal and, therefore, we claim that the computational results provide evidence that the algorithm using CE and local search is superior to the algorithm that only uses CE in terms of solution quality. The boxplot of Fig. 4
                         offers a graphical representation of the distributions of the three versions of the algorithm and, again, highlights the superiority, in terms of near-optimality, of the CE + LS version over both CE and R + LS.

In this section, we present the results produced by the proposed algorithm on a standard set of benchmark classification problems. All the datasets presented in this section have been obtained from the UCI Machine Learning Repository. This portion of the computational analysis has three main objectives:

                           
                              •
                              First, it is well known that the ability to solve to optimality (or near-optimality) problem IP(
                                    
                                       
                                       α
                                       ϕ
                                    
                                 ) does not necessarily translate into high classification accuracy. Therefore, after establishing that the proposed method produces near-optimal α-patterns, we aim now at measuring its classification accuracy. In other words, we want to determine whether the proposed metaheuristic produces good classifiers. We need to establish, e.g., how the proposed classification method compares with the classifier produced by the MIP-based approach of Bonates et al. (2008).

Second, we want to determine whether the findings of Section 5.1 in terms of algorithmic parameters are confirmed by the automatic fine-tuning mechanism we propose here. To summarize, in Section 5.1 we concluded that: (i) the proposed scheme is very effective when problem IP(
                                    
                                       
                                       α
                                       ϕ
                                    
                                 ) allows for a certain degree of fuzziness. We now want to determine whether the use of fuzzy patterns leads to higher classification accuracy; (ii) the use of the local search scheme allows to obtain patterns that are close to optimality. We now want to know whether this translates into higher classification accuracy. In addition, using the automatic fine-tuning mechanism we can determine the appropriate value of parameters N and δ for each dataset.

Third, we are interested in asserting the importance of one of the major features of the proposed algorithm, i.e., the pool management strategy of Sections 3.2. The major difference between the proposed approach and any other pattern generation MIP-based approach lies in the fact that we can use the CE scheme to generate a diverse set of α-patterns, where diversity can be defined using different metrics. Thus, we want to investigate whether the use of a pool of patterns is beneficial in terms of classification accuracy and, in addition, whether using a diversity metric to establish which patterns should be added to the pool (as opposed to, e.g., simply storing the best patterns obtained with CE) translates into an improvement in the accuracy of the LAD classifier.

The datasets used are all referred to binary classification problems. A typical dataset has a mix of binary as well as numerical (integer and real) attribute values. Given a dataset Ω divided in training and testing sets, ΩT
                         and Ωt
                         respectively, we thus build a LAD theory based on a training set by generating a pool of positive and negative patterns 
                           
                              P
                              +
                           
                         and 
                           
                              P
                              −
                           
                        . When a new testing observation ω ∈ Ωt
                         is given, such observation is classified using a discriminant function:

                           
                              (17)
                              
                                 
                                    Δ
                                    
                                       (
                                       ω
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                p
                                                ∈
                                                
                                                   P
                                                   +
                                                
                                             
                                          
                                          
                                             T
                                             p
                                          
                                          
                                             (
                                             ω
                                             )
                                          
                                       
                                       
                                          
                                             |
                                          
                                          
                                             P
                                             +
                                          
                                          
                                             |
                                          
                                       
                                    
                                    −
                                    
                                       
                                          
                                             ∑
                                             
                                                n
                                                ∈
                                                
                                                   P
                                                   −
                                                
                                             
                                          
                                          
                                             T
                                             n
                                          
                                          
                                             (
                                             ω
                                             )
                                          
                                       
                                       
                                          
                                             |
                                          
                                          
                                             P
                                             −
                                          
                                          
                                             |
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              T
                              (
                              ω
                              )
                              =
                              1
                           
                         if pattern 
                           T
                         covers observation ω and 
                           
                              T
                              (
                              ω
                              )
                              =
                              0
                           
                         otherwise. Observation ω is classified as positive if Δ(ω) > 0 and negative if Δ(ω) < 0. The observation is unclassified if 
                           
                              Δ
                              (
                              ω
                              )
                              =
                              0
                           
                        .

The overall accuracy of the algorithm is measured using a 10-fold cross-validation method, while the formula used to measure the overall classification accuracy (OCA) is the one presented in Boros et al. (1997) and in Bonates et al. (2008), i.e.:

                           
                              (18)
                              
                                 
                                    O
                                    C
                                    A
                                    =
                                    
                                       1
                                       2
                                    
                                    
                                       [
                                       a
                                       +
                                       e
                                       +
                                       
                                          1
                                          2
                                       
                                       
                                          (
                                          c
                                          +
                                          f
                                          )
                                       
                                       ]
                                    
                                 
                              
                           
                        where a, b, c (d, e, f ) represent the percentage of positive (negative) observations which are predicted by the LAD model to be positive, negative, or unclassified. (See Bonates et al. (2008) for a more detailed explanation.)

One of the common drawbacks of a number of data mining algorithms is that, in general, extensive calibration and fine tuning is required to determine the value of the numerous algorithmic parameters. The algorithm proposed in this paper requires the calibration of some parameters, i.e., the fuzziness level ϕ, the cross entropy population N, the smoothing factor δ, and the activation/deactivation of the local search scheme. To calibrate the proposed algorithm, we employed a recently proposed scheme known as biased random key genetic algorithm (bRKGA) (Gonçalves & Resende, 2011), in a fashion similar to what proposed in Morán-Mirabal, González-Velarde, and Resende (2013). The bRKGA is used to find a good configuration of the algorithmic parameters in an automatic fashion, thus reducing the time and effort of the calibration phase. In addition, as illustrated in Morán-Mirabal et al. (2013), the results obtained via automatic fine tuning are often superior to those obtained via manual configuration. bRKGA was implemented using the API available from Toso and Resende (2012). We setup the bRKGA using each individual of the genetic algorithm to encode a specific set of parameters, then employing the fitness value of the individual as a measure of the goodness of that specific choice of parameters value, where the fitness value of each configuration is the classification accuracy produced by the algorithm running that specific configuration of parameters. More specifically, we setup the following design (see Fig. 5
                            for a representation of the chromosome and Morán-Mirabal et al. (2013) for a deeper explanation of the method):

                              
                                 •
                                 fuzziness level ϕ ∈ {0.0, 0.05, 0.10, 0.15}

cross entropy population size N ∈ {10, 50, 100, 200}

cross entropy smoothing factor δ ∈ {0.1, 0.2, 0.5, 0.9}

with local search ls ∈ {0, 1}, where 0 indicates that the local search feature is not used, while 1 indicates that the local search is activated.

Consequently, the method works as follows: Let us suppose a chromosome is randomly produced as:
                           
                              
                                 
                                    
                                       x
                                       =
                                       
                                          [
                                          0.37
                                          ,
                                          0.54
                                          ,
                                          0.28
                                          ,
                                          0.61
                                          ]
                                       
                                    
                                 
                              
                           where each allele is associated to the parameter indicated in Fig. 5. Thus, chromosome x determines the following value of parameters: 
                              
                                 ϕ
                                 =
                                 0.05
                                 ;
                                 N
                                 =
                                 100
                                 ;
                                 δ
                                 =
                                 0.2
                                 ,
                              
                            and 
                              
                                 l
                                 s
                                 =
                                 1
                              
                           .
                              1
                           
                           
                              1
                              We use the following decoding scheme: A value for each allele is randomly generated in the interval [0, 1]. If the value of the first allele is between 0 and 0.25, we set 
                                    
                                       ϕ
                                       =
                                       0.0
                                    
                                 ; if it is between 0.26 and 0.5, 
                                    
                                       ϕ
                                       =
                                       0.05
                                    
                                 ; if it is between 0.51 and 0.75, 
                                    
                                       ϕ
                                       =
                                       0.1
                                    
                                  and if it is between 0.76 and 1, 
                                    
                                       ϕ
                                       =
                                       0.15
                                    
                                 . The same applies to the other alleles of the chromosome.
                            Next, we run the algorithm with this specific set of parameter values and we associate the OCA obtained with this configuration of parameters to the fitness value of the chromosome. The evolution of the population is governed by the rules of the bRKGA defined in Gonçalves and Resende (2011).

Interestingly, the bRKGA allows not only to automatically determine the numerical value of the parameters but also to test different versions of the same algorithm, via activation/deactivation of some portions of the algorithm itself. As mentioned above, the parameter ls takes value 0 when the algorithm that makes use of the CE only is employed, while it takes value 1 when the full algorithm CE + LS is used. Thus, via automatic fine-tuning we can determine what are the ingredients of the algorithm that should be used to achieve the best performance.

To evaluate the classification performance of the proposed algorithm, for each dataset we ran five 10-fold cross-validation experiments using the 10 datasets presented in Table 2. Table 3 presents the average results over five runs (and the standard deviation) of the OCA of the proposed algorithm, along with the parameters value used to achieve such classification accuracy. In the table, the first column provides the instance name; the second column collects the best know result obtained by one of five commonly used machine learning algorithms, i.e., Support Vector Machine (svo), C4.5 Decision Trees ( j48), Random Forest (rf), Multilayer Perceptron (mp), and Simple Logistic Regression (lr), as reported in Bonates et al. (2008) and obtained using the publicly available software package Weka (Witten & Frank, 2005) (the symbol next to the result in the second column indicates which of these five algorithms obtained the best performance); the third column provides the best available LAD result from the literature for the corresponding dataset (Bonates et al., 2008); the fourth column gives mean and standard deviation of the proposed algorithm (over five runs). Finally, columns five to eight present the parameters value used to obtain such results.
                        

The results collected in the table show that the proposed algorithm is quite competitive in terms of classification accuracy, both when compared with the best LAD implementation as well as with respect to other well-known machine learning approaches. To ensure a fair comparison, we kept the same experimental conditions used in Bonates et al. (2008), e.g., k-fold validation, OCA, etc. The proposed algorithm achieves a better classification accuracy than the best LAD result from the literature in 7 out of 10 datasets. The standard deviation, while a bit higher than the one reported in Bonates et al. (2008), is still in line with the results presented in the LAD literature. Interestingly, the automatic fine-tuning tool provides an answer to the question of whether the local search scheme actually contributes to the OCA of the algorithm. As indicated in the last column of Table 3, the value of parameter ls is fixed to one. Thus, we can conclude that the version of the algorithm that makes use of the cross entropy plus the local search is superior, in terms of OCA, to the version in which only the cross entropy is used. This result, obtained using the automatic tool, is in line with what reported in Table 1, where a statistical test was used to claim the superiority of CE + LS over CE. We can also notice that the CE method is fairly robust, as highlighted by the fact that both N and δ do not significantly vary over the different datasets. Finally, by comparing the results of the proposed LAD with those of some machine learning approaches, we can see that LAD is a competitive approach. One might argue that, with more extensive tuning, the machine learning approaches might lead to better results. However, the goal here is not to compare the performance of LAD with those of other machine learning methods. Rather, we would like to point out that (i) LAD results are in line with those of widely accepted machine learning methods; and (ii) the pool management strategy of CE might be of interest to other rule learning algorithms.

We can finally address the last question presented in this section: How important the pool management strategy is with respect to the final classification performance? To test the hypothesis that the diversification mechanism presented in Section 3.2 enhances the performance of the classification algorithm, we compared the results of Table 3 with the results obtained by the same algorithm (using the same set of parameters) but with different pool management strategies. More specifically, let us define three versions of the algorithm:

                              
                                 •
                                 LAD-pool: the full-feature algorithm presented in Section 4;

LAD-best: the pool management strategy is such that each 
                                       
                                          P
                                          α
                                          +
                                       
                                     contains the 10 best α-patterns, ignoring any measure of diversity;

LAD-one: the pool size is equal to one, i.e., we save in the pool only the best α-pattern.


                           Table 4
                            presents the OCA of the three versions of the algorithm for the 10 datasets. In the table, the first three columns are taken from Table 3, thus reporting the best result from the literature and the result of the full-feature algorithm. Columns four and five provide the mean OCA and the standard deviation for the two versions of the algorithm presented above. Again, for each dataset, we use a 10-fold cross-validation experiment, and we repeat the process five times. From the table, it can be observed that LAD-pool, i.e., the proposed algorithm that uses diversification as strategy for the management of the pool is the best among the three variations, thus confirming that introducing a degree of diversification in the patterns set (hence including patterns that might not have the maximum coverage and yet are structurally different from the other patterns) fosters the overall performance of the LAD algorithm.

Despite the fact that, right from the table, it is quite simple to see which configuration of the algorithm provides superior performance, we carried out a more rigorous statistical analysis aimed at asserting (i) whether the different configurations produce statistically different results, and (ii) whether we can define a ranking of the four LAD algorithms. With this goal in mind, we performed a two-step analysis:

                              
                                 1.
                                 We first ran a Friedman test, to rank the four configurations of the algorithm. The null hypothesis of the Friedman test is that the results of those configurations are indistinguishable, i.e., they all provide solutions that are approximately of the same quality and, thus, the ranking should be randomly distributed. In other words, if the pool management strategy did not affect the classification accuracy, we would expect the average ranking value of the three variations of the proposed algorithm to be quite similar. Let rij
                                     be the rank of the solution produced by configuration j on dataset i. The average rank of configuration i is thus defined as:

                                       
                                          
                                             
                                                
                                                   R
                                                   j
                                                
                                                =
                                                
                                                   
                                                      
                                                         ∑
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         N
                                                      
                                                      
                                                         ∑
                                                         
                                                            j
                                                            =
                                                            1
                                                         
                                                         k
                                                      
                                                      
                                                         r
                                                         
                                                            i
                                                            j
                                                         
                                                      
                                                   
                                                   N
                                                
                                             
                                          
                                       
                                    where k is the total number of configurations used, i.e., 
                                       
                                          k
                                          =
                                          4
                                          ,
                                       
                                     and N is the total number of datasets, i.e., 
                                       
                                          N
                                          =
                                          10
                                       
                                    .


                                    Table 5
                                     summarizes the results in terms of ranking values of the four configurations. The Friedman test produced a p-value of 0.02197 (
                                       
                                          
                                             χ
                                             2
                                          
                                          =
                                          9.631
                                          ,
                                          d
                                          f
                                          =
                                          3
                                       
                                    ), i.e., we can reject the null hypothesis and conclude that the four configurations do produce significantly different results.

Since the null hypothesis of the Friedman test was rejected, we now run a post-hoc analysis, as suggested in Demsar (2006). More precisely, we use the Nemenyi test to compare each configuration with the others, thus detecting for which configurations there exists a statistically significant difference in terms of ranking. The performance of two algorithms are significantly different if the corresponding average ranks differ by at least a “critical difference,” computed as:

                                       
                                          
                                             
                                                CD
                                                =
                                                
                                                   q
                                                   α
                                                
                                                
                                                   
                                                      
                                                         k
                                                         
                                                            (
                                                            k
                                                            +
                                                            1
                                                            )
                                                         
                                                      
                                                      
                                                         6
                                                         N
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    where the critical value qα is obtained from Demsar (2006), 
                                       
                                          k
                                          =
                                          4
                                       
                                     is the number of configurations used, and N is the total number of benchmark datasets, 
                                       
                                          N
                                          =
                                          10
                                       
                                    . From Demsar (2006), we obtain that, with a level of confidence of 95 percent, 
                                       
                                          
                                             q
                                             α
                                          
                                          =
                                          2.569
                                       
                                     and, therefore, 
                                       
                                          CD
                                          =
                                          1.48
                                       
                                    . The performance of two algorithms is thus significantly different if their corresponding average ranks differ by at least the critical distance CD. (See Demsar (2006) for more details.)

As we can observe from Fig. 6
                                    , algorithmic configuration LAD-pool is significantly different from the other configurations, with a level of confidence of 95 percent. Thus, from the post-hoc analysis we can conclude that, in terms of robustness, configuration LAD-pool (i.e., the algorithm that uses a measure of “diversity” as pool management strategy) seems to be significantly better than the other configurations and the best LAD algorithm from the literature.

@&#CONCLUSIONS@&#

This paper presented a novel metaheuristic algorithm for the pattern generation phase of Logical Analysis of Data (LAD). LAD is a successful data mining technique aimed at tackling binary classification problems, i.e., problems in which a set of observations in classified in one of two classes. The analysis of the literature makes evident that the key step in LAD is the generation of patterns. Historically, the pattern generation problem has been solved either using an enumerative, construction-based approach, or an approach based on the use of an MIP formulation. In this paper, we propose a third approach, based on the use of a metaheuristic technique. The metaheuristic takes in input an observation from the training set and makes use of a learning scheme, based on the cross entropy metaheuristic (CE), that progressively builds patterns of higher coverage. At each iteration of the algorithm, a learning scheme is employed, in an attempt to capture the features, i.e., combination of attributes, that compose high quality patterns. Thus, at each generation of the CE, good combinations of attributes become more and more predominant in the population.

The key advantage of the proposed approach is twofold: On the one hand, for each observation of the training set, a pool of patterns is generated, as opposed to collecting the best pattern (as done with an MIP formulation); on the other hand, the metaheuristic sensibly reduces the running time and, thus, allows to use LAD on larger datasets. As illustrated in the computational results, the idea of collecting, for each observation, a pool of patterns with pre-specified characteristics has an important effect on the final classification accuracy. In this paper, we tested three different pool management strategies (based on a diversity measure and a fitness value) and empirical evidence proves that the algorithm in which a pool of diverse patterns is collected is significantly better (in the statistical sense) than the other versions. This finding is in line with some of the remarks about overfitting presented in the literature. A number of authors claimed that the ability to get optimal patterns with respect to a pre-specified criterion (e.g., maximal α-patterns) does not automatically translate into superior classification accuracy. Conversely, it seems beneficial to create a LAD theory based on a pool of diverse patterns with good coverage (near-optimal α-patterns), hence striking the balance between diversification and intensification, and reducing the effect of overfitting the training set.

Finally, another contribution of the paper concerns the use of an automatic fine-tuning approach. We used the biased Random Key Genetic Algorithm to fine-tune all the parameters of the algorithm at once. Such automatic fine-tuning reduces the calibration effort and allows to determine which ingredients of the algorithm are needed to maximize the overall classification accuracy.

We tested the proposed scheme on a set of 10 benchmark instances from the UCI repository and compared the results hereby obtained with those of the best LAD algorithm from the literature as well as with some well-known machine learning approaches. The proposed approach compares favorably, both in terms of classification accuracy (we obtained a better classification accuracy for seven out of 10 datasets) as well as running time (the time required by the metaheuristic is around 5 percent of the time employed by the MIP approach) with the best known LAD approach.

An interesting extension of this work is to generalize the approach to a multi-class LAD algorithm and, subsequently, to a regression-type approach, in which the class each observation belongs to is defined on a continuum.

@&#REFERENCES@&#

