@&#MAIN-TITLE@&#Adapting a classification rule to local and global shift when only unlabelled data are available

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The model addresses binary classification of evolving populations.


                        
                        
                           
                           It bridges the gap between the observation of new features and their class labels.


                        
                        
                           
                           It helps to improve classification even when model re-estimation is impossible.


                        
                        
                           
                           The model addresses local and global drift.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Dataset shift

Concept drift

Local drift

Global drift

Verification latency

@&#ABSTRACT@&#


               
               
                  For evolving populations the training data and the test data need not follow the same distribution. Thus, the performance of a prediction model will deteriorate over the course of time. This requires the re-estimation of the prediction model after some time. However, in many applications e.g. credit scoring, new labelled data are not available for re-estimation due to verification latency, i.e. label delay. Thus, methods which enable a prediction model to adapt to distributional changes by using only unlabelled data are highly desirable. A shift adaptation method for binary classification is presented here. The model is based on mixture distributions. The conditional feature distributions are determined at the time where labelled data are available, and the unconditional feature distribution is determined at the time where new unlabelled data are accessible. These mixture distributions provide information on the old and the new positions of subpopulations. A transition model then describes how the subpopulations of each class have drifted to form the new unconditional feature distribution. Assuming that the conditional distributions are reorganised using a minimum of energy, a two-step estimation procedure results. First, for a given class prior distribution the transfer of probability mass is estimated such that the energy required to obtain the new unconditional distribution by a local transfer of the old conditional distributions is a minimum. Since the optimal solution of the resulting transportation problem measures the distance between the old and the new distributions, the change of the class prior distribution is found in a second step by solving the transportation problem for varying class prior distributions and selecting the value for which the objective function is a minimum. Using the solution of the transportation problem and the component parameters of the unconditional feature distribution, the new conditional feature distribution can be determined. This thus allows for a shift adaptation of the classification rule. The performance of the proposed model is investigated using a large real-world dataset on default rates in Danish companies. The results show that the shift adaptation improves classification results.
               
            

@&#INTRODUCTION@&#

Statistical classification is traditionally based on the assumption that the test data (target data) follow the same distribution as the training data for which the classification rule is estimated. However, in many real-world applications this assumption does not hold due to the evolution of the population. Typical examples here are credit scoring (Crook, Edelman, and Thomas, 2007; Hand, 2006; Kelly, Hand, and Adams, 1999; Yang, 2007), finance (Sun and Li, 2011), spam filtering (Bickel and Scheffer, 2007; Delany, Cunningham, Tsymbal, and Coyle, 2005), natural language processing (Jing and Xiang, 2007; Yamada, Sugiyama, and Matsui, 2010), remote sensing (Dinh, Duin, Piqueras-Salazar, and Loog, 2013), quality control (Forman, 2008), or bioinformatics (Tsymbal, Pechenizkiy, Cunningham, and Puuronen, 2008).

A changing population, i.e. a non-stationary environment (Ditzler, Rosen, and Polikar, 2012) is characterised by a change of the joint distribution P(X, Y) of the features (i.e. predictors) X and the class variable Y over the course of time. In the literature, such change is denoted as population drift (Kelly et al., 1999), as concept drift (Schlimmer and Granger, 1986), as dataset shift (Quinnero-Candela, Sugiyama, and Schwaighofer, 2009), as local, or as global drift (Hofer and Krempl, 2013), depending on which distribution changes. Unfortunately, consistent terminology has often been lacking in the drift literature. To simplify scientific discourse Moreo-Torres, Raeder, Alaiz-Rodríguez, Chawla, and Herrera (2012) thus made an attempt to unify frequently used terms. Depending on the distributions that are affected by the change, they distinguished four types of drift: (1), covariate shift, where the unconditional feature distribution P(X) changes but the posterior distributions P(Y|X) remain the same, (2), prior probability shift, where the class prior distribution P(Y) changes but the conditional feature distributions P(X|Y) remain the same, (3), concept shift, where the unconditional feature distribution P(X) is unchanged but the posterior distributions P(Y|X) changes, and (4), dataset shift, where the joint distribution P(X, Y) changes and none of the other cases hold.

Other definitions (see for example Ditzler et al., 2012; Krempl and Hofer, 2011), have a strong focus on the time component, i.e. changes are distinguished in terms of whether they are gradual (i.e. drift) or sudden (i.e. shift), fast or slow (further drift types see for example Forman, 2006). Hofer and Krempl (2013) distinguished between global drift and local drift, a distinction that is based on whether changes affect the whole feature space homogeneously or whether they only affect particular subpopulations and thus are observed in particular subregions of the feature space (see also Tsymbal et al., 2008). Global drift is caused by the change of the prior distribution and thus coincides with class prior probability shift as defined by Moreo-Torres et al. (2012), whereas local drift is a consequence of changing conditional feature distributions P(X|Y). Local drift falls into the category of dataset shift according to Moreo-Torres et al. (2012). In this present paper the terms ‘local’ and ‘global’ drift are preferred to the terms ‘dataset shift’ and ‘prior probability shift’, since they are very intuitive.

In changing environments the performance of a recently estimated prediction model will deteriorate over the course of time since it becomes less and less optimal with respect to the new distribution. As an example, consider a binary classification problem where the class prior probability of positives is P(Y = 1). According to the Bayesian classification rule a sample with the univariate features X is predicted to be positive when the posterior probability P(X|Y = 1) P(Y = 1) > P(X|Y = 0) P(Y = 0). However, when the class prior probability P(Y = 1) increases, the optimal classification rule which is defined by a single real value will shift to the right or to the left depending on the location of the conditional feature distribution P(X|Y = 1). As a consequence the false negative rate will increase when the old classification rule is still used instead of the new one. The situation becomes much more complicated when the conditional feature distributions P(X|Y) also change, i.e. in the case of local drift.

Thus, a dynamic environment requires the re-estimation of the prediction model after some time using new representative, labelled training data. However, in many real-world applications such new labelled data are not available due to a phenomenon called verification latency (Marrs, Hickey, and Black, 2010) or label delay (Kuncheva, 2008). Verification latency denotes the time span until the corresponding labels of given instances become accessible. In particular, it refers to applications in which it is cheap or easy to obtain instances, i.e. unlabelled data, but where it is expensive or impossible to obtain the class labels at the same time. For example, in credit scoring estimating a prediction model for the default of loans with a maturity of three years is based on training data collected three years ago. This results from the fact that the true class labels (default or not) of all training instances are not known before maturity of the loans, but estimation of the prediction model requires a full dataset, including the class labels. Thus, depending on the extent and speed at which changes occur, such data need not be representative, in particular for large time spans, until the class labels are accessible.

In the presence of verification latency only an incomplete dataset containing instances and their features, but not their labels, is available for tracking changes. However, since unlabelled data also provide information about the earlier distributional change, methods enabling adaptation of a prediction model using only such data are highly desirable. The procedure introduced in the present paper addresses this problem in the context of classification. In contrast to the previous paper (Hofer and Krempl, 2013) that deals with global changes, the present paper allows for both global and local changes.

In the literature a few methods were proposed for adapting a classification rule to local changes of distributions. Biernacki, Beninel, and Bretagnolle (2002) and Beninel, Biernacki, Bouveyron, Jacques, and Lourme (2012) addressed a particular problem of local drift, i.e. changes of distributions that can be described by a linear univariate transformation of the features. Thus, their model is limited to the situation where changes can be expressed in terms of diagonal transformation matrices that may differ between the classes. If the diagonal matrices are known, the means and variances of the test populations can be calculated and thus an adapted classification rule can be constructed. Since the populations in the various classes are Gaussian, the model requires continuous features (for an extension to binary predictors see Jacques and Biernacki, 2008). However, since mixture distributions are not considered, the model is inappropriate when subpopulations within a class behave differently over the course of time.


                     Alaiz-Rodríguez, Guerrero-Curieses, and Cid-Sueiro (2011) introduced an adaptive algorithm for a particular type of local drift based on neural networks. Their method is limited to the case where each class can be decomposed into several (unknown) subclasses, and all changes in data distributions arise from changes in class prior distributions and in prior subclass probabilities, while preserving invariance in subclass conditional densities. The model is not restricted to continuous features.


                     Krempl (2011) proposed a nonparametric method based on kernel density estimation. The author used an EM algorithm for the maximum likelihood estimation of the drift parameters. He showed that the maximisation step of the EM algorithm is equivalent to solving an assignment problem. However, since the assignment is carried out instancewise, the number of training instances and target instances is assumed to be equal. In addition, the method only applies to continuous features.


                     Krempl and Hofer (2011) proposed a method for only local changes where the parameters were estimated by means of an EM algorithm. Some of the disadvantages of this model are that the estimation of particular weights needs labelled data (for a discussion and the use of auxiliary techniques see Krempl and Hofer, 2011), and that the number of components in the mixture distribution is fixed.

To overcome the restriction of the above mentioned methods, a more general technique for local shift adaptation is proposed in the present paper. In contrast to Biernacki et al. (2002) and Beninel et al. (2012), it is assumed that the population contains several subpopulations, and that these components do not necessarily behave in the same way as the whole class to which they belong. In contrast to the model in Krempl and Hofer (2011), the number of subpopulations can change over the course of time. The method is based on the idea that unlabelled data give information on the new locations of subpopulations. These new locations can be estimated by a mixture distribution of the unlabelled data, but the class membership of the subpopulations is unknown due to the lack of labels. However, for each class the old locations of subpopulations are known from recent labelled data. Since the unconditional feature distribution is the weighted sum of the conditional feature distributions where the weights are the mixing proportions, a transition model can be constructed that describes how the subpopulations of each class drifted to form the new unconditional feature distribution.

The model assumes that the timespan between the estimation of a classification model using labelled data and its adaptation using unlabelled data is not too large, and thus, that the distance covered by the subpopulations is not too large. In particular, it is assumed that the given distribution is reorganised using a minimum of energy. This yields a two-step estimation procedure to estimate the shift. First, for a given class prior distribution the transfer of probability mass is estimated such that the product of distance covered and of mass transferred is a minimum. The objective function value corresponds to the energy that is required to obtain the new unconditional distribution by a local transfer of the old conditional distributions. This energy should be a minimum. This results in a transportation problem. The optimal solution of the transportation problem measures the distance between the old and the new distributions, and it can be used as estimate of the local change.

In a second step the change of the class prior distribution is found by solving the transportation problem for varying class prior distributions. The value for which the objective function takes a minimum is then selected to represent the shift of the class prior probability. Since the solution of the transportation problem yields the amount of probability mass that originates in the particular classes, they can be used together with the component parameters of the unconditional feature distribution to determine the new conditional feature distributions. These new conditional feature distributions and the new class prior distribution allow an update of the classification rule.

The model proposed is applicable when data are given at two different time points. Thus, the model addresses shift rather than drift. Drift denotes gradual changes that occur systematically over the course of time, whereas shift denotes a change between two particular time points (see for example Krempl and Hofer, 2011). In this respect, the proposed model handles data in batch mode, i.e. the instances arriving in a data stream are not processed upon arrival but, are collected, and then the model is applied to the whole dataset available at that time point.

As in Hofer and Krempl (2013) the present paper is based on a comparison of the conditional and unconditional feature distributions. In contrast to Hofer and Krempl (2013), the distributions are estimated as Gaussian mixture models and not as mixtures of normed B-spline basis functions, since B-spline basis functions increase the time complexity of the estimation procedure in the multivariate context.

The paper is organised as follows: Section 2 reviews the existing drift literature. In addition to an overview of the different types of drift and drift detection methods, this section discusses different drift adaptation procedures, and it discusses the position of the shift model with respect to the existing literature. Section 3 then describes the proposed shift model, introduces a two-step estimation procedure and shows how a classification rule is updated after the estimation. In addition, it addresses the problem of nominal features in the estimation process. Finally, the model is applied to a large real-world dataset in Section 4.

@&#RELATED WORK@&#

Even though most papers present techniques for adapting a classifier to changed distributions, some papers deal with detecting drift. For example, Cieslak and Chawla (2008) investigated three reasons for the deterioration of a classifier: sample selection bias, covariate shift, and shifting class prior. Their definitions of covariate shift and shifting class prior are similar to local and global drift as used by Hofer and Krempl (2013). While Cieslak and Chawla (2008) analysed nonparametric tests in order to identify different types of changes, Hofer and Krempl (2013) introduced an ex-ante and an ex-post drift mining method. In an ex-post drift analysis the estimated and the observed (unconditional and conditional) feature distributions can be compared according to any measure of discrepancy such as χ
                     2-test for goodness of fit, the likelihood ratio test, or symmetric Kullback–Leibler divergence. Ex-ante drift mining is based on the residuals between the unconditional feature distribution estimated from recent unlabelled data and the extrapolated unconditional feature distribution resulting from the old conditional feature distributions as derived from the drift model.

Depending on the type of drift, different methods have been introduced to adapt a classification rule or a regression model after changes in distributions have occurred. In addition, methods have also been proposed for clustering data that differ over time periods (Biernacki and Lourme, 2011; Lourme and Biernacki, 2013). These clustering methods for continuously distributed data are based on Gaussian mixture models or Student-t-mixture models and a linear transformation between the populations.

To cope with covariate shift in a regression context Shimodaira (2000) proposed a weighted maximum likelihood estimation. The weights applied to the training instances have the form P
                     1(X)/P
                     0(X), where P
                     1(X), and P
                     0(X), are the new and the old unconditional feature distribution, respectively. A different type of drift in the regression context was addressed by Bouveyron and Jacques (2010) and Bouveyron and Jacques (2014). The changes were expressed in terms of a linear transformation of the regression coefficients. Bouveyron and Jacques (2010) used various diagonal transformation matrices. The parameters were estimated by means of ordinary least squares or maximum likelihood. Bouveyron and Jacques (2014) introduced a Gaussian mixture model where for each component transformations as in Bouveyron and Jacques (2010) were used. The models require a relatively small set of labelled test data.

Under a covariate shift P(Y|X) is assumed to be unchanged. Even though a classification decision is regularly based on P(Y|X), a covariate shift is relevant in classification. As a result of non-optimality of the classification rule, the classification error increases for values of X in dense regions of the new distribution, i.e. when P
                     1(X) > P
                     0(X). The weighted kernel logistic regression proposed by Yamada et al. (2010) is an example of how Shimodaira’s importance weighting (Shimodaira, 2000) can be used in classification problems. Various authors have addressed the problem of estimating the weights such that an explicit estimation of P
                     1(X) and P
                     0(X) is not required (Bickel, Brückner, and Scheffer, 2009; Huang, Smola, Gretton, Borgwardt, and Schölkopf, 2006; Sugiyama, Nakajima, Kashima, von Bünau, and Kawanabe, 2008a; Sugiyama, Suzuki, Nakajima, Kashima, von Bünau, and Kawanabe, 2008b; Yamada et al., 2010). Among them, the procedure proposed by Huang et al. (2006), referred to as kernel mean matching, results in a quadratic optimisation problem for the weights. The weighting method can be applied in various classification and regression models to improve the predictive performance after a covariate shift.

Models on global drift, i.e. on the change of the class prior distribution, are addressed in the papers by Saerens, Latinne, and Decaestecker (2001), Forman (2008), Zhang and Zhou (2010), and Hofer and Krempl (2013). In these models the conditional feature distributions, P(X|Y), are assumed to remain constant while the class prior distribution P(Y) changes over time. As shown in Saerens et al. (2001), the new posterior probabilities P
                     1(Y = i | X) of class i can be expressed as the old posterior probabilities P
                     0(Y = i | X) weighted by the ratio of the new class priors P
                     1(Y = i) to the old class priors P
                     0(Y = i), i.e.

                        
                           
                              
                                 
                                    P
                                    1
                                 
                                 
                                    (
                                    Y
                                    =
                                    i
                                    
                                    |
                                    
                                    X
                                    )
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                P
                                                1
                                             
                                             
                                                (
                                                Y
                                                =
                                                i
                                                )
                                             
                                          
                                          
                                             
                                                P
                                                0
                                             
                                             
                                                (
                                                Y
                                                =
                                                i
                                                )
                                             
                                          
                                       
                                       
                                       
                                          P
                                          0
                                       
                                       
                                          (
                                          Y
                                          =
                                          i
                                          
                                          |
                                          
                                          X
                                          )
                                       
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          K
                                       
                                       
                                          
                                             
                                                P
                                                1
                                             
                                             
                                                (
                                                Y
                                                =
                                                j
                                                )
                                             
                                          
                                          
                                             
                                                P
                                                0
                                             
                                             
                                                (
                                                Y
                                                =
                                                j
                                                )
                                             
                                          
                                       
                                       
                                       
                                          P
                                          0
                                       
                                       
                                          (
                                          Y
                                          =
                                          j
                                          
                                          |
                                          
                                          X
                                          )
                                       
                                    
                                 
                                 
                                 .
                              
                           
                        
                     While Saerens et al. (2001), and similarly Zhang and Zhou (2010) used an EM estimation to find the new class prior probabilities using only unlabelled new data, Hofer and Krempl (2013) used least squares to estimate the factor δ by which the class prior changed. For this purpose the class prior evolution is modelled as P
                     1(Y) = δ 
                     P
                     0(Y). The new unconditional feature distribution P
                     1(X) is estimated using recent unlabelled data. Since the conditional feature distributions P(X|Y) are assumed to remain unchanged over time, they are estimated using old labelled data. Since the unconditional distribution can also be expressed as a mixture of the conditional distributions, where the mixing proportions are equal to the unknown class priors, the class prior change is estimated by minimising

                        
                           
                              
                                 
                                    
                                       
                                          SSQ
                                          (
                                          δ
                                          )
                                       
                                    
                                    
                                       =
                                    
                                    
                                       
                                          
                                             ∑
                                             
                                                X
                                                ∈
                                                Ω
                                             
                                          
                                          
                                             (
                                          
                                          
                                             P
                                             1
                                          
                                          
                                             (
                                             X
                                             )
                                          
                                          −
                                          δ
                                          
                                          
                                             P
                                             0
                                          
                                          
                                             (
                                             Y
                                             =
                                             1
                                             )
                                          
                                          
                                          
                                             P
                                             0
                                          
                                          
                                             (
                                             X
                                             
                                             |
                                             
                                             Y
                                             =
                                             1
                                             )
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          −
                                          
                                          
                                             (
                                             1
                                             −
                                             δ
                                             
                                             
                                                P
                                                0
                                             
                                             )
                                          
                                          
                                          
                                             P
                                             0
                                          
                                          
                                             (
                                             X
                                             
                                             |
                                             
                                             Y
                                             =
                                             0
                                             )
                                          
                                          
                                             )
                                          
                                          
                                             
                                             2
                                          
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     
                     Hofer and Krempl (2013) not only addressed shift but also proposed a drift model, i.e. how to use information on the evolution of class prior changes over a relatively large time range. Due to a nonparametric density estimation using a B-spline basis representation, their model can be used for quantitative as well as nominal features.

An alternative approach for estimating the class prior change was proposed by Forman (2008) and is based on the confusion matrix. Defining TP and FP as the true positive rate and the false positive rate, he estimated the probability of an instance being positive as 
                        
                           
                              P
                              ^
                           
                           
                              (
                              Y
                              =
                              1
                              )
                           
                           =
                           TP
                           
                           ·
                           P
                           
                              (
                              Y
                              =
                              1
                              )
                           
                           +
                           FP
                           
                           ·
                           
                              (
                              1
                              −
                              P
                              
                                 (
                                 Y
                                 =
                                 1
                                 )
                              
                              )
                           
                        
                     . Solving this equation for P(Y = 1) yields a formula for estimating the true class prior from the observed proportion 
                        
                           
                              P
                              ^
                           
                           
                              (
                              Y
                              =
                              1
                              )
                           
                        
                      and from TP and FP. Using recent labelled data, TP and FP are derived by cross-validation using the training dataset. 
                        
                           
                              P
                              ^
                           
                           
                              (
                              Y
                              =
                              1
                              )
                           
                        
                      is found by predicting the class membership of unlabelled test data using a model that is derived from the training dataset. Forman (2008) also introduced more sophisticated modifications of his model.

In contrast to existing ensemble methods, Tsymbal et al. (2008) proposed an ensemble technique that can handle local drift. In addition, it can also cope with global, gradual, and abrupt shift. The authors assigned each classifier a weight proportional to its local accuracy with regard to the test instance, and selected either the best classifier, or the classifiers were integrated using weighted voting. The performance of a classifier is predicted by k-nearest neighbours for the test instance.


                     Ditzler et al. (2012) introduced an ensemble method that is based on an idea similar to the shift adaptation method proposed in the present paper. However, the two papers differ in their estimation procedures and the classification method applied. Ditzler et al. (2012) also estimated the conditional feature distributions from recent labelled data and the unconditional feature distribution from new unlabelled data, and the authors used Gaussian mixture distributions. However, they related the components of the old conditional feature distributions to the components of the new unconditional feature distribution by a weighting function that depends on the posterior distribution of classes conditioned on the components. These weights are used to derive the classifier voting weights.

A different approach is proposed by Bifet and Gavaldà (2007) or Klinkenberg and Joachims (2000). They used a sliding-window technique with an adaptive window size. Depending on bounds on false positives and false negatives the window in Bifet and Gavaldà (2007) automatically grows when no changes are detected, and shrinks when the data change. As long as statistical tests do not indicate significant differences between the distributions of two sections of the current window, the current window is retained. Otherwise, the older window is dropped. This windowing method is combined with an incremental version of the naive Bayes predictor, and a particular ensemble method, i.e. bagging (Bifet, Holmes, Pfahringer, Kirkby, and Gavalda, 2009). In contrast, Klinkenberg and Joachims (2000) applies support vector machines trained in a sliding window. The window size is chosen such that the generalisation error, which is estimated as the average leave-one out error, is a minimum.

The model proposed here addresses changes in both class prior distributions and feature distributions. It applies to changes between two different time points and does not take gradual changes over a sequence of time points into account. The method handles data in batch mode, i.e. the instances are not processed upon arrival but all at once at a particular time point. The method is not designed for classic data stream processing. The proposed model thus addresses a shift of the distributions between training time and testing time. Owing to the assumed verification latency, new labelled data allowing for post-change re-estimation of the classification rule is not available. However, information on changes can be derived from existing unlabelled data.

Let Y ∈ {0, 1} denote the class label, and let 
                        X
                      ∈ Ω be the predictor variables where Ω denotes the feature space. In the present model Ω is considered to be static, even when the distribution in Ω changes over the course of time. ft
                     (
                        x
                     ) is the (unconditional) density of the predictors, pt
                      = pt
                     (1) = Pt
                     (Y = 1) = P(Y = 1|t) is the class prior probability of class y = 1 at time t. In particular, when the class is not explicit, the class prior distribution refers to class 1. ft
                     (
                        x
                      | y) is the feature distribution conditioned on classes, and Pt
                     (y | 
                        x
                     ) is the posterior probability. The index t indicates that the particular probability or density refers to the distribution at time t. Let t = 0 be the time of training, and t = 1 the time of prediction, i.e. the time at which the classification model is applied to new data drawn from the population at this time. The distribution ft
                     (
                        x
                     ) is assumed to be observable (in the form of unlabelled data) at the time at which the classification model is applied, whereas the posterior ft
                     (y | 
                        x
                     ), due to verification latency, is assumed to be unobservable. Below, f will denote the density of a continuous feature, although the model can also be extended to cope with categorical features. In contrast to Hofer and Krempl (2013), a multivariate approach is described here.

The population is assumed to be heterogeneous in the sense that it comprises homogeneous subpopulations where the latent variable forming the subpopulation need not coincide with the class variable Y. Depending on the statistical context the term homogeneity can have different meanings. Here, homogeneity denotes similarity with respect to the features, but not necessarily with respect to the class variable Y. As a consequence, subpopulations may exist which are not homogeneous with respect to the class variable Y, and thus cannot be classified solely by means of their feature distributions.

Thus, the conditional and the unconditional feature distributions are assumed to be mixture distributions. In particular, the conditional feature distribution ft
                        (
                           x
                        |y) of class y at time t has the form

                           
                              
                                 
                                    
                                       f
                                       t
                                    
                                    
                                       (
                                       x
                                       |
                                       y
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          K
                                          
                                             t
                                             y
                                          
                                       
                                    
                                    
                                       α
                                       
                                          i
                                          t
                                          y
                                       
                                    
                                    
                                    
                                       h
                                       
                                          i
                                          t
                                       
                                    
                                    
                                       (
                                       x
                                       |
                                       y
                                       )
                                    
                                    
                                    y
                                    =
                                    0
                                    ,
                                    1
                                    ,
                                 
                              
                           
                        where hit
                        (
                           x
                        |y) with i = 1, …, Kty
                         denoting the ith of the Kty
                         components of the mixture distribution for class y at time t, and αity
                         with i = 1, …, Kty
                         denoting the corresponding mixing proportions. Kty
                         ≥ 1 denotes the number of components. The model is not restricted to a particular mixture distribution. However, a convenient choice for continuous features would be a Gaussian mixture.

Thus, the unconditional feature distribution at time t is a mixture of the form

                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             
                                                f
                                                t
                                             
                                             
                                                (
                                                x
                                                )
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                p
                                                t
                                             
                                             
                                             
                                                f
                                                t
                                             
                                             
                                                (
                                                x
                                                |
                                                y
                                                =
                                                1
                                                )
                                             
                                             +
                                             
                                                (
                                                1
                                                −
                                                
                                                   p
                                                   t
                                                
                                                )
                                             
                                             
                                             
                                                f
                                                t
                                             
                                             
                                                (
                                                x
                                                |
                                                y
                                                =
                                                0
                                                )
                                             
                                             =
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                
                                                   L
                                                   t
                                                
                                             
                                             
                                                γ
                                                
                                                   j
                                                   t
                                                
                                             
                                             
                                                g
                                                
                                                   j
                                                   t
                                                
                                             
                                             
                                                (
                                                x
                                                )
                                             
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where gjt
                        (
                           x
                        ) with j = 1, …, Lt
                         denotes the jth component of the mixture. gjt
                        (
                           x
                        ) follows an appropriate distribution (in this paper the Gaussian distribution). Due to the above mentioned assumptions, the relation Lt
                         = ∑
                           y
                        
                        Kty
                         need not hold in general. In particular, if both classes have some components in common, the equality does not hold due to a sparse representation of ft
                        (
                           x
                        ).

Without loss of generality the subsequent description considers a shift from time point t = 0 to time point t = 1. Thus, the time index of the mixture components is omitted, i.e. h
                        
                           i0 = hi
                         and gj
                         = g
                        
                           j1. Correspondingly, α
                        
                           i0y
                         = αiy, γ
                        
                           i1 = γi, K
                        0y
                         = Ky
                        , and L
                        1 = L.

The model addresses two types of shift: the change of the class prior distribution (i.e. “global” shift) and the change of the feature distributions (i.e. “local” shift). The change of the class prior probability pt
                         = pt
                        (1) = P(Y = 1|t) from time t = 0 to time t = 1 is modelled as p
                        1 = δ 
                        p
                        0, where δ > 0 .

Local changes from t = 0 to t = 1 can affect the conditional feature distributions f
                        0(
                           x
                        |y), y = 0, 1 in two ways. On the one hand, the component parameters and the number of components may change. On the other hand, local changes can apply to the mixture proportions of the conditional feature distributions. However, modelling changes in the conditional feature distributions is affected by the lack of information resulting from verification latency. Due to verification latency the components gj
                        (
                           x
                        ), j = 1, …, L
                        1 of the unconditional distribution f
                        1(
                           x
                        ) in equation (1) are estimated from an incomplete dataset at time t = 1. Since this dataset comprises the features (predictors) but not the class variable Y, an explicit decomposition of f
                        1(
                           x
                        ) into the conditional distributions, as is required for classification, is impossible. However, the determination of the new conditional distributions f
                        1(
                           x
                        |y = 0) and f
                        1(
                           x
                        |y = 1) can be based on the components gj
                        (
                           x
                        ) j = 1, …, L of the unconditional distribution f
                        1(
                           x
                        ) and on a model of how these components relate to the old components hi
                        (
                           x
                        |y) for i = 1, …, Ky
                         and y = 0, 1 at time t = 0 (i.e. the last time point for which a full dataset is available). While this relation is described in the current section, the construction of the new conditional feature distributions is discussed in Section 3.3.

For modelling changes, let 
                           
                              
                                 π
                                 
                                    i
                                    j
                                 
                                 
                                    (
                                    1
                                    )
                                 
                              
                              ,
                              
                              
                                 π
                                 
                                    i
                                    j
                                 
                                 
                                    (
                                    0
                                    )
                                 
                              
                              ∈
                              
                                 [
                                 
                                 0
                                 ,
                                 
                                 1
                                 
                                 ]
                              
                              ,
                           
                         describe the amount of mass transferred from component hi
                        (
                           x
                        |y = 1) to gj
                        (
                           x
                        ), and hi
                        (
                           x
                        |y = 0) to gj
                        (
                           x
                        ), respectively. These parameters are subject to the constraints

                           
                              
                                 
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                L
                                             
                                             
                                                π
                                                
                                                   i
                                                   j
                                                
                                                
                                                   (
                                                   1
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             1
                                             
                                             ∀
                                             i
                                             =
                                             1
                                             ,
                                             …
                                             ,
                                             
                                             
                                                K
                                                1
                                             
                                             ,
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                L
                                             
                                             
                                                π
                                                
                                                   i
                                                   j
                                                
                                                
                                                   (
                                                   0
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             1
                                             
                                             ∀
                                             i
                                             =
                                             1
                                             ,
                                             …
                                             ,
                                             
                                             
                                                K
                                                0
                                             
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        Thus, taking the class prior probabilities and the mixing proportions into account, at time t = 1, component j may be determined as

                           
                              (2)
                              
                                 
                                    
                                       γ
                                       j
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          K
                                          1
                                       
                                    
                                    δ
                                    
                                       p
                                       0
                                    
                                    
                                    
                                       α
                                       
                                          i
                                          1
                                       
                                    
                                    
                                    
                                       π
                                       
                                          i
                                          j
                                       
                                       
                                          (
                                          1
                                          )
                                       
                                    
                                    +
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          K
                                          0
                                       
                                    
                                    
                                       (
                                       1
                                       −
                                       δ
                                       
                                       
                                          p
                                          0
                                       
                                       )
                                    
                                    
                                    
                                       α
                                       
                                          i
                                          0
                                       
                                    
                                    
                                    
                                       π
                                       
                                          i
                                          j
                                       
                                       
                                          (
                                          0
                                          )
                                       
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          
                                             K
                                             1
                                          
                                          +
                                          
                                             K
                                             0
                                          
                                       
                                    
                                    
                                       a
                                       
                                          i
                                          j
                                       
                                    
                                    ,
                                 
                              
                           
                        where

                           
                              
                                 
                                    
                                       a
                                       
                                          i
                                          j
                                       
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   δ
                                                   
                                                   
                                                      p
                                                      0
                                                   
                                                   
                                                   
                                                      α
                                                      
                                                         i
                                                         1
                                                      
                                                   
                                                   
                                                   
                                                      π
                                                      
                                                         i
                                                         j
                                                      
                                                      
                                                         (
                                                         1
                                                         )
                                                      
                                                   
                                                
                                             
                                             
                                                …
                                             
                                             
                                                
                                                   i
                                                   =
                                                   1
                                                   ,
                                                   …
                                                   ,
                                                   
                                                      K
                                                      1
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      (
                                                      1
                                                      −
                                                      δ
                                                      
                                                      
                                                         p
                                                         0
                                                      
                                                      )
                                                   
                                                   
                                                   
                                                      α
                                                      
                                                         i
                                                         0
                                                      
                                                   
                                                   
                                                   
                                                      π
                                                      
                                                         i
                                                         j
                                                      
                                                      
                                                         (
                                                         0
                                                         )
                                                      
                                                   
                                                
                                             
                                             
                                                …
                                             
                                             
                                                
                                                   i
                                                   =
                                                   
                                                      K
                                                      1
                                                   
                                                   +
                                                   1
                                                   ,
                                                   …
                                                   ,
                                                   
                                                   
                                                      K
                                                      1
                                                   
                                                   +
                                                   
                                                      K
                                                      0
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                    
                                    .
                                 
                              
                           
                        The model assumes that the transition of component i to component j does not modify the class membership of the component. Thus, if component i belongs to bad customers (i.e. positives) at time t = 0, and it evolves to component j at time t = 1, it still consists of bad customers.

The proposed shift model is flexible in the sense that it allows the components (i.e. subpopulations) to evolve differently over the course of time. In addition, the local drift is not restricted to a particular distribution or its parameters.

A classic maximum likelihood (ML) approach for parameter estimation yields a very complicated nonlinear maximisation problem under constraints that cannot be solved in a closed form. To reduce the computational effort, a stepwise parameter estimation is proposed here. This method combines a maximum likelihood estimation of the parameters of the mixture distributions with the minimisation of the distance between unconditional distributions. For this purpose, first, the parameters of the mixture distributions f
                        0(
                           x
                        |y) for y = 0, 1 and f
                        1(
                           x
                        ) are estimated by means of the EM algorithm using labelled data at t = 0 and unlabelled data at t = 1. Second, to find the remaining model parameters the distance between the unconditional distribution as given by the local drift model (see the mixture proportions in Eq. (2)) and the observed unconditional distribution f
                        1(
                           x
                        ) is minimised. This minimisation is carried out in a two-step estimation procedure: first, the distance between distributions is determined for varying values of δ; second, those parameters are chosen for which the distance between distributions is a minimum.

This approach has the advantage that the estimation procedure results in a well known and easily solvable optimisation problem that allows a very fast determination of the parameters. In addition, the values aij
                         determine the amount of positives and negatives in component j, and allow for a decomposition of the unconditional feature distribution at time t = 1 into the corresponding conditional feature distributions.

To measure the distance between distributions, the earth mover’s distance, EMD, is used. This distance has a nice geometric interpretation that fits the idea of modelling local changes as local displacement of probability mass. It measures the distance between two distributions as the amount of energy (costs) necessary to transport mass from one position to another one. It was first introduced by Rubner, Tomasi, and Gubias (2000, actually already first in a preprint as of September 1998 under the same title), who addressed the question of how to measure the distance between histograms, or more generally between signatures in image retrieval. Signatures can be defined as clusters of features and are more general than histograms for which the bins are fixed a-priori (Rubner et al., 2000). However, similar distances such as Mallow’s distance or the Wasserstein metric have been known since the work of Mallow (1972). A short overview and a discussion of the similarities of Mallow’s distance and the EMD can be found in Levina and Bickel (2001). See also Scobey and Kabe (1981) who showed an analogy between quadratic cost transportation problems and maximum likelihood estimation of regression coefficients.


                        Rubner et al. (2000) showed that the EMD is a metric when the compared signatures have the same weights and when the distance which the EMD is based on is a metric. According to the authors the major advantage of the EMD in comparison to other distances between histograms or signatures, such as the Kullback–Leibler divergence, is the fact that the EMD displays perceptual similarities since it allows for a cross-bin comparison. This aspect is nicely visualised in Fig. 1
                         in Rubner et al. (2000) and in Fig. 1 in Ling and Okada (2007). In addition, the EMD allows for partial matches, and it does not assume that the histograms and signatures have the same bins. Levina and Bickel (2001) showed that for distributions with equal masses Mallow’s distance and the EMD are the same.

To estimate the parameters of the shift adapted model the EMD is defined as

                           
                              
                                 
                                    EMD
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                
                                                   K
                                                   1
                                                
                                                +
                                                
                                                   K
                                                   0
                                                
                                             
                                          
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             L
                                          
                                          
                                             a
                                             
                                                i
                                                j
                                             
                                          
                                          
                                          
                                             d
                                             
                                                i
                                                j
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                
                                                   K
                                                   1
                                                
                                                +
                                                
                                                   K
                                                   0
                                                
                                             
                                          
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             L
                                          
                                          
                                             a
                                             
                                                i
                                                j
                                             
                                          
                                       
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          
                                             K
                                             1
                                          
                                          +
                                          
                                             K
                                             0
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          L
                                          1
                                       
                                    
                                    
                                       a
                                       
                                          i
                                          j
                                       
                                    
                                    
                                    
                                       d
                                       
                                          i
                                          j
                                       
                                    
                                 
                              
                           
                        where dij
                         is the base distance between component i and component j, and aij
                         the mass transferred. The denominator equals 1, since it represents the whole probability distribution.

The parameters aij
                         and δ are found by minimising EMD in a two step procedure. First, for any fixed δ > 0 for which p
                        0 
                        δ ∈ [0, 1], the earth mover’s distance, EMDmin(δ), is found by solving the minimisation problem

                           
                              
                                 
                                    
                                       
                                          
                                             EMD
                                             (
                                             δ
                                             )
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                min
                                                
                                                   a
                                                   
                                                      i
                                                      j
                                                   
                                                
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   
                                                      K
                                                      1
                                                   
                                                   +
                                                   
                                                      K
                                                      0
                                                   
                                                
                                             
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                L
                                             
                                             
                                                a
                                                
                                                   i
                                                   j
                                                
                                             
                                             
                                             
                                                d
                                                
                                                   i
                                                   j
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             s
                                             .
                                             t
                                             .
                                             
                                                5
                                                e
                                                x
                                             
                                             
                                                0
                                                e
                                                x
                                             
                                          
                                       
                                       
                                       
                                          
                                             
                                                30
                                                e
                                                x
                                             
                                             
                                                0
                                                e
                                                x
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             a
                                             
                                                i
                                                j
                                             
                                          
                                       
                                       
                                          ≥
                                       
                                       
                                          
                                             0
                                             
                                             ∀
                                             i
                                             ,
                                             j
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                L
                                             
                                             
                                                a
                                                
                                                   i
                                                   j
                                                
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                α
                                                
                                                   i
                                                   1
                                                
                                             
                                             
                                                p
                                                0
                                             
                                             δ
                                             
                                             ∀
                                             i
                                             =
                                             1
                                             ,
                                             …
                                             ,
                                             
                                                K
                                                1
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                L
                                             
                                             
                                                a
                                                
                                                   i
                                                   j
                                                
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                α
                                                
                                                   i
                                                   0
                                                
                                             
                                             
                                                (
                                                1
                                                −
                                                
                                                   p
                                                   0
                                                
                                                δ
                                                )
                                             
                                             
                                             ∀
                                             i
                                             =
                                             
                                                K
                                                1
                                             
                                             +
                                             1
                                             ,
                                             …
                                             ,
                                             
                                                K
                                                1
                                             
                                             +
                                             
                                                K
                                                0
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   
                                                      K
                                                      1
                                                   
                                                   +
                                                   
                                                      K
                                                      0
                                                   
                                                
                                             
                                             
                                                a
                                                
                                                   i
                                                   j
                                                
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                γ
                                                j
                                             
                                             
                                             ∀
                                             j
                                             =
                                             1
                                             ,
                                             …
                                             ,
                                             L
                                          
                                       
                                    
                                 
                              
                           
                        Since this optimisation problem is a transportation problem it can be solved quickly. In a second step, EMDmin(δ) is calculated for varying δ. Depending on the adjusted class prior probability with adjustment factor δ, the objective function value EMDmin(δ) measures the distance between the training distribution and the test distribution in terms of the energy (i.e. product of distance and transferred mass) required to obtain the testing distribution by local displacement of the training distribution. Thus, an estimate for δ is then derived as 
                           
                              δ
                              =
                              arg
                              
                                 min
                                 δ
                              
                              
                                 EMD
                                 min
                              
                              
                                 (
                                 δ
                                 )
                              
                           
                        .

The quality of the parameter estimation is influenced by the choice of the base distance dij
                        . In particular, any component is assumed to evolve preferably uniformly in the sense that splits are avoided if possible. This can be interpreted as different members of a subpopulation all having a similar development. However, the Euclidean distance between the centers of components need not necessarily satisfy this requirement. Since the objective function EMD is defined as the product of mass and distance, unwanted splits of components can happen, when some components move a large distance. However, this effect can be avoided by increasing the base distance dij
                         by

                           
                              
                                 
                                    
                                       f
                                       
                                          i
                                          j
                                       
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   
                                                      |
                                                   
                                                   
                                                      γ
                                                      j
                                                   
                                                   −
                                                   δ
                                                   
                                                   
                                                      p
                                                      0
                                                   
                                                   
                                                   
                                                      α
                                                      
                                                         i
                                                         1
                                                      
                                                   
                                                   
                                                      |
                                                   
                                                
                                             
                                             
                                                
                                                   i
                                                   =
                                                   1
                                                   ,
                                                   …
                                                   ,
                                                   
                                                      K
                                                      1
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      |
                                                   
                                                   
                                                      γ
                                                      j
                                                   
                                                   −
                                                   
                                                      (
                                                      1
                                                      −
                                                      δ
                                                      
                                                      
                                                         p
                                                         0
                                                      
                                                      )
                                                   
                                                   
                                                   
                                                      α
                                                      
                                                         i
                                                         0
                                                      
                                                   
                                                   
                                                      |
                                                   
                                                
                                             
                                             
                                                
                                                   i
                                                   =
                                                   
                                                      K
                                                      1
                                                   
                                                   +
                                                   1
                                                   ,
                                                   …
                                                   ,
                                                   
                                                      K
                                                      1
                                                   
                                                   +
                                                   
                                                      K
                                                      0
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                    
                                    .
                                 
                              
                           
                        Depending on the scale, a tuning parameter ψ is necessary, i.e. 
                           
                              
                                 d
                                 
                                    i
                                    j
                                 
                                 ′
                              
                              =
                              
                                 d
                                 
                                    i
                                    j
                                 
                              
                              +
                              ψ
                              
                              
                                 f
                                 
                                    i
                                    j
                                 
                              
                           
                         where 
                           
                              d
                              
                                 i
                                 j
                              
                              ′
                           
                         are the modified distances between component i and component j. This serves to penalise the transport of mass on a route where the demand deviates from supply. It thus prefers moving a component as a whole to another component even if the distances would suggest a splitting. Nevertheless, the model remains open to the merging as well as the splitting of components. In particular, the number of components at time t = 0 is not required to equal the number of components at time t = 1. The parameter ψ can be determined by cross-validation. The value of ψ is chosen for which the respective performance measure for classification takes a minimum.

No matter which base distance dij
                         between components is chosen, the question on how to determine this distance remains. The model on local changes is not restricted to a particular mixture of distributions. However, a Gaussian mixture turns out as convenient not only because standard tools for estimation are available but also because the parameters can directly be used to determine different distances between the components, in particular the Euclidean distance between the means. In addition, if the covariance matrix is included in the distance, the parameters of the Gaussian mixture also allow for simple calculation of the Bhattacharyya distance, the Mahalanobis distance, or the Kullback–Leibler distance. Cross-validation can be used to select a proper distance.

The major aim of the model on local and global changes is a shift adaptation of the classification rule making use of the limited information on the feature distribution at time t = 1. This can be achieved by splitting the mixing proportion γj
                         for j = 1, …, L into the parts resulting from the particular classes. For this purpose the proportions

                           
                              
                                 
                                    
                                       r
                                       
                                          1
                                          j
                                       
                                    
                                    =
                                    
                                       1
                                       
                                          δ
                                          
                                          
                                             p
                                             0
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          K
                                          1
                                       
                                    
                                    
                                       a
                                       
                                          i
                                          j
                                       
                                    
                                    
                                    
                                       r
                                       
                                          0
                                          j
                                       
                                    
                                    =
                                    
                                       1
                                       
                                          1
                                          −
                                          δ
                                          
                                          
                                             p
                                             0
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          
                                             K
                                             1
                                          
                                          +
                                          1
                                       
                                       
                                          
                                             K
                                             1
                                          
                                          +
                                          
                                             K
                                             0
                                          
                                       
                                    
                                    
                                       a
                                       
                                          i
                                          j
                                       
                                    
                                 
                              
                           
                        are determined for all components j = 1, …, L at time t = 1, where aij
                         are estimated by the transportation problem. r
                        1j
                         contains the probability mass that is transferred from class 1 components to component j, and r
                        0j
                         comprises probability mass of class 0 components. Since γj
                         = δ 
                        p
                        0 
                        r
                        1j
                         + (1 − δ 
                        p
                        0) r
                        0j
                         the unconditional feature distribution can be decomposed according to Eq. (1) into the conditional feature distributions at time t = 1 thus,

                           
                              
                                 
                                    
                                       
                                          
                                             
                                                f
                                                1
                                             
                                             
                                                (
                                                x
                                                |
                                                y
                                                =
                                                1
                                                )
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                L
                                             
                                             
                                                r
                                                
                                                   1
                                                   j
                                                
                                             
                                             
                                             
                                                g
                                                j
                                             
                                             
                                                (
                                                x
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                f
                                                1
                                             
                                             
                                                (
                                                x
                                                |
                                                y
                                                =
                                                0
                                                )
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                L
                                             
                                             
                                                r
                                                
                                                   0
                                                   j
                                                
                                             
                                             
                                             
                                                g
                                                j
                                             
                                             
                                                (
                                                x
                                                )
                                             
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        which now allows Bayes classification rule to be applied using shift adapted distributions.

Even though the proposed model does not assume a particular mixture distribution, it is primarily designed for continuous features. However, it is not restricted to such data. This is important since in practical applications categorical predictors are also involved. For example, if all predictors are categorical, the feature distribution can be chosen to be a multinomial mixture distribution and the proposed model can be applied in the usual manner. However, the base distance still has to be chosen appropriately. For example, mass shifted from category i to category j, may cause costs of dij
                         = 1, 
                           
                              i
                              ≠
                              j
                              ,
                           
                         but dii
                         = 0. Thus, the total amount of mass shifted to obtain the new unconditional distribution is minimised. Alternatively, the use of meta-information on how customers change a category can yield a more sophisticated distance, or the Kullback–Leibler divergence can be chosen.

When continuous and nominal features are available, a different approach is preferred. Let ϕ be any category derived as a combination of values of the nominal features. A sample from category ϕ and with continuous features 
                           x
                         is classified at time t according to Bayes classification rule

                           
                              
                                 
                                    
                                       
                                          
                                             
                                                max
                                                y
                                             
                                             
                                                P
                                                t
                                             
                                             
                                                (
                                                y
                                                |
                                                x
                                                ,
                                                ϕ
                                                )
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                max
                                                y
                                             
                                             
                                                
                                                   
                                                      f
                                                      t
                                                   
                                                   
                                                      (
                                                      x
                                                      |
                                                      y
                                                      ,
                                                      ϕ
                                                      )
                                                   
                                                   
                                                   
                                                      P
                                                      t
                                                   
                                                   
                                                      (
                                                      y
                                                      |
                                                      ϕ
                                                      )
                                                   
                                                   
                                                   
                                                      P
                                                      t
                                                   
                                                   
                                                      (
                                                      ϕ
                                                      )
                                                   
                                                
                                                
                                                   
                                                      P
                                                      t
                                                   
                                                   
                                                      (
                                                      x
                                                      ,
                                                      ϕ
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                max
                                                y
                                             
                                             
                                                f
                                                t
                                             
                                             
                                                (
                                                x
                                                |
                                                y
                                                ,
                                                ϕ
                                                )
                                             
                                             
                                             
                                                P
                                                t
                                             
                                             
                                                (
                                                y
                                                |
                                                ϕ
                                                )
                                             
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        
                        Pt
                        (y|ϕ) is the prior distribution with respect to category ϕ. Thus, in the presence of nominal features the model is applied within each category. The total class prior shift is then derived from

                           
                              
                                 
                                    
                                       p
                                       1
                                    
                                    
                                       (
                                       y
                                       )
                                    
                                    =
                                    
                                       ∑
                                       ϕ
                                    
                                    
                                       P
                                       1
                                    
                                    
                                       (
                                       y
                                       |
                                       ϕ
                                       )
                                    
                                    
                                    
                                       P
                                       1
                                    
                                    
                                       (
                                       ϕ
                                       )
                                    
                                    =
                                    
                                       ∑
                                       ϕ
                                    
                                    
                                       δ
                                       ϕ
                                    
                                    
                                    
                                       P
                                       0
                                    
                                    
                                       (
                                       y
                                       |
                                       ϕ
                                       )
                                    
                                    
                                    
                                       θ
                                       ϕ
                                    
                                    
                                    
                                       P
                                       0
                                    
                                    
                                       (
                                       ϕ
                                       )
                                    
                                 
                              
                           
                        as

                           
                              
                                 
                                    
                                       δ
                                       y
                                    
                                    =
                                    
                                       
                                          
                                             p
                                             1
                                          
                                          
                                             (
                                             y
                                             )
                                          
                                       
                                       
                                          
                                             p
                                             0
                                          
                                          
                                             (
                                             y
                                             )
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             ϕ
                                          
                                          
                                             δ
                                             ϕ
                                          
                                          
                                          
                                             θ
                                             ϕ
                                          
                                          
                                          
                                             P
                                             0
                                          
                                          
                                             (
                                             y
                                             |
                                             ϕ
                                             )
                                          
                                          
                                          
                                             P
                                             0
                                          
                                          
                                             (
                                             ϕ
                                             )
                                          
                                       
                                       
                                          
                                             ∑
                                             ϕ
                                          
                                          
                                             P
                                             0
                                          
                                          
                                             (
                                             y
                                             |
                                             ϕ
                                             )
                                          
                                          
                                          
                                          
                                             P
                                             0
                                          
                                          
                                             (
                                             ϕ
                                             )
                                          
                                       
                                    
                                    
                                 
                              
                           
                        for y = 0, 1, where δϕ
                         denotes the change of class prior distribution within category ϕ, and θϕ
                         denotes the change of the probability of category ϕ. The value of θϕ
                         can easily be derived using unlabelled data.

In the case where this approach requires a large number of models to be estimated, nominal features and their values can be combined. Such aggregation might be carried out with respect to the response variable. However, in practical applications some combinations of feature values will be more important than others such that for them explicit models must be estimated.

The performance of the shift model proposed is analysed using a real-world dataset on Danish tax defaults arising due to bankruptcy. The dataset, provided by the Danish tax authority SKAT, is a random sample of all Danish companies from the years 2006 to 2009. To obtain a more balanced dataset, i.e. one corresponding better to the ratio of marginal misclassification costs between classes (see Hand and Vinciotti, 2003), stratified random sampling (random in each class) was carried out such that the proportion of positives was 10 times higher in the sample (10.79 percent) than in the whole dataset (1.08 percent).

Negatives were defined as companies not defaulting in any of the five years, while bads were defined as companies in the year before default. Records of companies covering more than one year before default were excluded. The dataset is partitioned into training data and test data for each year resulting in four training datasets and four test datasets. All datasets are independent in the sense that any instance appears only in one dataset.

Among the many features, four categorical variables, and two continuous variables, were selected for experimental evaluation. Since the analysis aims at illustrating the performance of the shift model, the selection of the variables is rather arbitrary. However, in contrast to the paper on global drift (Hofer and Krempl, 2013), the features are selected such that they are subject to local changes. The continuous variables are transformed to the interval [0, 1] and then scattered by adding a random variable N(0, 0.3) to reach a high degree of anonymity. The nominal features which relate to legal status, region of operations and business branch were aggregated to form a new nominal feature such that combinations of those values of the given nominal features that are similar with respect to their probability of default were combined into one value. Such aggregation eliminated those combinations for which very few observations were available and resulted in one nominal feature with six different values. These values are denoted as groups below.


                        Table 1
                         shows the development of the class prior probability P(Y = 1) i.e. the probability of default in the full dataset, in the training dataset, and in the test dataset over the course of time. Based on the results of a proportions test the differences in the probability of default in the full dataset are significant (p-value < 0.01). Thus, the data reveal a significant increase of the probability of default during the years 2006 to 2008 and a significant decrease from 2008 to 2009. Due to the random sampling applied, the training data and the test dataset mirror the same change in the probabilities of default as the full dataset. However, as can be seen in Table 1, these probabilities are 10 times the probabilities found in the full dataset (see for example Hand and Vinciotti, 2003). However, sampling did not change the drift in the data.


                        Table 2
                         shows the evolution of the class prior probability P(Y = 1) within each group. Again, based on a proportions test (p-values < 0.02) the differences in the probabilities between two consecutive years are significant. Exceptions are only group 3 and 4 from 2008 to 2009 (p-values ≥ 0.16), and group 6 from 2008 to 2009 (p-value 0.07). In addition, the class prior probability grows in the years 2006 to 2008 and falls from 2008 to 2009, except for group 4. However, for this group the difference in the class prior probabilities in 2008 and 2009 is not significant (p-value 0.16).

The figures in Table 2 reflect the financial crisis that began in 2007 and that resulted in a significant increase of bankruptcies. Even though the crisis affected the various groups of firms differently with respect to the probability of default, none of them escaped the crisis. Unfortunately, the anonymisation of the data prohibits a precise and group-specific discussion.

The dataset not only reveals changes of the class prior distributions in the whole dataset and within the different groups. As can be seen from Figs. 1 and 2, it is also subject to local changes. Fig. 1
                         shows the univariate distributions f(x|y = 1) · P(Y = 1) and f(x|Y = 0) · P(Y = 0) of the first continuous variable within group 1 for the years 2006–2009. Local changes occur for both positives and negatives. However, even though the changes of the conditional distributions are not very large, they are significant for the positives, except for 2006–2007 (p-value of the Kolmogorov–Smirnov test < 0.01), but not for the negatives (p-values of the Kolmogorov-Smirnov test > 0.40). In particular, the main component of positives moves to the right over the course of time. As a consequence, the optimal decision boundary moves to the right from 2006 to 2009. Since the positives are located on the left-hand side and since the class prior probability for positives increases up to more than 0.50, the false negative rate would increase when the decision rule estimated in previous years is applied instead of a drift adapted one.

Similarly, Fig. 2 shows the distributions f(x|Y = 1) · P(Y = 1) and f(x|Y = 0) · P(Y = 0) for group 3 in 2006 (left) and 2008 (right). The figures show not only local displacement of the components within the classes, but also an increase of the class prior probability of positives. These changes obviously influence the classification results.

Since the data are subject to global and local drift, i.e. the class prior distribution and the feature distribution change over the course of time, the data are appropriate for investigating the performance of the proposed model. The effect of verification latency of different durations is investigated in such a way that a classification model is estimated from training data in year i, where i = 2006, 2007, 2008 and then applied to the test data in subsequent years j = i + 1, …, 2009. This results in six test situations where, in three of them, the test distribution mirrors changes that happened within a one-year timespan, in two of them within a 2-year shift, and in one within a 3-year shift.

Classification is carried out for each of the 6 groups that were derived from the nominal features. The performance of the shift adapted Gaussian mixture (EMD-based) model as proposed in this paper is compared to the following models:

                           
                              (a)
                              Gaussian mixture model (GM)

Global drift model (Global)

Logistic regression (LogR)

Bayesian classification for Gaussian classes (BG)

Linear feature shift model (LFS)

Importance weighting (Imp)

The conditional feature distributions in the Gaussian mixture model (GM) are not only used for classification but also to estimate the EMD-based model. Applying the GM to the test data in the subsequent years demonstrates the deterioration of the classification results due to global and local changes, and the extent to which the results can be improved by the EMD-based model. The number of components in the mixture distribution is determined by means of the Bayesian information criterion (BIC).

The global drift model as proposed by Hofer and Krempl (2013) assumes that the conditional feature distributions do not change. This assumption does not hold in general for the present dataset. Thus, the global drift model is used to show that an adaptation of the classification rule only with respect to the class prior probabilities need not be sufficient to improve classification results. As in Hofer and Krempl (2013), the estimated change of the class prior probability is used to adapt the results of a weighted logistic regression accordingly. In addition, the results of the weighted logistic regression (LogR) without the global drift adaptation are calculated. The weights in the logistic regression account for the imbalanced dataset.

Further models are: Bayesian classification for Gaussian classes (BG) and the linear feature shift model (LFS) as proposed by Biernacki et al. (2002). LFS assumes that ft
                        (
                           x
                        |y) ∼ N(
                           μ
                        
                        
                           ty
                        , 
                           Σ
                        
                        
                           ty
                        ) and that the conditional feature distributions between different time points are linked by a univariate linear transformation of the features such that 
                           X
                        
                        1 = D
                        
                           y
                        
                        
                           X
                        
                        0 within class y. Dy
                         for y = 0, 1 are diagonal matrices. The unknown class priors and the matrices D
                        
                           y
                         are determined by maximum likelihood estimation. This model does not account for changes that affect subpopulations of a class differently.

Finally, a weighted logistic regression is used where the weights account for covariate shift. These importance weights w(
                           x
                        ) = f
                        1(
                           x
                        )/f
                        0(
                           x
                        ), where ft
                        (
                           x
                        ) denotes the unconditional feature distributions at time t = 0 and t = 1, are determined by the Kullback–Leibler importance estimation procedure proposed by Sugiyama et al. (2008a); Sugiyama et al. (2008b). The weights are expressed in terms of a linear model of Gaussian basis functions and are determined using the new unlabelled data at time t = 1. Although each instance in the training dataset is weighted, this method only accounts for covariate shift that assumes constant posterior distributions. This model does not provide an estimate for the change of the class prior probability.

The quality of classification is measured by four characteristics:

                           
                              (a)
                              false positive rate FP

false negative rate FN

total classification error F
                              

Cohan’s kappa κ
                              

In addition, the EMD-based model, the global drift model and the LFS model also estimate the change of the class prior probability. These changes are compared to the true changes using a proportions test.


                        Tables 3
                         and 4 show the performance of the models considered. Column ‘Train’ denotes the time point of model estimation, and column ‘Test’ the time points of model application. G represents the group for which the model is estimated. δ in Table 3
                         denotes the true change of the class prior, and 
                           
                              δ
                              ^
                           
                         the corresponding estimate.

As can be seen in Table 3, in almost all test situations where the EMD-based model is used, no significant differences can be found between estimated changes in class priors and their true changes (p-value > 0.05). This is true no matter whether the class prior increases, as was the case between 2006 and 2008, or whether it falls, as was mainly true from 2008 to 2009. With respect to the factor δ, it initially appears that the estimated differences from the true value were only significant for group 4 for period 2006–2008. However, a proportions test yielded a p-value of 0.0521, while the p-value in Table 3 is rounded to 0.05. Thus, the EMD-based model turned out to be able to reliably estimate the change of the class prior in test situations where the conditional feature distributions are subject to local changes. In contrast, when LFS is used, class prior changes were often significantly different from the true value, and were more often too low than too large.

The global drift model that is designed for test situations where the class prior probability has changed, yielded estimates for δ that were significantly different from the true value in most test situations. This is not surprising since in almost all test situations the conditional distributions of at least one feature and in at least one class changed significantly (p-value of KS test < 0.05). In Table 3 a change of at least one feature is denoted by ° in class 0, and by * in class 1. Thus, the major assumption of the global drift model that conditional distributions remain constant, did not hold. While in this situation any estimate can be obtained by the global drift model, it is expected to work when the assumption holds. From 2006 to 2007 no significant changes of the conditional distributions were detected in group 3. In this test situation the estimate, 
                           
                              
                                 δ
                                 ^
                              
                              ,
                           
                         of the global drift model did not deviate significantly from the true value. In fact, the local changes increased over the course of time.


                        Table 4 compares the models with respect to their classification performance. For columns with pairs of models such as GM/EMD-based, the results of the first model are found in the grey row of a particular test situations, and the results of the second model (i.e. a shift adapted version) are displayed in the white row below. For example, when GM is estimated in group 1 using 2006 data and is then applied to 2007 data, FN is 0.14. When a shift adaptation by the EMD-based model is carried out, FN falls to 0.08. Similarly, the FN rate of 0.10 of LogR can be reduced to 0.08 when the global drift model is applied.

The EMD-based model which is a shift adapted version of GM improved the false negative rate FN in all classification problems compared to GM. However, in many test situations these improvements were achieved at the cost of a higher false positive rate FP than in the GM model, even though in cases of local changes FP does not necessarily increase when FN decreases.

A proportion test is carried out to compare FN and FP of the EMD-based model to the respective values of the global drift model. The bold figures in columns GM/EMD-based in Table 4 indicate when the EMD-based model is superior to the global drift model. Figures in columns LogR/Global, BG/LFS, or Imp are displayed as bold when they are significant compared to the EMD-based model.

While the EMD-based model is the superior model in general, the differences in FN between the EMD-based model and the global drift model are often not significant, especially in group 3 from 2006 to 2007. In this test situation the model assumption of the global drift model holds. In those cases where the global drift model has a significantly smaller FN than the EMD-based model, we find that other criteria (FP and/or κ) may be worse. Similar considerations hold for the few significant results of LFS. In addition, in group 5 from 2006 to 2007 FN is lower for Imp than for the EMD-based model (p-value 0.06). Imp assumes that the posterior probabilities remain constant. However, this assumption does not hold here (p-value of KS test < 0.01).

The questions which model to choose and, in particular, whether shift adaptation is desirable or not depend on the respective misclassification costs, given that the model assumptions are satisfied. Such costs reflect the fact that an imbalance exists between the economic effect of non-detected defaults (FN) and undetected non-defaults (FP). Nondetected default usually induces much higher costs than undetected non-default. As discussed in Hofer and Krempl (2013) the overall misclassification loss L = c
                        0 (1 − p) FP + c
                        1 
                        p FN defined by Hand (2009) can serve as a cost function. Here, c
                        0 and c
                        1 are the misclassification cost for class 0 and 1, respectively, and p represents the class prior of class 1. According to this criterion, a model is preferred when its overall misclassification loss is lower than that of the alternative model.

For demonstration, let FN and FP denote the false negative rate and the false positive rate for the EMD-based model, and 
                           
                              
                                 FN
                                 ˜
                              
                              ,
                           
                        
                        
                           
                              FP
                              ˜
                           
                         the respective values for an alternative model. The EMD-based model is preferred when

                           
                              
                                 
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   
                                                      c
                                                      1
                                                   
                                                   >
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            (
                                                            1
                                                            −
                                                            p
                                                            )
                                                         
                                                      
                                                      
                                                         p
                                                      
                                                   
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               FP
                                                               ˜
                                                            
                                                            −
                                                            FP
                                                         
                                                      
                                                      
                                                         
                                                            FN
                                                            −
                                                            
                                                               FP
                                                               ˜
                                                            
                                                         
                                                      
                                                   
                                                   
                                                   
                                                      c
                                                      0
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                   
                                                      for
                                                      
                                                   
                                                   FN
                                                   <
                                                   
                                                      FN
                                                      ˜
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      c
                                                      1
                                                   
                                                   <
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            (
                                                            1
                                                            −
                                                            p
                                                            )
                                                         
                                                      
                                                      
                                                         p
                                                      
                                                   
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               FP
                                                               ˜
                                                            
                                                            −
                                                            FP
                                                         
                                                      
                                                      
                                                         
                                                            FN
                                                            −
                                                            
                                                               FN
                                                               ˜
                                                            
                                                         
                                                      
                                                   
                                                   
                                                   
                                                      c
                                                      0
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                   
                                                      for
                                                      
                                                   
                                                   FN
                                                   >
                                                   
                                                      FN
                                                      ˜
                                                   
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        This criterion is obtained from solving the inequality for the overall misclassification loss with respect to c
                        1.

For example, consider the model estimation in 2008 and model application in 2009 for group 4. In this situation FN is 0.29 for the global drift model and 0.39 for the EMD-based model. Ignoring for a moment that the model assumption for the global drift model is not satisfied (see also the estimate of the class prior change), these figures would suggest that the global drift model is preferred to the EMD-based model due to the importance of detecting negatives. ‘Importance’ is evaluated by the cost of non-detection. However, the EMD-based model might be preferred even though FN is larger than for the global drift model, and even though c
                        1 > c
                        0. In particular, this is the case when the cost in class 1 is at most 14 times higher than the cost in class 0. In this case the overall misclassification loss will be smaller for the EMD-based model than for the global drift model, even though FN is significantly smaller for the global drift model.

However, in addition to cost considerations, model selection also needs to take account of the model assumptions. In this example, the univariate conditional distributions of both continuous features in both classes changed significantly (p-values of KS test < 0.01). Even though changes may be significant, they do not necessarily affect classification results and the estimation of the class prior change. However, the detected changes are a strong indicator that the global drift model is inappropriate in addressing the changes occurring.

With respect to Cohan’s κ the EMD-based model was in most cases superior. In group 4 the amount of defaults was very small and it was hard for the models to detect the positives. The results also show that the performance of the various methods is weaker when an adaptation over a relatively large period has to be carried out, as is the case for the time span between 2006 and 2009. While the differences of FN between a one-year latency and a 3-year latency are not significant for the EMD-based model, the LFS model has proven to be more sensitive since it only captures changes in terms of linear univariate transformations of features (p-value of Wilcoxon test 0.04). These transformations are the same for all subgroups within a class. This reaction to the amount of change was also strong for Imp, a model that only takes covariate shifts into account. Thus, the difference of the FN between a 1-year latency and a 3-year latency is significant (p-value of Wilcoxon test 0.01).

@&#CONCLUSION@&#

A major assumption of regression or statistical classification is that the distributions at the time of model application equal the distributions at the time of model training. However, when a population evolves over the course of time, this assumption does not hold. As a consequence, the prediction model will not be optimal for the test data distribution and thus its performance will deteriorate over the course of time. Even though nonstationary distributions make the re-estimation of the prediction model necessary, in many applications, such as credit scoring, this is impossible due to the lack of new labelled data, a phenomenon denoted as verification latency or label delay.

In the presence of verification latency methods that allow for adapting a prediction model using only unlabelled data are highly desirable. In the present paper a shift adaptation model for binary classification is presented. The model is based on a mixture distribution of the conditional feature distribution at the time where labelled data are available, and on the unconditional feature distribution at the time where new unlabelled data are accessible. These mixture distributions provide information on the old and the new positions of subpopulations. A transition model then describes how the subpopulations of each class drifted to form the new unconditional feature distribution.

The conditional distributions are assumed to be reorganised using a minimum of energy. This yields a two-step procedure for estimating the shift. First, for a given class prior distribution the transfer of probability mass is estimated such that the product of distance covered and of mass transferred is a minimum. This corresponds to minimising the energy required to obtain the new unconditional distribution in that transfer of the old conditional distributions occurs locally. The optimal solution of the resulting transportation problem measures the distance between the old and the new distributions, and it can be used as an estimate of the local change.

In a second step, the change of the class prior distribution is found by solving the transportation problem for varying class prior distributions and selecting the values for which the objective function takes a minimum. Since the solution of the transportation problem yields the amount of probability mass that originates in the particular classes, the unconditional feature distribution can be decomposed into the conditional feature distributions. These new conditional feature distributions and the new class prior distribution allow for an update of the classification rule.

Using a large real-world dataset the performance of the proposed shift model is demonstrated. The results are compared to those of a weighted logistic regression, to an importance weighted logistic regression, to a Bayesian classification for Gaussian classes and to a Gaussian feature shift model. The analysis showed that shift-adapted Gaussian mixture model was able to reduce the classification error. In addition, the estimated change of the class prior was close to the true change.

@&#ACKNOWLEDGEMENTS@&#

Great thanks go to Mads Krogh Nielsen MSc and the Danish tax authority SKAT, and to Georg Krempl for providing the anonymised data concerning credit defaults in Danish companies. The author also thanks the anonymous reviewers for their valuable suggestions.

@&#REFERENCES@&#

