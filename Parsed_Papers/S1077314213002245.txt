@&#MAIN-TITLE@&#Object detection based on spatiotemporal background models

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose two types of spatiotemporal background models (i.e., SLDP and StSIC).


                        
                        
                           
                           They integrate multiple different modeling approaches into a single framework.


                        
                        
                           
                           SLDP models an illumination-invariant local feature in a statistical framework.


                        
                        
                           
                           StSIC models spatiotemporal similarity of intensity changes among the pixels.


                        
                        
                           
                           Experimental results show our models are robust against various background changes.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Background model

Object detection

Illumination changes

Dynamic changes

Background Models Challenge

@&#ABSTRACT@&#


               
               
                  We present a robust background model for object detection and its performance evaluation using the database of the Background Models Challenge (BMC). Background models should detect foreground objects robustly against background changes, such as “illumination changes” and “dynamic changes”. In this paper, we propose two types of spatiotemporal background modeling frameworks that can adapt to illumination and dynamic changes in the background. Spatial information can be used to absorb the effects of illumination changes because they affect not only a target pixel but also its neighboring pixels. Additionally, temporal information is useful in handling the dynamic changes, which are observed repeatedly. To establish the spatiotemporal background model, our frameworks model an illumination invariant feature and a similarity of intensity changes among a set of pixels according to statistical models, respectively. Experimental results obtained for the BMC database show that our models can detect foreground objects robustly against background changes.
               
            

@&#INTRODUCTION@&#

One of the fundamental problems in computer vision is detecting regions or objects of interest from an image sequence. Background subtraction, which removes a background image from the input image, is widely used for detecting foreground objects in practical applications because it enables us to detect foreground objects without any prior knowledge. However, when background subtraction is applied to outdoor surveillance, the “long shot” scenes of cameras often include not only foreground objects but also background changes related to illumination conditions or disturbances in the scenes because the cameras are often installed in high locations to obtain a large field of view. In general, background changes that occur in outdoor scenes can be classified into two types; typical examples are shown in Fig. 1
                     .
                        
                           •
                           
                              Illumination changes – changes relating to lighting conditions such as the sun rising, setting, or being blocked by clouds (see Fig. 1(a)),


                              Dynamic changes – changes relating to the swaying motion of tree branches, leaves and grass, fleeting cloud, waves on water and so on (see Fig. 1(b)).

To robustly detect foreground objects, we need to be able to handle these background changes. Many researchers have proposed background modeling approaches for dealing with these effects [1,2].

Because illumination changes affect not only a target pixel but also its neighboring pixels, local feature-based approaches that use this characteristic have been proposed to cope with illumination changes [3–7]. The Local Binary Pattern (LBP) [5,6] is a well-known local feature for background modeling. The LBP is defined by the signed differences between a target pixel and neighboring pixels. The LBP is unaffected by local intensity changes caused by illumination changes because it is a binary pattern describing lower or higher intensity relations between neighboring pixels. The distance to a neighboring pixel depends on the scene context and should be decided on a case-by-case basis. The Radial Reach Filter (RRF) [7] extends the LBP to adaptively determine the distance. Both approaches assume that local features are unaffected by background changes. However, surveillance scenes also often include dynamic changes that significantly affect the local features in the background. It is therefore difficult for local feature-based background models to handle dynamic changes in the background.

To cope with dynamic changes, statistical methods [8–13] have been used. In these approaches, the background is modeled by a probability distribution of the previously observed intensity values of each pixel. Background pixel values are usually observed with higher probabilities if we assume foreground objects are moving. When we use multiple distributions for the pixels, we can treat multi-modal backgrounds caused by dynamic changes. A Gaussian mixture model is used to represent the multiple distributions in the literature [8,9]. Non-parametric statistical methods [10–13] that use kernel density estimation have also been proposed. However, it is difficult for statistical background models to handle illumination changes, which vary intensity values rapidly and significantly.

Hybrid methods [14–16], which use multiple different background models, have also been proposed. To avoid falsely classifying the object regions as background, Yoshimura et al. [14] used a local feature-based background model in addition to a model focused on each pixel, and combined the results using a logical OR operation. In contrast, to cope with both illumination and dynamic changes in the background, Shimada et al. [15] and Tanaka et al. [16] used both local feature-based and statistical background models, and combined the results using a logical AND operation. However, the methods cannot adapt to particular regions that are affected by both illumination and dynamic changes at the same time because they assume at least one of the background models employed by the hybrid methods can adapt to background changes correctly. These methods are a kind of tandem system, and a logical combination of the detection results does not improve the accuracy of the foreground detection. Zhao et al. [17] used a local feature defined by multiple point pairs that exhibit a stable statistical intensity relationship as a background model. However, their method is not suitable for online surveillance because it needs to scan the entire input sequence to analyze the stability between point pairs.

To solve these problems, we integrate the methodologies of statistical and local feature-based approaches into a single framework. In this paper, we propose two types of spatiotemporal background modeling frameworks suitable for outdoor surveillance.
                        1
                        Our target scenes are mainly “long shot” scenes in the outdoors, and our proposed method is not intended for “close-up shot” scenes in which a foreground object is very large.
                     
                     
                        1
                      The first type [18,19] is based on a spatiotemporal local feature, where a statistical framework is introduced for an illumination-invariant local feature. The second type is based on a spatiotemporal feature, where similarity of intensity changes among a set of pixels is modeled using a statistical framework. By considering the similarity of intensity changes among the pixels, the second type can use the spatiality to handle not only illumination but also dynamic changes, while the first model uses spatial information only in defining an illumination-invariant feature to adapt to illumination changes. Our background modeling frameworks have properties of both statistical and local feature-based approaches because they use spatiotemporal information. Therefore, our spatiotemporal models can adapt to various background changes, even if some regions are affected by different types of background changes at the same time. To verify the effectiveness of our approaches, we report evaluation results obtained using the database of the Background Models Challenge (BMC
                        2
                        1st ACCV Workshop on Background Models Challenge: http://bmc.univ-bpclermont.fr/.
                     
                     
                        2
                     ).

We present a spatiotemporal background model by applying a statistical framework to a local feature-based approach [18,19]. In practice, we apply a Gaussian mixture model (GMM) to a local feature called the local difference (LD) to get a statistical local feature called the statistical local difference (SLD). Finally, we define the statistical local difference pattern (SLDP) [18,19] for the background model using several SLDs.

In most cases where illumination changes, there are small changes in the difference between a target pixel and its neighboring pixel because the values of pixels in a localized region increase or decrease proportionally. Owing to the invariance of the difference value with respect to illumination changes, the SLDP has the ability to tolerate the changes as shown in Fig. 2
                     (a) because it uses the difference value as a local feature. Furthermore, our proposed method can also cope with dynamic changes because the SLDP can learn the variety of the changes as shown in Fig. 2(b). This is because a GMM, which can handle a multi-modal background, is applied to the LD, which is an important component of the SLDP. Thus, our background model can combine the concepts of statistical and local feature-based approaches into a single framework.

A target pixel and its neighboring pixel in an observed image are described by the vectors 
                           
                              
                                 
                                    p
                                 
                                 
                                    c
                                 
                              
                              =
                              
                                 
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          c
                                       
                                    
                                    ,
                                    
                                       
                                          y
                                       
                                       
                                          c
                                       
                                    
                                    )
                                 
                                 
                                    T
                                 
                              
                           
                         and 
                           
                              
                                 
                                    p
                                 
                                 
                                    j
                                 
                              
                              =
                              
                                 
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                    ,
                                    
                                       
                                          y
                                       
                                       
                                          j
                                       
                                    
                                    )
                                 
                                 
                                    T
                                 
                              
                           
                        , respectively. 
                           
                              f
                              (
                              p
                              )
                           
                         represents the image intensity at pixel 
                           
                              p
                           
                        . We can then define the LD as 
                           
                              
                                 
                                    X
                                 
                                 
                                    j
                                 
                              
                              =
                              f
                              (
                              
                                 
                                    p
                                 
                                 
                                    c
                                 
                              
                              )
                              -
                              f
                              (
                              
                                 
                                    p
                                 
                                 
                                    j
                                 
                              
                              )
                           
                        . In cases where illumination changes occur, the changes in the LD are small because the pixels in the localized region show a similar change. Therefore, the value of LD is stable under the illumination changes as shown in Fig. 2(a).

We apply a GMM to the LD to represent probability density functions (PDF) for the LD. This gives the SLD. We define the SLD 
                           
                              P
                              (
                              
                                 
                                    X
                                 
                                 
                                    j
                                 
                                 
                                    t
                                 
                              
                              )
                           
                         (PDF for LD) at time t by
                           
                              (1)
                              
                                 P
                                 (
                                 
                                    
                                       X
                                    
                                    
                                       j
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          m
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 
                                    
                                       w
                                    
                                    
                                       j
                                       ,
                                       m
                                    
                                    
                                       t
                                    
                                 
                                 η
                                 (
                                 
                                    
                                       X
                                    
                                    
                                       j
                                    
                                    
                                       t
                                    
                                 
                                 |
                                 
                                    
                                       μ
                                    
                                    
                                       j
                                       ,
                                       m
                                    
                                    
                                       t
                                    
                                 
                                 ,
                                 
                                    
                                       Σ
                                    
                                    
                                       j
                                       ,
                                       m
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 ,
                              
                           
                        
                     

where 
                           
                              
                                 
                                    w
                                 
                                 
                                    j
                                    ,
                                    m
                                 
                                 
                                    t
                                 
                              
                              ,
                              
                                 
                                    μ
                                 
                                 
                                    j
                                    ,
                                    m
                                 
                                 
                                    t
                                 
                              
                           
                         and 
                           
                              
                                 
                                    Σ
                                 
                                 
                                    j
                                    ,
                                    m
                                 
                                 
                                    t
                                 
                              
                           
                         are the weight, the mean and the covariance matrix of the mth Gaussian in the mixture at time t respectively, and 
                           
                              η
                           
                         is the Gaussian probability density. We construct the background model by updating the GMM (SLD). The updating method for the GMM is based on the method proposed by Shimada et al. [9].
                           3
                           This updating method [9] enables us to automatically control M (the number of Gaussian distributions) in response to background changes. In particular, for the pixels that observe background changes, M increases with the addition of new distributions. Conversely, for stable pixels whose intensity values are constant for a while, M decreases with the elimination or integration of the distributions. We can then achieve high accuracy with low computational cost compared with the conventional approach [8].
                        
                        
                           3
                         The SLD can handle dynamic background changes because its GMM can learn the variety of background hypotheses as shown in Fig. 2(b).

In our method, each pixel has a pattern of the SLD in the background model; i.e., the SLDP [18,19]. The SLDP at time t is defined as 
                           
                              
                                 
                                    S
                                 
                                 
                                    t
                                 
                              
                              =
                              {
                              P
                              (
                              
                                 
                                    X
                                 
                                 
                                    1
                                 
                                 
                                    t
                                 
                              
                              )
                              ,
                              …
                              ,
                              P
                              (
                              
                                 
                                    X
                                 
                                 
                                    j
                                 
                                 
                                    t
                                 
                              
                              )
                              ,
                              …
                              ,
                              P
                              (
                              
                                 
                                    X
                                 
                                 
                                    N
                                 
                                 
                                    t
                                 
                              
                              )
                              }
                           
                         using a target pixel 
                           
                              
                                 
                                    p
                                 
                                 
                                    c
                                 
                              
                           
                         and N neighboring pixels 
                           
                              
                                 
                                    p
                                 
                                 
                                    j
                                 
                              
                           
                         that radiate out from 
                           
                              
                                 
                                    p
                                 
                                 
                                    c
                                 
                              
                           
                        . Here, N represents the number of neighboring pixels (Fig. 2 shows an example for 
                           
                              N
                              =
                              6
                           
                        ). Note that all the neighboring pixels lie on a circle with radius r centered at a target pixel 
                           
                              
                                 
                                    p
                                 
                                 
                                    c
                                 
                              
                           
                        .

Foreground detection using the SLDP involves a voting method that judges whether a target pixel 
                           
                              
                                 
                                    p
                                 
                                 
                                    c
                                 
                              
                           
                         belongs to the background or the foreground. When the pattern of N LDs is given as 
                           
                              
                                 
                                    D
                                 
                                 
                                    t
                                 
                              
                              =
                              {
                              
                                 
                                    X
                                 
                                 
                                    1
                                 
                                 
                                    t
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    X
                                 
                                 
                                    j
                                 
                                 
                                    t
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    X
                                 
                                 
                                    N
                                 
                                 
                                    t
                                 
                              
                              }
                           
                        , foreground detection based on SLDP is decided according to
                           
                              (2)
                              
                                 Φ
                                 (
                                 
                                    
                                       p
                                    
                                    
                                       c
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   background
                                                
                                                
                                                   if
                                                   
                                                   
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            j
                                                            =
                                                            1
                                                         
                                                         
                                                            N
                                                         
                                                      
                                                   
                                                   ϕ
                                                   (
                                                   
                                                      
                                                         D
                                                      
                                                      
                                                         j
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         S
                                                      
                                                      
                                                         j
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   )
                                                   ⩾
                                                   
                                                      
                                                         T
                                                      
                                                      
                                                         B
                                                      
                                                   
                                                   ,
                                                
                                             
                                             
                                                
                                                   foreground
                                                
                                                
                                                   otherwise
                                                   ,
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

where 
                           
                              
                                 
                                    T
                                 
                                 
                                    B
                                 
                              
                           
                         is a threshold for determining whether a target pixel 
                           
                              
                                 
                                    p
                                 
                                 
                                    c
                                 
                              
                           
                         belongs to the background or the foreground. In Eq. (6), 
                           
                              ϕ
                              (
                              
                                 
                                    D
                                 
                                 
                                    j
                                 
                                 
                                    t
                                 
                              
                              ,
                              
                                 
                                    S
                                 
                                 
                                    j
                                 
                                 
                                    t
                                 
                              
                              )
                           
                         is a function that returns 0 or 1, depending on whether the LD 
                           
                              
                                 
                                    X
                                 
                                 
                                    j
                                 
                                 
                                    t
                                 
                              
                           
                         matches the SLD 
                           
                              P
                              (
                              
                                 
                                    X
                                 
                                 
                                    j
                                 
                                 
                                    t
                                 
                              
                              )
                           
                         at time t. For further details, we refer the reader to the literature [9].

In Section 2, we presented a spatiotemporal background model (SLDP) by applying a statistical framework to a local feature-based approach. In case of the SLDP, the spatial information is used only to define local features to adapt to illumination changes. However, the spatial information is also useful in adapting to dynamic changes more robustly. Therefore, in this section, we propose another spatiotemporal background modeling, where the spatial information is used to adapt not only to illumination changes but also to dynamic changes. In other words, region-level statistical information, instead of pixel-level statistical information, is introduced.

Previous statistical approaches [8–13] model the background using a probability distribution of the previously observed pixel values for each pixel (i.e., at the pixel level). Such pixel-level statistical background models cannot adapt to illumination changes that are not observed in the previous frames, and they also have difficulty handling heavy dynamic changes that result in complicated distributions far from usual dynamic changes. To solve these problems, we define a spatiotemporal background model by considering pixel clusters where all the pixels are close to one another and are similar in intensity change as shown in Fig. 3
                     . The spatiotemporal background model can then adapt to dynamic changes and illumination changes robustly as follows.
                        
                           
                              Adaptivity to dynamic changes: Each of such clusters usually consists of pixels that have a similar characteristic of dynamic changes because the characteristic of each dynamic change is associated with the place (e.g., when trees shiver in the wind, the sway of leaves causes similar changes in neighboring pixels of the trees.). Modeling the observation probability of features of the pixels within each cluster then allows the estimation of the effects of dynamic changes in each pixel, and therefore, the model can adapt to heavy dynamic changes more robustly. This is why spatial information is useful for adapting to heavy dynamic changes robustly.


                              Adaptivity to illumination changes: By referring to similarity of intensity changes among the pixels within such clusters, we can easily identify whether each pixel of the same cluster belongs to foreground objects or the background (illumination changes) as shown in Fig. 3. This is because, in cases of illumination changes that affect not only the target pixel but also its neighboring pixels, the relationships among the pixels belonging to the same cluster are maintained essentially constant. Therefore, a feature, such as a difference among pixel values, is not affected by illumination changes, even if illumination changes affect the pixel values significantly. This is why spatial information is effective for adapting to illumination changes robustly.

Using the spatial characteristics described above, we propose the spatiotemporal background modeling framework. In the rest of this section, we explain how to cluster the pixels and model the spatial characteristics.

It is noted that scene points having similar surface normals show similar intensity changes against a smoothly moving distant light source. Koppal et al. divided a complex scene into geometrically consistent clusters (scene points that have the same or very similar surface normals) irrespective of their material properties and lighting [20]. The results of the literature [20] at least show that the pixels that are positionally and chromatically close to each other belong to the same cluster. Therefore, we assume that the pixels that are geographically and chromatically close to one another show similar changes against illumination changes. Under this assumption, we classify pixels into several clusters, in each of which all the pixels show similar changes against illumination changes. We employ K-means clustering to acquire the clusters.

For the K-means algorithm, we use a chromatic and positional feature. We assume that, at the beginning, we can have a background image in which any foreground object does not show up. The chromatic and positional feature is defined as the quintet 
                           
                              F
                              =
                              {
                              Nx
                              ,
                              Ny
                              ,
                              NY
                              ,
                              NU
                              ,
                              NV
                              }
                           
                        , where Nx and Ny are the image coordinates normalized by the image size, and 
                           
                              NY
                              ,
                              NU
                           
                         and NV are the normalized pixel values in YUV color space. Using this feature, we classify the pixels into several clusters as shown in Fig. 4
                        . Fig. 4 at least shows that each cluster contains the pixels that belong to the same part of the scene context (e.g., grass, road, sky, and walls of the buildings). Then, according to the literature [20], we regard that the pixels belonging to the same cluster show similar changes against illumination changes.

We consider a target pixel at 
                           
                              (
                              x
                              ,
                              y
                              )
                           
                         belonging to a certain cluster 
                           
                              C
                           
                        , and define an illumination invariant feature 
                           
                              X
                              =
                              {
                              D
                              ,
                              U
                              ,
                              V
                              }
                           
                         in YUV color space. Here, D is the difference between the target pixel intensity Y and the representative intensity of its cluster 
                           
                              
                                 
                                    Y
                                 
                                 
                                    r
                                 
                              
                           
                        . We use a median intensity of the cluster as its representative intensity 
                           
                              
                                 
                                    Y
                                 
                                 
                                    r
                                 
                              
                           
                        . We can then estimate the PDF 
                           
                              P
                              (
                              
                                 
                                    X
                                 
                                 
                                    t
                                 
                              
                              )
                           
                         at time t by kernel density estimation (KDE) using the S past samples. The proposed method employs a fast algorithm [12]
                        
                           4
                           In the case of the parameter settings used for the experiments (
                                 
                                    S
                                    =
                                    250
                                 
                               and 
                                 
                                    h
                                    =
                                    9
                                 
                              ), the computation using the rectangular kernel [12] is about 3 times faster than that using a conventional Gaussian kernel [10].
                        
                        
                           4
                         of PDF estimation, which uses a rectangular function (Parzen window) as the kernel function W, instead of the Gaussian function that is often used in KDE.
                           
                              (3)
                              
                                 W
                                 (
                                 u
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         1
                                                      
                                                      
                                                         
                                                            
                                                               h
                                                            
                                                            
                                                               d
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   if
                                                   
                                                   |
                                                   u
                                                   |
                                                   ⩽
                                                   
                                                      
                                                         h
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                   ,
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   otherwise
                                                   ,
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where h is a parameter representing the width of the kernel (i.e., some smoothing parameter) and d is the dimension of the color space. Using this kernel, the PDF is represented as
                           
                              (4)
                              
                                 P
                                 (
                                 
                                    
                                       X
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       S
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          S
                                       
                                    
                                 
                                 W
                                 
                                    
                                       
                                          |
                                          
                                             
                                                X
                                             
                                             
                                                t
                                             
                                          
                                          -
                                          
                                             
                                                X
                                             
                                             
                                                t
                                                -
                                                i
                                             
                                          
                                          |
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

By accumulating the PDF of the pixels belonging to the cluster 
                           
                              C
                           
                        , we also estimate the PDF of the cluster as
                           
                              (5)
                              
                                 
                                    
                                       P
                                    
                                    
                                       C
                                    
                                 
                                 (
                                 
                                    
                                       X
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       S
                                       |
                                       C
                                       |
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          e
                                          =
                                          1
                                       
                                       
                                          |
                                          C
                                          |
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          S
                                       
                                    
                                 
                                 W
                                 
                                    
                                       
                                          |
                                          
                                             
                                                X
                                             
                                             
                                                t
                                             
                                          
                                          -
                                          
                                             
                                                X
                                             
                                             
                                                e
                                             
                                             
                                                t
                                                -
                                                i
                                             
                                          
                                          |
                                       
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              |
                              C
                              |
                           
                         is the number of pixels belonging to the cluster 
                           
                              C
                           
                        . We construct the background model by updating two PDFs (i.e., 
                           
                              P
                              (
                              
                                 
                                    X
                                 
                                 
                                    t
                                 
                              
                              )
                           
                         and 
                           
                              
                                 
                                    P
                                 
                                 
                                    C
                                 
                              
                              (
                              
                                 
                                    X
                                 
                                 
                                    t
                                 
                              
                              )
                           
                        ), and call this model “Spatiotemporal Similarity of Intensity Changes (StSIC).” The updating method for the PDF is based on a fast computation method proposed by Tanaka et al. [12]. Foreground detection using StSIC uses a threshold:
                           
                              (6)
                              
                                 Ψ
                                 (
                                 
                                    
                                       X
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   background
                                                
                                                
                                                   if
                                                   
                                                   P
                                                   (
                                                   
                                                      
                                                         X
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   )
                                                   ⩾
                                                   
                                                      
                                                         T
                                                      
                                                      
                                                         S
                                                      
                                                   
                                                   
                                                   or
                                                   
                                                   
                                                      
                                                         P
                                                      
                                                      
                                                         C
                                                      
                                                   
                                                   (
                                                   
                                                      
                                                         X
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   )
                                                   ⩾
                                                   
                                                      
                                                         T
                                                      
                                                      
                                                         S
                                                      
                                                   
                                                   ,
                                                
                                             
                                             
                                                
                                                   foreground
                                                
                                                
                                                   otherwise
                                                   ,
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    T
                                 
                                 
                                    S
                                 
                              
                           
                         is a threshold for determining whether the target pixel 
                           
                              (
                              x
                              ,
                              y
                              )
                           
                         belongs to the background or the foreground.

StSIC is robust against dynamic changes because StSIC can learn the variety of changes that are observed repeatedly. By referring to the PDF of the cluster, StSIC can also adapt to changes happening around a target pixel. Furthermore, StSIC is robust against illumination changes that affect not only a target pixel but also its neighboring pixels. This is because StSIC classifies the pixels into several clusters so that the pixels belonging to the same cluster show similar changes against illumination changes. Therefore, StSIC can tolerate the effects of both illumination and dynamic changes by modeling the difference between the target pixel intensity and the representative intensity of its cluster.

@&#EVALUATION@&#

We evaluated our spatiotemporal background models (i.e., the SLDP and StSIC) on the database provided for the BMC. The human-annotated ground truth is also available for all videos and is used for performance evaluation. Thus, exhaustive competitive comparison of methods is possible on this database.

All videos were processed with a unique set of parameters that are tuned according to 10 synthetic videos for the learning phase. The parameters of the SLDP are radial distance 
                           
                              r
                              =
                              20
                           
                        , the number of neighboring pixels 
                           
                              N
                              =
                              6
                           
                         and the detection threshold for the SLDP 
                           
                              
                                 
                                    T
                                 
                                 
                                    B
                                 
                              
                              =
                              5
                           
                        . Although the details of the GMM are not explained in Section 2.2, we give the parameter settings in the GMM for reproducibility as a learning rate 
                           
                              α
                              =
                              0.01
                           
                        , initial weight 
                           
                              W
                              =
                              0.05
                           
                         and threshold of choosing the background model 
                           
                              T
                              =
                              0.7
                           
                        .

The parameters of StSIC are the number of clusters 
                           
                              K
                              =
                              imagesize
                              /
                              35
                              ×
                              35
                           
                         (e.g., if 
                           
                              imagesize
                              =
                              320
                              ×
                              240
                           
                        , then 
                           
                              K
                              =
                              62
                           
                        ), the number of past samples 
                           
                              S
                              =
                              250
                           
                         and the size of the rectangular kernel 
                           
                              h
                              =
                              9
                           
                        . We use the first image of each sequence as the background image.

To evaluate the effectiveness of the statistical and local feature-based approaches respectively, we compared the performance of foreground detection with five different background models: two statistical models [9,13], a local feature-based model and two hybrid models [15,16].
                           
                              
                                 GMM 
                                 [9] removes the local feature-based framework from the SLDP, and is consistent with a statistical approach using a Gaussian mixture model.


                                 KDE 
                                 [13] is a statistical model based on KDE and employs shadow elimination, and is similar to StSIC without spatial information.


                                 Adaptive RRF is our revised version of a conventional local feature-based approach using radial reach correlation (RRC) [7], and a scheme
                                    5
                                    RRC [7] is defined as the magnitude relations between a target pixel and its neighboring pixels, and foreground objects are detected according to whether the current magnitude relationship is similar to that of the background. In Adaptive RRF, the distances of a target pixel and its neighboring pixels are updated according to the background changes so that the RRC can adapt to the temporal changes of the background.
                                 
                                 
                                    5
                                  updating the length of the reach, which defines the locality of the feature, is introduced.


                                 STLBP&GMM 
                                 [15] is a hybrid model, which extracts the regions detected as foreground by both local feature-based (STLBP) and statistical (GMM) models.


                                 KDE&ARRF 
                                 [16] is a hybrid model, which extracts the regions detected as foreground by both statistical (KDE) and local feature-based (Adaptive RRF) models, and also compensates the background images when significant illumination changes are observed.


                        Tables 1 and 2
                        
                         show evaluation results on the BMC database including 10 synthetic and nine real videos for the evaluation phase. As shown in Tables 1 and 2, except for real video 002, our spatiotemporal background models achieve an F-measure that is similar to or higher than that of the existing methods. To demonstrate the effectiveness of our approaches, we also present examples of foreground detection results in Fig. 5
                        : “Street 312” is a sunny scene where the illumination condition changes, “Street 512” is a windy scene where tree leaves flutter in the wind, “Rotary 422” is a foggy scene where fog is coming in and “Real Applications 008” is a real scene with both illumination and dynamic background changes.


                        Fig. 5(a) and (c) shows that the statistical methods (i.e., GMM [9] and KDE [13]) reveal many false-positive pixels that are affected by illumination changes and by the fog, and Table 1 shows their precision values are low for “Street 312” and “Rotary 422.” In contrast, the other models (Adaptive RRF, STLBP&GMM, KDE&ARRF, SLDP and StSIC) revealed few false-positive pixels. This is because these methods use a spatial relationship among the pixels, which is not significantly affected by illumination changes or the fog. These results are typical evidence of the effectiveness of spatial information, such as a local feature, regarding illumination changes that affect a target pixel value in proportion with others. We also see that Adaptive RRF falsely detects the movement of tree leaves from Fig. 5(b), and its corresponding precision value is low (see Table 1). Meanwhile, the other models (GMM, KDE, STLBP&GMM, KDE&ARRF, SLDP and StSIC) can detect the foreground objects robustly against the movement of tree leaves. Modeling temporal information using a statistical framework, these methods can tolerate the effects of dynamic changes. These results are typical evidence of the effectiveness of temporal information, which can be modeled by a statistical framework, regarding dynamic background changes.

In cases of real scenes, illumination changes and dynamic changes are often observed at the same time as shown in Fig. 5(d) and (e). Then, both statistical (GMM and KDE) and local feature-based (Adaptive RRF) methods cannot adapt to such scenes because they falsely detect regions affected by illumination changes and dynamic changes, respectively (see the sky and the grass in Fig. 5(d) and (e)). If illumination changes and dynamic changes affect different regions as shown in Fig. 5(d), the hybrid methods (STLBP&GMM and KDE&ARRF) can adapt to background changes as is the cases in the synthetic videos discussed above. However, in cases where illumination and dynamic changes affect the same regions at the same time, hybrid methods also falsely detect such regions as shown in Fig. 5(e). This is because the hybrid background models assume that at least one of their multiple different models can adapt to background changes, but their assumption does not hold when the regions are affected by different types of background changes (e.g., illumination changes and dynamic changes) at the same time. Conversely, the SLDP and StSIC use spatiotemporal information by integrating a statistical approach and a local feature-based approach into a single framework. This is why, if any regions are affected by multiple different background changes at the same time, the SLDP and StSIC can detect foreground objects robustly against such regions. As a result, the precision values of the SLDP and StSIC are much higher than those of computational methods on “Real Applications 008” in Table 2.

We discuss the limitation of our spatiotemporal background models. Our models assume that illumination changes affect all the pixels in a particular localized region: an area within radius r for the SLDP and within a cluster for StSIC. Therefore, our models can absorb the effects of illumination changes that affect the localized region entirely. At the same time, there is a problem that our models confuse particular foreground objects with the background-affected illumination changes especially in the close-up shot scenes. In the case of the SLDP approach, it is difficult to detect an object with a uniform texture on the uniform texture background because the SLDP uses the difference between a target pixel and its neighboring pixels. In the case of StSIC, the same problem occurs for foreground objects that entirely cover a cluster and that are similar in color to the cluster. This is why the recall of our methods is sometimes low compared with the existing methods, especially on “Real Applications 002” (see Table 2). However, these mis-detections can be reduced using a post processing such as Graph-cuts.

As discussed in Section 4.3, the locality controls the adaptivity of our spatiotemporal background models to illumination changes. This problem is closely related to the locality of each method. In the SLDP approach, the locality can be controlled by the radial distance r. The bigger r becomes, the weaker the adaptivity of the SLDP to illumination changes becomes. However, when the radial distance r is small, the SLDP often confuses foreground objects with the background as discussed in Section 4.3. Additionally, the SLDP cannot change the size of locality depending on the scene context. Conversely, StSIC controls its locality indirectly by the number of clusters K. In other words, StSIC can use appropriate localities depending on the scene context, and this is an advantage of StSIC. Therefore, StSIC can adapt to illumination changes and fog more robustly compared with the SLDP. These findings are confirmed from “Street 312, 412” and “Rotary 322, 422” in Table 1, in which the precision of StSIC is higher than that of the SLDP.

Furthermore, StSIC can use spatial information to adapt not only to illumination changes but also to dynamic changes, and this is an advantage of StSIC. This is why StSIC can also adapt to dynamic changes more robustly than the SLDP. This is confirmed from “Street 512” and “Rotary 522” in Table 1, in which the precision of StSIC is higher than that of the SLDP.

@&#CONCLUSION@&#

In this paper, we have proposed spatiotemporal background models (i.e., the SLDP and StSIC) by integrating the concepts of a local feature-based approach and a statistical approach into a single framework. Our spatiotemporal background models can adapt to both illumination and dynamic changes in the background. This is because our methods effectively model both spatial information and temporal information, which are useful in tolerating the effects of illumination and dynamic changes in the background respectively. The SLDP uses illumination-invariant local features that have the ability to tolerate the effects of illumination changes, and describes their distribution by GMMs that can learn the variety of the dynamic background. StSIC models the intensity change similarity among the pixels, which is effective in adapting to both dynamic and illumination changes robustly, using the KDE. As a result of evaluation on the BMC database, we have confirmed that the SLDP and StSIC can detect the foreground objects robustly against various background changes. In our future work, using other object detection databases in addition to the BMC database, we will rigorously evaluate our models and show statistical significance according to a statistical test.

@&#REFERENCES@&#

