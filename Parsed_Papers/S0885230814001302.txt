@&#MAIN-TITLE@&#Experimenting a discriminative possibilistic classifier with reweighting model for Arabic morphological disambiguation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We perform Arabic morphological disambiguation on unlabeled vocalized corpora.


                        
                        
                           
                           We experiment possibilistic measures for imprecise morphological data classification.


                        
                        
                           
                           We assess the impact of a reweighting model and a possibilistic lexical likelihood.


                        
                        
                           
                           Possibilistic classification is accurate in modern and classical texts disambiguation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Morphological analysis

Morphological disambiguation

Discriminative possibilistic classifier

Reweighting model

@&#ABSTRACT@&#


               
               
                  In this paper, we experiment a discriminative possibilistic classifier with a reweighting model for morphological disambiguation of Arabic texts. The main idea is to provide a possibilistic classifier that acquires automatically disambiguation knowledge from vocalized corpora and tests on non-vocalized texts. Initially, we determine all the possible analyses of vocalized words using a morphological analyzer. The values of their morphological features are exploited to train the classifier. The testing phase consists in identifying the accurate class value (i.e., a morphological feature) using the features of the preceding and the following words. The appropriate class is the one having the greatest value of a possibilistic measure computed over the training set. To discriminate the effect of each feature, we add the weights of the training attributes to this measure. To assess this approach, we carry out experiments on a corpus of Arabic stories and on the Arabic Treebank. We present results concerning all the morphological features and we discern to which degree the discriminative approach improves disambiguation rates and extract the dependency relationships among the features. The results reveal the contribution of possibility theory for resolving ambiguities in real applications. We also compare the success rates in modern versus classical Arabic texts. Finally, we try to evaluate the impact of the lexical likelihood in morphological disambiguation.
               
            

Many applications in the field of Arabic Natural Language Processing (ANLP) need to deal with the complex morphology of this language. Morphological analysis and disambiguation is an important step in Automatic Speech Recognition (ASR) (Diehl et al., 2012; Kirchhoff et al., 2006), Arabic text phonetization (El-Imam, 2004) and summarization (Azmi and Al-Thanyyan, 2012). Besides, information access applications need to index documents and extract relevant features about their meaningful entities (Bounhas et al., 2011b; Elayeb, 2009; Elayeb et al., 2009, 2011, 2014). Indeed, Information Retrieval and Knowledge Extraction Systems (IRKES) require recognizing useful entities in texts such as words, expressions and concepts. The basic level concerns the structure of words; i.e., the morphological level. Indeed, a given word may have many interpretations at this level, what is called morphological ambiguity. This phenomenon is more challenging with morphologically rich languages such as Arabic (Diab et al., 2004). Thus, a non-vocalized Arabic word may have more than 12 interpretations (Habash and Rambow, 2007; Habash et al., 2009b).

In this paper, we study existent morphological disambiguation approaches applied for Arabic texts. Then, we present our framework, which allows to, automatically, learn contextual knowledge required for disambiguation from vocalized texts. This framework tries to avoid the limits of existent systems, which require manually encoded knowledge or labeled corpora. It is also an attempt to consider Arabic classical texts; because most of the existent tools were trained and assessed on modern corpora (cf. Section 1.2). Another important concern is lexical likelihood, which differs from one type of text to another. This issue is carefully studied in this paper; we examine, therefore, the effect of this factor on morphological disambiguation. Our framework is illustrated through examples (cf. Section 3) and assessed through experimental results (cf. Section 4). Indeed, the basic version of our possibilistic classifier was presented in two conference papers (Ayed et al., 2012a,b). This new contribution stands out by the following aspects.

First, we present a reweighting model which tries to evaluate the discriminative power of attributes. We also take into account the discriminative power of the values of each attribute. Indeed, it is the first time that the necessity measure is being used in possibilistic classification, as the state-of-the art possibilistic classifier used only the possibility measure (Haouari et al., 2009). We also propose a new version of information entropy adapted for attribute reweighting in imprecise data. In the whole, we obtain six different classifiers (combining possibility, necessity and entropy). Besides, we compute, in the training phase, the lexical likelihood to take into account the dependencies between a given word and its features.

Second, we fully re-experiment these classifies, thus assessing the impact of these discriminative weights and the lexical likelihood. In addition, this paper is a fully revised version which provides a more detailed interpretation of results. In fact, we employ the Wilcoxon Matched-Pairs Signed-Ranks Test (Demsar, 2006) to assess our results, besides computing the disambiguation rates.

Finally, the experiments in Ayed et al. (2012a,b) were performed in a non-standardized traditional corpus. In this paper, we assess our model on the Arabic Treebank, thus showing its performance in a modern standard corpus.

Morphological analysis tools, like Hajic's analyzer (Hajic, 2000), allow to recognize the stem of a given word and its flectional marks. The analyzer interprets a given word out of context and returns a set of possible solutions (analyses), each having different morphological features.
                           1
                        
                        
                           1
                           POS (verb, noun, particle, etc.), number (singular, dual or plural), gender (male or female), voice (active or passive), mode of the verb (indicative, subjunctive, etc.), person (first, second or third), aspect (perfect or imperfect), etc.
                         A word is ambiguous if it has more than one solution. Disambiguation is the task of choosing, among these solutions, the most appropriate given context of the word (Ayed et al., 2012a, 2014a,b). However, this task is not easy to achieve, because of the complexity of the Arabic language morphology (Kirchhoff et al., 2006).

We analyze the main sources of ambiguities in the Arabic language and their consequences as follows. In fact, this language is agglutinative, derivational and inflectional. For example, the word “وضوء” (wDw′) may be analyzed as “وُضُوء” (wuDuw′: ablution),“وَضُوء” (waDuw′: water for ablution) or 
                           
                         (Dw′: light). In this example, the letter “و” is interpreted either as a conjunction or as the first letter of the lemma. Even in the second case, we obtain two possible lemmas diacriticized differently. In fact, the main source of ambiguity is the lack of diacritics in most existing Arabic texts. Morphological ambiguities make it difficult to extract simple terms, because the morphological analyzers enumerate for each word many possible lemmas (Bounhas et al., 2011b).

Morphological ambiguities affect the other levels of analysis and mislead the results of IRKES. At the syntactic level, it is hard to identify the grammatical function of a word in a sentence. For example, the expression “
                           
                        ” may be interpreted as 
                           
                         (a whole sentence meaning “The man searched”), where the first word (بَحَثَ, searched) is the verb of the sentence. It may be also read as: 
                           
                         
                        
                           
                         (an annexation compound noun meaning “The research of the man”). In the same way, the structure of sentences or expressions in Arabic may affect morphological disambiguation. We mainly talk about a commonly recognized phenomenon in Arabic texts, which is called the “free word order” (Attia, 2008). For example, the previous expression (i.e., 
                           
                        ) may be replaced by 
                           
                         without changing the meaning. However, this may mislead morphological disambiguation tools as we will see in the experimental results (Section 4.2). At the semantic level, the first example (i.e., “وضوء” (wDw′)) shows that a word may have several meanings according to its morphological interpretation. From these examples, it is clear that shorts vowels (diacritics) have a great importance in understanding the grammatical category, the function and the meaning of words. Thus, vocalized texts are less ambiguous than non-vocalized ones.

To disambiguate Arabic texts at the morphological level, we may exploit linguistic and/or statistical knowledge. Linguistic approaches employ rules written by linguists to label the morphological features. In related literature, we find approaches based on heuristics, contextual and non-contextual rules (cf. Daoud, 2009; Daoud and Daoud, 2009; Othman et al., 2004 for more reading). For example, Daoud (2009) and Daoud and Daoud (2009) proposed to use Universal Networking Language (UNL) and EnCo
                           2
                        
                        
                           2
                           
                              http://libraries.unl.edu/.
                        ; a rule-based programming language specialized for the writing of EnConverters (i.e., parsers) to define disambiguation rules. They define several types of rules modeling morphological and syntactic contextual dependencies. However, it is hard to assess the coverage, the reusability and the accuracy of such an approach, especially because the authors did not present any experimental evaluation. In the same perspective, Othman et al. (2004) used a syntactic analyzer to resolve ambiguities in the morphological level. Purely linguistic approaches are fast, efficient and reliable compared to statistical ones (Daoud and Daoud, 2009). However, many of these approaches require syntactic knowledge, while Arabic syntactic analysis tools are not yet mature. Although a lot of work has been done in this field (e.g., Attia, 2008), there are no available tools to exploit in an efficient manner. Besides, the complexity of Arabic texts is hard to handle with rigid grammar formalisms, thus it is required to model several degrees of closeness between tokens and contextual information or to use fuzzy knowledge.

To model fuzziness in contextual relations, Support Vector Machines (SVM) (Vapnik, 1999) and several probabilistic models (e.g., Markov Models) are used for Arabic and other languages (Diab et al., 2004; Khoja, 2001; Kirchhoff et al., 2006; Merialdo, 1994; Nguyen and Vogel, 2008; Zitouni and Sarikaya, 2009). While purely linguistic approaches need only the manual intervention of a linguist, approaches using statistical information require a training phase which may be performed in monolingual or bilingual corpora. Thus, some researchers tried to exploit the robustness of morphological taggers in other languages to resolve ambiguities in Arabic texts. For example, Nguyen and Vogel (2008) proposed to use the GIZA++ tool to align an English–Arabic bilingual corpus at the words’ level. However, we must assess the accuracy of the alignment process, which is not guaranteed. As example of approach using a monolingual tagged corpus, we may cite Khoja (2001) and Diab et al. (2004). The former computes two probabilities: (a) the lexical probability: the probability of attributing a given morphological feature (a tag) to a word; and (b) the contextual probability: the probability that a given tag follows another one. From these probabilities, grammatical rules are defined and the whole system reaches more than 90% of accuracy. However, such a tool is a tagger aiming to identify the grammatical categories of words (Par-Of-Speech, POS). Some other tools allow only restoring diacritics for non-vocalized texts. For example, Zitouni and Sarikaya (2009) used Maximum Entropy Models to perform this task.

IRKES and ASR need much more information than POS and diacritics, thus covering the other morphological features. MADA (Alkuhlani et al., 2013; Habash and Rambow, 2005, 2007; Roth et al., 2008) is a first attempt for full Arabic morphological analysis and disambiguation, providing rich information about words. It is a unique tool of its type using a SVM classifier, but it was trained on modern texts. For example, it has been integrated in the ASR tool of Diehl et al. (2012).

Although a great effort is being done in the field of Arabic morphological analysis and disambiguation, we feel yet the need of new approaches insuring more coverage with less effort in the learning step. Indeed, existing approaches need manually encoded linguistic knowledge or tagged corpora. Manual intervention is hard and time consuming, especially with the richness of Arabic language whose words have several features (cf. Section 2.1). It is also difficult to ensure the coverage of linguistic rules or the tagged corpora. Thus, it is necessary to develop automatic approaches allowing learning from large textual collections. Besides, it is crucial to consider classical Arabic texts, which represent an important side of the Arabic civilization and the content of the actual Web.

We present an automatic approach for learning morphological feature dependencies which are used to disambiguate Arabic texts. The approach avoids the manual intervention of the user on the learning step by exploiting vocalized texts which are less ambiguous. We model the disambiguation task as a classification problem as implemented in many state-of-the-art systems (Diab et al., 2004; Habash and Rambow, 2005, 2007; Khoja, 2001; Roth et al., 2008). For example, to solve the ambiguity of the grammatical category (POS), we define, first, the appropriate attributes that describe each instance. The POS of a word is closely related to the features of its preceding and following words. We specify a window that controls the number of words (before and after) considered as attributes describing the class of an instance. In many existent approaches, the size of the window is 2 (e.g., in Habash and Rambow, 2005). Thus, to classify the POS of a specific word w, we define the attributes POS−2, POS−1, POS+1 and POS+2 if the window's size is 2. They indicate, respectively, the POS of the two preceding and the two following words of the word w. We also use the other morphological features of the preceding and the following words to determine the class value of the current word (i.e., its POS). Hence, the first step is to provide the different possible solutions and attributes of words in vocalized texts and non-vocalized texts in input.

Unlike existent tools (Diab et al., 2004; Habash and Rambow, 2005, 2007; Khoja, 2001; Roth et al., 2008) which learn from tagged corpora, we build our classifier from untagged vocalized texts. This learning method is widely used for unsupervised disambiguation, because it is less-consuming (Niu et al., 2007; Seo et al., 2004). In our case, the context used to disambiguate a given word is itself ambiguous, thus needing to treat this task as a case of imprecision. Because probabilistic are not suitable to deal with such type of data, we are based on possibility theory which applies, naturally, to this kind of imperfection. To the best of our knowledge, the only operational possibilistic classifier was developed by Haouari et al. (2009). Thus, we adopt possibility theory, as it is the only framework suitable for imprecision treatment, and we present an enhanced version of this classifier (Alkuhlani et al., 2013; Bounhas et al., 2013, 2014). Consequently, our work is the first of its kind which applies this theory for morphological disambiguation.

Morphological analysis tools (e.g., Hajic (2000)), try to identify the different features of a given word out of context. In this work, we use an updated version of BAMA 1.2.1 (Buckwalter, 2004) (AraMorph), which can interpret vocalized texts. AraMorph identifies the grammatical categories
                           3
                        
                        
                           3
                           
                              http://www.nongnu.org/aramorph/english/grammatical_categories.html.
                         of the prefixes, the suffixes and the stem of each word. We use the same 14 morphological features of MADA (Habash and Rambow, 2005, 2007; Roth et al., 2008), which are: POS, conjunction, particle, determiner, pronoun, person, voice, aspect, gender, number, case, preposition, mode and adjective (Ayed et al., 2012b, 2014a,b). Conjunction, particle, determiner, preposition and adjective have binary values indicating if a word contains or not, respectively, a conjunction, a particle, a determiner, a preposition or an adjective. The value of a feature which starts with the “N” means that this word does not contain it. POS, pronoun, person, voice, aspect, gender, number, case and mode provide the values of the corresponding features (Ayed et al., 2012a,b, 2014a,b).

We prepare data by defining, for each vocalized word, an instance extracted from its morphological analysis. An instance is described by a list of attribute values (e.g., POS−2, POS−1, pronoun+2, etc.). Finally, possibility distributions (see Section 2.2.1) are computed for all the available instances (as discussed in Section 2.3). A subset of the generated test collection is used for training and the remaining instances are exploited to assess the accuracy of our classifiers according to the cross-validation method.

Possibility Theory was inspired by Gaines and Kohout (1975), invented in 1977 by Zadeh (1978) and developed by several authors (Dubois and Prade, 1985, 1998). It is devoted to deal with incomplete information specifically uncertain and imprecise (Alkuhlani et al., 2013; Bounhas et al., 2013, 2014). Unlike the probability theory, possibility theory differentiates between uncertainty and imprecision. There is imprecision whenever a reality state is equivocal i.e., it is described by a multi-valued propositional variable (fuzzy set (Zadeh, 1978)) (Ayed et al., 2012a). Uncertainty is a further information that supports the fact of not knowing or providing a statement to determine the truth value of a proposal (probability) (Dubois and Prade, 2000, 2010; Jaynes and Bretthorst, 2003). Indeed, the morphological ambiguities are cases of imprecision. Then, to disambiguate a given word, we use the features of the preceding and following words which are themselves ambiguous (i.e., multi-valued propositional variables). In the following sub-sections, we present the main concepts of possibility theory. For more details, we refer to Dubois and Prade (1985, 1998).

We denote Ω
                           ={ω
                           1, ω
                           2, …, ω
                           
                              n
                           } the universe of discourse that characterizes a set of activities (i.e., states of the real world). We designate π a possibility distribution. It represents a mapping from Ω to an ordered scale (i.e., the interval [0,1]). The possibility degree is the affected value. The possibility distribution provides a thorough knowledge about the actual state ω
                           
                              i
                           . It distinguishes what is plausible from what is less plausible. If a possibility degree is equal to 0, the state ω
                           
                              i
                            is revealed as impossible. The state becomes absolutely possible once the degree is equal to 1.

We determine the degree of plausibility and certainty of a state by calculating two measures which are: possibility and necessity. We designate A a subset of states counted in the universe of discourse Ω. We describe the possibility measure of A, given a possibility distribution π (defined on Ω), as follows:
                              
                                 (1)
                                 
                                    
                                       Π
                                       (
                                       A
                                       )
                                       =
                                       
                                          
                                             max
                                          
                                          
                                             ω
                                             ∈
                                             A
                                          
                                       
                                       π
                                       (
                                       ω
                                       )
                                    
                                 
                              
                           
                        

We present the necessity measure (N) which is derived from the possibility measure:
                              
                                 (2)
                                 
                                    
                                       N
                                       (
                                       A
                                       )
                                       =
                                       
                                          
                                             min
                                          
                                          
                                             ω
                                             ∉
                                             A
                                          
                                       
                                       
                                          
                                             1
                                             −
                                             π
                                             (
                                             ω
                                             )
                                          
                                       
                                       =
                                       1
                                       −
                                       Π
                                       (
                                       
                                          A
                                          ¯
                                       
                                       )
                                    
                                 
                              
                           
                        


                           
                              
                                 
                                    A
                                    ¯
                                 
                              
                            symbolizes the complement of A, i.e., the elements of Ω that do not belong to the event A. Π(A) assesses the consistence degree of the event A with the knowledge π whereas N(A) appraises to what extent A is certainly inferred by the knowledge represented by π. It defines the degree to which we expect the occurrence of the event A (Dubois and Prade, 1985).

An object or a case is described by the values of a given set of attributes. Classifiers ascribe a class, from a predefined set, to this object or case considering the values of attributes. There are many methods of supervised classification including neural networks (Bishop, 2007), Naïve Bayesian networks (Pearl, 1997), decision trees (Quinlan, 1986), K-nearest neighbors (Cover and Hart, 1967) and Support Vector Machines (SVM) (Vapnik, 1999). Previous works attested the effectiveness of the Naïve Bayesian networks to accomplish an appropriate classification. Nevertheless, these classifiers encounter problems as they are unable to treat imprecision. To treat this problem, Haouari et al. (2009) made out the Naïve Possibilistic Network Classifier. We take advantage of the simplicity and accuracy of this classifier to output a disambiguation approach to which we add the discriminative aspect. Therefore, we present in the next paragraphs, the basic measures and functions used in the training and test phases. Besides, we explain the discriminative aspect and the reweighting model of our classifier.

In this step, we train a classifier for each feature. We should take into account the fact that the attributes and/or the classes of the classification instances are imprecise i.e., having many possible values. The imprecision is handled by possibility distributions denoted by π. Let T be a collection of data and I
                              
                                 k
                               the set of the values of the attributes of the instance number k. We also denote A
                              
                                 j
                               the attribute number j from this set and a
                              
                                 jL
                               a possible value of A
                              
                                 j
                              . Inspired from Haouari et al. (2009) and the possibilistic information retrieval model (Bounhas et al., 2011b; Elayeb et al., 2009; Elayeb, 2009), we compute the normalized frequency of an attribute value (a
                              
                                 jL
                              ) for a class c
                              
                                 i
                               as follows:
                                 
                                    (3)
                                    
                                       
                                          Freq
                                          (
                                          
                                             a
                                             
                                                j
                                                L
                                             
                                          
                                          ,
                                          
                                             c
                                             i
                                          
                                          )
                                          =
                                          
                                             
                                                Occ
                                                (
                                                
                                                   a
                                                   
                                                      j
                                                      L
                                                   
                                                
                                                ,
                                                
                                                   c
                                                   i
                                                
                                                )
                                             
                                             
                                                Ma
                                                
                                                   x
                                                   
                                                      L
                                                      =
                                                      1
                                                   
                                                   
                                                      |
                                                      
                                                         A
                                                         j
                                                      
                                                      |
                                                   
                                                
                                                Occ
                                                (
                                                
                                                   a
                                                   
                                                      j
                                                      L
                                                   
                                                
                                                ,
                                                
                                                   c
                                                   i
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              where Occ(a
                              
                                 jL
                              ,
                              c
                              
                                 i
                              ) is the number of instances that have the class c
                              
                                 i
                               and the value a
                              
                                 jL
                               for the attribute A
                              
                                 j
                               and |A
                              
                                 j
                              | is the number of possible values of A
                              
                                 j
                              . Indeed, we used the MAX operator to obtain the normalized frequencies such used in possibilistic information retrieval (Bounhas et al., 2011b; Elayeb, 2009; Elayeb et al., 2009, 2011, 2014). Then, the sum of the frequencies of all the values of an attribute, given a class c
                              
                                 i
                              , is not equal to 1, which is one of the main hypothesizes of possibility theory to deal with imperfect data.

In the imperfect case, the number of occurrences of an attribute value is fuzzy:
                                 
                                    (4)
                                    
                                       
                                          Occ
                                          (
                                          
                                             a
                                             
                                                j
                                                L
                                             
                                          
                                          ,
                                          
                                             c
                                             i
                                          
                                          )
                                          =
                                          
                                             ∑
                                             
                                                k
                                                =
                                                1
                                             
                                             
                                                |
                                                T
                                                |
                                             
                                          
                                          
                                             
                                                β
                                                
                                                   j
                                                   k
                                                
                                             
                                             *
                                             
                                                ∅
                                                
                                                   i
                                                   j
                                                   k
                                                   L
                                                
                                             
                                          
                                       
                                    
                                 
                              where β
                              
                                 jk
                               indicates the imprecision rate of the attribute A
                              
                                 j
                               in the instance I
                              
                                 k
                              . As our corpus is unlabeled, we adopt the uniformity assumption which leads to the fact that, in the learning step, if an attribute (or the class) has many possible values, all these values have equal possibility to be correct. Thus, if, in a given instance, an attribute has two possible values and the class has only one possible value, the imprecision rate is 0.5. β
                              
                                 jk
                               is computed as follows:
                                 
                                    (5)
                                    
                                       
                                          
                                             β
                                             
                                                j
                                                k
                                             
                                          
                                          =
                                          
                                             1
                                             
                                                |
                                                
                                                   A
                                                   
                                                      j
                                                      k
                                                   
                                                
                                                |
                                                *
                                                |
                                                
                                                   C
                                                   k
                                                
                                                |
                                             
                                          
                                       
                                    
                                 
                              |A
                              
                                 jk
                              | denotes the cardinal of the set of the values of A
                              
                                 j
                               in the instance I
                              
                                 k
                              . In the same way, |C
                              
                                 k
                              | is the number of possible classes for I
                              
                                 k
                              . ∅
                                 ijkL
                               is equal to 1 if the value a
                              
                                 jL
                               exists in the possible values of A
                              
                                 j
                               in the instance I
                              
                                 k
                              , and the class c
                              
                                 i
                               exists in the possible classes of I
                              
                                 k
                               and 0 elsewhere. Thus, the more the instance is imprecise (in its attributes and/or class values), the more this factor β
                              
                                 jk
                               is decreased.

We compute the possibility of each class c
                              
                                 i
                               given an imperfect instance (I
                              
                                 k
                              ) having m attributes. This measure was inspired from the Naïve Possibilistic Network Classifier of Haouari et al. (2009). The possibility measure is the product of the frequencies of all the attributes. However, a specific factor is added for imprecise attributes. For example, if an attribute has three possible values, we compute the product of the frequencies of these three values, then we introduce a weight (β
                              
                                 jk
                              ) equal to 1/3, as we adopt the uniformity assumption. Hence, we introduce the possibility measure as follows:
                                 
                                    (6)
                                    
                                       
                                          Π
                                          (
                                          
                                             C
                                             i
                                          
                                          |
                                          
                                             I
                                             k
                                          
                                          )
                                          =
                                          
                                             ∏
                                             
                                                j
                                                =
                                                1
                                             
                                             m
                                          
                                          
                                             
                                                ∏
                                                
                                                   L
                                                   =
                                                   1
                                                
                                                
                                                   |
                                                   
                                                      A
                                                      
                                                         j
                                                         k
                                                      
                                                   
                                                   |
                                                
                                             
                                             
                                                Freq
                                                (
                                                
                                                   a
                                                   
                                                      j
                                                      L
                                                   
                                                
                                                ,
                                                
                                                   c
                                                   i
                                                
                                                )
                                                *
                                                
                                                   β
                                                   
                                                      j
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           

We finally have:
                                 
                                    (7)
                                    
                                       
                                          socre
                                          (
                                          
                                             c
                                             i
                                          
                                          |
                                          
                                             I
                                             k
                                          
                                          )
                                          =
                                          Π
                                          (
                                          
                                             c
                                             i
                                          
                                          |
                                          
                                             I
                                             k
                                          
                                          )
                                       
                                    
                                 
                              
                           

The basic classifier inspired from Haouari et al. (2009) does not evaluate the discriminative power of attribute values, since it uses only the possibility measure (formula (6)). However, we may discover that some values of a given attribute have greater impact in determining the correct class. Possibility theory models this fact by the necessity measure. That is, possibilistic IRKES (Bounhas et al., 2011b; Elayeb, 2009; Elayeb et al., 2009, 2011, 2014) use this measure to evaluate the discriminative power of query terms; i.e., terms which exist only in few documents have greater impact in relevance evaluation. Based on this idea and inspired from previous works (Bounhas et al., 2011b; Elayeb, 2009; Elayeb et al., 2009, 2011, 2014), we propose to compute the necessity measure to classify instances in the testing step as follows:
                              
                                 (8)
                                 
                                    
                                       N
                                       (
                                       
                                          c
                                          i
                                       
                                       |
                                       
                                          I
                                          k
                                       
                                       )
                                       =
                                       1
                                       −
                                       
                                          ∏
                                          
                                             j
                                             =
                                             1
                                          
                                          m
                                       
                                       
                                          
                                             ∏
                                             
                                                L
                                                =
                                                1
                                             
                                             
                                                |
                                                
                                                   A
                                                   
                                                      j
                                                      k
                                                   
                                                
                                                |
                                             
                                          
                                          
                                             (
                                             1
                                             −
                                             
                                                φ
                                                
                                                   i
                                                   j
                                                   k
                                                
                                             
                                             /
                                             
                                                β
                                                
                                                   j
                                                   k
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        

Here again, we are obliged to consider all the possible values of the imprecise attributes, by computing the product 
                              
                                 (
                                 
                                    Π
                                    
                                       L
                                       =
                                       1
                                    
                                    
                                       |
                                       
                                          A
                                          
                                             j
                                             k
                                          
                                       
                                       |
                                    
                                 
                                 )
                              
                            and the imprecision rate (β
                           
                              jk
                           ). As in IRKES, we have:
                              
                                 (9)
                                 
                                    
                                       
                                          φ
                                          
                                             i
                                             j
                                             k
                                          
                                       
                                       =
                                       
                                          log
                                          
                                             10
                                          
                                       
                                       
                                          
                                             
                                                P
                                                
                                                   n
                                                   
                                                      C
                                                      
                                                         j
                                                         L
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       *
                                       Freq
                                       (
                                       
                                          a
                                          
                                             j
                                             L
                                          
                                       
                                       ,
                                       
                                          c
                                          i
                                       
                                       )
                                    
                                 
                              
                           
                        

In this formula, P is the number of possible classes and nC
                           
                              jL
                            is the number of classes having a non-null frequency with the value a
                           
                              jL
                            (i.e., Freq(a
                           
                              jl
                           , c
                           
                              i
                           )>0).

The necessity may be directly used for classification as follows:
                              
                                 (10)
                                 
                                    
                                       socre
                                       (
                                       
                                          c
                                          i
                                       
                                       |
                                       
                                          I
                                          k
                                       
                                       )
                                       =
                                       N
                                       (
                                       
                                          c
                                          i
                                       
                                       |
                                       
                                          I
                                          k
                                       
                                       )
                                    
                                 
                              
                           
                        

Also, we may combine it with the possibility measure as defined in possibilistic IRKES (Bounhas et al., 2011b, 2013, 2014; Elayeb, 2009; Elayeb et al., 2009, 2011, 2014):
                              
                                 (11)
                                 
                                    
                                       socre
                                       (
                                       
                                          c
                                          i
                                       
                                       |
                                       
                                          I
                                          k
                                       
                                       )
                                       =
                                       Π
                                       (
                                       
                                          c
                                          i
                                       
                                       |
                                       
                                          I
                                          k
                                       
                                       )
                                       +
                                       N
                                       (
                                       
                                          c
                                          i
                                       
                                       |
                                       
                                          I
                                          k
                                       
                                       )
                                    
                                 
                              
                           
                        

In our experiments, we will compare three alternatives adopting, respectively formulae (7), (10) and (11) to identify the best class. In all the cases, the best class for the instance I
                           
                              k
                            is the one having the greater score among all the classes:
                              
                                 (12)
                                 
                                    
                                       
                                          c
                                          *
                                       
                                       =
                                       arg
                                       
                                          
                                             max
                                          
                                          
                                             
                                                c
                                                i
                                             
                                          
                                       
                                       score
                                       (
                                       
                                          c
                                          i
                                       
                                       |
                                       
                                          I
                                          k
                                       
                                       )
                                    
                                 
                              
                           
                        

The reweighting model allows assigning absolute weights to the classification attributes. Disambiguation tools, like MADA (Alkuhlani et al., 2013; Habash and Rambow, 2005, 2007; Roth et al., 2008), implement such models to relatively evaluate these attributes and enhance classification rates. This permits to reduce the contextual space required to disambiguate a given feature and thus simplifying the disambiguation process.

In our case, we compute the information gain based on information entropy (Blansché, 2006; Quinlan, 1986), which has been used in many areas of artificial intelligence such as privacy in online services (Krause and Horvitz, 2008), clustering (Liping et al., 2007) and classification (Yue, 2012). The use of entropy in classification is justified by its ability to partition the space into non-overlapping decision regions for classification. This is exactly what is required for morphological disambiguation as the different values of a class (i.e., noun and verb and so on for the POS) allow partitioning the words into completely separated sets. As explained by Lee et al. (2001), “both the complexity and computational load of the classifier are reduced and thus the training time and classification time are extremely short”. In our case, this factor is important as the full morphological analysis is time consuming and our tool will be used in IRKES, which require short response time. In addition, Lee et al. (2001) claim that the “entropy-based feature selection procedure not only reduces the dimensionality of a problem but also discards noise-corrupted, redundant and unimportant features”.

The entropy may be used to weight attributes as follows (Quinlan, 1986). However, we will be obliged to adapt the formulae to the imperfect case. The main change is to introduce factors allowing to assign lower weights to the attributes or the classes, which are imperfect (i.e., have many possible values). This change is a constraint imposed by the kind of application, namely the disambiguation task. The examples in Section 3 will explain and show the importance of these factors.

Let S be a given set of instances. If we have P classes, then S is segmented into P subsets: S
                           ={S
                           1, …, S
                           
                              P
                           } where the instances of each S
                           
                              i
                            belong to the same class.

The information entropy of S, in the perfect case, is given by (Blansché, 2006):
                              
                                 (31)
                                 
                                    
                                       I
                                       (
                                       S
                                       )
                                       =
                                       −
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          P
                                       
                                       
                                          
                                             
                                                |
                                                
                                                   S
                                                   i
                                                
                                                |
                                             
                                             
                                                |
                                                S
                                                |
                                             
                                          
                                       
                                       
                                          
                                             log
                                          
                                          2
                                       
                                       
                                          
                                             |
                                             
                                                S
                                                i
                                             
                                             |
                                          
                                          
                                             |
                                             S
                                             |
                                          
                                       
                                    
                                 
                              
                           
                        

We suppose that the attribute A
                           
                              j
                            has v different values. The entropy or the information required to classify the instances of all the subsets of T, according to the attribute A
                           
                              j
                           , is given by:
                              
                                 (14)
                                 
                                    
                                       E
                                       (
                                       
                                          A
                                          j
                                       
                                       )
                                       =
                                       
                                          ∑
                                          
                                             L
                                             =
                                             1
                                          
                                          v
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               I
                                                               k
                                                            
                                                            ∈
                                                            T
                                                            |
                                                            
                                                               A
                                                               j
                                                            
                                                            =
                                                            
                                                               a
                                                               
                                                                  j
                                                                  L
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                |
                                                T
                                                |
                                             
                                          
                                          *
                                          I
                                          (
                                          
                                             I
                                             k
                                          
                                          ∈
                                          T
                                          |
                                          
                                             A
                                             j
                                          
                                          =
                                          
                                             a
                                             
                                                j
                                                L
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        

In the imprecise case, it is not possible to compute directly 
                              
                                 
                                    
                                       
                                          
                                             
                                                I
                                                k
                                             
                                             ∈
                                             T
                                             |
                                             
                                                A
                                                j
                                             
                                             =
                                             
                                                a
                                                
                                                   j
                                                   L
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           . It is also necessary to adapt formula (14) in order to compute I(I
                           
                              k
                           
                           ∈
                           T|A
                           
                              j
                           
                           =
                           a
                           
                              jL
                           ). Indeed, we cannot attribute a binary value to A
                           
                              j
                           
                           =
                           a
                           
                              jL
                           , because each attribute may contain several possible values, which are equally weighted.

We perform as follows:
                              
                                 (15)
                                 
                                    
                                       E
                                       (
                                       
                                          A
                                          j
                                       
                                       )
                                       =
                                       
                                          ∑
                                          
                                             L
                                             =
                                             1
                                          
                                          v
                                       
                                       
                                          
                                             
                                                
                                                   F
                                                   
                                                      j
                                                      L
                                                   
                                                
                                             
                                             
                                                |
                                                T
                                                |
                                             
                                          
                                          *
                                          
                                             I
                                             ′
                                          
                                          (
                                          
                                             I
                                             k
                                          
                                          ∈
                                          T
                                          |
                                          
                                             A
                                             j
                                          
                                          =
                                          
                                             a
                                             
                                                j
                                                L
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           where F
                           
                              jL
                            is a fuzzy count of the instances having the value a
                           
                              jL
                           . It is given by:
                              
                                 (16)
                                 
                                    
                                       
                                          F
                                          
                                             j
                                             L
                                          
                                       
                                       =
                                       
                                          ∑
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             |
                                             T
                                             |
                                          
                                       
                                       
                                          
                                             1
                                             
                                                |
                                                
                                                   A
                                                   
                                                      j
                                                      k
                                                   
                                                
                                                |
                                             
                                          
                                          *
                                          
                                             ∅
                                             
                                                j
                                                k
                                                L
                                             
                                          
                                       
                                    
                                 
                              
                           where ∅
                              jkL
                            is equal to 1 if the value a
                           
                              j
                            exists in the possible values of A
                           
                              j
                            in the instance I
                           
                              k
                            and 0 elsewhere. Again, we divide by |A
                           
                              jk
                           | to decrease the weights of the imprecise case. We also have:
                              
                                 (17)
                                 
                                    
                                       
                                          I
                                          ′
                                       
                                       (
                                       
                                          I
                                          k
                                       
                                       ∈
                                       T
                                       |
                                       
                                          A
                                          j
                                       
                                       =
                                       
                                          a
                                          
                                             j
                                             L
                                          
                                       
                                       )
                                       =
                                       −
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          P
                                       
                                       
                                          
                                             
                                                Occ
                                                (
                                                
                                                   a
                                                   
                                                      j
                                                      L
                                                   
                                                
                                                ,
                                                
                                                   c
                                                   i
                                                
                                                )
                                             
                                             
                                                
                                                   F
                                                   
                                                      j
                                                      L
                                                   
                                                
                                             
                                          
                                       
                                       
                                          log
                                          2
                                       
                                       
                                          
                                             Occ
                                             (
                                             
                                                a
                                                
                                                   j
                                                   L
                                                
                                             
                                             ,
                                             
                                                c
                                                i
                                             
                                             )
                                          
                                          
                                             
                                                F
                                                
                                                   j
                                                   L
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where Occ(a
                           
                              jL
                           ,c
                           
                              i
                           ) is calculated according to formula (4). The information gain (Blansché, 2006) of the attribute A
                           
                              j
                            is computed as follows:
                              
                                 (18)
                                 
                                    
                                       Gain
                                       (
                                       
                                          A
                                          j
                                       
                                       )
                                       =
                                       
                                          I
                                          ′
                                       
                                       (
                                       T
                                       )
                                       −
                                       E
                                       (
                                       
                                          A
                                          j
                                       
                                       )
                                    
                                 
                              
                           
                        


                           I′(T) is computed as follows:
                              
                                 (19)
                                 
                                    
                                       
                                          I
                                          ′
                                       
                                       (
                                       T
                                       )
                                       =
                                       −
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          P
                                       
                                       
                                          
                                             
                                                Occ
                                                (
                                                
                                                   c
                                                   i
                                                
                                                )
                                             
                                             
                                                |
                                                T
                                                |
                                             
                                          
                                       
                                       
                                          log
                                          2
                                       
                                       
                                          
                                             Occ
                                             (
                                             
                                                c
                                                i
                                             
                                             )
                                          
                                          
                                             |
                                             T
                                             |
                                          
                                       
                                    
                                 
                              
                           where Occ(c
                           
                              i
                           ) is a fuzzy count of the instances having the class c
                           
                              i
                           . It is given by:
                              
                                 (20)
                                 
                                    
                                       Occ
                                       (
                                       
                                          c
                                          i
                                       
                                       )
                                       =
                                       
                                          ∑
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             |
                                             T
                                             |
                                          
                                       
                                       
                                          
                                             1
                                             
                                                |
                                                
                                                   C
                                                   k
                                                
                                                |
                                             
                                          
                                          *
                                          
                                             ∅
                                             
                                                i
                                                k
                                             
                                          
                                       
                                    
                                 
                              
                           where ∅
                              ik
                            is equal to 1 if the value c
                           
                              i
                            exists in the possible classes of the instance I
                           
                              k
                            and 0 elsewhere.

Finally, we introduce the information gain in the possibility and the necessity measures by modifying formulae (6) and (8) as follows:
                              
                                 (21)
                                 
                                    
                                       Π
                                       (
                                       
                                          c
                                          i
                                       
                                       |
                                       
                                          I
                                          k
                                       
                                       )
                                       =
                                       
                                          ∏
                                          
                                             j
                                             =
                                             1
                                          
                                          m
                                       
                                       
                                          
                                             ∏
                                             
                                                L
                                                =
                                                1
                                             
                                             
                                                |
                                                
                                                   A
                                                   
                                                      j
                                                      k
                                                   
                                                
                                                |
                                             
                                          
                                          
                                             Freq
                                             (
                                             
                                                a
                                                
                                                   j
                                                   L
                                                
                                             
                                             ,
                                             
                                                c
                                                i
                                             
                                             )
                                             *
                                             
                                                β
                                                
                                                   j
                                                   k
                                                
                                             
                                             *
                                             Gain
                                             (
                                             
                                                A
                                                j
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (22)
                                 
                                    
                                       N
                                       (
                                       
                                          c
                                          i
                                       
                                       |
                                       
                                          I
                                          k
                                       
                                       )
                                       =
                                       1
                                       −
                                       
                                          ∏
                                          
                                             j
                                             =
                                             1
                                          
                                          m
                                       
                                       
                                          
                                             ∏
                                             
                                                L
                                                =
                                                1
                                             
                                             
                                                |
                                                
                                                   A
                                                   
                                                      j
                                                      k
                                                   
                                                
                                                |
                                             
                                          
                                          
                                             
                                                
                                                   1
                                                   −
                                                   
                                                      
                                                         
                                                            φ
                                                            
                                                               i
                                                               j
                                                               L
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            β
                                                            
                                                               i
                                                               k
                                                            
                                                         
                                                         *
                                                         Gain
                                                         (
                                                         
                                                            A
                                                            j
                                                         
                                                         )
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Every single instance of the training and the test sets describes the attributes values of one word. Thus, the possibilistic classifier does not take into account the word value to determine its accurate analysis (class). Hence, including the word value to compute the possibilistic score (cf. formula (12)) becomes crucial. The lexical likelihood is integrated in the most language taggers (Jurafsky and Martin, 2009). It studies the coexistence of a word with a particular class over a training set. If w
                           
                              i
                            is the word of the test instance I
                           
                              k
                           , then its lexical likelihood is P(w
                           
                              i
                           
                           |c
                           
                              i
                           ). The latest measure defines the probability that the word, in the current test's instance, is w
                           
                              i
                            knowing that the associated class is c
                           
                              i
                           . In other terms, this lexical likelihood answers the question: if we were expecting a class c
                           
                              i
                           , for the instance I
                           
                              k
                           , how likely is it that the related word would be w
                           
                              i
                           ? Therefore, the chosen class for the instance I
                           
                              k
                            is the one having the highest score combining the feature-based score (score(c
                           
                              i
                           |I
                           
                              k
                           )) and the lexical likelihood of its word (score(c
                           
                              i
                           |w
                           
                              k
                           )). This score is computed in the same manner; i.e., we can use the possibility and/or the necessity.
                              
                                 (23)
                                 
                                    
                                       
                                          c
                                          *
                                       
                                       =
                                       arg
                                       
                                          
                                             max
                                          
                                          
                                             
                                                c
                                                i
                                             
                                          
                                       
                                       (
                                       score
                                       (
                                       
                                          c
                                          i
                                       
                                       |
                                       
                                          I
                                          k
                                       
                                       )
                                       *
                                       socre
                                       (
                                       
                                          c
                                          i
                                       
                                       |
                                       
                                          w
                                          k
                                       
                                       )
                                       )
                                    
                                 
                              
                           
                        

The lexical likelihood requires the coexistence of a word w
                           
                              i
                            with a class value c
                           
                              i
                            in the training set, otherwise, the c* value turns into zero. Thus, this measure is considered that when this condition is fulfilled. If score(c
                           
                              i
                           |w
                           
                              k
                           ) is equal to zero then we omit it and we turn to compute the formula 
                              
                                 
                                    c
                                    *
                                 
                                 =
                                 arg
                                 
                                    
                                       max
                                    
                                    
                                       
                                          c
                                          i
                                       
                                    
                                 
                                 (
                                 score
                                 (
                                 
                                    c
                                    i
                                 
                                 |
                                 
                                    I
                                    k
                                 
                                 )
                                 )
                              
                           .

This section presents examples for the different steps of morphological disambiguation; i.e., morphological analysis, training and testing with and without reweighting.

We extract information about all the morphological features of each word in vocalized texts. A vocalized word provides often a single analysis. Examples of words from a vocalized text (see Fig. 1
                        ) with some of their possible solutions are shown in Table 1
                        . We give a description of each feature in the second column. For instance, ÝóíóÃúßõáõ (faya>okulu: “and then he eats”) contains a conjunction (CONJ); does not contain a particle (NPART), does not contain a determiner (NDET) (since it is not a name), has no case or mode (NCASE and NMODE), does not contain preposition (NPREP) and is not an adjective. This word is an imperfect verb in the 3rd person masculine singular (IV3MS).

After analyzing the vocalized words, we build the training set composed of instances which are described by attributes and class values generated by the morphological analyzer. We ascribe to each word an instance. We assume, in this example, that we classify the POS using the attributes describing the POS of the two preceding and the two following words. We consider a training set made up of 4 instances presented in Table 2
                        . The second instance contains an imperfect attribute that is POS−1 (i.e., β
                        11
                        =1/2=0.5). We compute the frequencies as detailed in Table 3
                        .

For example, to compute Occ (POS+1=VERB_IMPERFECT, POS=NOUN_PROP), we search for instances which have NOUN_PROP in the class (POS); i.e., the first instance. For this instance, the imprecision rate is 1/2, because the class has two possible values. Thus, we have: Occ (POS+1=VERB_IMPERFECT, POS=NOUN_PROP)=(1/2)*1=0.5. The same approach is used to handle attribute imprecision. That is, the number Occ (POS−1=VERB_PERFECT, POS=NOUN) is equal to 1*(1/2)=0.5, because VERB_PERFECT appears only once in the attribute POS−1 with the class NOUN and the attribute has two possible values.

We compute the global information quantity as follows:
                           
                              
                                 
                                    
                                       I
                                       ′
                                    
                                    (
                                    T
                                    )
                                    =
                                    −
                                    
                                       
                                          
                                             
                                                2.5
                                             
                                             4
                                          
                                       
                                    
                                    
                                       log
                                       2
                                    
                                    
                                       
                                          
                                             
                                                2.5
                                             
                                             4
                                          
                                       
                                    
                                    −
                                    
                                       1
                                       4
                                    
                                    
                                       log
                                       2
                                    
                                    
                                       
                                          
                                             1
                                             4
                                          
                                       
                                    
                                    −
                                    
                                       
                                          0.5
                                       
                                       4
                                    
                                    
                                       log
                                       2
                                    
                                    
                                       
                                          
                                             
                                                0.5
                                             
                                             4
                                          
                                       
                                    
                                    =
                                    1.29
                                 
                              
                           
                        
                     

We, also, compute the information gain of the four attributes as detailed in Table 4
                        .

We define a new instance (see Table 5
                        ) for which we want to identify the POS, using the POS−2, POS−1, POS+1 and POS+2 attributes, based on our calculus in the training step. This instance is assigned to a non-vocalized word whose POS is ambiguous i.e., has more than one morphological analysis.

We compute the possibility measure for the three classes as follows:
                           
                              
                                 
                                    Π
                                    (
                                    
                                       c
                                       i
                                    
                                    =
                                    NOUN
                                    |
                                    
                                       I
                                       k
                                    
                                    )
                                    =
                                    Freq
                                    (
                                    POS
                                    −
                                    2
                                    =
                                    NOUN
                                    _
                                    PROP
                                    ,
                                    
                                       c
                                       i
                                    
                                    =
                                    NOUN
                                    )
                                    *
                                    0
                                    .5
                                    *
                                    Freq
                                    (
                                    POS
                                    −
                                    2
                                    =
                                    PRON
                                    _
                                    3
                                    MS
                                    ,
                                    
                                       c
                                       i
                                    
                                    =
                                    NOUN
                                    )
                                    *
                                    0
                                    .5
                                    *
                                    Freq
                                    (
                                    POS
                                    −
                                    1
                                    =
                                    VERB
                                    _
                                    PERFECT
                                    ,
                                    
                                       c
                                       i
                                    
                                    =
                                    NOUN
                                    )
                                    *
                                    
                                       1
                                    
                                    *
                                    Freq
                                    (
                                    POS
                                    +
                                    1
                                    =
                                    PRP
                                    ,
                                    
                                       c
                                       i
                                    
                                    =
                                    NOUN
                                    )
                                    *
                                    
                                       1
                                    
                                    *
                                    Freq
                                    (
                                    POS
                                    +
                                    2
                                    =
                                    VERB
                                    _
                                    IMPERFECT
                                    ,
                                    
                                       c
                                       i
                                    
                                    =
                                    NOUN
                                    )
                                    *
                                    
                                       1
                                    
                                    =
                                    0
                                    *
                                    0
                                    .5
                                    *
                                    0
                                    *
                                    0
                                    .5
                                    *
                                    0.5
                                    *
                                    
                                       1
                                    
                                    *
                                    0
                                    *
                                    
                                       1
                                    
                                    *
                                    1
                                    *
                                    
                                       1
                                    
                                    =
                                    0
                                 
                              
                           
                        
                     

Thus, the values of the first attribute (POS−2) are weighted by β
                        1k
                        
                        =0.5, because this attribute has two possible values. In the same manner, we have:
                           
                              
                                 
                                    Π
                                    (
                                    
                                       c
                                       i
                                    
                                    =
                                    VERB
                                    _
                                    PERFECT
                                    |
                                    
                                       I
                                       k
                                    
                                    )
                                    =
                                    0
                                    *
                                    0
                                    .5
                                    *
                                    1
                                    *
                                    0
                                    .5
                                    *
                                    0
                                    *
                                    1
                                    *
                                    0
                                    *
                                    1
                                    *
                                    0
                                    *
                                    1
                                    =
                                    0
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    Π
                                    (
                                    
                                       c
                                       i
                                    
                                    =
                                    NOUN
                                    _
                                    PROP
                                    |
                                    
                                       I
                                       k
                                    
                                    )
                                    =
                                    0
                                    *
                                    0
                                    .5
                                    *
                                    0
                                    *
                                    0
                                    .5
                                    *
                                    0
                                    *
                                    1
                                    *
                                    0
                                    *
                                    1
                                    *
                                    0
                                    *
                                    1
                                    =
                                    0
                                 
                              
                           
                        
                     

We remark that all the possibility values are null due to the negligible size of the training set. However in real situations much more frequencies will be non-null, thus leading to non-null possibility values.

The necessity is computed as:
                           
                              
                                 
                                    N
                                    (
                                    
                                       c
                                       i
                                    
                                    =
                                    NOUN
                                    |
                                    
                                       I
                                       k
                                    
                                    )
                                    =
                                    1
                                    −
                                    [
                                    (
                                    1
                                    −
                                    0
                                    /
                                    0.5
                                    )
                                    *
                                    (
                                    1
                                    −
                                    
                                       log
                                       
                                          10
                                       
                                    
                                    (
                                    3
                                    /
                                    1
                                    )
                                    *
                                    0
                                    /
                                    0.5
                                    )
                                    *
                                    (
                                    1
                                    −
                                    
                                       log
                                       
                                          10
                                       
                                    
                                    (
                                    3
                                    /
                                    1
                                    )
                                    *
                                    0.5
                                    /
                                    1
                                    )
                                    *
                                    (
                                    1
                                    −
                                    0
                                    /
                                    1
                                    )
                                    *
                                    (
                                    1
                                    −
                                    
                                       log
                                       
                                          10
                                       
                                    
                                    (
                                    3
                                    /
                                    1
                                    )
                                    *
                                    1
                                    /
                                    1
                                    )
                                    ]
                                    =
                                    0
                                    .6
                                    0
                                 
                              
                           
                        
                     

For the remaining classes, we obtain: N (c
                        
                           i
                        
                        =VERB_PERFECT|I
                        
                           k
                        )=
                        0.95 and
                           
                              
                                 
                                    N
                                    (
                                    
                                       c
                                       i
                                    
                                    =
                                    NOUN
                                    _
                                    PROP
                                    |
                                    
                                       I
                                       k
                                    
                                    )
                                    =
                                    
                                       0
                                    
                                 
                              
                           
                        
                     

When we introduce the information gain of the attributes, we obtain:
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                Π
                                                (
                                                
                                                   c
                                                   i
                                                
                                                =
                                                NOUN
                                                |
                                                
                                                   I
                                                   k
                                                
                                                )
                                                =
                                                Π
                                                (
                                                
                                                   c
                                                   i
                                                
                                                =
                                                VERB
                                                _
                                                PERFECT
                                                |
                                                
                                                   I
                                                   k
                                                
                                                )
                                                =
                                                Π
                                                (
                                                
                                                   c
                                                   i
                                                
                                                =
                                                NOUN
                                                _
                                                PROP
                                                |
                                                
                                                   I
                                                   k
                                                
                                                )
                                                =
                                                0
                                             
                                          
                                       
                                       
                                          
                                             
                                                N
                                                (
                                                
                                                   c
                                                   i
                                                
                                                =
                                                NOUN
                                                |
                                                
                                                   I
                                                   k
                                                
                                                )
                                                =
                                                1
                                                −
                                                [
                                                (
                                                1
                                                −
                                                0
                                                /
                                                (
                                                0.5
                                                *
                                                0.89
                                                )
                                                )
                                                *
                                                (
                                                1
                                                −
                                                
                                                   
                                                      log
                                                   
                                                   
                                                      10
                                                   
                                                
                                                (
                                                3
                                                /
                                                1
                                                )
                                                *
                                                0
                                                /
                                                (
                                                0.5
                                                *
                                                0.89
                                                )
                                                )
                                                *
                                                (
                                                1
                                                −
                                                
                                                   
                                                      log
                                                   
                                                   
                                                      10
                                                   
                                                
                                                (
                                                3
                                                /
                                                1
                                                )
                                                *
                                                0.5
                                                /
                                                (
                                                1
                                                *
                                                0.7
                                                )
                                                )
                                                *
                                                (
                                                1
                                                −
                                                0
                                                /
                                                (
                                                1
                                                *
                                                0.54
                                                )
                                                )
                                                *
                                                (
                                                1
                                                −
                                                
                                                   
                                                      log
                                                   
                                                   
                                                      10
                                                   
                                                
                                                (
                                                3
                                                /
                                                1
                                                )
                                                *
                                                1
                                                /
                                                (
                                                1
                                                *
                                                0.54
                                                )
                                                )
                                                ]
                                                =
                                                0
                                                .9
                                                2
                                             
                                          
                                       
                                       
                                          
                                             
                                                N
                                                (
                                                
                                                   c
                                                   i
                                                
                                                =
                                                VERB
                                                _
                                                PERFECT
                                                |
                                                
                                                   I
                                                   k
                                                
                                                )
                                                =
                                                1
                                                .0
                                                7
                                             
                                          
                                       
                                       
                                          
                                             
                                                N
                                                (
                                                
                                                   c
                                                   i
                                                
                                                =
                                                NOUN
                                                _
                                                PROP
                                                |
                                                
                                                   I
                                                   k
                                                
                                                )
                                                =
                                                
                                                   0
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

That is, if we use the necessity or we combine the two measures with or without reweighting, we select “VERB_PERFECT” as a POS for this instance. However, the attribute weights changed the scores assigned to the different classes, which will have greater impact in real situations.

This section presents the test collection used in our experiments, the evaluation method and the experimental results highlighting several aspects of our classifiers.

The major aim of our approach is to acquire morphological dependencies from vocalized texts and test on non-vocalized ones. Besides, we consider classical Arabic texts, which have been ignored in previous related works. Therefore, we use a collection of Arabic stories i.e., “Hadith”
                              4
                           
                           
                              4
                              
                                 http://www.islamweb.net/hadith/index.php.
                            which have been the issue of several works (e.g., (Bounhas et al., 2010, 2011b; Harrag et al., 2009, 2013)). Hadiths talk about all the real-life concerns and cover common and universal knowledge. To justify our choice, we assess that the corpus of hadith is one of the few vocalized Arabic corpora. Besides, the hadith corpus is bigger than TreeBanks (Habash et al., 2009a; Maamouri et al., 2009; Maamouri and Bies, 2004), since it contains 1400 vocalized books of hadith, each one holding thousands of Arabic stories.3 The six most famous and reliable books contain more than 2.5 million words and more than 95,000 fragments (titles and paragraphs). Moreover, this corpus is well structured and the titles of chapters and sub-chapters represent relevant contextual information to disambiguate texts (Bounhas et al., 2011b). In addition, the books of hadith are widely used in information retrieval and knowledge extraction, because of their huge size (Harrag et al., 2009, 2013). However, we recognize that most of the hadiths are assigned to one person (i.e., the prophet PBSL), which may make of it not an ideal source for modeling. Nevertheless, many hadiths are speeches of other persons like the companions of the prophet (PBSL) and many narrators add other expressions especially when they narrate stories and not speeches, thus enriching the corpus and making it less specific.

We use, among the hadith corpus, the well-recognized six encyclopedic books organized by theme which are: Sahih Al-Bukhari, Sahih Muslim, Sunan Abi Dawud, Sunan Ettermidhi, Sunan Ibn Majah and Sunan Annasaii (Al-Echikh, 1998). As detailed in Table 6
                           , we limit our experiments to three sub-corpora corresponding to the following domains of interest: 
                              
                            (Al>$rbp “drinks”), 
                              
                            (AlzwAj “marriage”) and 
                              
                            (AlThArp “purification”) (Ayed et al., 2012a).

The cross-validation method (Kohavi, 1995) is commonly used over a dataset that can be divided into significant sub-sets. Indeed, we apply this method on the six books of hadith to estimate the performance of our possibilistic classifier, thus procuring six combinations. For each combination, five books are used for training and the remaining one is employed for testing. The average success rate is computed over the 5+1 combinations. To get these rates, we perform as follows: (a) the vocalized texts are analyzed and the correct morphological solutions are stored; (b) the short vowels of the same texts are removed; (c) the obtained texts are disambiguated with our classifier and the results are stored; and (d) we compare the two results (Ayed et al., 2012a,b).

@&#EXPERIMENTAL RESULTS@&#


                        Ayed et al. (2012a,b) presented preliminary results which showed a high accuracy of the basic possibilistic classifier. It, also, allowed us to study the domain dependency of our classifier. However, we should also assess our discriminative possibilistic classifier, the reweighting model and the effect of introducing the lexical likelihood.

Our previous work focused on the basic possibilistic classifier which uses only the possibility measure to determine the most plausible class. It assigns a uniform weight to all the attributes involved in the classification procedure. We discern, in this section, our discriminative possibilistic classifier and the reweighting model. We provide some details of the learning process and the classification rates obtained with several combinations of classification models. Starting with some examples, Table 7
                            gives some possibility distributions (i.e. frequencies) obtained for the POS with the most common values of the POS−1 attribute.

We can, also, extract the dependency relationships of the POS with other features. Table 8
                            illustrates the information gain (IG
                           
                              J
                           ) for each attribute (A
                           
                              j
                           ) given the POS as class. These weights attest that the POS of a given word is more related to the POS of the preceding and the following words compared to the other features. We assert that, to disambiguate the POS, we require the involvement of the POS, the PRONOUN and the DETERMINER of the preceding and the following words. These features have the highest information gain averages. This assumption can be linguistically proved. Let us have the prior example 
                              
                            (And we find lightly wine grapes in the city). We may try to disambiguate the word “
                              
                           ”, having only information about the POS, PRONOUN and DETERMINER of the previous and following words. 
                              
                            (in the city) and 
                              
                            (grapes) are both nouns, which have a determiner and do not contain pronouns. They are preceded by a verb, which is “
                              
                           ” (we find). We select mostly the NOUN as a POS for the word “
                              
                           ”.

As far as the other features are concerned, we present in Table 9
                           , the three top values of information gain of each feature. These values are assigned to the most discriminative attributes of every morphological feature. We remark that the most related attributes are those of the adjacent words (the preceding and the following word). Besides, we can remark that all the features depend on the POS, since at least, one attribute is associated with the POS appears in its list of the highest information gain measures. This fact is normal, because the POS determines the grammatical category of the word on which the other attributes depend closely. We justify, once again, the close and communal relationship between the features POS, PRONOUN and DETERMINER. As we know, determiners are applicable only for nouns and the kind of pronoun depends on the POS (verb or noun).

Finally, we present the disambiguation rates (DR) for all the features using all the combinations of classification models in Table 10
                           . We obtain, entirely, six combinations (i.e., six classifiers). П, N and Π
                           +
                           N denote that we use, respectively, the possibility, the necessity and the sum of the two measures. “Without reweighting” and “with reweighting” determine the fact of including or not the information gain in the possibility and necessity measures (see formulae (21) and (22)). We also compute the difference between the results of the five last models with the basic classifier (DB). For example, when moving from the basic classifier (Π) to the model using only the necessity measure (N), we lose 0.61% in the disambiguation rate of the POS. The experiments showed that most features are closely inter-dependent. They often provide more than 70% of disambiguation rates. For most features, the success rates, given by the П measure, are better than those given by the N measure. The sum of the two measures provides better results. This may be explained by the free word order phenomenon (cf. Section 1.1). Thus, the values of a given attribute (a feature of a preceding or a following word) are like to be equally distributed over the values of the class (a feature of the current word).

The use of information gain increased the disambiguation rates for the three measures except for some values of possibility or/and necessity of some features (e.g., N-CONJUNCTION, N-DETERMINER, N-PERSON, П-PERSON, П-NUMBER and N-MODE). This deterioration in rates is minor and does not exceed 0.47%. The overall averages of the disambiguation rates of all the features give high values when using the information gain, which shows the usefulness of this model for attribute reweighting.

To compare the six classifiers in terms of disambiguation rate (DR), we use the Wilcoxon Matched-Pairs Signed-Ranks Test as proposed by Demsar (2006) and used by Bounhas et al. (2013). It is a non-parametric alternative to the paired t-test that enables us to compare two classifiers over multiple features. The given values are computed by comparing the (Π
                           +
                           N) with reweighting classifier to each from the other five remaining classifiers.

Comparison results given in Table 11
                            shows that the (Π
                           +
                           N) with reweighting is always significantly better (p-value<0.05) than the five other classifiers for all the features.


                           Table 12
                            presents the disambiguation rates of the POS in the three domains “Drinks”, “Marriage” and “Purification” using our basic possibilistic classifier. We used only the POS of the two following and the two preceding words. The average disambiguation rate through the three domains for the POS was about 88.38%.

We obtained similar results for the three domains. Hence, we preview that context-based approaches (i.e., the possibilistic classifier) may be used for any domain or type of text. To improve our assessment, we conduct our experiments on the Arabic Treebank Part 2 (Habash et al., 2009a; Maamouri et al., 2009; Maamouri and Bies, 2004). It is a tagged text corpus that was produced by Linguistic Data Consortium and includes more than 500 stories from the Ummah Arabic News articles (Egyptian newspaper). It contains about 144K of annotated words. The annotation denotes the POS tag to each word. Table 13
                            presents the POS disambiguation average rates of words in the hadith and the Arabic Treebank corpora. We obtain close results with high rates. We use the “(П
                           +
                           N) with reweighting” model as it gives, mostly, the best results (cf. Section 4.2.1).

These results reveal that the possibilistic disambiguation approach is reusable through domains and types of text, as it provides good disambiguation average rates (over than 80%) on news texts as well as the Hadith texts. There is, however, a difference of about 5% in the global rates. As the sizes of the two corpora are almost equal, we can explain this fact that the hadith corpus contains some recurrent expressions, which exist both in the training and the test corpus (e.g., 
                              
                           ; Peace and Blessing be upon him).

We compute the lexical likelihood of each word from the test sets. Table 14
                            gives the disambiguation rates of the morphological features applying the six classification models. The chosen class corresponds to the one that has the highest value of formula (23). Comparing to Table 10, the lexical likelihood enhances the disambiguation rates of all the morphological features. Likewise, the “(П
                           +
                           N) with reweighting” model gives, generally, the best disambiguation results. Hence, we assess that the lexical likelihood improves the morphological features disambiguation as it provides an enhancement average rate of about 3%. The existence of some words with their correct class values, in the training set, explains this improvement. However, this constraint is not always satisfied. Consequently, the lexical likelihood becomes equal to zero and we turn to the use of our discriminative possibilistic classifier without computing the lexical likelihood.

This paper constitutes the first attempt to apply possibility theory to morphological disambiguation. We presented and studied a discriminative possibilistic classifier with a reweighting model used to disambiguate the morphological features of non-vocalized Arabic texts. This classifier was trained on vocalized texts and tested on non-vocalized ones. The first part of this approach (i.e., the training phase) generated the morphological solutions of vocalized texts using AraMorph. The second part of this approach (i.e., the testing phase) determined, for non-vocalized texts, the corresponding instances described by the same attributes used for the training set. The non-vocalized words of these texts gave, commonly, more than one value of each morphological feature. To disambiguate one feature of an ambiguous word, we are based on the training set of this feature. We disambiguated these features by identifying the class that corresponds to the highest possibility and/or necessity measures computed over the specific training set. Indeed, we defined three classification models that are based on different measures to conclude the accurate class. These measures are: the possibility, the necessity and the sum of possibility and necessity. To compute the impact of each attribute in the disambiguation of the morphological features, we integrated the information gain, based on entropy, to these measures, thus acquiring six classifiers in total.

We performed experimentations with cross-validation on the 14 morphological features and we applied the six aforementioned classification models. We reached better disambiguation rates with the reweighting model, using the sum of possibility and necessity measures. Besides, we discerned likewise the dependency relationships between different features. Indeed, we specified for each feature the information gain measures of the attributes which presented the highest discriminative weights. We concluded that the POS is involved in the disambiguation of all the morphological features. We should, also, note that only religious texts (hadiths and Quran) and some children’ didactic texts are entirely vocalized. The big size of these collections allows precise training. Based on the same idea, we extended our work by testing on the Treebank corpus which represents another type of text (i.e., modern texts of newspaper articles). Indeed, our experiments revealed that our classifier is reusable through domains and types of texts.

Despite these encouraging results, we remarked that our approach registered some unsatisfactory results. This may be explained by the “free word order” phenomena, but also by the inability to disambiguate some particles which have high ambiguity rate even in vocalized texts. As future work, we may deal with these problems, by adopting one of two paths. On the one hand, we may enlarge the training set and the hadith corpus allows this kind of perspective. On the other hand, integrating morphological analysis and disambiguation tools will enhance the performance of search engines. It will also allow assessing these tools in a practical context. As far as our tool is concerned, we will evaluate its accuracy in an information retrieval application which deals with vocalized and non-vocalized texts, by introducing a primeval phase of queries and documents disambiguation. Just here, we can renounce the particles’ disambiguation as they are considered stop-words and are not used for indexing. Besides, the morphological features computed by our tools are useful for other levels of analysis (e.g., the syntactic and the semantic levels (Bounhas and Slimani, 2009; Bounhas et al., 2011a; Lahbib et al., 2013)). Furthermore, we should carry a comparison with the state-of-the art tool MADA. The two tools should be trained and assessed on the same data set to ensure an objective comparison. Finally, our work, in this paper, constituted an attempt to treat imprecision in a practical case of application. We introduced new factors to possibilistic classification (namely using the necessity and the information gain), which need to be more investigate in other domains.

@&#REFERENCES@&#

