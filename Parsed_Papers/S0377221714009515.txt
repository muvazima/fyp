@&#MAIN-TITLE@&#Tracking global optima in dynamic environments with efficient global optimization

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Metamodel-based optimization for expensive dynamic black box functions.


                        
                        
                           
                           Novel adaptation of efficient global optimization to dynamic environments.


                        
                        
                           
                           Four approaches to decrease reliance on old information empirically compared.


                        
                        
                           
                           Comparisons with naive approaches of re-optimization or ignoring change show significant improvement.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Heuristics

Dynamic global optimization

Efficient global optimization

Gaussian processes

Response surfaces

@&#ABSTRACT@&#


               
               
                  Many practical optimization problems are dynamically changing, and require a tracking of the global optimum over time. However, tracking usually has to be quick, which excludes re-optimization from scratch every time the problem changes. Instead, it is important to make good use of the history of the search even after the environment has changed. In this paper, we consider Efficient Global Optimization (EGO), a global search algorithm that is known to work well for expensive black box optimization problems where only few function evaluations are possible. It uses metamodels of the objective function for deciding where to sample next. We propose and compare four methods of incorporating old and recent information in the metamodels of EGO in order to accelerate the search for the global optima of a noise-free objective function stochastically changing over time. As we demonstrate, exploiting old information as much as possible significantly improves the tracking behavior of the algorithm.
               
            

@&#INTRODUCTION@&#

Many practical optimization problems are dynamically changing over time, e.g., because new jobs arrive over time in scheduling, the quality of the raw material changes in production processes, or new information becomes available in portfolio management. In dynamic environments, rather than finding the global optimum, the goal is to track the changing optimum over time. Because tracking usually has to be quick, re-optimization from scratch every time the problem changes is not an option. However, because the problem in most applications changes only slightly from one stage to the next, it should be possible to re-use some information from the search process in previous stages to solve the current stage more quickly. On the other hand, completely relying on old information may be misleading, and prevent the algorithm from finding the new optimum.

The idea of using stochastic processes to model a dynamically changing function was first introduced by Kushner (1962) for a one dimensional problem. We are not aware of any other work in this direction. On the other hand, in recent years, dynamic optimization problems have attracted a lot of attention in the evolutionary computation area. Algorithms proposed in this area range from simply continuing with the old population but temporarily increasing the mutation rate in order to allow the algorithm to leave the current local optimum, to sophisticated multi-population approaches that try to simultaneously track multiple optima over time. Recent surveys on this topic can be found in Cruz, González, and Pelta (2011) and Nguyen, Yang, and Branke (2012). In most cases, however, the information transferred from one stage of the problem to the next is information about a set of previously known good solutions.

In this paper, we consider the Efficient Global Optimization (EGO) algorithm proposed by Jones, Schonlau, and Welch (1998). This algorithm constructs and sequentially updates a Gaussian process surrogate model (Kriging metamodel) of the fitness landscape, and uses this model to decide where to sample next, based on the principle of the largest expected improvement (EI). EGO has received significant attention in the literature, and is recognized as a powerful global search algorithm in particular for expensive black box optimization problems, where the number of solutions that can be evaluated is severely limited. This is one of the reasons why we consider EGO a suitable candidate for dynamic optimization problems, as dynamic environments usually require a quick response, thus limiting the number of function evaluations. The other reason is that EGO explicitly maintains a model of the entire fitness landscape, whereas evolutionary algorithms (EA) can only do so implicitly by maintaining a small set of solutions in good regions of the search space. Such a model of the entire fitness landscape should capture most of the information relevant to speed up optimization after a change, in particular information about all the “good” regions in the search space.

We therefore propose and empirically evaluate in this paper various adaptations of EGO to dynamic optimization problems. These adaptations aim at integrating, in the surrogate model, information from previous stages of the problem, acknowledging, however, the fact that such information is old and not fully reliable. We show that the developed variants significantly outperform the straightforward strategies of simply restarting after a change or ignoring that a change happened.

The main contributions of this paper are, first, three out of the four mathematical models for incorporating old and new information together to create a response surface using a Gaussian process. Second, an extensive comparison of the proposed models aiming to help in the model selection according to the dynamics of the problem to be optimized, and third the exploitation of the structure of the EI function to accelerate its maximization using a set of local hill climbers with carefully selected starting points. This paper extends Morales-Enciso and Branke (2014) by introducing three new models, by considering the two dimensional case of the problem, and by comparing all the models in a more thorough manner. Apart from the extended conference proceedings paper, to the authors’ best knowledge, this paper constitutes the first adaptation of the EGO algorithm to approach dynamically changing optimization problems.

The paper is structured as follows. We start with a survey of related work in Section 2. Then, in Section 3, the concepts and techniques of Gaussian processes and EGO are explained, which are the bases for the proposed sampling strategies detailed in Section 4. Some numerical experiments and results are provided and analyzed in Section 5. The paper concludes with a summary and some ideas for future work in Section 6.

@&#RELATED WORK@&#

Global optimization tackles the problem of finding the best solution over the entire search space (Floudas and Gounaris, 2009; Pardalos and Romeijn, 2002; Zhigljavsky and Zilinskas, 2008). Typical approaches include the Nelder–Mead algorithm (Nelder and Mead, 1965), and meta-heuristics such as EAs (Eiben and Smith, 2003), particle swarm optimization (PSO) (Kennedy, Kennedy, and Eberhart, 2001), and GRASP (Hirsch, Pardalos, and Resende, 2010). This paper considers global optimization in the context of expensive evaluations of dynamic black box functions. Black box refers to the lack of an analytic expression of the objective function so methods requiring an analytic expression of the objective function or of its gradient cannot be applied. Expensive evaluation means that every sample or observation taken from the objective function requires a relatively large amount of resources as compared to the additional cost of creating a model to aid the search. This situation is likely to arise for instance in engineering design or when dealing with complex simulations (Burl and Wang, 2009; Fu, 2002; Shelokar, Jayaraman, and Kulkarni, 2008).

The static version of this problem has been extensively studied. Shan and Wang (2010) conclude that current research does not focus on trying to directly model and understand black boxes, but focuses instead on sampling strategies and finding clever uses of the scarce observed data in order to determine promising areas to sample.

Response surfaces (or surrogate models) are approximations of the input–output function of the underlying simulation models or real systems created using available data. They are the output of some sort of regression and imply approximations of the objective function. These models are used when a direct measurement of the function is not practical, for instance if a solution’s quality is not easy to measure or if each measurement is expensive to obtain in time, money, or any other cost unit. Some of the most widely used response surface techniques include, but are not limited to, radial basis functions (RBF) (Regis, 2011), support vector machines (SVM), artificial neural networks (ANN), and Gaussian processes (GPs), also known as kriging (Jones, 2001; Kleijnen, 2009; Lim, Jin, Ong, and Sendhoff, 2010).

In global optimization, the use of surrogates as a replacement for expensive objective functions is a common practice. Surrogate models have been used in two different ways. One way is simply as add-on to some other local search heuristic, filtering out solutions that are likely to perform poorly. For example in combination with EAs, solutions are generated using the standard operators such as crossover and mutation, but before they are evaluated with the expensive objective function, they are evaluated based on the surrogate model. Then, only the most promising candidates, according to the response surface, are accurately evaluated using the expensive objective function. The most commonly used candidate generating techniques are EA (e.g. Zhou and Zhang, 2010; Zhou, Ong, Nair, Keane, and Lum, 2007, see Jin and Branke, 2005 for a survey) and more recently PSO (e.g. Kattan and Arif, 2012).

The second approach is to explore and analyze the generated response surface to decide where to sample next, i.e., to use the generated model directly to propose one or more candidate solutions. A naive way to choose the next best (or most promising) sample is to find the global optimum of the surrogate model and choose it as the next estimate to be evaluated. A far better use of the surrogate, as shown by Jones et al. (1998), is to sample where the EI is maximized. This technique is called EGO and, due to its simplicity in concept and good performance, has become a popular choice in literature with many variations and adaptations. An information theoretic approach which accounts for the overall information gain on the optimizer obtained from a new evaluation has also been presented. This is known as informational approach to global optimization (IAGO), and uses entropy as a measure for information (Villemonteix, Vazquez, and Walter, 2009).

More recently, a generalization to EGO based on a dynamic programming approach known as knowledge gradient-policy for correlated beliefs (KGCB) was proposed by Frazier, Powell, and Dayanik (2008, 2009), and then extended for the continuous case by Scott, Frazier, and Powell (2011) under the name of knowledge gradient for continuous parameters policy (KGCP). Before taking any new sample, KGCP estimates a new response surface for a given possible sample (one step look ahead), and then compares the maximum of this anticipated estimation with the current maximum of the initial surface (as opposed to the maximum observed value used in EGO) in order to calculate the expected gain that would have been achieved had a particular sample been taken. Even with the approximations presented by Scott et al. (2011), this technique is computationally very intensive.

We base our paper on EGO, because it is well established in the field and has proven to be useful in a wide variety of applications, requires far less computational resources than KGCP, and provides a more analytically tractable framework than IAGO. For a study on convergence rates for EGO, the reader is referred to Bull (2011).

The dynamic version of expensive black box optimization deals with tracking the global optimum changing over time. This calls for a more sophisticated exploration strategy capable of keeping track of promising solutions that might become useful at later times. In the general case, the changes can happen each time the function is evaluated, after a given number of evaluations, or after a given period of time. The frequency of the changes depends on the nature of the problem to be solved, for instance, after a given number of performed experiments, or at the beginning of every season. Some studies focus on change detection (Richter, 2009), but this paper assumes the frequency of changes to be known in advance in terms of function evaluations.

If the problem changes completely and there are no similarities between the objective function before and after the change, the best one can do is re-start optimization after every change. In most real world scenarios, however, changes are subtle, and thus it should be possible to transfer some useful information from the search process so far to the search after a change. On the other hand, care must be taken to maintain the search capabilities of the algorithm and not overly rely on outdated information that may be misleading. Optimization in dynamic environments has been a hot research topic over the last years in the areas of computational intelligence, EA, PSO, and ant colony optimization. Surveys on this topic can be found, e.g., in Jin and Branke (2005), Nguyen et al. (2012), and Cruz et al. (2011).

In most of the previous work the information transferred from one search stage to the next is in the form of previously found good solutions. In this paper we explore ways to transfer and adapt the response surface model used by the global optimization algorithm.

A standard dynamic benchmark problem is the Moving Peaks Benchmark (MPB) (Branke, 1999) that we also use here. It consists of a number of peaks that change slowly and independently in location, height and width.

Models which build a response surface using old samples updated with new information from a changed environment are not found in the literature. So, in the remainder of this paper, four techniques to track the global optima of a dynamic expensive black box function based on a response surface are described and compared.

The advantage of GP as a technique to build response surfaces over other methods such as RBF, SVM, and ANN is the analytical tractability it provides not only for the predictions but also for the confidence on its predictions. Furthermore, it provides a natural framework to incorporate old information for the dynamic case as is shown in Section 4.

Let us denote the observed dataset 
                           
                              D
                              =
                              
                                 {
                                 
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       ,
                                       
                                          y
                                          i
                                       
                                       )
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 }
                              
                              =
                              
                                 {
                                 X
                                 ,
                                 Y
                                 }
                              
                           
                         of n D-dimensional samples taken at 
                           x
                        
                        
                           i
                         = [x
                        
                           i, 1, …, x
                        
                           i, D
                        ] with corresponding response values yi
                        .

A GP is fully defined by a mean function which allows to introduce any prior information available into the model, and a covariance function which expresses the covariance between the data points (Rasmussen and Williams, 2006). As a result of applying GP for regression to a dataset, we obtain a distribution on the function that generated the data, also called latent function f.

                           
                              (1)
                              
                                 
                                    f
                                    ∼
                                    GP
                                    (
                                    m
                                    (
                                    x
                                    )
                                    ,
                                    k
                                    (
                                    x
                                    ,
                                    x
                                    )
                                    )
                                 
                              
                           
                        
                     

Unless stated otherwise, throughout this paper, a zero mean prior function (2) and the squared exponential covariance function (3) are used. The two main reasons for using a zero mean prior function are, first, that it allows for a simpler treatment of the equations without loss of generality (as it is argued in Chapter 2.7 of Rasmussen and Williams, 2006), and second, that the initial data set available before starting the sequential optimization process is not necessarily large enough for fitting any model that could serve as mean prior, as it is sometimes advised by some authors. Let

                           
                              (2)
                              
                                 
                                    m
                                    (
                                    x
                                    )
                                    =
                                    0
                                 
                              
                           
                        and

                           
                              (3)
                              
                                 
                                    k
                                    
                                       (
                                       x
                                       ,
                                       
                                          
                                             x
                                          
                                          ′
                                       
                                       )
                                    
                                    =
                                    
                                       σ
                                       f
                                       2
                                    
                                    exp
                                    
                                       (
                                       −
                                       
                                          ∑
                                          
                                             d
                                             =
                                             1
                                          
                                          D
                                       
                                       
                                          
                                             
                                                (
                                                
                                                   x
                                                   d
                                                
                                                −
                                                
                                                   x
                                                   d
                                                   ′
                                                
                                                )
                                             
                                             2
                                          
                                          
                                             2
                                             
                                                θ
                                                d
                                                2
                                             
                                          
                                       
                                       )
                                    
                                    +
                                    
                                       σ
                                       n
                                       2
                                    
                                    δ
                                    
                                       (
                                       x
                                       ,
                                       
                                          x
                                          ′
                                       
                                       )
                                    
                                 
                              
                           
                        denote the mean prior and the covariance function respectively, where 
                           
                              σ
                              f
                              2
                           
                         is the variance of the GP metamodel. The characteristic length-scales for each dimension 
                           
                              θ
                              =
                              [
                              
                                 θ
                                 1
                              
                              ,
                              …
                              ,
                              
                                 θ
                                 D
                              
                              ]
                           
                         (sometimes also called correlation coefficients) represent how much data points influence each other – independently for each dimension – as a function of the distance, and this parameter is key for the proposed sampling method described in Section 4.7. Finally, 
                           
                              σ
                              n
                              2
                           
                         is the noise associated with the sampling process, and δ(
                           x, x′
                        ) is the Kronecker delta function. Since only deterministic objective functions are considered in this paper, 
                           
                              σ
                              n
                              2
                           
                         will be set to zero except for one specific case detailed in Section 4.6 where this parameter plays a major role as a proxy to discount reliability in old samples. However, even in that case 
                           
                              σ
                              n
                              2
                           
                         will not be learned from the data, so only D + 1 parameters are to be inferred. In the general case, there are D + 2 parameters in total which are learned from the available data 
                           D
                         by using maximum likelihood estimation (MLE) (4).

                           
                              (4)
                              
                                 
                                    log
                                    
                                       (
                                       L
                                       
                                          (
                                          
                                             σ
                                             f
                                             2
                                          
                                          
                                             ,
                                             θ
                                             |
                                             D
                                          
                                          )
                                       
                                       )
                                    
                                    =
                                    −
                                    
                                       1
                                       2
                                    
                                    
                                       
                                          Y
                                       
                                       T
                                    
                                    
                                       
                                          K
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    Y
                                    −
                                    
                                       1
                                       2
                                    
                                    log
                                    
                                       |
                                       K
                                       |
                                    
                                    −
                                    
                                       n
                                       2
                                    
                                    log
                                    
                                       (
                                       2
                                       π
                                       )
                                    
                                 
                              
                           
                        
                     

Let 
                           K
                         denote the matrix containing the covariances evaluated at all training points. To estimate the objective function value at a new point 
                           x
                        
                        
                           p
                        , 
                           K
                         is augmented to include this additional data point, leading to the following matrix 
                           K
                        
                        
                           p
                        .

                           
                              (5)
                              
                                 
                                    
                                       K
                                       p
                                    
                                    =
                                    
                                       [
                                       
                                          
                                             
                                                K
                                             
                                             
                                             
                                                
                                                   k
                                                   (
                                                   
                                                      x
                                                      1
                                                   
                                                   ,
                                                   
                                                      x
                                                      p
                                                   
                                                   )
                                                
                                             
                                          
                                          
                                             
                                             
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                
                                                   k
                                                   (
                                                   
                                                      x
                                                      p
                                                   
                                                   ,
                                                   
                                                      x
                                                      1
                                                   
                                                   )
                                                
                                             
                                             
                                                ⋯
                                             
                                             
                                                
                                                   k
                                                   (
                                                   
                                                      x
                                                      p
                                                   
                                                   ,
                                                   
                                                      x
                                                      p
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                       ]
                                    
                                 
                              
                           
                        
                     

Then, we can make a prediction 
                           
                              
                                 y
                                 ^
                              
                              p
                           
                         using (6), and the confidence about that prediction is given by (7), which allows us to characterize the prediction on the outcome yp
                         at the test point 
                           x
                        
                        
                           p
                         with a normal distribution (8), where k(
                           x
                        
                        
                           p
                        , 
                           X
                        ) denotes the last row of 
                           K
                        
                        
                           p
                        .

                           
                              (6)
                              
                                 
                                    μ
                                    :
                                    =
                                    
                                       
                                          y
                                          ^
                                       
                                       p
                                    
                                    =
                                    m
                                    
                                       (
                                       x
                                       )
                                    
                                    +
                                    k
                                    
                                       (
                                       
                                          x
                                          p
                                       
                                       ,
                                       X
                                       )
                                    
                                    
                                       
                                          K
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    Y
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    σ
                                    :
                                    =
                                    Var
                                    
                                       [
                                       
                                          y
                                          p
                                       
                                       ]
                                    
                                    =
                                    k
                                    
                                       (
                                       
                                          x
                                          p
                                       
                                       ,
                                       
                                          x
                                          p
                                       
                                       )
                                    
                                    −
                                    k
                                    
                                       (
                                       
                                          x
                                          p
                                       
                                       ,
                                       X
                                       )
                                    
                                    
                                       
                                          K
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    k
                                    
                                       
                                          (
                                          
                                             x
                                             p
                                          
                                          ,
                                          X
                                          )
                                       
                                       T
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    
                                       y
                                       p
                                    
                                    ∼
                                    N
                                    
                                       (
                                       μ
                                       ,
                                       σ
                                       )
                                    
                                 
                              
                           
                        
                     


                        Eq. (6) is an unbiased estimator, however (7) provides a biased estimator for the variance of the predicted value since it ignores the fact that the parameters in (3) must be estimated. Even though Kleijnen, van Beers, and Van Nieuwenhuyse (2012) recently proposed an adaptation of the EGO algorithm that implements an unbiased estimator for the predictor variance that is calculated through parametric bootstrapping, using such an approximation is standard practice when implementing the EGO algorithm. Furthermore, based on their empirical results, the same authors report that “in general, ...the classic EI [using the biased estimator] may be considered a robust global optimizer”. A complete and formal description on GP is given by Rasmussen and Williams (2006) or MacKay (2003).

Once the surrogate model is available, a sampling strategy such as EGO (Jones et al., 1998) can be followed to determine where the next observation should be taken. EGO looks for the sample that maximizes the expectation of improvement over the currently known best sample, which is possible because the GP provides an analytic expression of the probability distribution for each predicted value (8).

In order to calculate the EI 
                           
                              E
                              [
                              I
                              
                                 (
                                 
                                    x
                                    p
                                 
                                 )
                              
                              ]
                           
                         
                        (11) at the test point 
                           x
                        
                        
                           p
                        , and assuming a maximization problem without loss of generality, the best observed value so far 
                           
                              
                                 y
                                 *
                              
                              =
                              
                                 max
                                 
                                    i
                                    =
                                    1
                                 
                                 n
                              
                              
                                 (
                                 
                                    y
                                    i
                                 
                                 )
                              
                              ,
                           
                         is taken as a reference. Then, the EI is given by the probability of the predicted value yp
                         
                        (9) times the obtained improvement (10), integrated over all possible values better than y* (11).

                           
                              (9)
                              
                                 
                                    P
                                    
                                       (
                                       
                                          y
                                          p
                                       
                                       )
                                    
                                    =
                                    
                                       1
                                       
                                          
                                             2
                                             π
                                             
                                                σ
                                                2
                                             
                                             
                                                )
                                             
                                          
                                       
                                    
                                    exp
                                    
                                       (
                                       −
                                       
                                          1
                                          2
                                       
                                       
                                          
                                             (
                                             
                                                
                                                   μ
                                                   −
                                                   
                                                      y
                                                      p
                                                   
                                                
                                                σ
                                             
                                             )
                                          
                                          2
                                       
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (10)
                              
                                 
                                    I
                                    
                                       (
                                       
                                          x
                                          p
                                       
                                       )
                                    
                                    =
                                    max
                                    
                                       (
                                       y
                                       −
                                       
                                          y
                                          *
                                       
                                       ,
                                       0
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (11)
                              
                                 
                                    E
                                    
                                       [
                                       I
                                       
                                          (
                                          
                                             x
                                             p
                                          
                                          )
                                       
                                       ]
                                    
                                    =
                                    
                                       ∫
                                       
                                          
                                             y
                                             *
                                          
                                       
                                       ∞
                                    
                                    
                                    I
                                    
                                       (
                                       
                                          x
                                          p
                                       
                                       )
                                    
                                    P
                                    
                                       (
                                       
                                          y
                                          p
                                       
                                       )
                                    
                                    d
                                    
                                       y
                                       p
                                    
                                    =
                                    
                                       (
                                       
                                          y
                                          p
                                       
                                       −
                                       
                                          y
                                          *
                                       
                                       )
                                    
                                    Φ
                                    
                                       (
                                       
                                       
                                          
                                             
                                                y
                                                p
                                             
                                             −
                                             
                                                y
                                                *
                                             
                                          
                                          σ
                                       
                                       
                                       )
                                    
                                    +
                                    σ
                                    ϕ
                                    
                                       (
                                       
                                       
                                          
                                             
                                                y
                                                p
                                             
                                             −
                                             
                                                y
                                                *
                                             
                                          
                                          σ
                                       
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

The next sample 
                           x
                        
                        n + 1 is finally taken where the EI is maximized (12) and, together with the observed response, is added to 
                           D
                        . This sampling strategy has proven to be successful in a variety of applications for static problems (Biermann, Weinert, and Wagner, 2008).

                           
                              (12)
                              
                                 
                                    
                                       x
                                       n+1
                                    
                                    =
                                    
                                       
                                          arg
                                          
                                          max
                                       
                                       
                                          
                                             x
                                             p
                                          
                                          ∈
                                          
                                             R
                                             D
                                          
                                       
                                    
                                    
                                       (
                                       E
                                       
                                          [
                                          I
                                          
                                             (
                                             
                                                x
                                                p
                                             
                                             )
                                          
                                          ]
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

The following sequential sampling strategies for parameter optimization of dynamic black box problems build on the principles of the static version. So, each time a new sample is obtained, the response surface is rebuilt by updating the GP with the new observation as described in Section 3.1. Once the response surface has been built, the EGO mechanism is used to determine where to sample next.

The key difference when building response surfaces in dynamic environments is that data of different age are available. Old data should be considered less reliable than recent data because of the changes that have taken place since the time the data were collected.

As stated in Section 2.2, we address the problem where the objective function changes after a known number of evaluations cf
                      (change frequency), and the periods in between changes are referred to as epochs (t) numbered in increasing order. However, it is not the epoch at which each sample was obtained that is relevant to discount the reliability of the sample, but rather how long ago it was taken with respect to the current epoch (tc
                     ). So instead of using the epoch number, it is the age of a sample with respect to the present (τ = tc
                      − t) that is considered to reduce reliability of the samples.

Below, eight strategies are described. The first four are simple strategies used as benchmarks. First, a random sampling strategy is proposed to compare against the completely uninformed case. Then, given the great attention that dynamic optimization problems have attracted from the evolutionary computation area in recent years, we implemented an EA with hypermutation as a more sophisticated benchmark. Furthermore, two limiting cases are presented: the reset strategy as a memoryless model that starts solving the problem from scratch after every change, and the ignore strategy that disregards all the changes and considers all the information equally reliable. The last four sampling strategies are proposed as different ways of reusing and transferring information from old epochs to the new ones, exploiting different properties of GP, and constitute the main contribution of this paper. These eight methods are compared through numerical experiments in Section 5.

In order to build a response surface using a GP, it is necessary to start with at least λ = 2 data samples to be able to estimate the length-scale parameters of the process. So, for the first epoch it is assumed that there are at least λ ≥ 2 observations previously obtained following – for instance – the random sampling strategy described in Section 4.1, or any other traditional design of experiments technique such as Latin hypercube or stratified sampling (Hernandez, Lucas, and Sanchez, 2012; McKay, Beckman, and Conover, 1979).

Let 
                        D
                      be the set of all the samples collected throughout the history of the experiment, and 
                        
                           
                              D
                              τ
                           
                           ⊂
                           D
                        
                      the set of data points of age τ.

The random sampling strategy explores the parameter space 
                           
                              x
                              ∈
                              
                                 R
                                 D
                              
                           
                         by independently drawing a random number from a uniform distribution for each dimension. This technique serves only as a benchmark in order to set a reference to assess the improvements of the other techniques, and there is no response surface built.

The selected EA strategy is a simple (μ + λ)-ES which generates a random initial population for the first epoch, but keeps in memory the last population of the previous epoch to use it as starting population after a change happens. This creates a memory effect. Population size μ = λ = 5 has been used for 1D problems and 10 for 2D problems. To avoid premature convergence and re-introduce diversity after a change, a hypermutation strategy (Cobb, 1990) has been implemented that re-initializes the mutation step size to a high value after a change and linearly decreases it with iterations. This strategy does not require a response surface.

This strategy discards all the previously obtained samples every time a change of the objective function happens. This is equivalent to a re-start, as if this were a new problem. So, at the current epoch (τ = 0), the response surface will be estimated using only current information in 
                           
                              D
                              0
                           
                         (see Section 3.1). Since previous samples are not considered, at the beginning of each epoch λ observations need to be sampled in order to start building the response surface one more time.

The reset strategy also serves as a reference to measure the improvement obtained by other sampling strategies. Besides, it is useful in the presence of very drastic changes where there is no similarity between the objective function before and after each change.

As its name suggests, the ignore strategy ignores the fact that a change has happened, which means that all the available samples in 
                           D
                         are used to fit the response surface. Not only is this a bad strategy to find the global optima of a changing function because old information is taken to be as valid as new one, potentially misguiding the search, but also because it unnecessarily increases the computational cost of generating the GP model. This is the opposite extreme to the reset strategy and serves as another benchmark. The ignore strategy is useful when the magnitude of the changes is negligible and the problem is thus similar to a static problem.


                        Reset* differs from reset 
                        (Section 4.3) only in the way the first samples of a new epoch (other than the first epoch) are taken. Instead of taking λ initial observations at the beginning of a new epoch (τ = 0), reset* looks for the best response found in the immediate previous epoch (τ = 1) and resamples at the same place where this previously best response was obtained. Furthermore, the length-scale parameters (
                           θ
                        ) found at the end of the immediate previous epoch are reused in order to overcome the inability of fitting a GP with only one data point and allow to take a second sample. Once the second sample has been obtained, the sampling process continues as the reset strategy (i.e. refitting the GP parameters from the available data (
                           
                              D
                              0
                           
                        ) every time a new sample becomes available) until the next function change.


The idea behind this strategy is to consider newly obtained samples as deterministic – as it has been done throughout this paper –, but to introduce some artificial measurement noise in order to discount the old samples. The recent observations, being treated as deterministic (no noise added), force the response surface to go exactly through the measured sample, while the old observations, treated as noisy observations, allow the response surface to pass within some distance of the actually observed response values (proportional to the magnitude of the introduced noise) but not necessarily through them. By considering old information but discounting its accuracy, the search is guided to the regions where there used to be good responses in order to explore if that is still the case, but it is acknowledged that the landscape might have changed.

GPs provide a natural way of introducing noise in different magnitudes for each data sample through the noise measurement term (
                           
                              σ
                              n
                              2
                           
                        ) in Eq. (3). Furthermore, the introduced noise can be a function of the age of the observations. This modification gives rise to Eq. (13) which is used to calculate the covariance matrix needed to generate the response surface for the DIN model.

                           
                              (13)
                              
                                 
                                    k
                                    
                                       (
                                       x
                                       ,
                                       
                                          
                                             x
                                          
                                          ′
                                       
                                       )
                                    
                                    =
                                    
                                       σ
                                       f
                                       2
                                    
                                    exp
                                    
                                       (
                                       −
                                       
                                          ∑
                                          
                                             d
                                             =
                                             1
                                          
                                          D
                                       
                                       
                                          
                                             
                                                (
                                                
                                                   x
                                                   d
                                                
                                                −
                                                
                                                   x
                                                   d
                                                   ′
                                                
                                                )
                                             
                                             2
                                          
                                          
                                             2
                                             
                                                θ
                                                d
                                                2
                                             
                                          
                                       
                                       )
                                    
                                    +
                                    
                                       σ
                                       n
                                       2
                                    
                                    
                                       (
                                       τ
                                       )
                                    
                                    δ
                                    
                                       (
                                       x
                                       ,
                                       
                                          
                                             x
                                          
                                          ′
                                       
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 σ
                                 n
                                 2
                              
                              
                                 (
                                 τ
                                 )
                              
                           
                         is now a function of the age of the samples and no longer a constant as in Eq. (3), and can be any strictly increasing function in τ such that 
                           
                              
                                 σ
                                 n
                                 2
                              
                              
                                 (
                                 
                                    τ
                                    c
                                 
                                 )
                              
                              =
                              0
                              ,
                           
                         for instance 
                           
                              
                                 σ
                                 n
                                 2
                              
                              
                                 (
                                 τ
                                 )
                              
                              =
                              τ
                              
                                 s
                                 2
                              
                              ,
                           
                         where s is some constant noise level.

The introduced noise 
                           
                              
                                 σ
                                 n
                                 2
                              
                              
                                 (
                                 τ
                                 )
                              
                           
                         increases as a function of the age of the samples following a predefined functional form which is user defined rather than learned.

Since DIN uses samples from previous epochs, it is not necessary to generate any random samples nor to reuse the GP parameters from previous epochs. Nonetheless, the first sample of each epoch is taken where the best response was obtained at the previous epoch, following the same procedure as in reset* 
                        (Section 4.5).

An illustration of this model is provided in Fig. 1
                        , which shows the sequential sampling process using discounted information through noise (s
                        2 = 2.0). Fig. 1(a) shows the end of an epoch, along with the GP response surface (continuous line) generated using samples obtained at t (circles). The confidence interval presented for the response surface (gray area) corresponds to ± σ away from the predicted mean μ. The EI is shown using a different scale, displayed on the right hand side, and is the curve displayed at the bottom of the box. The vertical line shows where the next best sample should be taken according to EGO. In Fig. 1(b) the new sample has been taken (square), but the true objective function has changed (semi-dashed line). When fitting the GP, the response surface passes exactly through the new observation even if there are other old samples in the region (circles). But in the absence of recent information in other regions, old data are used to guide the response surface. Fig. 1(c) and (d) shows the next two samples taken sequentially.

Another way of using the temporal information is by considering time as an additional dimension (D + 1). The advantage of this method is that it learns the time correlations from the data instead of relying on arbitrarily chosen functions or noise levels as it is done in the DIN method explained in Section 4.6. The learning process takes place when estimating the introduced parameter θ
                        
                           D + 1 and it is done by maximizing the likelihood. Nevertheless, having an additional parameter to estimate during the likelihood maximization increases the difficulty of finding the optimal parameters as compared to the other models.

Introducing an additional dimension is naturally done through the covariance function (3). Consider an observed data point 
                           
                              x
                              ∈
                              
                                 R
                                 D
                              
                              ,
                           
                         then let 
                           
                              
                                 x
                                 ˜
                              
                              ∈
                              
                                 R
                                 
                                    D
                                    +
                                    1
                                 
                              
                           
                         be the vector containing 
                           x
                         augmented by the age of the samples τ. Then, the correlation function between two augmented samples can be written as:

                           
                              (14)
                              
                                 
                                    k
                                    
                                       (
                                       
                                          x
                                          ˜
                                       
                                       ,
                                       
                                          
                                             x
                                             ˜
                                          
                                          ′
                                       
                                       )
                                    
                                    =
                                    
                                       σ
                                       f
                                       2
                                    
                                    exp
                                    
                                       (
                                       −
                                       
                                          ∑
                                          
                                             d
                                             =
                                             1
                                          
                                          
                                             D
                                             +
                                             1
                                          
                                       
                                       
                                          
                                             
                                                (
                                                
                                                   
                                                      x
                                                      ˜
                                                   
                                                   d
                                                
                                                −
                                                
                                                   
                                                      x
                                                      ˜
                                                   
                                                   d
                                                   ′
                                                
                                                )
                                             
                                             2
                                          
                                          
                                             2
                                             
                                                θ
                                                d
                                                2
                                             
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

The procedure for estimating the response surface is not changed. When evaluating the response surface from the surrogate model, τ must be set to zero so as to make valid predictions for the current time (i.e. 
                           
                              
                                 
                                    x
                                    ˜
                                 
                                 
                                    D
                                    +
                                    1
                                 
                                 ′
                              
                              =
                              0
                           
                        ).

The first sample of each epoch is chosen exactly as in the DIN strategy (Section 4.6).

The models presented so far have all assumed a zero mean prior GP, as proposed in Eq. (2). However, since in the regions where there are no data to support any prediction the GP tends to the mean prior, the mean prior of a GP can be used to introduce any information available. The rate at which the response surface tends to the mean prior in each dimension depends on the corresponding inferred length-scale θd
                        . An example of this behavior can be seen in Fig. 1 throughout all the subfigures in the sample space interval [ − 3, −2], where the predicted response surface tends to zero lacking data to support any other predictions.

PSMP exploits this property of GP in order to transfer information about good regions found at previous epochs to the current epoch through the mean prior by providing a tailored estimate of the expected value at each point. The information is introduced into the current model through the mean prior function in the shape of the response surface available at the end of the previous epoch, which is a GP as well.

This gives rise to a recurrent definition where the mean prior of a GP at the current epoch (m
                        0), which uses the data set 
                           
                              
                                 D
                                 0
                              
                              ,
                           
                         is the GP fitted at the end of the previous epoch (m
                        1) using the data 
                           
                              D
                              1
                           
                        . For the first epoch (t = 0), PSMP uses the mean of the first λ samples as a constant mean prior (15), which carries subindex τ = t since it will always be t epochs old with respect to the current epoch.

                           
                              (15)
                              
                                 
                                    
                                       m
                                       t
                                    
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    
                                       m
                                       ¯
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             λ
                                          
                                          
                                             y
                                             i
                                          
                                       
                                       λ
                                    
                                 
                              
                           
                        
                     

The first observation of any other epoch (t > 0) is taken where the best response was obtained at t − 1, following the procedure explained for reset* 
                        (Section 4.5) including the reuse of the parameters. Once there is at least one previous epoch and two data points in 
                           
                              
                                 D
                                 0
                              
                              ,
                           
                         the mean prior at any point is simply the value of the response surface model from the previous epoch evaluated at this point. This means m
                        1 is used as the prior mean for constructing the response surface of age 0 (m
                        0), and in general the response surface at epoch τ is given by:

                           
                              (16)
                              
                                 
                                    
                                       m
                                       τ
                                    
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    GP
                                    
                                       (
                                       
                                          m
                                          
                                             τ
                                             +
                                             1
                                          
                                       
                                       
                                          (
                                          x
                                          )
                                       
                                       ,
                                       k
                                       
                                          (
                                          x
                                          ,
                                          x
                                          )
                                       
                                       |
                                       
                                          D
                                          τ
                                       
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (17)
                              
                                 
                                    
                                       m
                                       
                                          τ
                                          +
                                          1
                                       
                                    
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   GP
                                                   (
                                                   
                                                      m
                                                      
                                                         τ
                                                         +
                                                         2
                                                      
                                                   
                                                   
                                                      (
                                                      x
                                                      )
                                                   
                                                   ,
                                                   k
                                                   
                                                      (
                                                      x
                                                      ,
                                                      x
                                                      )
                                                   
                                                   |
                                                   
                                                      D
                                                      
                                                         τ
                                                         +
                                                         1
                                                      
                                                   
                                                   )
                                                
                                             
                                             
                                                
                                                   
                                                   if
                                                   
                                                   τ
                                                   <
                                                   t
                                                
                                             
                                          
                                          
                                             
                                                
                                                   m
                                                   ¯
                                                
                                             
                                             
                                                
                                                   
                                                   if
                                                   
                                                   τ
                                                   =
                                                   t
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Before fitting the parameters (
                           θ
                        ) by MLE using Eq. (4), care must be taken to subtract the mean prior from the vector of observations 
                           Y
                        
                        0. Eq. (2) in the definition of the GP is then replaced by (17). Finally, when using the fitted GP to maximize the EI – or for making any predictions –, the previously removed mean must be added back, which is already considered in Eq. (6).

@&#EXPERIMENTS AND RESULTS@&#

The various methods proposed in the previous section are compared empirically using the MPB, a standard benchmark used for dynamic global optimization. In this section, a brief description of the benchmark along with two performance measures is provided, followed by a detailed description of the experiments performed to test the performance of each of the eight presented approaches. Finally, a statistical analysis of the experiments is done to support the results.

The MPB (Branke, 1999) consists of a D dimensional continuous function defined in a given interval with N peaks. Each peak (pi
                        ) is defined by its position 
                           
                              
                                 x
                                 i
                              
                              ∈
                              
                                 R
                                 D
                              
                              ,
                           
                         height hi
                        , and width wi
                         (i ∈ [1, …, N]). At every change, each of the peaks changes slightly in position, height, and width. For the position of the peaks, the magnitude of the change (vL
                        ) is fixed, but the direction is random. The changes in height and width of the peaks are independent, normally distributed, and scaled according to their corresponding severity parameter. Besides, these three components are bounded by their corresponding upper and lower bounds: (
                           x
                        
                        
                           l
                        , 
                           x
                        
                        
                           u
                        ), (hl, hu
                        ), and (wl, wu
                        ). In all cases, the boundary conditions are reflective, which means that after a change, if the updated parameter falls outside the bounds by some magnitude, such parameter would bounce back with the same magnitude.

The benchmark is one of the classic benchmarks for dynamic optimization problems in the evolutionary computation area. To be successful, an algorithm has to be able to track a moving peak, but also to jump from one peak to another if the heights change such that another peak becomes the highest peak.

A standard performance measure for dynamic optimization problems is the offline error (Branke, 1999). The offline error is the time-averaged error of the best solution found so far in the epoch. More specifically, let all evaluations be numbered consecutively from 1…T, and denote by jτ
                         the number of the first evaluation in each epoch. Then, at a particular time t in epoch τ, i.e., t ∈ [jτ
                        , …, j
                        
                           τ + 1 − 1], the current error 
                           
                              ϵ
                              t
                              c
                           
                         is the error of the best evaluation within this period, or

                           
                              (18)
                              
                                 
                                    
                                       ϵ
                                       t
                                       c
                                    
                                    =
                                    
                                       min
                                       
                                          i
                                          =
                                          
                                             j
                                             τ
                                          
                                          …
                                          
                                             j
                                             
                                                τ
                                                +
                                                1
                                             
                                          
                                          −
                                          1
                                       
                                    
                                    
                                       f
                                       τ
                                       *
                                    
                                    −
                                    
                                       y
                                       i
                                    
                                    .
                                 
                              
                           
                        The offline error is then simply the average over all current errors:

                           
                              (19)
                              
                                 
                                    
                                       ϵ
                                       T
                                       o
                                    
                                    =
                                    
                                       1
                                       T
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       T
                                    
                                    
                                       ϵ
                                       i
                                       c
                                    
                                    ,
                                 
                              
                           
                        
                     

The offline error assumes that evaluations are done offline (hence the name), i.e. the best known solution found so far since the last change is actually implemented in the real world while the search for a better solution continues in a separate process.

Another performance measure we look at is the average error, defined as

                           
                              (20)
                              
                                 
                                    
                                       
                                          ϵ
                                          T
                                       
                                       ¯
                                    
                                    =
                                    
                                       1
                                       T
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       T
                                    
                                    
                                       ϵ
                                       i
                                    
                                    ,
                                 
                              
                           
                        which measures the average deviation from the global optimum of each function evaluation performed so far. For both performance measures, offline and average errors, if the T index is dropped, we refer to the error measured at the end of the run.

@&#IMPLEMENTATION DETAILS@&#

The implementation of the MPB simulates the sequential sampling process applying the eight different strategies described in Section 4 in the attempt of tracking the global optima. The parameters governing the dynamics of the objective function are detailed in Table 1
                        .

All the simulations start with an initial number of λ = 4 samples, and when applicable the same number of initial samples is used at the beginning of later epochs. Then, one of the proposed strategies is followed to fit a GP to the available data.

The EGO policy is followed, so the EI function (11) needs to be maximized. The benefit of generating a surrogate model is that the EI is cheap to evaluate relative to the real objective function, so direct optimization methods can be applied to find the maximum EI. Nevertheless, the EI is itself highly multimodal.

The problem with using a local hill climber with random multi-start is that as the number of data points used to fit the GP increases, the problem gets harder. More specifically, a new local maximum can be generated in the hyper box created by each possible pair of neighbor samples. So the more samples there are, the more regions possibly containing local maxima must be explored. Even with a large number of starting points for the local hill climbers, the probability of leaving at least one hyper box unexplored increases exponentially with the number of samples used to generate the response surface (
                           D
                        ).

Even though Jones et al. (1998) proposed a limited memory branch-and-bound algorithm fine-tuned with a local search in order to maximize the EI, one of the same authors proposed later a method to generate starting points for local maximizers together with a method for clustering the results (Jones, 2001). More recently reported methods to maximize the EI are mainly based on genetic algorithms (Forrester, Sóbester, and Keane, 2007).

In this paper we propose a different method for finding good starting locations for local hill climbers by exploiting the structure of the EI function. The local hill climbers’ starting points are placed at the center of each hyper box created by every pair of neighbor samples, resulting in 
                           
                              ζ
                              =
                              
                                 
                                    (
                                    |
                                    D
                                    |
                                    +
                                    1
                                    )
                                 
                                 D
                              
                           
                         starting points. Their initial step size is chosen such that each local hill climber does not get out of its corresponding region in the first step, forcing it to explore the locality. This method is still a heuristic and does not guarantee to find the global maxima of the EI, but it shows better performance than random multi-start locations.

In this particular implementation, each time the EI is maximized, ζ local hill climbers – each implementing the Nelder–Mead method – are used. For efficiency purposes, methods using data from previous epochs only consider old data from the immediate previous epoch (i.e. τ = 1).

In order to compare the different models presented in Section 4, a set of experiments with different parameter settings was run. Table 1 shows the parameters for the MPB. Values in bold are the default parameters used for the base case whenever more than one value was tested for a parameter. In this section only the base case is considered, and the parameter analysis is left for Section 5.4.2. The parameters chosen for the base case are D = 1, vL
                            = 0.25, hs
                            = 7.0, and cf
                            = 25. The simulations are run for 80 epochs, and all results are averaged over R = 64 independent replications.


Since the DIN sampling strategy (Section 4.6) requires parameter tuning for the noise level, each experiment has to be run in two steps. The first step is to find out the optimal noise level s* by running a first set of simulations of the optimizer using the DIN sampling strategy with different noise discount values, and empirically choosing the one with best performance. In this case, offline error is chosen as the preferred measure of performance, so the remainder of the experiments focuses mainly on this performance measure, but the same procedure would apply for the average error. Since the changes of the objective function are stochastic, several replications are required to provide statistical significance to the interpretation of the results. So, R = 64 replications were run in this first part of the experiment.


                           Fig. 2
                            (a) shows how the performance of the DIN strategy changes according to the chosen discount noise. A discount noise level of s = 0 (no discount at all) corresponds to a version of the ignore strategy with the first solution evaluated after a change being the best found solution from the previous epoch, while an infinite discount of the old samples (s → ∞) makes the approach more similar to the reset* sampling strategy but does not take the initial λ samples at the beginning of new epochs. For comparison purposes, the performance of three other strategies (reset*, TasD + 1, and PSMP) is displayed in the background as well. We observe that for some values of s, TasD + 1 and PSMP outperform the DIN strategy, but for well tuned values of s, the DIN strategy outperforms the others. This reveals the importance of choosing an appropriate value for the discount noise, although an exhaustive tuning might not be required since values close to the optimum do not vary drastically in their performance.

Once the DIN strategy has been tuned, all the strategies can be run to asses their performance. For this part of the experiment another R = 64 replications were run using common random numbers across strategies. These replications do not share common random numbers with the previous step. For each of the eight sampling strategies, the results at the end of the 80 epochs are shown in Fig. 3
                            using box plots, which allows an easy visual comparison, although it is not as powerful as the statistical tests performed in Section 5.4.3. We observe that the dominating strategy depends on the performance measure selected. If we consider offline error, DIN dominates all the other strategies, whereas if we consider average error, PSMP is the best. Nevertheless, the advantage of reusing information in a discounted manner is consistent across performance measures, which is evidenced by the fact that reset*, EA, DIN, TasD + 1, and PSMP – all reusing information from previous epochs – outperform the strategies that either do not use it (reset) or do not discount it (ignore). The EA with hypermutation is outperformed as well, demonstrating that EGO can be made competitive with other metaheuristics for dynamic expensive black box optimization problems.


In terms of computational cost, strategies that use data only from the current epoch (reset, reset*, and PSMP) to fit the GP are much faster than those using data from previous epochs (ignore, DIN, and TasD + 1). The difference is due to the fact that fitting a GP has computational complexity 
                              
                                 O
                                 (
                                 |
                                 D
                                 
                                    |
                                    3
                                 
                                 )
                              
                           . Comparing DIN and TasD + 1, the latter has an additional parameter to fit, and so TasD + 1 has a larger computational cost when maximizing the likelihood. However, this is negligible when compared to the parameter tuning for DIN that requires several replications, each fitting a GP, for each attempted parameter value. Fig. 4
                            illustrates running times for all the strategies compared.

In order to better understand how the offline error behaves throughout the simulation and to verify that the comparison of the final values happens after convergence, the whole evolution across time is visualized in Fig. 2(b).

Finally, the current error plots, displayed in Fig. 5
                           , are useful to understand the effect of each sampling strategy. We can see that, at the beginning of each new epoch, strategies dismissing old samples (random and reset) start with a similarly poor solution in every epoch, and require some time to find good solutions. The ignore strategy benefits of old samples at the beginning of later epochs, but because the information it relies on is outdated, it does not manage to improve much after that and its performance seems to deteriorate from epoch to epoch. The first sample of the last four strategies is taken at the location where the best observation was made in the previous epoch, which explains the fact that they start with a large advantage in terms of current error. However, they all treat old data in different ways which accounts for the difference in performance. A better comparison of the convergence to the global optimum at each epoch is shown in the plot with the overlapped current errors, where we can see that even if reset* has a good starting point, it gets stuck in good – yet not optimal – regions, which is perhaps due to the lack of memory about good regions from previous epochs. DIN, TasD + 1 and PSMP do not seem to get stuck, although they seem to have different convergence rates, with the DIN strategy being the fastest to find the global optima.


The sampling strategies make different use of the available information, so we expect their performance to depend on the magnitude of the variations suffered by the dynamic objective function. In order to explore this, two more experiments were run. First, the configuration (vL
                            = 0.5, hs
                            = 7.0) was used to simulate an increase in the distance for which the locations of the peaks change, and second the parameters were set to (vL
                            = 0.25, hs
                            = 15.0) so as to simulate a scenario where the peaks move as fast as the reference, but the global maximum is more likely to change from one peak to another. Both scenarios make historic information less reliable than the base case. The offline errors measured at the end of these simulations for each strategy are summarized in Fig. 6
                           , where we observe that even if the ranking is preserved, for an increased vL
                           , all the strategies using old information perform worse than in the base case, which is expected since old information is less reliable due to the greater magnitude of the changes in between epochs. When increasing hs
                           , the performance of reset* deteriorates quite significantly, while the other strategies with memory preserve a similar performance with respect to the base case (compare Fig. 6(b) with Fig. 3(a)). Given that an increased height severity means that the global maximum jumps from peak to peak more frequently, it makes sense that strategies preserving information about different good regions in previous epochs (and not only about the best region as reset*) are better at tracking the global optimum in this case.

To verify whether these results extend to higher dimensions, two more experiments were run, this time for the two dimensional case, including the base case, and the increase in the distance for which the locations of the peaks change. Except for the length of the simulations and the frequency of changes, all the other parameters were kept as for the 1D base case. Since the difficulty of finding the global maximum of a function increases with the dimension of the function, the frequency of changes was decreased to allow more function evaluations in between changes. For these experiments, there are cf
                            = 50 function evaluations in each epoch, the experiments were run for 20 epochs, and R = 64 replications of each experiment were performed. The results obtained are compiled in Fig. 7
                           .

In the experiments performed, the DIN sampling strategy outperforms TasD + 1. This might be due to the fact that TasD + 1 has to learn an additional parameter (time scale). However, since DIN has a parameter that requires manual tuning (through an extensive set of simulations), TasD + 1 might be preferred unless similar problems are solved repeatedly, in which case the effort of tuning parameters may be justified.

PSMP significantly outperforms the reset* sampling strategy in the one dimensional case, but the effective difference for the two dimensional case is small. This is probably related to the number of samples used to approximate the mean prior, which for the one dimensional case is 25 and seems to be enough to provide an accurate approximation of the previous epoch’s landscape. However, since the number of required samples to create a good approximation of the landscape is expected to increase as a power of the dimension, 50 samples might not be enough to create a good approximation of the previous epoch in two dimensions.

These simulations show that the DIN model outperforms all others for the tested scenarios, but the fact that a previous tuning needs to be made to find the optimum discount noise level must not be discarded, since it requires a lot of computation. So, using the DIN strategy only makes sense if similar problems are solved repeatedly, and for practical purposes or under the constraints of limited resources the TasD + 1 strategy might be preferred.

The box plots shown above provide a visual guide on the individual performance of the presented sampling strategies for each experiment, but they do not exploit the advantage of using common random numbers for the simulations. To compare the performance of the different models taking advantage of the pairing, we use the paired Wilcoxon signed-rank statistical test. This test is preferred to the Student’s t-test because the latter assumes that the compared samples are drawn from a normal distribution, which is not the case (Fay and Proschan, 2010; Wilcoxon, 1945).

Consider the eight models compared, and number them in ascending order as they were presented so that the random sampling strategy is 1, EA strategy is 2, reset* is 3, and so on. For a given experiment, let 
                              
                                 
                                    ϵ
                                    ˜
                                 
                                 i
                              
                            (i ∈ [1, …, 8]) denote the median offline error of the R replications of strategy i. And let the null hypothesis (H
                           0) be that the median difference between the pairs of observations is zero. So the alternative hypothesis (H
                           1) is that the median difference of the pairs is not zero 
                           (21). After performing the set of tests and calculating the corresponding p-values (pij
                           ), 
                              
                                 H
                                 
                                    i
                                    j
                                 
                                 0
                              
                            can be rejected – in favor of 
                              
                                 H
                                 
                                    i
                                    j
                                 
                                 1
                              
                            – at the αij
                            = 1 − pc
                            significance level if pij
                            < pc
                           .

                              
                                 (21)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   H
                                                   
                                                      i
                                                      j
                                                   
                                                   0
                                                
                                                :
                                                
                                                   
                                                      ϵ
                                                      ˜
                                                   
                                                   i
                                                
                                                =
                                                
                                                   
                                                      ϵ
                                                      ˜
                                                   
                                                   j
                                                
                                                ,
                                                
                                                i
                                                ,
                                                j
                                                ∈
                                                
                                                   [
                                                   1
                                                   ,
                                                   …
                                                   ,
                                                   8
                                                   ]
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   H
                                                   
                                                      i
                                                      j
                                                   
                                                   1
                                                
                                                :
                                                
                                                   
                                                      ϵ
                                                      ˜
                                                   
                                                   i
                                                
                                                ≠
                                                
                                                   
                                                      ϵ
                                                      ˜
                                                   
                                                   j
                                                
                                                ,
                                                
                                                i
                                                ,
                                                j
                                                ∈
                                                
                                                   [
                                                   1
                                                   ,
                                                   …
                                                   ,
                                                   8
                                                   ]
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           These null hypotheses were tested for all of the configurations described in order to rank the strategies according to their performance in offline error. We found that for all the 1D cases 
                              
                                 H
                                 
                                    i
                                    j
                                 
                                 0
                              
                            can be rejected using pc
                            = 0.002, or – equivalently – at the 99.8 percent significance level. For the 2D scenarios the same is true for almost all comparisons using pc
                            = 0.003. A ranking of the different strategies evaluated according to the median offline error for all the experiments described is presented in Table 2
                           , where we can see that the ranking of the strategies does not change across different settings.

@&#DISCUSSION AND CONCLUSIONS@&#

An adaptation of EGO to track global optima in dynamic environments has been proposed and tested in this paper. Specifically, four sequential sampling strategies relying on GP to build a surrogate model have been described. Different properties of GP have been exploited to construct the response surface using both old and new information to enhance tracking of the global optima for dynamic expensive black box optimization problems. These four new sampling strategies, together with four other benchmark sampling strategies have been compared through numeric simulations implementing the moving peaks benchmark and using the offline error as performance measure.

The poor performance of the random strategy throughout the different experiments confirms the advantages of using informed selection of the points to be sampled. The simple trick of re-evaluating the previous best found solution at the beginning of an epoch highly improves the performance in the experiments considered. This idea has been used in all the newly proposed strategies. The experiments also show that sampling strategies using old information in an explicit way (DIN, TasD + 1, and PSMP) systematically perform significantly better than those which either discard it (reset and reset*) or treat it in the same way as recent information (ignore).

Future work in this area might focus on how to combine the different sampling strategies here presented. For example, building a more accurate response surface from old samples using the TasD + 1 method and using it as mean prior for the PSMP strategy. Or perhaps, trying to remove the tuning component for the DIN strategy by learning the amount of noise to be introduced online. Another direction could be to extend this work for dynamic objective functions with continuous changes, as opposed to changes happening only after a given number of function evaluations. Although TasD + 1 could naturally cope with this case, individual levels of noise might be required for each sample if using DIN, and additional modifications would be required for PSMP to work. A more interesting extension would be to extend these strategies to be able to cope with constraints.

The methods exposed in this paper could further be adapted for creating response surfaces using information coming from simulations with different fidelities. This case is common while looking for optimal designs in fluid dynamics for instance, where simulations which accurately model all the interactions between particles are computationally extremely expensive whereas simplified models are much faster to run but can only provide rough approximations. By correctly taking into account the origins of the data using adaptations of these models, improved response surfaces can be built.

@&#REFERENCES@&#

