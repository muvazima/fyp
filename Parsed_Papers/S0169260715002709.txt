@&#MAIN-TITLE@&#Image processing based automatic diagnosis of glaucoma using wavelet features of segmented optic disc from fundus image

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           In this work glaucoma identification is done using wavelet features of optic disk.


                        
                        
                           
                           Wavelet features are extracted from segmented and blood vessels removed optic disk.


                        
                        
                           
                           Several machine learning algorithms are used for prominent feature selection.


                        
                        
                           
                           Genetic algorithm is used to reduce the dimensionality of feature vector.


                        
                        
                           
                           Accuracy of glaucoma identification achieved in this work is 94.7%.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Glaucoma

Fundus image

Blood vessels

Wavelet transform

Feature extraction

Classification

@&#ABSTRACT@&#


               
               
                  Glaucoma is a disease of the retina which is one of the most common causes of permanent blindness worldwide. This paper presents an automatic image processing based method for glaucoma diagnosis from the digital fundus image. In this paper wavelet feature extraction has been followed by optimized genetic feature selection combined with several learning algorithms and various parameter settings. Unlike the existing research works where the features are considered from the complete fundus or a sub image of the fundus, this work is based on feature extraction from the segmented and blood vessel removed optic disc to improve the accuracy of identification. The experimental results presented in this paper indicate that the wavelet features of the segmented optic disc image are clinically more significant in comparison to features of the whole or sub fundus image in the detection of glaucoma from fundus image. Accuracy of glaucoma identification achieved in this work is 94.7% and a comparison with existing methods of glaucoma detection from fundus image indicates that the proposed approach has improved accuracy of classification.
               
            

@&#INTRODUCTION@&#

Glaucoma is a disease of retina in which the optic nerve undergoes a damage caused by the increase in the intraocular pressure (IOP) of the eye. There is a fluid called aqueous humor that flows through the pupil and is absorbed by the bloodstream. In case of glaucoma the flow of this fluid becomes clogged. This results more intraocular pressure in the eye which damages the highly sensitive optic nerve causing vision impairment. It mainly affects the portion inside the optic disk where the size of the optic cup increases resulting in a high cup-to-disk ratio. It causes the successive narrowing of the field of view of affected patients.

After diabetic retinopathy, glaucoma is the second highest cause of blindness across the world. Glaucoma is not curable and the loss of vision cannot be regained but with early diagnosis it is possible to prevent further loss of vision by proper medication and surgery.

One serious threat and cause of concern for glaucoma is unlike many other diseases the signs and symptoms of glaucoma are not immediately felt and experienced [1] by the patient. By the time the patient experiences the signs and symptoms of glaucoma, the damage in the retina is done. According to data available [1] it is estimated that more than 2.2 million Americans have glaucoma but unfortunately 50% of them are not aware that they have it. The situation is even worse in developing countries of Asia and Africa where there is a shortage of trained ophthalmologist for diagnosis of glaucoma. Hence, there is a need to develop automatic and efficient computer based methods for diagnosis of such diseases.

Some work has been reported in computer vision based identification of glaucoma from digital fundus image. An image processing based glaucoma identification process [2] was presented in which several image based features were analyzed and combined to capture signs of glaucoma. Wavelet-Fourier analysis was used [3] for characterization of neuroanatomic changes in glaucoma images. The results reported in this work are encouraging and may attract more work using wavelet for identification of glaucoma from digital fundus images.

There are many studies where segmentation of the optic cup and optic disc from the color fundus image is done for calculation of the optic cup to disk ratio (CDR) to identify symptoms of glaucoma. An optic disc segmentation technique is presented [4] which use the local image information around some points of interest in multi-dimensional feature space and it seems to provide robustness against some variants around the optic disk region.

A statistical model based method [5] for the segmentation of the optic disc and optic cup from digital color fundus images is presented where knowledge based circular Hough Transform is used. A CDR calculation based method is presented [6] where a double threshold based approach is used, one for removing blood vessels and background and second threshold for segmenting the super intensity pixels contained by the optic disc and optic cup.

Multiple features such as area of blood vessels in Inferior, Superior, Nasal and Temporal side near to optic disk (ISNT ratio), neuroretinal rim analysis, distance between optic disc center and optic nerve head [7] has been used for identification of glaucomatous eyes.

Proper orthogonal decomposition (POD) is a technique [8] that uses structural features to identify glaucomatous progression. The use of texture features and higher order spectra (HOS) features were proposed [9,10] by for glaucomatous image classification. The use of wavelet-Fourier analysis (WFA) for the characterization of neuroanatomic disruption in glaucoma [11,12] was proposed and the results are encouraging. Discrete wavelet based features of fundus image are used [13] to identify glaucomatous images.

Some high dimensional feature vectors from the fundus images are compressed and combined [14] and is classified using SVM classifiers. A super-pixel based classification method [16] for glaucoma screening from fundus images is presented where in optic disc segmentation, histograms and some statistical parameters are used to classify each super-pixel as disc or non-disc.

In all these methods of glaucoma detection, features are extracted from the whole fundus image or a sub-image containing optic disc. The features from the image portions outside the optic disc do not carry any significant information about glaucoma and are redundant and may also adversely affect the automatic classification. Keeping this in mind this paper presents a method where the optic disc is first automatically segmented from the fundus image and then the features are extracted from the segmented optic disc.

The main contribution of this paper is a novel method that achieves 94.7% accuracy in classification of glaucoma from fundus images. This outperforms accuracy of comparable works [7,9,12–14,16]. Detailed comparative information is provided in Section 5. The idea behind this paper is an automatic glaucoma identification algorithm from digital fundus images where feature extraction and classification is done from the segmented region of optic disc. Unlike the existing method where the features have been extracted from the complete fundus image or a sub-image this method has the advantage of reducing the redundant features which do not contribute to glaucoma identification. In this way only features from the glaucoma informative regions are considered which will improve the accuracy of identification. There may be a possibility that the features from non-informative regions may adversely affect the classifiers during training and testing stages and this method prevents such situation to arise.

Another important contribution of this work is removal of the blood vessels from the segmented optic disc before features are extracted from the segmented optic disc. Blood vessel pixels are considered as noisy pixels in the case of glaucoma identification and the removal of the blood vessels before feature extraction may improve the accuracy of classification. First level wavelet features from segmented and blood vessel in-painted optic disc image are extracted. These features are more prominent in comparison to spatial domain features for classification which will enhance sensitivity and specificity of glaucoma image classification. To improve the performance of glaucoma image classification, extracted features from this blood vessel removed image are subjected to feature selection scheme and then prominent features are considered for further classification.

Another highlight of this work is to improve the performance of classifiers used for glaucoma image classification where evolutionary attribute selection algorithm is used to select the most prominent features. Genetic algorithm with operator optimized selection (evolutionary) is employed for feature selection to reduce the dimensionality of feature vector which enhances the performance efficiency of the classifiers.

The rest of the paper is organized as follows. Section 2 includes the explanation of the fundus image data base used in this paper. The next section describes the justification of feature selection from segmented optic disc. Section 4 describes the proposed methodology of glaucoma detection including optic disc segmentation, feature extraction, feature selection and description of the classifiers and classifier parameters employed for our experiments. Section 5 includes experimental results obtained using proposed method. Finally Section 6 provides conclusion to the paper.

The fundus images used for this work were collected from the Venu Eye Research Center, New Delhi, India. In this work total 63 images were used for experimentation from patients aged 18–75. Total of 63 images out of which 44 images are used to train the classifiers and remaining 19 images are used to check the performance of classifiers. All the images were used anonymously and had an ethical clearance from the hospital committee. The inbuilt imaging software of the fundus camera is used to take pictures of the inner surface of the eye in the JPEG format and had a resolution of 2544×1696 pixels. The combined anterior and posterior segment digital photography eye camera with IR lighting source was used to capture the images. This camera is used by doctors to diagnose eye diseases like diabetic retinopathy, glaucoma, etc. and to monitor their progression. An example of a normal and a glaucoma fundus image is shown in Fig. 1(a) and (b) respectively. Fig. 1(c) shows the fundus camera used to capture the images. It can be clearly seen from Fig. 1 that in case of the glaucoma image the optic cup has spread more than normal image.

In the existing methods of glaucoma detection from feature based classification, either the complete fundus image or retinal sub-image containing optic disc is considered for feature extraction. For example, a combination of texture feature and higher order spectra features of fundus image [9] are used for glaucomatous image classification. Pixel intensity values, FFT coefficients and B-spline coefficients of retinal sub-image containing optic disc [14] are used for glaucoma risk evaluation. Discrete wavelet based features of the whole fundus image [12] are used to identify glaucoma images.

It is known that optic disc is the main area where glaucoma symptoms occur. The features that may help to classify glaucoma in the fundus image are limited inside the optic disc and the portion of the image outside the optic disc does not contribute significantly toward glaucoma identification.

As a research hypothesis for glaucoma identification, features extracted from segmented optic disc images seem to be more prominent in comparison to features of the whole fundus image. This is because the features extracted from the regions outside the optic disc of the fundus image may not have information regarding glaucoma and may be redundant. Such features may also pose problems in training a classifier.

The proposed method involves the features that are extracted from the segmented optic disc image for glaucoma identification. Also the segmented optic disc is in-painted for the blood vessel removals and this seems to be more promising for feature extraction. The research hypothesis used in the proposed method seems to be more effective for classification of glaucoma from fundus images.

The proposed method of glaucoma detection involves wavelet feature extraction and analysis of segmented optic disc image. Optic disc is automatically segmented from the fundus image and then wavelet features are extracted for analysis and classification. This glaucoma detection algorithm does not require any manual intervention to take decision whether the input fundus image is normal or glaucomatous. The input image will be automatically processed and the decision is made without any manual input or intervention. A flow diagram of the proposed method is presented in Fig. 2
                     .

To segment the optic disc from the digital fundus image, initially the center of the optic disc is localized and then optic disc is segmented using bit plane analysis.

Center of the optic disc is calculated using a double windowing method and a circular sub-image is cropped as a region of interest (ROI) by considering the center of optic disc as a center of the cropped circular area. As it is medically known that optic disc is the brightest object of fundus image so optic disc is segmented accurately using strategic combination of 6th, 7th and 8th bit planes. To get the prominent features of optic disc, blood vessels are removed using an in-painting method from the segmented optic disc image. This is important as the blood vessel pattern in fundus images are arbitrary and has no known pattern which varies from sample to sample. These blood vessels cover the retinal features and cause in image analysis. First level wavelet features are then extracted from this image and these features are processed for further glaucoma image classification.

In this paper feature processing is done using two approaches to improve the performance of classifiers and make an efficient glaucoma detection system. In the first approach high prominent features are selected using evolutionary attribute selection method and then only more prominent feature are considered for further classification. Feature selection is done to improve the accuracy of classifier in this approach.

In the other approach of feature analysis the extracted features are compressed using feature reduction technique by applying principle component analysis (PCA). Only two main principle components which are highly discriminatory are considered for further classification purpose. This approach of glaucoma detection is less complex as it does not involve feature ranking or feature selection techniques. This improves the accuracy as only distinct features are considered for glaucoma image classification.

The proposed method of glaucoma detection system is broadly divided into following four main parts:
                        
                           A.
                           Optic disc segmentation and blood vessel removal.

Wavelet feature extraction from segmented optic disk image.

Prominent feature selection.

Glaucoma image classification.

To extract the features from optic disc there is a need to segment out the optic disc from the fundus image. In the proposed method, features are extracted from segmented optic disc image so that only the prominent features are considered for glaucoma detection. In addition to this blood vessels are removed from the segmented optic disc region as these noisy pixels cover the retinal features and may affect the accuracy of classification. The center of the optic disc is detected using a double windowing technique and the optic disc is segmented using bit plane analysis. After the segmentation of optic disc, blood vessels are removed from that segmented optic disc region using morphological image processing. Optic disc segmentation and blood vessel removal process is briefly explained in the following sub-sections.

Optic disc (OD) is the brightest feature of fundus image. The OD is the location where the optic nerve exits, and it is usually a vertically slightly elliptical shaped object. Hence the strategy adopted in this paper is to detect the large bright intensity objects in the fundus image. The steps of the proposed method are given in Algorithm 1
                           
                           .
                              Algorithm 1
                              //Optic disc center detection//
                                    
                                       
                                          
                                          
                                             
                                                
                                                   
                                                      Step 1:
                                                    Extraction of Green channel from RGB Fundus image as green channel has maximum information content needed for OD centre detection.
                                             
                                             
                                                
                                                   
                                                      Step 2:
                                                    A “Kaiser window” of dimensions W
                                                   ×Total no. of columns in the fundus image is created. Where W equals to of 1/10th of the total no. of rows in the fundus image. Fig. 3 shows the Kaiser window.
                                             
                                             
                                                
                                                   
                                                      Step3:
                                                    This window is passed through all the rows of fundus image and a maximum is selected to get the x-coordinate of center of OD.
                                             
                                             
                                                
                                                   
                                                      Step4:
                                                    To get the y-coordinate of OD center, a “Hat shaped Kaiser Window” of dimensions W
                                                   ×
                                                   W as shown in the Fig. 4 is created. This window is passed through all the columns of the obtained row of step 3. The Maxima is selected as y-coordinate of center of OD.
                                             
                                          
                                       
                                    
                                 
                              

On analyzing the RGB components of the image, it was found that the optic disc can be easily discriminated from Red channel. In the proposed method, optic disc is segmented from fundus image using a bit plane technique. As OD is clearly discriminated in red channel of fundus image hence it is proposed to segment the OD using logical combination of bit planes for three higher most significant bits (MSB's). In this paper data base images used has 8 bit representation, hence 6th, 7th and 8th bit planes of red channel of fundus image are considered for optic disc segmentation. The algorithm for segmentation of optic disc from fundus image is explained in Algorithm 2.
                           Algorithm 2
                           //Segmentation of optic disc from fundus image//
                                 
                                    
                                       
                                       
                                          
                                             
                                                
                                                   Step1
                                                : I is the red channel of RGB fundus image and (X
                                                o, Y
                                                o) is the centre of optic disc.
                                          
                                          
                                             
                                                
                                                   Step2:
                                                 Extraction of Circular sub image I
                                                
                                                   s
                                                 using centre (X
                                                o, Y
                                                o) and radius R
                                                o.
                                          
                                          
                                             
                                                
                                                where
                                          
                                          
                                             
                                                
                                                   
                                                      (1)
                                                      
                                                         
                                                            
                                                               R
                                                               o
                                                            
                                                            =
                                                            
                                                               1
                                                               2
                                                            
                                                            
                                                               
                                                                  
                                                                     c
                                                                     6
                                                                  
                                                                  +
                                                                  
                                                                     r
                                                                     4
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             where r and c are no. of rows and no. of columns in fundus image respectively. This sub-image will cover the whole OD and this calculation also provides some additional margin area.
                                          
                                          
                                             
                                                
                                                   Step3:
                                                 Segment the common area of 7th and 8th bit planes of I
                                                
                                                   s
                                                
                                             
                                          
                                          
                                             
                                                
                                                
                                                I
                                                1
                                                =
                                                I
                                                7
                                                +
                                                I
                                                8
                                             
                                          
                                          
                                             Where I
                                                7 and I
                                                8 are 7th and 8th bit planes of Circular sub image I
                                                
                                                   s
                                                 respectively and “+” denotes logical “AND” operation.
                                          
                                          
                                             
                                                
                                                   Step4:
                                                 it is found that if OD is segmented accurately using strategic logical combinations of 7th and 8th bit plane then bright pixels near to OD area will be very less, otherwise there will be considerably more number of bright pixels that will appear near to segmented OD. This seems to be a good choice and is used to consider this as a check point to know whether OD is accurately segmented using these bit planes of the image.
                                          
                                          
                                             
                                                
                                                   Step5:
                                                 If OD is segmented accurately using 7th and 8th bit plane and passes the check point in step 4 then final segmented OD (I
                                                
                                                   od
                                                )
                                          
                                          
                                             
                                                
                                                
                                                I
                                                od
                                                =
                                                I
                                                1
                                             
                                          
                                          
                                             
                                                Else
                                          
                                          
                                             
                                                
                                                go to Step 6.
                                          
                                          
                                             
                                                
                                                   Step 6:
                                                 Segment common area of 6th bit plane and I
                                                1
                                             
                                          
                                          
                                             
                                                
                                                
                                                I
                                                2
                                                =
                                                I
                                                6
                                                +
                                                I
                                                1
                                             
                                          
                                          
                                             Where I
                                                6 is 6th bit plane of Circular sub image I
                                                
                                                   s
                                                 and “+” denotes logical “AND” operation.
                                          
                                          
                                             
                                                
                                                   Step 7: I
                                                
                                                   od
                                                
                                                =
                                                I
                                                2
                                             
                                          
                                          
                                             
                                                
                                                   Step 8:
                                                 Removal noisy pixels from I
                                                
                                                   od
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           

To remove noisy pixels from final segmented OD, the objects are labeled and objects are selected which has largest area and roundness. Object having largest area and roundness is considered as the optic disc.


                        Fig. 5
                         is the images of the various steps involved in which OD is segmented from fundus image using the method discussed above. Two samples of fundus image, its circular sub-image containing OD and segmented OD are shown in Fig. 5(a)–(c) respectively.

The blood vessels act as noisy pixels which covers the OD and it has to be removed for improved accuracy of OD detection. Removal of blood vessels in this paper is done using an algorithm based on the fact that pixel intensity is a function of the distance from the center of optic disc. The gray level pixel values decrease as the distance from the optic disc center increases outwards. It follows that all the pixels that are at equal distance from the OD center have similar intensity values and any deviation from this pattern may indicate the presence of a blood vessel. This logic seems to be a good choice to remove blood vessel pixels from the optic disc image without tampering the OD pixels. The algorithm for blood vessel in-painting is explained stepwise in Algorithm 3.
                           Algorithm 3
                           //Blood vessel in-painting//
                                 
                                    
                                       
                                       
                                          
                                             
                                                
                                                   Step 1: I is optic disc image and (X
                                                o, Y
                                                o) is centre of OD.
                                          
                                          
                                             
                                                
                                                   Step 2:
                                                 Let P
                                                ={P
                                                1, P
                                                2,…,
                                                P
                                                
                                                   q
                                                ,….,
                                                P
                                                
                                                   k−1, P
                                                
                                                   k
                                                } is a set of pixels that are on equal distance R from the optic disc centre.
                                          
                                          
                                             
                                                
                                                   Step 3:
                                                 Calculate
                                          
                                          
                                             
                                                
                                                   
                                                      (2)
                                                      
                                                         
                                                            T
                                                            =
                                                            
                                                               min
                                                            
                                                            (
                                                            
                                                               P
                                                            
                                                            )
                                                            
                                                               +
                                                            
                                                            
                                                               
                                                                  {
                                                                  
                                                                     max
                                                                  
                                                                  (
                                                                  P
                                                                  )
                                                                  −
                                                                  
                                                                     min
                                                                  
                                                                  (
                                                                  P
                                                                  )
                                                                  }
                                                               
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   Step 4:
                                                 if Pq
                                                <T
                                          
                                          
                                             
                                                
                                                then P
                                                
                                                   q
                                                 is assumed to be a blood vessel pixel.
                                          
                                          
                                             
                                                
                                                   Step 5:
                                                 To remove the blood vessel pixel P
                                                
                                                   q
                                                , the pixel value P
                                                
                                                   q
                                                 is replaced by the median of these k pixels of P.
                                          
                                          
                                             
                                                
                                                   
                                                      (3)
                                                      
                                                         
                                                            
                                                               P
                                                               
                                                                  q
                                                                  (
                                                                  n
                                                                  e
                                                                  w
                                                                  )
                                                               
                                                            
                                                            =
                                                            m
                                                            e
                                                            d
                                                            i
                                                            a
                                                            n
                                                            (
                                                            
                                                               P
                                                               1
                                                            
                                                            ,
                                                            
                                                               P
                                                               2
                                                            
                                                            ,
                                                            …
                                                            ,
                                                            
                                                               P
                                                               k
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           


                        Fig. 6
                         shows two samples of optic disc containing sub-image, corresponding segmented optic disc image and blood vessels in-painted optic disc image. It can be seen from the figure that blood vessel pixels are removed from the optic disc image.

The texture features of image in spatial domain are roughly defined as the spatial variation of gray scale values of pixel intensity across the image. These features are not effective enough for glaucoma image classification as these are limited to pixel intensity values and there is no information about hidden frequency content. The wavelet transform captures both the spatial and frequency information of an image. In the wavelet transform, the image is represented in terms of the frequency of content of local regions over a range of scales. This representation provides a good framework for the analysis of image features, which are size independent and can often be characterized by their frequency domain properties.

In the proposed method, blood vessel in-painted optic disc image is transformed in wavelet domain for feature extraction.

Discrete wavelet transform (DWT) is a multi-scale analysis method, in which analysis can be performed on various scales and levels. Each level of the transformation provides an analysis of the source image at a different resolution, resulting in its independent approximation and detailed coefficients. The features may be more categorical if higher level DWT decomposition is used but computation time increases for DWT higher level decomposition. In the proposed method first level decomposition is used as the size of the segmented OD is small in size. Keeping in consideration that the segmented optic disc is very small in size there is a possibility that some information may be lost in the higher level decomposition of DWT. On the basis of experiments, it was found that there is enough discriminatory features for glaucoma detection in first level DWT decomposition. Hence first level DWT feature are used in the proposed method as discriminatory features for glaucoma detection system making it computationally cheap and suitable for real time applications. Higher order decomposition may make the computational cost more and not required in this case as the first order decomposition provides enough discriminatory variation. Fig. 7
                         represents two samples of optic disc image used for feature extraction.

Wavelet domain first level approximation and detailed coefficients (horizontal, vertical and diagonal coefficients) are extracted using filters db3, Symlet3, Haar and Biorthogonal filters (bio 3.3, bio 3.5, bio3). Since the number of elements in these matrices of wavelet coefficients is high and only a single number as a representative feature is needed, the average and energy parameters are calculated for each matrix of wavelet coefficients to determine such single-valued features. This strategy is employed to get a feature vector as a representative of the optic disc image. Algorithm 4 is the stepwise procedure for wavelet feature extraction from the segmented vessel in-painted optic disc image.
                           Algorithm 4
                           //Wavelet based feature extraction for OD image//
                                 
                                    
                                       
                                       
                                          
                                             
                                                
                                                   Step 1:
                                                 Read the segmented and blood vessels-removed optic disc image as the input image I.
                                          
                                          
                                             
                                                
                                                   Step 2:
                                                 Input image I is transformed into wavelet domain using two-dimensional discrete wavelet transform and first level approximation and detailed wavelet coefficients are obtained as explained in Eq. (1).
                                          
                                          
                                             
                                                
                                                   
                                                      (4)
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           A
                                                                        
                                                                        
                                                                           
                                                                              C
                                                                              h
                                                                           
                                                                        
                                                                        
                                                                           
                                                                              C
                                                                              v
                                                                           
                                                                        
                                                                        
                                                                           
                                                                              C
                                                                              d
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                            =
                                                            d
                                                            w
                                                            t
                                                            (
                                                            I
                                                            )
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             Where “dwt” represents two-dimensional discrete wavelet transform, A is first level approximation coefficient and Ch, Cv and Cd represents detailed horizontal, vertical and diagonal coefficients respectively.
                                          
                                          
                                             
                                                
                                                   Step 3:
                                                 Single-valued features mean and energy are calculated for first level horizontal, vertical and diagonal wavelet coefficients. The definition of mean and energy features for horizontal detailed coefficient Ch is explained in Eqs. (2) and (3) respectively
                                          
                                          
                                             
                                                
                                                   
                                                      (5)
                                                      
                                                         
                                                            M
                                                            e
                                                            a
                                                            n
                                                             
                                                            C
                                                            h
                                                            =
                                                            
                                                               1
                                                               
                                                                  p
                                                                  ×
                                                                  q
                                                               
                                                            
                                                            
                                                               ∑
                                                               
                                                                  x
                                                                  =
                                                                  {
                                                                  p
                                                                  }
                                                                  ,
                                                                     
                                                                  y
                                                                  =
                                                                  {
                                                                  q
                                                                  }
                                                               
                                                            
                                                            
                                                               |
                                                               c
                                                               h
                                                               (
                                                               x
                                                               ,
                                                               y
                                                               )
                                                               |
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      (6)
                                                      
                                                         
                                                            E
                                                            n
                                                            e
                                                            r
                                                            g
                                                            y
                                                             
                                                            C
                                                            h
                                                            =
                                                            
                                                               1
                                                               
                                                                  
                                                                     p
                                                                     2
                                                                  
                                                                  ×
                                                                  
                                                                     q
                                                                     2
                                                                  
                                                               
                                                            
                                                            
                                                               ∑
                                                               
                                                                  x
                                                                  =
                                                                  {
                                                                  p
                                                                  }
                                                                  ,
                                                                     
                                                                  y
                                                                  =
                                                                  {
                                                                  q
                                                                  }
                                                               
                                                            
                                                            
                                                               
                                                                  
                                                                     (
                                                                     c
                                                                     h
                                                                     (
                                                                     x
                                                                     ,
                                                                     y
                                                                     )
                                                                     )
                                                                  
                                                                  2
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             where p and q represents number of pixels in the rows and columns of horizontal wavelet coefficient ch respectively.
                                          
                                          
                                             
                                                
                                                   Step 4:
                                                 These features are calculated for wavelet coefficients obtained using db3, Symlet3, Haar and Biorthogonal filters. A feature vector containing 18 features is obtained to represent optic disc image in wavelet feature space.
                                          
                                       
                                    
                                 
                              
                           

To improve the accuracy of glaucoma image classification only prominent features from the 18 features extracted are selected for further processing and used in the classification.

The performance of glaucoma image classification may be increased if only the prominent features are considered for classification and redundant features are ignored. The complexity of classifier will reduce if less number of features is used for classification. Keeping in mind the issue of higher accuracy and less complexity, only prominent features are used for further glaucoma image classification.

In this proposed work two different techniques are used for prominent feature selection. In the first method prominent features are selected using evolutionary attribute selection method and then only selected prominent features are considered for further classification. Another technique tried was based on the feature reduction using principal component analysis (PCA) and then only two main principle components of compressed feature set which are more discriminatory are selected for glaucoma image classification. This technique of feature selection is less complex and efficient to select more discriminatory features for classification. Both these methods are discussed in the following subsection.

To improve the performance of classifiers used for glaucoma image classification, evolutionary attribute selection algorithm is used to select the most prominent features before classification. It is found that this algorithm selects the most prominent features for the feature set extracted from segmented optic disc image. Evolutionary attribute selection algorithm selects the prominent features depending on the classification algorithm which may improve the accuracy of classifiers. In this paper genetic algorithm [17–20] with operator optimized selection (evolutionary) is employed [24] for feature selection to reduce the dimensionality of feature vector consisting of 18 wavelet features.

Genetic algorithm generates set of features (60 for one generation) and these features are used for training model on the training data. This trained model was validated on testing data. Number of generations has fluctuated around 5 for this feature set. This feature selection algorithm selects the set of prominent features depending on the classification algorithm used which will surely improve the accuracy of classification algorithms. It can be seen from Table 1
                            that different features are selected for different classifier to improve the performance. Selected features are represented as “√” in Table 1 for the classifiers used. This table shows the best possible combination to achieve best accuracy for the particular classifier used.

To make the glaucoma image classification system less complex principal component analysis (PCA) is used for discriminatory feature selection. Here the feature set extracted from segmented optic disk image are compressed using the feature reduction technique principal component analysis (PCA) and then principle components which has higher weightage are considered. On analysis it is found that only two main principle components of compressed feature set has higher weightage and more discriminatory for glaucoma image classification. Hence two main principle components are finally selected for glaucoma image classification. In addition to improve the performance of PCA compression, features set are normalized before compression using z-score normalization method.

Each of the 18 features extracted from segmented optic disc image is subjected to z-score normalization [15]. In the process of z-score normalization, a sample (vector) consisting of 18 features is converted to zero mean and unit variance.

The mean and standard deviation of the input vector are computed as follows:
                              
                                 (7)
                                 
                                    
                                       
                                          y
                                          
                                             n
                                             e
                                             w
                                          
                                       
                                       =
                                       
                                          
                                             
                                                Y
                                                
                                                   o
                                                   l
                                                   d
                                                
                                             
                                             −
                                             m
                                             e
                                             a
                                             n
                                          
                                          
                                             s
                                             t
                                             d
                                          
                                       
                                    
                                 
                              
                           where Y
                           
                              old
                            is the original value, Y
                           
                              new
                            is the new value, mean and std are the mean and standard deviation of the original data range, respectively. Normalized feature vector are compressed using PCA [28] and then glaucoma image classification is done by considering only two main principle components of compressed feature set.

Considering the prominent features, the performance of most popular classification algorithms Decision Tree, k-NN, Random Forest, K-star, support vector machine (SVM), artificial neural network (ANN) with various parameter configuration are measured and then the classifier which has the best accuracy are finally considered for glaucoma detection. Once the prominent features are selected for the classification process, performance of classification algorithm is measured for different possible parameters and the best possible setting parameters which have the best performance are finally selected.


                        Table 2
                         presents the best parameters setting for the classification algorithms for the features selected using evolutionary attribute selection method and feature set compressed using PCA. Classification algorithms used for glaucoma image classification and their main control parameters are explained below:

In random forest classification algorithm [21] number of trees is the main control parameter which specifies the number of random trees to generate. Accuracy of this classification algorithm is measured for different values of tree and tree value which provides the best accuracy is selected for classification.

In the Naïve Byes [22] classifier Laplace Correction is an expert parameters used to avoid the case of zero probabilities. The value of Laplace correction can be selected as True/False depending on the performance of classification.

The value of no. of nearest neighbors, weighted vote and distance measure metrics are the main control parameters for k-NN classification algorithm [23]. Accuracy of this classification algorithm highly depends upon the no. of nearest neighbors used.

The control parameters for ANN classifiers [25] are number of hidden layers, number of neurons in the hidden layers, training cycles, learning rate, momentum, decay and shuffle. Performance of ANN classification algorithm highly depends on some main control parameters number of neurons in the hidden layers and training cycles used to train the ANN classifier.

The complexity parameter and accuracy of SVM classifier [26, 27] depends on the kernel function used to distribute the data into different classes.

Performance of SVM classifier is measure for different kernel functions and the best kernel function which has the best accuracy is finally selected for classification.

Experiments have been carried out in a local data base of 63 fundus images and results of the experiments are presented and discussed in this section. Out of 63 images, 44 images are used to train the classifier and remaining 19 images are used to test the accuracy of classifier. Initially training images are categorized into two classes named as normal image and glaucoma image and they were classified by professional ophthalmologist. These are used to train the classifier. There is no overlap in the data used in testing and training set of images. This section provides a detailed description of the results of glaucoma detection using different classifiers. Experiments has been performed for different classifiers using feature vector selected by evolutionary attribute selection method and the feature vector compressed using dimension reduction technique PCA.


                     Table 3
                      represents the performance of different classifiers considering only prominent features to improve the accuracy of classification. Prominent features are selected using evolutionary attribute selection technique and then only selected features are considered for further classification. It is clearly shown in the table that ANN classifier and random forest classifier provides 94.7% accuracy with feature vector selected using evolutionary attribute selection technique.


                     Table 4
                      shows the accuracy of different classifiers using feature vector compressed using the dimension reduction technique of PCA. To make the classification accurate and less complex, only two main principle components which are more discriminatory are selected for classification. On the basis of experiments, it is seen that K-NN and SVM classifier provides the best accuracy for the feature vector compressed using PCA.


                     Figs. 8 and 9
                     
                      show the plot of two main principle components of PCA which are selected for classification using SVM and K-NN respectively. As seen from Table 4 these two classifiers provide 94.7% accuracy for this feature vector having two main principle components of PCA. It is clearly visible in the figure that selected principle components are more discriminatory for classification.


                     Table 5
                      shows the total number of training and testing samples which are further categorized into normal and glaucoma class for classification purpose. Total of 63 images out of which 44 images are used to train the classifiers and remaining 19 images are used to check the performance of classifiers. For further validation sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV) analysis is done and results are reported in Tables 6 and 7
                     
                     .

With respect to this statistical treatment sensitivity is the probability that, glaucoma class is identified as glaucoma and specificity is defined as the probability of normal class is classified as normal. The PPV and NPV describe the performance of a diagnostic test.

All these five parameters of performance measurement are calculated using following equations as shown below:
                        
                           (8)
                           
                              
                                 Sensitivity
                                  
                                 or
                                  
                                 True
                                  
                                 Positive
                                  
                                 Rate
                                 =
                                 
                                    
                                       
                                          TP
                                       
                                    
                                    
                                       
                                          TP+FN
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (9)
                           
                              
                                 Specificity
                                  
                                 or
                                  
                                 True
                                  
                                 Negative
                                  
                                 Rate
                                 =
                                 
                                    
                                       T
                                       N
                                    
                                    
                                       T
                                       N
                                       +
                                       F
                                       P
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (10)
                           
                              
                                 Positive
                                  
                                 Predictive
                                  
                                 Value
                                  
                                 or
                                  
                                 Precision
                                 =
                                 
                                    
                                       T
                                       P
                                    
                                    
                                       T
                                       P
                                       +
                                       F
                                       P
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (11)
                           
                              
                                 Negative
                                  
                                 Predictive
                                  
                                 Value
                                 =
                                 
                                    
                                       T
                                       N
                                    
                                    
                                       T
                                       N
                                       +
                                       F
                                       N
                                    
                                 
                              
                           
                        
                     
                     
                        
                           (12)
                           
                              
                                 Accuracy
                                 =
                                 
                                    
                                       T
                                       P
                                       +
                                       T
                                       N
                                    
                                    
                                       T
                                       P
                                       +
                                       F
                                       P
                                       +
                                       T
                                       N
                                       +
                                       F
                                       N
                                    
                                 
                              
                           
                        
                     
                  

Where True positive (TP), is defined as number of glaucoma image classified as glaucoma and True negative (TN) is the number of normal image classified as normal. False positive (FP) is the number of normal image is identified as glaucomatous and False negative (FN) is the number of glaucoma image is classified as normal. Tables 5 and 6 represent the results of sensitivity and specificity analysis for each of the classifier using feature selection and feature reduction technique.


                     Fig. 10
                      shows the accuracy of glaucoma image classification using different classification algorithms. It is seen in Fig. 10 that SVM and k-NN classification algorithm provides highest accuracy for features selected using PCA. Random forest and neural network classifier provides highest accuracy of features selected using evolutionary attribute selection as clearly visible in Fig. 10.

On the basis of experiments performed it is found that Random forest and neural network classifier provides the best accuracy which is 94.7% for the feature set selected using attribute selection method. It can be clearly seen from Tables 3 and 6 that neural network classifier provides 87.50% specificity and 100% sensitivity and random forest classifier detected glaucoma with 90.91% sensitivity and 100% specificity. These results can be considered as a significant contribution in glaucoma detection. Two main principle components of PCA which are found more discriminatory are considered for classification to make this approach less complex.


                     Tables 4 and 7 clearly represent that K-NN and SVM classifier provides the best performance for the feature vector compressed using PCA. SVM classifier provides 94.75% accuracy with 100% sensitivity and 87.50% specificity and K-NN classifier detected glaucoma with 100% sensitivity and 90.91% specificity. Experimental results indicate that proposed method of glaucoma detection using attribute selection technique with neural network and random forest classifier and using PCA with K-NN and SVM classifier can be considered significant contribution in glaucoma identification having high levels of accuracy.

The average running time for OD segmentation followed by blood vessels inpainting was 25second per image and for wavelet feature extraction was 4.5second per image (size of image is 1696
                     ×
                     2544). The computational time for prominent feature selection using PCA for all 44 training samples and 19 testing samples was 5second. The computational time to train the classifier using 44 training samples and to test the performance of classifier using 19 testing samples was approximate 5.5second and 1.3second respectively.

The algorithms were implemented in MATLAB R2011b (Math Works) software using CPU@ 2.3GHz, 4Gb RAM, 32 bit operating system.

Since, the image data base used is relatively small and using more samples for testing can create over fitting of the samples, so leave one out cross validation (LOOCV) classification method [29] was also employed for classification of the normal and glaucomatous samples. In this validation method one image is used for testing and other remaining images are used as training data at one time then this process is repeated for whole data of images.

If total images are n then (n
                        −1) images are used for training and one image for testing. The average sensitivity, specificity and accuracy of all the samples are calculated to measure the performance of glaucoma image classification.

The LOOCV method are used for feature set compressed using PCA and SVM classifier are selected to measure the performance of classification as this PCA+SVM combination provides the highest accuracy of glaucoma detection.

In this proposed method total dataset is of 63 samples are used for leave one out cross validation (LOOCV). Out of 63 samples, 62 samples are used for training and remaining one sample is used for testing the performance of classifier. Similarly one by one all samples are tested and average sensitivity, specificity and accuracy of all the samples are calculated to measure the performance of glaucoma image classification. Table 8
                         represents the results of cross validation using LOOCV.

This cross validation method provides an accuracy of 95.24% with specificity of 93.33% and sensitivity of 96.97% for SVM classifier. Positive predictive and negative predictive value for glaucomatous image classification is 94.12% and 96.55% respectively. The result of LOOCV shows that proposed glaucoma detection method performs well and can be considered as a significant contribution in glaucoma screening system.


                        Table 9
                         gives a comparative analysis of performance of the proposed method with some existing methods of glaucoma detection. It can be seen from the table that all the existing methods used features of full fundus image or sub-image containing OD for glaucoma detection. It can be seen from the experimental results that accuracy of glaucoma detection is improved if features are extracted from blood vessel removed segmented optic disc image as done in the proposed method.

@&#CONCLUSION@&#

In this work, an automatic image analysis based system is proposed for diagnosis of glaucoma from the digital fundus image using wavelet features from the segmented optic disc. The noisy blood vessels were removed from the segmented optic disc and the features were extracted from this de-noised image. Experimental results indicate those glaucoma images were classified with 94.7% accuracy using first level wavelet features from segmented and blood vessels in-painted optic disc image which outperforms the related works.

Performance of glaucoma image classification were measured for five supervised classifiers using prominent feature selected using evolutionary attribute selection and principal component analysis. It is observed that all the five classifiers present accuracy more that 85%. Random forest and artificial neural network classifiers can identify the presence of glaucoma with 94.7% accuracy for feature set selected using evolutionary attribute selection whereas SVM and k-NN presents 94.7% accuracy for feature selected using principal component analysis. This proposed study is clinically significant, as the accuracy obtained is comparable to the accuracy achieved by existing methods. Some future work in this direction may be use of other feature selection methods and different classification approaches to improve accuracy and efficiency.

@&#ACKNOWLEDGEMENT@&#

This work was supported in parts by the Grants from Department of Science and Technology, Govt. of India No. “DST/TSG/ICT/2013/37” and National Sustainability Program under grant LO1401, Czech Republic. Authors also express their thankfulness to Dr. S.C. Gupta, Medical Director of Venu Eye research center for his contribution in classification of the images.

@&#REFERENCES@&#

