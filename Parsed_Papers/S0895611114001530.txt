@&#MAIN-TITLE@&#Robust multi-scale superpixel classification for optic cup localization

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Glaucoma is a leading cause of vision loss, & optic cup detection is of great interest.


                        
                        
                           
                           An optimal model integration framework to robustly localize the optic cup is presented.


                        
                        
                           
                           It addresses performance variations from random repeated training.


                        
                        
                           
                           Multiple superpixel scales are also integrated for better cup boundary adherence.


                        
                        
                           
                           It outperforms the intra image learning approach in cup localization accuracy.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Optic cup localization

Glaucoma

Model selection

Superpixel classification

Sparse learning

@&#ABSTRACT@&#


               
               
                  This paper presents an optimal model integration framework to robustly localize the optic cup in fundus images for glaucoma detection. This work is based on the existing superpixel classification approach and makes two major contributions. First, it addresses the issues of classification performance variations due to repeated random selection of training samples, and offers a better localization solution. Second, multiple superpixel resolutions are integrated and unified for better cup boundary adherence. Compared to the state-of-the-art intra-image learning approach, we demonstrate improvements in optic cup localization accuracy with full cup-to-disc ratio range, while incurring only minor increase in computing cost.
               
            

@&#INTRODUCTION@&#

As the global leading cause of irreversible blindness, glaucoma is estimated to visually impair 80 million people by 2020 [1]. It is the most prevalent of eye diseases in the United States and around the world. There are approximately 2.2 million diagnosed cases of glaucoma in the United States [2] and 60 million worldwide; however, this figure is significantly underreported due to the slow, asymptomatic progression of the disease, resulting in low disease detection until vision lost sets in. Undiagnosed rates are reported to be nearly 50% in United States and Australia [2] and as high as 90% in Singapore [3,4]. Glaucoma, which is a group of ocular disease, causes permanent damage to the optic nerve. Vision loss due to glaucoma usually begins from the peripheral vision and progress inwards, resulting in a “tunnel vision”. This “sneaky thief of sight” is a silent disease as most people do not develop onset symptoms, and do notice the lost of their peripheral vision. When glaucoma patients are referred to the ophthalmologists, severe irreversible visual impairment has often already occurred.

Although vision loss due to this disease is permanent and cannot be restored, studies have shown that early detection and proactive medical intervention are effective in preserving vision and slowing, or even halting, the progression of the disease [5]. A recent economic study has found that technology-based assessment would be a cost-effective option in improving glaucoma detection for eye examination [6].

One of the methods to detect glaucoma is to study the structural damage in the optic nerve head in retinal fundus photographs. In order to gauge the health of the optic nerve in a fundus image, the optic nerve has to be inspected and delineated to identify the optic disc, optic cup and neuroretinal rim [7]. Vertical elongation of the optic cup is a characteristic feature of glaucomatous optic neuropathy [8]. Assessment of the optic cup and optic disc provides important information for glaucoma evaluation, such as the cup-to-disc ratio (CDR). Clinically, these are manually annotated by a trained ophthalmologist.

Several automated methods have been proposed to reduce the labor intensive manual workload. Computerized segmentation techniques for optic disc includes Hough transform [9], template matching [10,11], pixel feature classification [12], vessel geometry [13], deformable models [14] and level sets [15].

In this paper, we address the challenging problem of optic cup detection. Broadly, existing approaches for optic cup segmentation can be categorized into image processing-based strategies and machine learning models. Examples of techniques employing image processing approaches includes thresholding based on the image's intensity [16], active contour [17], level sets [18], active shape models [19]. To further improve on the segmentation, domain priors such as vessel kinks [20], and r-bends [21] have been incorporated. Recently, [22] proposed using graph cuts to segment the optic cup in fundus images, using location and shape priors obtained from OCT.

In recent years, machine learning based approaches have become popular in this domain due to its better accuracy. The use of machine learning methods can be subcategorized into pixel-based, sliding windows and superpixel classification approaches. In [12], the authors proposed using color opponency gaussian filter banks and stereo features on a k-nearest neighbour classifier for pixel-level feature classification. Xu et al. [23] presented a machine learning framework based using a regression model for optic cup segmentation. However, the method is reported to take a long time as it seeks to identify the optic cup as a whole, using a bundle of sliding windows of varying sizes for feature extraction and a RBF support vector regression model to rank each candidate region.

Recently, the use of superpixels for feature classification has been widely adopted in retinal imaging for diseases like pathological myopia [24], age-related macula degeneration (AMD) [25] and glaucoma [26]. Examples of such image over-segmentation methods to acquire superpixels includes graph-based segmentation [27], SLIC superpixels[28], and TurboPixel [29,30]. In particular, for optic cup detection, the supervised superpixel-based classification [26,31] has shown to achieve state-of-the-art performance against other existing approaches [18,21,19]. In [31], a centre surround statistics (CSS) feature descriptor based on biological inspired features were used as features for a non-linear support vector machine (SVM) to perform the task of optic cup segmentation. In [26], besides using the general supervised superpixel classification method, domain prior based on intra-image learning, where each model is learnt from samples from each test image without pre-labeling, is introduced to overcome the illumination differences between training and testing images. A refinement scheme is then used to include structural priors and local context for the final cup detection to further boost its performance. However, the approach assumes a CDR ranging between 0.2 and 0.9, which leads to low accuracy in small and large cup localization. Furthermore, at larger superpixel scales, this will lead to fewer training samples, thus, affecting the overall performance. Using an unsupervised approach, [32] proposed using domain priors, derived from cup pallor and optic cup origin, together with features extracted from superpixels. A label refinement using k-means clustering is then used to achieve the task of optic cup localization. Nonetheless, the accuracy of the supervised approaches [26,31] outperforms the unsupervised method, using fewer assumptions (i.e. domain priors).

In this work, we identify two limitations in the existing superpixel-based framework. First, an alternative strategy to reduce the effects of varying illumination between images is proposed and experimentally validated. Next, a general multi-scale superpixel classification strategy is proposed to improve the accuracy and robustness for optic cup localization. We build upon the existing single scale superpixel classification framework in [26,32], and incorporated two novel contributions. First, a multiple model selection and integration scheme using sparse learning is proposed to provide stability in performance variations, arising from repeated random samples selection of training data. Second, multiple superpixel resolutions are integrated for better optic cup boundary adherence and localization. We believe this general framework is also adaptable to other similar classification-based ocular disease CAD applications (Fig. 1
                     ).

Similar to the setup of the work in [26], we start with a disc image to localize the optic cup. First, blood vessels are extracted and the input disc image is enhanced by a contrast normalization scheme. Next, the contrast enhanced image is then segmented into superpixels and blood vessels which overlap with the superpixels are removed. Features are then extracted across multiple superpixel scales, and multiple classification models are trained for each respective scale. To obtain an unique label for each pixel with higher accuracy, optimal superpixel classification models are selected and integrated using a sparse learning approach. The final optic cup area is then identified by using an ellipse to enclose all the pixels predicted as ‘cup’. The framework of our overall approach is illustrated in Fig. 2
                     .

To reduce the effects on rim/cup misclassification, we used a multi-scale difference-of-closing vessel extraction algorithm by [34] for blood vessel removal. The benefits of this method lies in its flexibility in quick parameter tuning to extract only thicker blood vessels, and the vessel extraction process is simple and fast, requiring only basic morphological operations. A disk structuring element of radius of 16 pixels and 3 pixels for the dilation and erosion operations respectively. As the images are processed at superpixels scale, a blood vessel mask is generated based on superpixels that overlap the vessel extraction mask by at least 75%.

The influence of illumination variances between training and learning images can affect the performance of the trained machine learning models. To reduce these effects, a pre-processing using histogram stretching is performed. For each RGB channel in a disc image, the image intensities are represented as a histogram and stretched into [0, 255] to expand the dynamic range and achieve consistency across all the images. This pre-processing histogram normalization accomplishes two goals, 1) it provides better illumination alignment for all images, 2) it helps to enhance the contrast between the optic cup and rim.

Instead of pixel-level classification, labels are assigned for each superpixel segment for efficiency. This bottom-up segmentation of superpixels not only reduces the complexity of our problem, but also summarizes image redundancy and provides highly desirable spatial and local boundaries adhering properties. Superpixels are acquired using the SLIC (Simple Linear Iterative Clustering) algorithm [28] which groups pixels by adapting a k-means clustering approach. Each SLIC superpixel corresponds to a cluster in a five-dimensional color (CIELAB) and image location plane space. The advantages of using the SLIC algorithm to perform superpixel segmentation are 1) its efficiency to generate compact, nearly uniform superpixel segments with O(N) complexity, and, 2) only requiring a single parameter, which is to specify the number of superpixels.

Experimental studies in [26] demonstrated the effects of different superpixel scales and their resulting performance accuracy. In particular, it was found that larger superpixel segments provides richer and more discriminative features compared to smaller superpixels, but are prone to over-segmentation in ambiguously-labeled boundary superpixels. In contrast, smaller superpixel regions offers less distinctive features but are able to adhere to the cup boundary with greater precision. This trade-off is unavoidable in a single scale classification framework. Instead of using a “one-size-fits-all” strategy, our approach proposes to merge multiple superpixel scales segments to unify their advantages for better boundary fit.

Using the superpixels as building blocks, features can be extracted to describe each segment. Examples of features that had been used to represent superpixels in object recognition and human body segmentation, includes size, shapes, texture, colors, boundary contours, and thumbnail appearances [35,36]. Following the features extracted in [26], for each j-th superpixel at superpixel scale S
                        
                           c
                         (i.e. S
                        
                           c
                        
                        ∈{S
                        1, …, S
                        
                           β
                        }), we extract a feature vector 
                           
                              
                                 
                                    f
                                 
                              
                              j
                              c
                           
                         that consists of position information (coordinates and distance of each superpixel centroid with respect to the disc centre) denoted by (x
                        
                           j
                        , y
                        
                           j
                        , ρ
                        
                           j
                        ), mean RGB colors (r
                        
                           j
                        , g
                        
                           j
                        , b
                        
                           j
                        ) and a 256-bin histogram (
                           
                              h
                              j
                              r
                           
                           ,
                           
                              h
                              j
                              g
                           
                           ,
                           
                              h
                              j
                              b
                           
                        ) for each color channel. To avoid magnitude differences among the features, they are each normalized to the range of [0, 1], with L
                        1-normalization of each histogram. Finally, for each scale S
                        
                           c
                        , the feature matrix of all superpixels 
                           F
                        
                        
                           
                              c
                           
                         can be obtained.

At each scale S
                        
                           c
                        , α linear SVM models are trained to predict each test superpixel to be either disc(−1) or cup (+1). For each test superpixel, with feature 
                           
                              
                                 
                                    f
                                 
                              
                              j
                              c
                           
                        , using a pre-learnt linear SVM model 
                           
                              
                                 
                                    w
                                 
                              
                              j
                              c
                           
                         at scale S
                        
                           c
                        , the predicted label can be obtained by 
                           
                              
                                 (
                                 
                                    
                                       
                                          w
                                       
                                    
                                    j
                                    c
                                 
                                 )
                              
                              ⊤
                           
                           
                              
                                 
                                    f
                                 
                              
                              j
                              c
                           
                         (i.e., the superpixel is classified as cup(+1) when 
                           
                              
                                 (
                                 
                                    
                                       
                                          w
                                       
                                    
                                    j
                                    c
                                 
                                 )
                              
                              ⊤
                           
                           
                              
                                 
                                    f
                                 
                              
                              j
                              c
                           
                           ≥
                           0
                         with this pre-learned model, otherwise it is classified as disc(−1)).

Using a one-time random sampled superpixels with manual labels (determined by an overlapping ratio with ground truth cup), a base model can be learned using a linear SVM. However, since one time random sampling only learns a model which represents a small area in the feature space, its performance may vary significantly. To overcome this bias, multiple base models can be obtained by repeated random sampling. Furthermore, in order to avoid unbalanced training data distribution, equal positive and negative samples are selected for each training round. In total, αβ base models are trained for β scales. At each scale, the α base models can be used to get α predictions simultaneously without much additional cost. This is performed simply by concatenating each single model projection vector 
                           
                              
                                 
                                    w
                                 
                              
                              j
                              c
                           
                        , as columns, to form a projection matrix 
                           W
                        
                        
                           c
                        . The prediction labels for a single scale can then be found by 
                           
                              
                                 (
                                 
                                    
                                       
                                          W
                                       
                                    
                                    
                                       
                                          c
                                       
                                    
                                 
                                 )
                              
                              ⊤
                           
                           
                              
                                 
                                    F
                                 
                              
                              j
                              c
                           
                        .

In the training phases, for each pixel, αβ base models are used to classify it as disc or cup. To get a unique and accurate classification with minimal computation cost, we solve the following problem:


                        
                           
                              (1)
                              
                                 
                                    min
                                    
                                       
                                          ω
                                       
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          a
                                          =
                                          1
                                       
                                       τ
                                    
                                    
                                       ∥
                                       
                                          l
                                          a
                                       
                                       −
                                       
                                          
                                             
                                                ω
                                             
                                          
                                          ⊤
                                       
                                       
                                          
                                             
                                                d
                                             
                                          
                                          a
                                       
                                       
                                          ∥
                                          2
                                          2
                                       
                                    
                                    +
                                    λ
                                    
                                       
                                          ∥
                                          
                                             
                                                ω
                                             
                                          
                                          ∥
                                       
                                       1
                                    
                                 
                              
                           
                        where 
                           d
                        
                        
                           a
                         is the αβ predictions from the linear SVM base models and, l
                        
                           a
                         is the training label. The first term is to minimize the prediction error, and the second term is regularized by λ to enforce the sparsity of 
                           ω
                        , which means, only a few linear base models are selected for the unique final prediction for each pixel. In our implementation, the SLEP [37] toolbox is used to train the integration model. The solution of 
                           ω
                         indicates which base models are chosen to generate the final feature D
                        
                           s
                         of each pixel, i.e., the ith base model is selected when |
                           ω
                        
                        
                           i
                        |>10−3. A unique integration model 
                           Ω
                         is then obtained by consolidating the weight parameters of these K selected base models in 
                           ω
                        . In real applications, only a small number of selected models are used to compute the intermediate results for making final classification. This allows a boost to the performance without much additional cost.

To summarize, in the testing phase, four steps are sequentially performed to obtain the final pixel-level label from the original image. 1) Preprocessing for blood vessel extraction and contrast enhancement, 2) Superpixel segmentation, blood vessel removal and feature extraction at multiple scales, S
                        
                           c
                        
                        ∈{512, 1024, 2048}, 3) Using the selected base models, a feature vector for the s-th pixel, 
                           D
                        
                        
                           s
                        , is formed by concatenating the respective classification decision values of selected models indicated by 
                           ω
                        . 4) The integration model 
                           Ω
                         is then used to assign each pixel with a unique label 
                           Ω
                        
                        ⊤
                        
                           D
                        
                        
                           s
                        .

As the given ground-truth for optic cup segmentation is an elliptical reference [33], after obtaining the final labels for each pixel, a minimum ellipse that encloses all positive labeled pixels is computed to produce the final optic cup boundary estimate. Another major reason to introduce ellipse fitting is to include superpixels lying on the blood vessels which had been previously removed. Moreover, the use of ellipse-fitting to produce an accurate single optic cup region has also been widely used and reported in related works [18,23,26,31,32]. In our implementation, the ellipse-fitted optic cup is represented by its centre and elongation parameters 
                           (
                           u
                           ,
                           v
                           ,
                           υ
                           ,
                           ν
                           )
                        . Fig. 3
                         illustrates the major steps of our approach.

@&#EXPERIMENTS@&#

Our approach is implemented in MATLAB R2013b, and tested on a four-core 3.4GHz PC with 12GB RAM. A total of 650 2-D fundus photographs from the online clinical dataset ORIGA 
                        [33] were used for the experiments. The color fundus images were acquired using a 45∘ field-of-view (FOV) Canon CR-DGi retinal fundus camera with a Canon 10D single-lens reflex camera (SLR). The color images were captured at an image resolution of 3072 × 2048 pixels and saved in JPEG format. The ORIGA 
                        [33] dataset were divided in two sets Set
                        
                           I
                         and Set
                        
                           II
                        , which consisted of 482 normal and 168 glaucoma images, in total. The normal and glaucoma images are distributed within the 650 images. The first 325 images from the dataset, forms Set
                        
                           I
                         and the rest forms Set
                        
                           II
                        . Specifically, in Set
                        
                           I
                        , there are 55 glaucoma images, while in Set
                        
                           II
                        , there are 113 glaucoma cases.

In the learning phase, the first 150 images (with 15 glaucoma images) from image set Set
                        
                           I
                         were used as the training set. This setting is consistent with [23,26]. A 2-fold cross-validation was performed to determine the optimal parameters for the linear SVM classifiers by setting the parameters as C
                        ∈{10−3, 10−2, ..., 102, 103}. Similar to [26], the optimal SVM parameter was determined to be C
                        =100, and the remaining models were trained by repeated random sampling, using the same optimal regularization parameter. A total of α
                        =100 models were trained for each superpixel scale, S
                        
                           i
                        
                        ∈{512, 1024, 2048}. For sparse model selection, the optimal parameter was chosen from λ
                        ∈{1024, 2048, ..., 32768} by using cross-validation.

As the fundamental basis of this approach is based on superpixel classification, we assess the multi-models’ superpixel classification performance, at superpixel scale precision. Similarly, to evaluate the localization of the unique optic cup for a given retinal image, two widely recognised metrics are used in this paper, following the previous works in [23,26,32].

ROC curves presents a comprehensive representation to summarize the accuracy of the superpixel classification predictions of our learning models, where each point on the curve represents the true-positive rate and false-positive rate. As the linear models are trained using partial images from image set Set
                           
                              I
                           , Area Under ROC (AUC) and a balanced accuracy with a fixed 85.0% specificity are assessed on image set Set
                           
                              II
                           . The balanced accuracy (
                              
                                 P
                                 ¯
                              
                           ), sensitivity (P
                           +) and specificity (P
                           −) are defined as
                              
                                 (2)
                                 
                                    
                                       
                                          
                                             
                                                P
                                                ¯
                                             
                                             =
                                          
                                          
                                             
                                                
                                                   
                                                      P
                                                      +
                                                   
                                                   +
                                                   
                                                      P
                                                      −
                                                   
                                                
                                                2
                                             
                                             ,
                                             
                                             
                                                P
                                                +
                                             
                                             =
                                          
                                          
                                             
                                                TP
                                                
                                                   TP
                                                   +
                                                   FN
                                                
                                             
                                             ,
                                                
                                             
                                                P
                                                −
                                             
                                             =
                                          
                                          
                                             
                                                TN
                                                
                                                   TN
                                                   +
                                                   FP
                                                
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           where TP and TN denote the number of true positives and negatives, respectively, and FP and FN denote the number of false positives and negatives, respectively.

Cup localization error is evaluated using non-overlap ratio (m
                           1) and absolute CDR error (δ), defined as:
                              
                                 (3)
                                 
                                    
                                       
                                          
                                             
                                                m
                                                1
                                             
                                             =
                                          
                                          
                                             1
                                             −
                                             
                                                
                                                   area
                                                   (
                                                   
                                                      E
                                                      dt
                                                   
                                                   ⋂
                                                   
                                                      E
                                                      gt
                                                   
                                                   )
                                                
                                                
                                                   area
                                                   (
                                                   
                                                      E
                                                      dt
                                                   
                                                   ⋃
                                                   
                                                      E
                                                      gt
                                                   
                                                   )
                                                
                                             
                                             ,
                                          
                                       
                                       
                                          
                                             δ
                                             =
                                          
                                          
                                             
                                                
                                                   |
                                                   
                                                      D
                                                      dt
                                                   
                                                   −
                                                   
                                                      D
                                                      gt
                                                   
                                                   |
                                                
                                                DD
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           where E
                           
                              dt
                            denotes a detected cup region, E
                           
                              gt
                            denotes the ground-truth ellipse, D
                           
                              dt
                            is the vertical diameter of the detected cup, D
                           
                              gt
                            is the vertical diameter of the ground-truth cup, DD is the vertical diameter of optic disc.


                           
                              It is worth mentioning that m1 is the only evaluation metric related to cup localization accuracy used in this paper
                           . This is also widely used for accuracy evaluation in general object detection challenges [38]. Absolute CDR error (δ), however, is associated to glaucoma diagnosis and used in many previous works [23,26,32], thus is also considered in this work, but m
                           1 is the dominant criteria in the experiments.

The experiments using these two criterions are carried out on Set
                           
                              I
                            and Set
                           
                              II
                           , and compared with existing state-of-the-art methods. This evaluation is comparable to many real applications, where the main objective is to localize the unique cup as accurately as possible to the ground truth. Additionally, the influence of parameters and computation speed of the overall performance are also reported and discussed.

Two sets of experiments were performed to identify two current limitations in the existing superpixel-based cup localization framework. In the first experiment, we examine the effects of illumination variations between images and verified if our contrast normalization scheme mitigates these influences. In the second experiment, we study the consequences of random sampling of training data, and its influence on the accuracy in optic cup localization.

In the existing superpixel-based approach of [26], an intra-image learning is proposed to avoid illumination inconsistencies between training and testing images. Though this is highly desirable, the approach assumes that each image has a CDR of between 0.2 and 0.9. In most cases, this assumption may be valid, however, in extreme cases of small (CDR ≤ 0.2) and large (CDR ≥ 0.9) optic cups, the training samples becomes insufficient. Furthermore, the intra-image learning approach is unable to localize CDR sizes beyond its assumed range. Instead, we propose using a histogram-based contrast normalization approach, as an alternative to intra-image strategy, to reduce this inter-image illumination variations.

In [26], it was observed that the cup localization results performs most favorably at superpixel scale 2048. Table 1
                            compares the results when the proposed contrast normalization approach was adopted and was not adopted within the same experimental framework. The mean errors of the α
                           =100 models at superpixel scale 2048 are used in this experiment for a fair comparison. They show that our normalization pre-processing method improves feature discriminative power, and is able to boost the overall cup localization performance in m
                           1 and δ. Comparing our implementation without contrast normalization against the pre-learned approach in [26], we observe similar m
                           1 performance, however, a slightly higher δ is observed. This could be due to the performance variations from the models trained with randomly selected samples. Compared to the intra-image learning, our approach achieves a relative error reduction by 1.50% in overlap error (m
                           1) but has a trade-off in 16.0% δ increment, without using the proposed multiple models and multiple scale fusion strategy.

As observed in [26], the cup detection error varies at different superpixel scales. Similarly, we observe in this experiment that, the cup localization performance varies with different random training data samples. To illustrate these variabilities, a box plot is computed for m
                           1 and δ errors for all the αβ models in our framework, Fig. 4
                           .

To further explore and understand the performance variabilities, we tabulate 3 measures (classification prediction accuracy, overlap error m
                           1 on training set and overlap error m
                           1 on testing set) for all the α training models in the same scale. Each model is ranked according to their respective measures, and sorted according to their prediction accuracy, Fig. 5
                           (Top), and training m
                           1 error, Fig. 5(Bottom). We can observe from both plots that a trained model with high prediction accuracy does not necessarily result in a high pixel-level cup localization performance, m
                           1. Likewise, models that perform well in m
                           1 during training, does not produce the best overall m
                           1 results. However, a trained model with poor prediction accuracy or high m
                           1 training error, often results in weak overall m
                           1 localization performance. This further highlights the challenges in finding optimal models for accurate cup localization, where models selected based on high accuracy or low training overlap error does not guarantee the performance for cup localization in testing.

In this section, we evaluate the superpixel classification performance for each superpixel scale (S
                        1, S
                        2, S
                        3) of the trained multi-models on 325 images from Set
                        
                           II
                         of the ORIGA dataset, to avoid bias from the training samples used from Set
                        
                           I
                        .

For clarity, base models from each superpixel scale will be modeled as a single ROC curve to represent the 100 ROC curves. For each scale, the single ROC curve is found by calculating the mean of all true positive points at each false positive interval from the 100 ROC curves. The AUC derived from the single ROC curve is also found to be about the same as the mean AUCs calculated from each base model, as the variances between each base model's performance is minimal. Next, similar to our proposed framework which integrates multiple models, a majority voting scheme, which is simple and has been shown to be effective in many applications, is used as a baseline comparison. As the integration model assigns a label for each pixel, in order to obtain a distinct decision value for a given superpixel segment, an averaging operation on the predicted labels of all pixels within each superpixel unit is performed. Lastly, the performance of our model integration approach is also compared.

Results of the superpixel classification performances are shown in Fig. 6
                         and Table 2
                        . It is observed that the classification performances for single model is the highest at superpixel scale S
                        3
                        =2048, Fig. 6(a), for both AUC and balanced accuracy. Using multiple models, the Majority Voting scheme, which assigns equal weights to all trained models, is shown to have improved performances at 85% sensitivity, where the test samples are easier to differentiate and both the optimal and sub-optimal models have consistent decisions. This also demonstrates that multiple model integration can improve the accuracy, as compared to single model approaches. However, in test samples that are more challenging and ambiguous, the optimal and sub-optimal models have different decisions. This is evident in Fig. 6(b), where the tail end of the ROC curves starts to taper at a false positive rate of 0.3, leading to a decrease in overall AUC performance when compared against single model approaches. Compared to Majority Voting, the proposed model selection scheme, which selects and integrates optimal models with different weights, produces consistent predictions for both easy and challenging test cases, thus improving both AUC and balanced accuracy.

We compare the cup localization performance within our framework and against several other published existing techniques. Majority Voting denotes using a majority voting scheme, in place of our proposed model selection, to integrate the results from multiple scales. We also evaluated the mean performance of all the trained αβ models and the mean performance of the models in each superpixel scale, S
                        1, S
                        2, S
                        3. Other established approaches such as window-based 
                        [23], pixel-based level set [18] and superpixel-based methods (Intra-image 
                        [26], Pre-learned 
                        [26], Unsupervised 
                        [32]) were also included in this experiment. We present the evaluation in Set
                        
                           I
                        , Set
                        
                           II
                         and Overall (Set
                        
                           I
                         
                        & Set
                        
                           II
                        
                        ) to provide a uniform and consistent overview from different methods (with and without training samples) on the same dataset. For a fair comparison across the supervised approaches of window-based 
                        [23] and Pre-learned 
                        [26], the readers can refer to the results from Set
                        
                           II
                        . The evaluation performances of these methods are listed in Table 3
                        .

In general, we observe that the superpixel-based approaches offers a significant performance improvement compared to pixel-based [18] and window-based [23] methods. Comparing the mean performance of our models (mean of αβ models, means of S
                        1, S
                        2 and S
                        3 models) with existing superpixel-based cup localization methods, we note that the proposed alternative contrast normalization method improves feature discrimination power and provides an improvement in cup overlap error, m
                        1, but has a slight trade-off in the absolute CDR error δ. We also noticed that the mean performance at each individual scale was almost similar (i.e. 80–350 pixels per superpixel segment), this was also observed in [26]. When the size of the superpixel segment is not within this range, the performance changes significantly.

Although from Table 3, the overall results of the three superpixel scales are similar; but for a specific test image with comparable m
                        1 results at different scales, the localized cup boundaries may not appear as identical, and our proposed multiple scale fusion can further reduce the localization error with closer boundary fit. An example for this can be seen in Fig. 7
                        .

Compared with the mean models and existing single superpixel-scale approaches [23,26,32], the combination of multiple models from multiple scales, as used in Majority Voting and in our proposed approach, offers an improvement in cup localization accuracy. This demonstrates that using multiple scales and multiple models does improve optic cup localization accuracy. However, from Fig. 4, we can observe that individual trained models have large variances. These performance variations from individual models is mainly caused by the bias of randomly selected training samples. Multi-model integration is able to improve localization performance, mainly because it has an equivalent effect of increasing the training sample density and reducing the sampling bias.

Compared to the Majority Voting scheme, the proposed model selection approach yielded improvements in both performance criteria. The error reduction is due to our model selection strategy which is able to select and integrate optimal models across different scales. This result also suggests that our use of sparse learning does reduce redundancy and improve the classification performance, similar to [39]. Furthermore, measured against the current state-of-the-art intra-image superpixel-based method [26], our proposed model selection approach offers robust cup localization with lower or similar cup localization performance at minimal additional computation cost. An illustration of the sample results of our experiments is shown in Fig. 8
                        .

In this section, we review the influence of the regularization parameter for sparse model selection and the computation speed of our proposed framework.

Identifying and using only the effective models can provide higher localization precision with minimal computational cost increase. The role of the regularizer λ, is to penalize and enforce sparsity on 
                              ω
                            for optimal model selection. Fig. 9
                            shows the selected SVM base models across different superpixel scales, and its associated weights as λ increases. The influence of different λ settings on the overall performance is shown in Table 4
                           .

From Table 4, it is observed that λ
                           =4096 provides the best overall results, selecting 6% of the models, roughly 2% at each scale. As λ increases or decreases, the number of selected models changes. When too few models are selected, the generalization condition becomes overly strict; conversely, selecting too many models introduces redundancy, thereby, also significantly affecting the overall localization accuracy.

On closer inspection of Fig. 9, it can be observed that the weight of the selected model(s) in S
                           3
                           =2048, is the highest. A similar observation is reported in [26], where a superpixel of scale 2048 provides the lowest localization errors. However, as demonstrated experimentally, the linear combination of selected optimal models from multiple scales is able to enhance the localization performance with higher classification accuracy, provide better adherence to the optic cup boundary and improved performance stability, with minimal additional computational costs. We also noticed that these selected models may not perform well in training and testing (in terms of m
                           1 and δ) individually.

Computation speed was also evaluated in our experiments. The contrast normalization process takes 0.12s per image, while the morphological-based vessel extraction operation required 0.85s. Superpixel segmentation, using SLIC, is performed in 0.64s for a 400×400 image. Training and testing of linear SVM classifiers is also quick. The most time consuming process is feature extraction for each superpixel and at multiple scales, and the overall processing time increases near linearly with the number of superpixels. On average, the overall computation for a given test image takes 27.97s. In comparison, the intra-image learning in [26] takes 20.2s per image. In short, our proposed approach provides a gain of 6.74% in cup localization accuracy at an additional 38.46% increase in computational cost. The improved robustness and accuracy in cup localization makes a worthwhile trade-off as increases in computational cost can be often be addressed by using faster computational hardware and more optimized algorithm implementations.

@&#CONCLUSION@&#

Based on the state-of-the-art superpixel classification approach, this paper has presented an optimal model integration approach to robustly localize the optic cup in retinal images. Our framework provides two major contributions. Firstly, motivated by the performance variations due to random selection of training samples, we present a sparsity-based optimal models selection and integration solution for robust and stable optic cup localization. Secondly, we extend the multiple model selection to include multiple superpixel scales for better optic cup boundary adherence and richer feature descriptions.

In our experiments, we demonstrate that this generalized multi-model framework is able to produce optic cup localization accuracy that is 7.12% higher than the current state-of-the-art intra-image learning method [26], without a restricted CDR range of 0.2–0.9. We believe that the framework that is proposed is general enough to be adapted to other similar classification-based ocular disease CAD applications. For future works, other domain knowledge and information and be investigated for integration into this framework. Such information may include vessel bends and contexture statistics. The proposed approach in this paper could also be used for other applications. For example, discriminative features to describe and distinguish the optic disc from its periphery neighbors can be considered and investigated to perform optic disc localization using this multi-scale multi-model approach.

None of the authors have any conflict of interest to disclose.

@&#ACKNOWLEDGEMENTS@&#

This work is funded by Singapore A*STAR SERC Grant (092-148-00731).

@&#REFERENCES@&#

