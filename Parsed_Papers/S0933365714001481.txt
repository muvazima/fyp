@&#MAIN-TITLE@&#Classifying GABAergic interneurons with semi-supervised projected model-based clustering

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Semi-supervised clustering of interneurons.


                        
                        
                           
                           Accurate discrimination among four types of interneurons.


                        
                        
                           
                           Identification of potential interneuronal subtypes.


                        
                        
                           
                           Axonal morphometric properties are better predictors than dendritic ones.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Semi-supervised projected clustering

Gaussian mixture models

Automatic neuron classification

Cerebral cortex

@&#ABSTRACT@&#


               
               
                  Objectives
                  A recently introduced pragmatic scheme promises to be a useful catalog of interneuron names. We sought to automatically classify digitally reconstructed interneuronal morphologies according to this scheme. Simultaneously, we sought to discover possible subtypes of these types that might emerge during automatic classification (clustering). We also investigated which morphometric properties were most relevant for this classification.
               
               
                  Materials and methods
                  A set of 118 digitally reconstructed interneuronal morphologies classified into the common basket (CB), horse-tail (HT), large basket (LB), and Martinotti (MA) interneuron types by 42 of the world's leading neuroscientists, quantified by five simple morphometric properties of the axon and four of the dendrites. We labeled each neuron with the type most commonly assigned to it by the experts. We then removed this class information for each type separately, and applied semi-supervised clustering to those cells (keeping the others’ cluster membership fixed), to assess separation from other types and look for the formation of new groups (subtypes). We performed this same experiment unlabeling the cells of two types at a time, and of half the cells of a single type at a time. The clustering model is a finite mixture of Gaussians which we adapted for the estimation of local (per-cluster) feature relevance. We performed the described experiments on three different subsets of the data, formed according to how many experts agreed on type membership: at least 18 experts (the full data set), at least 21 (73 neurons), and at least 26 (47 neurons).
               
               
                  Results
                  Interneurons with more reliable type labels were classified more accurately. We classified HT cells with 100% accuracy, MA cells with 73% accuracy, and CB and LB cells with 56% and 58% accuracy, respectively. We identified three subtypes of the MA type, one subtype of CB and LB types each, and no subtypes of HT (it was a single, homogeneous type). We got maximum (adapted) Silhouette width and ARI values of 1, 0.83, 0.79, and 0.42, when unlabeling the HT, CB, LB, and MA types, respectively, confirming the quality of the formed cluster solutions. The subtypes identified when unlabeling a single type also emerged when unlabeling two types at a time, confirming their validity. Axonal morphometric properties were more relevant that dendritic ones, with the axonal polar histogram length in the [π, 2π) angle interval being particularly useful.
               
               
                  Conclusions
                  The applied semi-supervised clustering method can accurately discriminate among CB, HT, LB, and MA interneuron types while discovering potential subtypes, and is therefore useful for neuronal classification. The discovery of potential subtypes suggests that some of these types are more heterogeneous that previously thought. Finally, axonal variables seem to be more relevant than dendritic ones for distinguishing among the CB, HT, LB, and MA interneuron types.
               
            

@&#INTRODUCTION@&#

GABAergic interneurons of the cerebral cortex are key elements in many aspects of cortical function in both health and disease. Nevertheless, the classification of GABAergic interneurons is a difficult task and has been a topic of debate for a long time, since the pioneering work of Santiago Ramón y Cajal on the characterization and identification of interneurons [1]. The difficulty stems from the high variability of these cells according to morphological, electrophysiological and molecular features [2]. The scientific community lacks an accepted catalog of neuron names [3] which makes it difficult to organize and share knowledge [2]. There is some agreement on the set of morphological, molecular, and physiological features that can be used to distinguish among types of GABAergic interneurons [2]. However, a comprehensive classification according to those features is difficult to perform in practice [3]. A recent experiment enabled 42 expert neuroscientists from all around the world to classify interneurons by visual inspection and according to pre-selected neuron names [3]. It showed that the experts agree on the morphological definitions of some of the pre-selected types while disagreeing on the definitions of others. In particular, some types seemed to overlap in terms of the cells that were assigned to them by the experts. In [3], the authors also showed that supervised classification models can automatically categorize interneurons in accordance with the opinion of the majority of the experts.

Automatic classification of interneurons has mainly been done with (unsupervised) clustering; see, e.g., [4–8]. However, supervised approaches can be more accurate when there is prior knowledge about neuronal types [9]. In this study, such knowledge comes from the experts who participated in the experiment described in [3]. We can use this knowledge to guide classification and simultaneously discover subtypes using semi-supervised clustering, an approach that lies between the supervised and unsupervised approaches. In doing this, we follow the cluster assumption 
                     [10], i.e., we consider that the instances within a cluster are likely to belong to the same class whereas a class may consist of several clusters. In semi-supervised learning [10,11], some data instances are labeled whereas others are not. Since all our neurons were labeled by the experts, we fitted the semi-supervised scenario by removing the labels of (a) one type at a time; (b) two types at a time; and (c) half the instances of each type, simultaneously. By doing this we sought to discover possible subtypes and see if the types could be automatically discriminated. We used an adaptation of the semi-supervised projected model-based clustering algorithm (SeSProc) introduced in [12]. This is a probabilistic clustering algorithm which estimates the number of clusters and the relevance of each predictive feature for each of the clusters. The estimation of feature relevance within model-based clustering was introduced in [13].

We quantified the neurons with nine simple axonal and dendritic morphological variables, such as the axonal length close to the soma, and labeled them according to the choices of the expert neuroscientists. In [3] each instance was given up to 42 labels—coming from the 42 experts that concluded the study. Following a common practice in supervised learning [14], we reduced this vector of 42 labels to its mode (i.e., the most common value), thus obtaining a single label per neuron. However, since experts frequently disagreed, such labels were often not reliable, i.e., they were backed by few experts. To cope with the label noise 
                     [15,16] that expert disagreements may be introducing, we analyzed three subsets of our neuron population, each with a different minimum of ‘label reliability’, i.e., such that the label of each neuron in the subset was agreed upon by at least th experts, with th being a ‘label reliability threshold’.

This paper is an extension of [17] and is the result of close collaboration between experts in neuroanatomy and machine learning.
                        1
                     
                     
                        1
                        See the affiliations of the two institutions involved.
                      We extend the mentioned paper by refining some of the predictor variables, adapting the SeSProC algorithm, and considering two additional experimental settings. The remainder of this paper is organized as follows: Section 2 describes the materials and methods we used; Section 3 reports and discusses the obtained results; while Section 4 provides conclusions.

We used 237 three-dimensional (3D) reconstructions of interneurons from several areas and layers of the cerebral cortex of the mouse, rat, and monkey. These neurons were used in [3], and were originally extracted from NeuroMorpho.Org [18]. From this population of neurons, we formed subsets by imposing minimums on the number of experts that agreed on the label of an included cell (i.e., a ‘label reliability threshold’), considering that a higher threshold yields more confidence in the cells’ labels. We used thresholds 18, 22 (half plus one out of the 42 experts), and 26 to build three databases: th18, th22, and th26, respectively. These data sets contained interneurons of four different types (classes): common basket (CB), horse-tail (HT), large basket (LB), and Martinotti (MA). Table 1
                         shows the distribution of different types at the three label reliability thresholds.

We characterized each neuron using nine features of axonal and dendritic morphology. While one may compute many morphological features (e.g., [3] used over 2000 features for classification), none are known, so far, as good predictors of interneuron type. Since detailed morphometric information on 3D reconstructed cortical interneurons is relatively scarce (a few hundred reconstructed neurons are available, comprising different types), it might be counterproductive to use many predictor variables. Therefore, we kept the number of variables low by defining variables which capture how, in our opinion, an expert classifies an interneuron upon visual examination.

We consider that an expert classifies an interneuron by estimating the distribution and the orientation of axonal and dendritic arborizations. We therefore measured the axonal and dendritic length according to the Sholl (5 features) and polar histogram (4 features) analyses from NeuroExplorer, the data analysis companion to Neurolucida [19]. Sholl analysis computes axonal and dendritic length at different distances from the soma whereas the polar histogram [20] describes the overall direction of dendritic growth; we only distinguished between two halves of the histogram, namely, the bifurcation angles falling in the [0, π) interval and those falling in the [π, 2π) interval. See Table 2
                         and Fig. 1
                         and for further details on predictor variables. We standardized all variables (transformed them so to have zero mean and unit standard deviation) prior to classification.

While an expert who classifies using a similar rationale can only roughly estimate these features, our classifier used exact values, thus possibly being more objective. This is important as some of the features that we use, such as the length of the axonal arbor at a certain distance from the soma, are rather hard for an expert to estimate.

We used the semi-supervised projected model-based clustering (SeSProC) algorithm, introduced in [12]. This algorithm allows clusters to exist in different feature subspaces, estimating the relevance of each feature for every cluster. We modified SeSProc's definition of feature relevance and its heuristic for initializing new clusters (see below for details on both modifications). SeSProC handles partially labeled data (i.e., some instances are labeled whereas others are not) by assigning unlabeled instances into either the a priori known clusters (given by the class labels) or to new clusters that it may have discovered.

Let 
                              X
                              =
                              {
                              
                                 
                                    
                                       x
                                    
                                 
                                 1
                              
                              ,
                              …
                              ,
                              
                                 
                                    
                                       x
                                    
                                 
                                 N
                              
                              }
                            be observed data, with 
                              
                                 
                                    
                                       x
                                    
                                 
                                 i
                              
                              ∈
                              
                                 
                                    
                                       ℝ
                                    
                                 
                                 F
                              
                              ,
                              ∀
                              i
                              ∈
                              {
                              1
                              ,
                              …
                              ,
                              N
                              }
                           , where F denotes the number of features. In model-based clustering, we assume that the data are generated from a finite mixture of K components. The density function for an instance x
                           
                              i
                            is


                           
                              
                                 
                                    f
                                    (
                                    
                                       
                                          
                                             x
                                          
                                       
                                       i
                                    
                                    ∣
                                    Θ
                                    )
                                    =
                                    
                                       ∑
                                       
                                          m
                                          =
                                          1
                                       
                                       K
                                    
                                    
                                       π
                                       m
                                    
                                    f
                                    (
                                    
                                       
                                          
                                             x
                                          
                                       
                                       i
                                    
                                    ∣
                                    
                                       
                                          
                                             θ
                                          
                                       
                                       m
                                    
                                    )
                                    ,
                                 
                              
                           with π
                           
                              m
                           
                           ∈[0, 1] and 
                              
                                 ∑
                                 
                                    m
                                    =
                                    1
                                 
                                 K
                              
                              
                                 π
                                 m
                              
                              =
                              1
                           , and 
                              θ
                           
                           
                              m
                            being the parameters of component m, with Θ={
                              Θ
                           
                           1, …, 
                              Θ
                           
                           
                              K
                           , π
                           1, …, π
                           
                              K
                           }.

SeSProC encodes the clustering solution with a latent K-dimensional binary variable 
                              Z
                           , with z
                           
                              im
                           
                           =1 if x
                           
                              i
                            is assigned to component m, and z
                           
                              im
                           
                           =0 otherwise. In a semi-supervised setting, we observe the class labels c
                           
                              i
                           , c
                           
                              i
                           
                           ∈{1, …, C}, of some instances i
                           ∈{1, …, L}, L
                           <
                           N. We use the class labels to set the minimum number of components in the mixture, with one component per class label (i.e., K
                           =
                           C). We also use the labels to fix 
                              
                                 z
                                 im
                              
                              =
                              
                                 
                                    I
                                 
                              
                              (
                              
                                 c
                                 i
                              
                              =
                              m
                              )
                           , where 
                              
                                 
                                    I
                                 
                              
                              (
                              ·
                              )
                            returns 1 if (·) is true and 0 otherwise, for i
                           <
                           L. This means that the algorithm does not change z
                           
                              im
                            for the labeled x
                           
                              i
                           —they always belong to clusters specified by their labels; the goal is to cluster the unlabeled instances x
                           
                              i
                           , L
                           <
                           i
                           ≤
                           N.


                           Localized feature selection. Not all predictor variables are necessarily relevant for clustering. Following [13], we assume that a feature j is irrelevant if its distribution is independent on the components.
                              2
                           
                           
                              2
                              This differs from the original SeSProC method which follows the notion of irrelevance given in [21]. Also, our model differs from the one used in [13] in that it considers the per-component relevance of each feature.
                            Such a feature then follows some distribution f(·∣
                           λ
                           
                              j
                           ), independent of any particular component, instead of following a distribution f(·∣
                           θ
                           
                              mj
                           ), dependent on a component m. SeSProc then models the distribution of a feature X
                           
                              i
                            as a mixture of f(·∣
                           λ
                           
                              j
                           ) and f(·∣
                           θ
                           
                              mj
                           ). Assuming that the features are independent given the (hidden) mixture component, this transforms the density function into


                           
                              
                                 
                                    f
                                    (
                                    
                                       
                                          
                                             x
                                          
                                       
                                       i
                                    
                                    ∣
                                    Θ
                                    )
                                    =
                                    
                                       ∑
                                       
                                          m
                                          =
                                          1
                                       
                                       K
                                    
                                    
                                       π
                                       m
                                    
                                    
                                       ∏
                                       
                                          j
                                          =
                                          1
                                       
                                       F
                                    
                                    (
                                    
                                       ρ
                                       mj
                                    
                                    f
                                    (
                                    
                                       x
                                       ij
                                    
                                    ∣
                                    
                                       θ
                                       mj
                                    
                                    )
                                    +
                                    (
                                    1
                                    −
                                    
                                       ρ
                                       mj
                                    
                                    )
                                    f
                                    (
                                    
                                       x
                                       ij
                                    
                                    ∣
                                    
                                       λ
                                       j
                                    
                                    )
                                    )
                                    ,
                                 
                              
                           where ρ
                           
                              mj
                            is the probability of feature j being relevant for component m. ρ is encoded with an additional latent variable 
                              V
                           , with 
                              
                                 v
                                 mj
                              
                              =
                              1
                            if feature j is relevant for component m and 
                              
                                 v
                                 mj
                              
                              =
                              0
                            otherwise, and 
                              
                                 ρ
                                 mj
                              
                              =
                              p
                              (
                              
                                 v
                                 mj
                              
                              =
                              1
                              )
                           . The full parameter set of the finite mixture of K components is then 
                              Θ
                              =
                              
                                 
                                    {
                                    
                                       θ
                                       mj
                                    
                                    ,
                                    
                                       λ
                                       j
                                    
                                    ,
                                    
                                       ρ
                                       mj
                                    
                                    ,
                                    
                                       π
                                       m
                                    
                                    }
                                 
                                 
                                    m
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    K
                                    ;
                                    j
                                    =
                                    1
                                    ,
                                    …
                                    ,
                                    F
                                 
                              
                           .

If 
                              Z
                            and 
                              V
                            were observed, the log-likelihood function would be


                           
                              
                                 
                                    log
                                    L
                                    (
                                    Θ
                                    ∣
                                    X
                                    ,
                                    Z
                                    ,
                                    V
                                    )
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       ∑
                                       
                                          m
                                          =
                                          1
                                       
                                       K
                                    
                                    
                                       
                                          
                                             
                                                z
                                                im
                                             
                                             log
                                             
                                                π
                                                m
                                             
                                             +
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                F
                                             
                                             (
                                             
                                                z
                                                im
                                             
                                             [
                                             
                                                v
                                                mj
                                             
                                             (
                                             log
                                             
                                                ρ
                                                mj
                                             
                                             +
                                             log
                                             f
                                             (
                                             
                                                x
                                                ij
                                             
                                             ∣
                                             
                                                θ
                                                mj
                                             
                                             )
                                             )
                                             +
                                             (
                                             1
                                             −
                                             
                                                v
                                                mj
                                             
                                             )
                                             (
                                             log
                                             (
                                             1
                                             −
                                             
                                                ρ
                                                mj
                                             
                                             )
                                             +
                                             log
                                             f
                                             (
                                             
                                                x
                                                ij
                                             
                                             ∣
                                             
                                                λ
                                                j
                                             
                                             )
                                             )
                                             ]
                                             )
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        

However, they are not observed, and we cannot maximize the log-likelihood function analytically. Instead, we approximate the solution with the iterative expectation-maximization (EM) algorithm [22]. The EM produces a sequence of parameter estimates by alternating an E-step—‘filling in’ the missing observations with their expected values given the current parameters, and an M-step —using the completed data to compute maximum likelihood estimates of the parameters. At a given iteration t of the EM procedure, the expectation of the log-likelihood function is given by


                           
                              
                                 
                                    
                                       
                                          
                                             E
                                          
                                       
                                       
                                          Z
                                          ,
                                          V
                                          ∣
                                          X
                                          ,
                                          
                                             Θ
                                             
                                                t
                                                −
                                                1
                                             
                                          
                                       
                                    
                                    [
                                    log
                                    L
                                    (
                                    
                                       Θ
                                       
                                          t
                                          −
                                          1
                                       
                                    
                                    ∣
                                    X
                                    ,
                                    Z
                                    ,
                                    V
                                    )
                                    ]
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       ∑
                                       
                                          m
                                          =
                                          1
                                       
                                       K
                                    
                                    γ
                                    (
                                    
                                       z
                                       im
                                    
                                    )
                                    log
                                    
                                       π
                                       m
                                    
                                    +
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       ∑
                                       
                                          m
                                          =
                                          1
                                       
                                       K
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       F
                                    
                                    γ
                                    (
                                    
                                       z
                                       im
                                    
                                    )
                                    γ
                                    (
                                    
                                       v
                                       mj
                                    
                                    )
                                    (
                                    log
                                    
                                       ρ
                                       mj
                                    
                                    +
                                    log
                                    f
                                    (
                                    
                                       x
                                       ij
                                    
                                    ∣
                                    
                                       θ
                                       mj
                                    
                                    )
                                    )
                                    +
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       ∑
                                       
                                          m
                                          =
                                          1
                                       
                                       K
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       F
                                    
                                    γ
                                    (
                                    
                                       z
                                       im
                                    
                                    )
                                    (
                                    1
                                    −
                                    γ
                                    (
                                    
                                       v
                                       mj
                                    
                                    )
                                    )
                                    (
                                    log
                                    (
                                    1
                                    −
                                    
                                       ρ
                                       mj
                                    
                                    )
                                    +
                                    log
                                    f
                                    (
                                    
                                       x
                                       ij
                                    
                                    ∣
                                    
                                       λ
                                       j
                                    
                                    )
                                    )
                                    ,
                                 
                              
                           where Θ
                              t−1 are the parameters from iteration t
                           −1 and γ() is the expectation function.

Assuming that both f(·∣
                           θ
                           
                              mj
                           ) and f(·∣
                           λ
                           
                              j
                           ) are Gaussian distributions, the parameters are updated in the M-step by maximum likelihood, as follows:


                           
                              
                                 
                                    
                                       π
                                       m
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             L
                                          
                                          
                                             z
                                             im
                                          
                                          +
                                          
                                             ∑
                                             
                                                i
                                                =
                                                L
                                                +
                                                1
                                             
                                             N
                                          
                                          γ
                                          (
                                          
                                             z
                                             im
                                          
                                          )
                                       
                                       N
                                    
                                    ,
                                 
                              
                           
                           
                              
                                 
                                    
                                       ρ
                                       mj
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          γ
                                          (
                                          
                                             z
                                             im
                                          
                                          )
                                          γ
                                          (
                                          
                                             v
                                             mj
                                          
                                          )
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             L
                                          
                                          
                                             z
                                             im
                                          
                                          +
                                          
                                             ∑
                                             
                                                i
                                                =
                                                L
                                                +
                                                1
                                             
                                             N
                                          
                                          γ
                                          (
                                          
                                             z
                                             im
                                          
                                          )
                                       
                                    
                                    ,
                                 
                              
                           
                           
                              
                                 
                                    
                                       μ
                                       
                                          
                                             θ
                                             mj
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          γ
                                          (
                                          
                                             z
                                             im
                                          
                                          )
                                          γ
                                          (
                                          
                                             v
                                             mj
                                          
                                          )
                                          
                                             x
                                             ij
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          γ
                                          (
                                          
                                             z
                                             im
                                          
                                          )
                                          γ
                                          (
                                          
                                             v
                                             mj
                                          
                                          )
                                       
                                    
                                    ,
                                 
                              
                           
                           
                              
                                 
                                    
                                       σ
                                       
                                          
                                             θ
                                             mj
                                          
                                       
                                       2
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          γ
                                          (
                                          
                                             z
                                             im
                                          
                                          )
                                          γ
                                          (
                                          
                                             v
                                             mj
                                          
                                          )
                                          
                                             
                                                (
                                                
                                                   x
                                                   ij
                                                
                                                −
                                                
                                                   μ
                                                   
                                                      
                                                         θ
                                                         mj
                                                      
                                                   
                                                
                                                )
                                             
                                             2
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          γ
                                          (
                                          
                                             z
                                             im
                                          
                                          )
                                          γ
                                          (
                                          
                                             v
                                             mj
                                          
                                          )
                                       
                                    
                                    ,
                                 
                              
                           
                           
                              
                                 
                                    
                                       μ
                                       
                                          
                                             λ
                                             j
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             ∑
                                             
                                                m
                                                =
                                                1
                                             
                                             K
                                          
                                          γ
                                          (
                                          
                                             z
                                             im
                                          
                                          )
                                          (
                                          1
                                          −
                                          γ
                                          (
                                          
                                             v
                                             mj
                                          
                                          )
                                          )
                                          
                                             x
                                             ij
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             ∑
                                             
                                                m
                                                =
                                                1
                                             
                                             K
                                          
                                          γ
                                          (
                                          
                                             z
                                             im
                                          
                                          )
                                          (
                                          1
                                          −
                                          γ
                                          (
                                          
                                             v
                                             mj
                                          
                                          )
                                          )
                                       
                                    
                                    ,
                                 
                              
                           
                           
                              
                                 
                                    
                                       σ
                                       
                                          
                                             λ
                                             j
                                          
                                       
                                       2
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             ∑
                                             
                                                m
                                                =
                                                1
                                             
                                             K
                                          
                                          γ
                                          (
                                          
                                             z
                                             im
                                          
                                          )
                                          (
                                          1
                                          −
                                          γ
                                          (
                                          
                                             v
                                             mj
                                          
                                          )
                                          )
                                          
                                             
                                                (
                                                
                                                   x
                                                   ij
                                                
                                                −
                                                
                                                   μ
                                                   
                                                      
                                                         λ
                                                         j
                                                      
                                                   
                                                
                                                )
                                             
                                             2
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             ∑
                                             
                                                m
                                                =
                                                1
                                             
                                             K
                                          
                                          γ
                                          (
                                          
                                             z
                                             im
                                          
                                          )
                                          (
                                          1
                                          −
                                          γ
                                          (
                                          
                                             v
                                             mj
                                          
                                          )
                                          )
                                       
                                    
                                    ,
                                 
                              
                           
                        

for m
                           =1, …, K
                           ;
                           j
                           =1, …, F.

Assuming that we have C classes in the data, the initial mixture contains C components. Its parameters are estimated with the EM procedure and its quality evaluated (see below) and recorded. Then, in the next iteration of SeSProC, this mixture is augmented with an additional component (see below), its parameters again estimated with EM and its quality computed and recorded. The process is repeated until a mixture 
                              
                                 M
                                 K
                              
                            with K components is better than a mixture 
                              
                                 M
                                 
                                    K
                                    +
                                    1
                                 
                              
                           , and yields 
                              
                                 M
                                 K
                              
                            as the final model. We evaluate the quality of a model with the AIC score [23],


                           
                              
                                 
                                    AIC
                                    =
                                    −
                                    2
                                    log
                                    L
                                    +
                                    2
                                    R
                                    ,
                                 
                              
                           where R, the number of parameters in a model, is a function of the number of components K and the number of features F:
                              3
                           
                           
                              3
                              The first addend in the formula for R corresponds to θ's, second to λ's, third to π's and fourth to ρ's.
                           
                        


                           
                              
                                 
                                    R
                                    =
                                    2
                                    KF
                                    +
                                    2
                                    F
                                    +
                                    (
                                    K
                                    −
                                    1
                                    )
                                    +
                                    KF
                                    .
                                 
                              
                           
                        

Additionally, we halt mixture augmentation if 
                              
                                 M
                                 K
                              
                            contains a component m with less than two instances (i.e., such that m is the most likely component for less than two instances), returning 
                              
                                 M
                                 
                                    K
                                    −
                                    1
                                 
                              
                           , unless K = C, in which case 
                              
                                 M
                                 K
                              
                            is returned.

When starting the EM procedure for a model 
                              
                                 M
                                 K
                              
                           , we use the class labels to initialize the θ parameters for the classes, i.e., we estimate 
                              θ
                           
                           
                              m
                           
                           =(θ
                           
                              m1, …, θ
                           
                              mF
                           ), for m
                           ≤
                           C, from instances belonging to class m. If K
                           >
                           C
                           +1 (i.e., if there are already newly found components in the mixture), then we use the 
                              θ
                           
                           
                              m
                           , C
                           <
                           m
                           <
                           K, from 
                              
                                 M
                                 
                                    K
                                    −
                                    1
                                 
                              
                            as their initial values in 
                              
                                 M
                                 K
                              
                           , since there are no labels that could guide the estimation of their initial values. The initial 
                              θ
                           
                           
                              K
                            for the new component K are estimated from a number of unlabeled data points; the following paragraph describes how these data points are selected.


                           Initializing a new component K. We have modified SeSProC to use the following heuristic for initializing new components. Starting from the previous mixture, 
                              
                                 M
                                 
                                    K
                                    −
                                    1
                                 
                              
                           , we consider the neighborhood of each unlabeled point as (a part of) a potential new cluster. Thus, we take the ct nearest unlabeled neighbors (according to the Euclidean distance in full dimensionality), where ct is a parameter of the algorithm, to a point x
                           
                              i
                            and assign them to a new cluster, by setting z
                           
                              jK
                           
                           =1 and z
                           
                              jm
                           
                           =0, m
                           ≠
                           K for all x
                           
                              j
                            in the neighborhood of x
                           
                              i
                           ; we then update all parameters (this includes the 
                              θ
                           
                           
                              K
                           ) by maximum likelihood (i.e., with an M-step) and compute the likelihood of the thereby obtained model. The neighborhood that yields the most likely model is then used to initialize 
                              θ
                           
                           
                              K
                            in the new model 
                              
                                 M
                                 K
                              
                           .All assignments to 
                              Z
                            here described are then undone after 
                              θ
                           
                           
                              K
                            is initialized (i.e., these assignments were only temporary).

We tested SeSProC's capacity to discriminate among interneuron classes and explored the existence of their subtypes in three experimental settings, corresponding to three rules for hiding class labels. First, we unlabeled all the cells of a single class and then ran the algorithm once for each class. Here, there was initially a cluster for each of the other (labeled) types and the desired result was to assign unlabeled instances to a (one of) newly formed cluster(s), allowing us to explore the potential subtypes of each class separately. Second, we simultaneously unlabeled all cells of each pair of classes, yielding six clustering scenarios; this allowed to assess whether cells of different classes would be clustered together and whether, and to what extent, would the subtypes identified in the previous setting re-appear. Finally, we simultaneously unlabeled a portion of cells of each of the four classes. This allowed unlabeled cells to be placed in their ‘true’ cluster, other classes (i.e., be misclassified), or assigned to a new cluster, providing insight into the homogeneity of each interneuron class. Here we unlabeled half the instances of each class (rounding down when necessary). We selected unlabeled cells by random sampling at each label reliability threshold (thus a cell might have been unlabeled at th26 but not at th18, for example), and repeated the sampling ten times. We used these three unlabeling settings for each label reliability threshold, i.e., for th18, th22, and th26.

For the first two settings, we defined per-class discrimination accuracy as acc
                           t
                        
                        =
                        c
                        
                           t
                        /u
                        
                           t
                        , where u
                        
                           t
                         is the number of cells of the unlabeled class t and c
                        
                           t
                         the cardinality of the subset of u
                        
                           t
                         assigned to a (one of) newly formed cluster(s) (and therefore not assigned to one of the other classes). In the second setting, we averaged acc
                           t
                         across the three ‘scenarios’ in which t was unlabeled (e.g., HT was unlabeled together with CB, LB, and MA). For the third setting, we defined two measures —‘error’ and ‘accuracy’—as follows: 
                           
                              err
                              t
                           
                           =
                           
                              
                                 
                                    ∑
                                    
                                       
                                          t
                                          ′
                                       
                                       ≠
                                       t
                                    
                                 
                                 
                                    a
                                    
                                       
                                          tt
                                          ′
                                       
                                    
                                 
                              
                              /
                              
                                 
                                    u
                                    t
                                 
                              
                           
                        , where 
                           
                              a
                              
                                 
                                    tt
                                    ′
                                 
                              
                           
                         is the number of unlabeled cells of class t assigned to class t′ and u
                        
                           t
                         the number of unlabeled cells of class t, and acc
                           t
                        
                        =
                        a
                        
                           tt
                        /u
                        
                           t
                        . ‘Accuracy’ considers the proportion of unlabeled cells of class t classified as t whereas ‘error’ does not penalize assignments to newly formed clusters.

When starting the EM procedure for a mixture model 
                           
                              M
                              K
                           
                        , one has to choose how to initialize the parameters. Furthermore, when K
                        >
                        C one can keep or adapt the parameters from the previous model, 
                           
                              M
                              
                                 K
                                 −
                                 1
                              
                           
                        . We initialize the parameters with the following heuristics:
                           
                              •
                              For m
                                 ≤
                                 C, estimate 
                                    θ
                                 
                                 
                                    m
                                  from cells labeled as belonging to class m.

Keep the θ and ρ parameters from 
                                    
                                       M
                                       
                                          K
                                          −
                                          1
                                       
                                    
                                  for C
                                 <
                                 m
                                 <
                                 K. That is, 
                                    
                                       
                                          
                                             θ
                                          
                                       
                                       m
                                       K
                                    
                                    =
                                    
                                       
                                          
                                             θ
                                          
                                       
                                       m
                                       
                                          K
                                          −
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          
                                             ρ
                                          
                                       
                                       m
                                       K
                                    
                                    =
                                    
                                       
                                          
                                             ρ
                                          
                                       
                                       m
                                       
                                          K
                                          −
                                          1
                                       
                                    
                                    ,
                                    
                                    
                                    C
                                    <
                                    m
                                    <
                                    K
                                  where 
                                    ρ
                                 
                                 
                                    m
                                 
                                 =(ρ
                                 
                                    m1, …, ρ
                                 
                                    mF
                                 ).

Estimate 
                                    θ
                                 
                                 
                                    K
                                  from the instances selected as described in paragraph ‘Initializing a new component’ in Section 2.2.3.

For components m
                                 ∈{1, …, C}∪{K}, i.e., those of fixed classes and the newly introduced one, make all features equally relevant and irrelevant: ρ
                                 
                                    mj
                                 
                                 =0.5, ∀
                                 m
                                 ∈{1, …, C}∪{K}, ∀
                                 j
                                 ∈{1, …, F}.

Adapt π from 
                                    
                                       M
                                       
                                          K
                                          −
                                          1
                                       
                                    
                                  to give more weight to newly discovered components m
                                 >
                                 C than to class components m′≤
                                 C: 
                                    
                                       π
                                       m
                                       K
                                    
                                    =
                                    2
                                    
                                       π
                                       
                                          
                                             m
                                             ′
                                          
                                       
                                       
                                          K
                                          −
                                          1
                                       
                                    
                                    ,
                                    ∀
                                    m
                                    >
                                    C
                                    ,
                                    ∀
                                    
                                       m
                                       ′
                                    
                                    ≤
                                    C
                                 .

Estimate λ as if ρ
                                 
                                    mj
                                 
                                 =0.5, ∀
                                 m, ∀
                                 j, i.e., as if each feature was equally relevant and irrelevant for every component, in order to fully ‘reset’ the λ estimates.

The EM procedure iterates until log-likelihood converges or up to 25 iterations. We set ct
                        =5 for initializing new components, because this value produced the best results in preliminary experiments.


                        Extreme probabilities due to maximum likelihood. Estimating the parameters of a Gaussian distribution by maximum likelihood can result in zero variance, driving an instance's density to infinity. To avoid this, we use 0.1 as a minimum variance for all Gaussian distributions, i.e., for all such distributions we set σ
                        2
                        =max{0.1, σ
                        2}, where σ
                        2 is the estimated variance.

@&#RESULTS AND DISCUSSION@&#

When unlabeling a single class, HT and MA cells were better distinguished from other classes when label reliability increased; the opposite happened for LB, while the discrimination accuracy for CB cells was rather unaffected (see Fig. 2
                           a). At th26, HT and MA cells were identified with rather high accuracy.


                           HT was the most easily identified class: with perfect accuracy at th22 and th26, and high accuracy (0.89) at th18.

Although accurately identified at th22 and th26, MA cells were confused with all the other classes, particularly with HT at th18 and th22, and CB at th26. In this respect, the MA seemed to be the most heterogeneous interneuron type with respect to the used variables.


                           LB and CB cell types were often confused with each other but easily distinguished from other types (with the exception of CB at th18, where it was heavily confused with HT; see Table 3
                           ). This confusion is not surprising, as even expert neuroscientists often struggle to discern these two classes [3].

When hiding two classes at a time, discrimination accuracy generally increased with label reliability (see Fig. 3
                           a). CB cells were better discriminated than when hiding a single class, HT cells equally well, whereas LB and MA cells better on some label reliability thresholds and worse on others.

At th26, which contained the most reliably labeled cells, all classes were identified more accurately than when unlabeling a single class (see Figs. 2a and 3a). Furthermore, the classes were well separated in the formed clusters: only six out of the 20 clusters formed at th26 (columns ‘A’, ‘B’, etc., in Table 4
                           a–f) were ‘mixed’, i.e., contained instances of more than one class (shown in black). Thus, even though MA cells were misclassified as CB at th26 (when unlabeling the MA type), these two types were neatly separated when unlabeled simultaneously (see Table 4a). HT cells were almost never placed in clusters containing other cell types (only a single CB cell was assigned to a HT cluster; see Table 4b). LB cells were least separated in the formed clusters: they were mixed with both CB and MA cells in two clusters (see Table 4e and f).

In this setting, discrimination accuracy generally improved with label reliability (see Fig. 4
                           a), except for MA and CB at th22, where it decreased due to more instances being assigned to new clusters (indicated by their low error at th22; Fig 4). Likewise, discrimination error generally decreased, except for MA and LB at th26.

At th26, the most accurately classified and most homogeneous types were HT and CB, as they had lowest error and highest accuracy. MA and LB, on the other hand, displayed low accuracies but relatively low errors, suggesting they were more heterogeneous than HT and CB at th26.

@&#SUMMARY@&#

Summarizing the previous three sections, we can note that, despite some fluctuations—possibly due to small data samples— discrimination accuracy tended to increase with label reliability in all three experimental settings (e.g., accuracy was almost universally higher and error lower at th26 than th18, see, e.g., Figs. 2a, 3a, 4a and b). This might suggest that the degree of label noise decreased with label reliability threshold, and that therefore, the most reliable results were obtained at th26.

The number of subtypes generally decreased with label reliability (see Figs. 2b and 3b). This may indicate that there was more heterogeneity among less reliably labeled cells; nonetheless, this heterogeneity may simply be due to the higher number of instances at lower thresholds (especially for the CB, LB, and HT types). HT appeared as the most compact class as all of its cells were clustered together (in a single cluster) at th22 and th26, in both the first and the second labeling scenario (see Figs. 2b and 3b and Table 4b–d). MA, on the other hand, seemed to be the most heterogeneous —at th26 
                        MA cells were clustered into at least three subtypes (see Fig. 2b and Tables 4a, d and e). Thus, we focused on th26 for analyzing the formed subtypes as it contained the most reliably labeled interneurons.


                           MA cells were clustered in three groups (see Fig. 5
                            for representative examples), with six, five, and five cells each, whereas a single distinct subtype was identified for the CB and LB classes, counting five and seven cells each, respectively. Since all HT cells were placed in a single cluster (see Fig. 2b), no potential subtypes of HT were identified.

Overall, MA subtypes showed relatively sparse axonal arbors, as indicated by their low or medium values for X
                           1 and X
                           2 (see Figs. 5a–c), with MA-A cells being less sparse than MA-B and MA-C ones. MA-A cells had plenty of axonal arborization far from soma (high X
                           5 values in Fig. 5a) and dendritic polar histogram length in the [π, 2π) interval (X
                           7). MA-B cells exhibited medium values for all variables (Fig. 5b) whereas MA-C had the sparsest axons (low values for X
                           1 and X
                           2 in Fig. 5c) and, like MA-A, plenty of axon far from soma (X
                           5) and dendrites in the [π, 2π) polar histogram interval (X
                           7). CB-A cells displayed relatively sparse axons (medium X
                           1 and low X
                           2 values in Fig. 5d), with little axon far from the soma (low X
                           5 values) and little dendritic arborization far from soma and in the [π, 2π) polar histogram interval (low X
                           7 and X
                           9 values). LB-A cells exhibited the most dense axonal and dendritic arbors, with high or medium values for X
                           1, X
                           2, X
                           6 and X
                           7; the X
                           4 values (axonal length at medium distance from soma), was especially high (see Fig. 5e).


                           Validating the produced clusters. While the discovered subtypes are relatively small—their sizes ranging from five to seven cells, they may nonetheless be relevant in the domain of neuronal classification, where 3D neuronal reconstructions, and reliably classified reconstructions in particular, are scarce. Cluster quality indices [24,25] may thus help assess the goodness of the clustering solution.

One type of indices compares the obtained (crisp) clustering partition with the original one, given by class labels. We performed such an analysis in Section 3.1 when we computed accuracy, and now we report a measure more commonly used for this end, the Adjusted Rand index (ARI; [26,27]),


                           
                              
                                 
                                    ARI
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                c
                                                =
                                                1
                                             
                                             C
                                          
                                          
                                             ∑
                                             
                                                m
                                                =
                                                1
                                             
                                             K
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               N
                                                               cm
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      c
                                                      =
                                                      1
                                                   
                                                   C
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     N
                                                                     
                                                                        c
                                                                        .
                                                                     
                                                                  
                                                               
                                                            
                                                            
                                                               
                                                                  2
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   ∑
                                                   
                                                      m
                                                      =
                                                      1
                                                   
                                                   K
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     N
                                                                     
                                                                        .
                                                                        m
                                                                     
                                                                  
                                                               
                                                            
                                                            
                                                               
                                                                  2
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               N
                                                            
                                                         
                                                         
                                                            
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             1
                                             2
                                          
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         c
                                                         =
                                                         1
                                                      
                                                      C
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        N
                                                                        
                                                                           c
                                                                           .
                                                                        
                                                                     
                                                                  
                                                               
                                                               
                                                                  
                                                                     2
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   +
                                                   
                                                      ∑
                                                      
                                                         m
                                                         =
                                                         1
                                                      
                                                      K
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        N
                                                                        
                                                                           .
                                                                           m
                                                                        
                                                                     
                                                                  
                                                               
                                                               
                                                                  
                                                                     2
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      c
                                                      =
                                                      1
                                                   
                                                   C
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     N
                                                                     
                                                                        c
                                                                        .
                                                                     
                                                                  
                                                               
                                                            
                                                            
                                                               
                                                                  2
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   ∑
                                                   
                                                      m
                                                      =
                                                      1
                                                   
                                                   K
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     N
                                                                     
                                                                        .
                                                                        m
                                                                     
                                                                  
                                                               
                                                            
                                                            
                                                               
                                                                  2
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               N
                                                            
                                                         
                                                         
                                                            
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where N
                           
                              cn
                            is the number of instances of class c assigned to cluster m. ARI rewards two types of agreements: (a) clustering a pair of instances together (i.e., as members of a same cluster) in both partitions; and (b) clustering a pair of instances separately (i.e., as members of different clusters) in both partitions. It reaches its maximum value, 1, when the two partitions agree perfectly, and this occurs when unlabeling HT cells (see Table 5
                           ). We also achieved high ARI values when unlabeling CB and LB cells whereas we obtained a low one when unlabeling MA cells; this is due to the discovery of three MA subtypes in the setting, which was not favored by ARI because it increased the difference among the two partitions.

We cannot easily compute a second type of clustering quality indices, based on assessing properties such intra-cluster compactness and inter-cluster separation, because that would require computing distances among data points, something which is unclear how to do when points are located in different feature subspaces. When then assessed our clustering in terms of probabilistic concordance, considering that a clustering is good if cluster membership probabilities, p(z
                           
                              i
                           ), are similar among the members of a same cluster and different among members of different clusters. We measured the similarities among cluster membership probabilities of two data instances with Jensen–Shannon divergence,


                           
                              
                                 
                                    
                                       d
                                       JS
                                    
                                    (
                                    p
                                    (
                                    
                                       
                                          
                                             z
                                          
                                       
                                       i
                                    
                                    )
                                    ,
                                    p
                                    (
                                    
                                       
                                          
                                             z
                                          
                                       
                                       j
                                    
                                    )
                                    )
                                    =
                                    
                                       1
                                       2
                                    
                                    (
                                    
                                       d
                                       KL
                                    
                                    (
                                    p
                                    (
                                    
                                       
                                          
                                             z
                                          
                                       
                                       i
                                    
                                    )
                                    ,
                                    r
                                    )
                                    +
                                    
                                       d
                                       KL
                                    
                                    (
                                    p
                                    (
                                    
                                       
                                          
                                             z
                                          
                                       
                                       j
                                    
                                    )
                                    ,
                                    r
                                    )
                                    )
                                    ,
                                 
                              
                           where 
                              r
                              =
                              
                                 1
                                 2
                              
                              (
                              p
                              (
                              
                                 
                                    
                                       z
                                    
                                 
                                 i
                              
                              )
                              +
                              p
                              (
                              
                                 
                                    
                                       z
                                    
                                 
                                 j
                              
                              )
                              )
                            and d
                           
                              KL
                           (p(z
                           
                              i
                           ), p(z
                           
                              j
                           )) is the Kullback–Leibler divergence [28] between p(z
                           
                              i
                           ) and p(z
                           
                              j
                           ),


                           
                              
                                 
                                    
                                       d
                                       KL
                                    
                                    (
                                    p
                                    (
                                    
                                       
                                          
                                             z
                                          
                                       
                                       i
                                    
                                    )
                                    ,
                                    p
                                    (
                                    
                                       
                                          
                                             z
                                          
                                       
                                       j
                                    
                                    )
                                    )
                                    =
                                    
                                       ∑
                                       
                                          m
                                          =
                                          1
                                       
                                       K
                                    
                                    p
                                    (
                                    
                                       z
                                       im
                                    
                                    )
                                    log
                                    
                                       
                                          
                                             
                                                
                                                   p
                                                   (
                                                   
                                                      z
                                                      im
                                                   
                                                   )
                                                
                                                
                                                   p
                                                   (
                                                   
                                                      z
                                                      jm
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        

Note that p(z
                           
                              jm
                           ) is simply γ(z
                           
                              im
                           ) computed in the final step of the EM algorithm. Unlike Kullback–Leibler divergence, Jensen–Shannon divergence is symmetric, does not require absolute continuity (i.e., that p(z
                           
                              im
                           )=0⇒
                           p(z
                           
                              jm
                           )=0), its square root is a metric, and it is bounded: 0≤
                           d
                           
                              JS
                           
                           ≤1 [29].

Using Jensen–Shannon divergence as the measure of distance among data instances, we computed the Silhouette width [30] clustering index, thus measuring intra-cluster compactness and inter-cluster separation in terms of this distance. The Silhouette width is given by


                           
                              
                                 
                                    SW
                                    =
                                    
                                       1
                                       N
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       
                                          
                                             b
                                             i
                                          
                                          −
                                          
                                             a
                                             i
                                          
                                       
                                       
                                          max
                                          (
                                          
                                             b
                                             i
                                          
                                          ,
                                          
                                             a
                                             i
                                          
                                          )
                                       
                                    
                                    ,
                                 
                              
                           where a
                           
                              i
                            is the average distance between x
                           
                              i
                            and other points in its cluster, while b
                           
                              i
                            is its average distance to the points in the closest cluster (defined as that yielding the lowest b
                           
                              i
                           ). We achieved maximum Silhouette values (its values range from −1 to 1) for all labeling scenarios (see Table 5), showing that the cluster membership probabilities had converged reasonably.

A partial cause for high Silhouette widths was that we only unlabeled a subset of instances in each setting, and it was only those instances’ p(z
                           
                              i
                           ) that were estimated by our algorithm, while the rest instances’ p(z
                           
                              i
                           ) were fixed to degenerate distributions (probability 1 for the class labels’ cluster; 0 for all other clusters) which were different among the clusters. Thus, e.g., when unlabeling the HT cells, we estimated the p(z
                           
                              i
                           ) of only four instances, the overall Silhouette width therefore necessarily being high, due to the inter-cluster differences among the fixed p(z
                           
                              i
                           ). Yet, as Table 6
                            shows, Silhouette widths were also high for the newly discovered clusters, not only for those corresponding to the known types (and thus containing many cells with fixed degenerate p(z
                           
                              i
                           )).

Finally, in the next section we validated the discovered subtypes with a different experimental setting—the hiding of two classes, thus evaluating their ‘stability’, i.e., their robustness to different labeling scenarios.

The validity of the above-described subtypes of the MA, CB, and LB types, identified when hiding a single class, was confirmed when hiding two classes simultaneously. That is, in almost every labeling scenario in the second setting (i.e., for every pair of hidden classes), there was a cluster that greatly resembled the corresponding subtypes. So, for example, when hiding CB and MA, cluster E (Table 4a) was identical to subtype CB-A. Furthermore, four cells from the CB-A subtype were clustered together in every labeling scenario, that is, the intersection of CB-A and cluster E in Table 4a, cluster B in Table 4b, and cluster B in Table 4f consisted of four cells. This subset of CB-A cells emerged as its ‘core’ of highly similar instances, showing the robustness of this subtype (which totalled five cells).

A ‘core’ of the MA-C subtype (i.e., the MA-C cells that were always clustered together) emerged, consisting of four cells which formed the intersection of the pure MA clusters C, A, and C in Table 4a, d, and e, respectively (the latter two clusters being identical). Regarding MA-A, a ‘core’ of three cells emerged, defined by the intersection of clusters A, C and A in Table 4a, d, and e, respectively (the latter being a mixed cluster). Finally, clusters C and D in Table 4a and d contained four and three MA-B cells, respectively.

A LB-A core of five cells emerged, contained in clusters A, B, and A in Table 4c, e and f, respectively. The latter two contained a larger LB-A core of six cells.

We focused on the first setting (i.e., hiding a single class) and th26 to analyze the estimated relevance of predictor variables. Overall, all predictor variables seemed useful, as each one was very likely relevant (around 100% chance of being relevant; dark green boxes in Fig. 6
                           ) for at least two subtypes/classes identified at th26. Feature X
                           2—axonal polar histogram in the [π, 2π) interval— appeared to be the most useful as it was very likely relevant for all the subtypes/classes (see Fig. 6). While its relevance for the HT class and MA subtypes is clear—an MA's axon grows predominantly upwards from the soma whereas the opposite holds for HT—it is interesting that it was relevant for the CB and LB subtypes as well. Features X
                           4 and X
                           5, which capture the length of axonal arborization at 150–300μm and over 300μm from soma, were relevant for five (out of six) subtypes. On the other hand, feature X
                           9—length of dendritic arborization at over 90μm from soma—appeared as least useful as it was very likely relevant for only two subtypes (CB-A and MA-B). In general, axonal features (X
                           1 to X
                           5) were more likely to be relevant than dendritic ones (X
                           6 to X
                           9). For example, an axonal feature was, on average, very likely relevant for 4.6 subtypes/classes in Fig. 6 whereas a dendritic one was for 2.5. Likewise, a dendritic feature was, on average, probably irrelevant (below 50% chance of being relevant; red and brown boxes in Fig. 6) for more subtypes than an axonal feature.

Moreover, as shown by Fig. 6, the relevance of axonal features differed among the subtypes/classes. All axonal features were very likely relevant for all MA subtypes (except X
                           1 for MA-A: MA-A, unlike MA-B and MA-C, did not stand out regarding X
                           1—note the olive green box for X
                           1 in Fig. 5) and for the HT class. This suggests that the axonal arborizations of the MA subtypes and the HT class were distinct among themselves and from the CB-A and LB-A subtypes according to all axonal features. On the other hand, two and three (out of five) axonal features were relevant for the CB-A and LB-A subtypes, respectively, and, in particular, polar histogram in the [0, π) interval and close to soma axonal arborization length (X
                           1 and X
                           3, respectively) were relevant with only 50% chance for both of those subtypes. This suggests that CB-A and LB-A were not particularly distinct according to those variables.

In summary, the results clearly show that the length of axonal polar histogram in the [π, 2π) interval (X
                           2) is particularly relevant, followed by axonal length relatively near to the cell body (X
                           4) and the extent of the axon far from the cell body (X
                           5). In addition, the dendritic arborization was highly relevant for the characterization of CB cells and MA cells. These findings may be useful for generating an accurate automatic classifier of 3D reconstructed interneurons.

@&#CONCLUSIONS@&#

Automatic classification of interneurons is a hard problem because data are scarce, the cells are morphologically, molecularly, and physiologically variable, and experts disagree on the definitions of features that distinguish them. Thus, we introduced a semi-supervised approach to this problem. Besides discriminating between types, this approach leads to the discovery of new types of neurons. It uses the available knowledge—in the shape of class labels—and also attempts to identify the relevance of each feature for each type and subtype.

We presented results on the classification of common basket, large basket, horse-tail, and Martinotti cells. We quantified the neurons with simple morphological features which describe the distribution and the orientation of axonal and dendritic arbors, seeking to mimic the way in which an expert visually classifies a neuron. The algorithm accurately discriminated among the different types when one and two types were unlabeled at a time and when half the instances of all types were unlabeled. Importantly, it identified potential subtypes of common basket, large basket, and Martinotti cells, suggesting that these types are more heterogeneous than previously thought. Although the identified subtypes are small, they may be indicative of the characteristics that differentiate cells belonging to the same interneuron type.

The defined morphological variables seemed useful for discriminating among the types. Axonal features seemed more useful than dendritic ones, with the axonal polar histogram length in the [π, 2π) interval appearing to be the most useful feature. The defined variables might then be considered in future studies of interneuron classification. Still, it is possible that more complex variables, such as those considering both the distance and position with respect to the soma (e.g., over 300μm from soma and above it) could further improve discrimination accuracy.

Overall, the results suggest that a semi-supervised approach may be helpful in neuronal classification and characterization. Further studies, with different morphological features and more neurons, would be needed in order to obtain more conclusive results. Although we focused on a subpopulation of GABAergic interneurons, the presented methodology could be applied to other types of cells as well.

The used algorithm, SeSProC, is open to further improvements, like the inclusion of uncertainty into labels. Instead of a single label—the class most voted by the experts—per data instance, a probability distribution could be learned from the experts’ votes, thus taking all votes into account. Alternatively, an approach similar to [31] might be applied: a separate model could be learned for each set of experts with similar opinions, and these models then combined into a final, consensus model. Regarding the data, although it is generally thought that the same morphological types of neurons are found in all species, we cannot discard the possibility of inter-species variability. Thus, we plan to further analyze the data taking into account the different species in order to find types of neurons that may be representative of particular species.

Finally, it has been shown that GABAergic interneurons are affected in several brain diseases but not all of these interneurons are equally affected. For example, alterations of chandelier cells expressing the calcium binding protein parvalbumin have been associated with certain forms of epilepsy and schizophrenia. In addition, possible alterations of interneurons expressing somatostatin, neuropeptide Y and vasoactive intestinal peptide, probably including Martinotti and horse-tail (or double bouquet) interneurons, have been reported in schizophrenia (e.g., [32–35]). Since the algorithm used in the present study accurately discriminated between different interneuron types and new groups can be discovered, its application to study brain diseases may shed light on interneuron pathology in these diseases, providing a tool to more accurately determine which subtypes may be affected.

@&#ACKNOWLEDGEMENTS@&#

The authors thank anonymous reviewers whose comments have greatly helped to improve this paper. This work was supported by grants from the following entities: the Spanish Ministry of Economy and Competitiveness (grants TIN2013-41592-P to B.M., P.L., and C.B; BFU2012-34963 to J.DF.), CIBERNED CB06/05/0066 to J.DF., the Cajal Blue Brain Project (Spanish partner of the Blue Brain Project initiative from EPFL) to B.M., J.DF., P.L. and C.B.; by the Regional Government of Madrid through the S2013/ICE-2845-CASI-CAM-CM project to B.M., P.L., and C.B; and the European Union's Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 604102 (Human Brain Project) to J.DF, P.L. and C.B. R.B.-P. was supported by the Spanish Ministry of Economy and Competitiveness (CSIC).

@&#REFERENCES@&#

