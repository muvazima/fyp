@&#MAIN-TITLE@&#Expressive signals in social media languages to improve polarity detection

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           To capture the sentiment of messages, several expressive forms are investigated.


                        
                        
                           
                           Expressive signals enrich the feature space of baseline and ensemble classifiers.


                        
                        
                           
                           Only adjectives play a fundamental role as expressive signal.


                        
                        
                           
                           Pragmatic particles and expressive lengthening could lead to the de finition of erratic polarity classifiers.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sentiment analysis

Polarity detection

Expressive signals

@&#ABSTRACT@&#


               
               
                  Social media represents an emerging challenging sector where the natural language expressions of people can be easily reported through blogs and short text messages. This is rapidly creating unique contents of massive dimensions that need to be efficiently and effectively analyzed to create actionable knowledge for decision making processes. A key information that can be grasped from social environments relates to the polarity of text messages. To better capture the sentiment orientation of the messages, several valuable expressive forms could be taken into account. In this paper, three expressive signals – typically used in microblogs – have been explored: (1) adjectives, (2) emoticon, emphatic and onomatopoeic expressions and (3) expressive lengthening. Once a text message has been normalized to better conform social media posts to a canonical language, the considered expressive signals have been used to enrich the feature space and train several baseline and ensemble classifiers aimed at polarity classification. The experimental results show that adjectives are more discriminative and impacting than the other considered expressive signals.
               
            

@&#INTRODUCTION@&#

The goal of sentiment analysis is to define automatic tools able to extract subjective information, such as opinions and sentiments from natural language texts, in order to create structured and actionable knowledge to be used by either a decision support system or a decision maker. This issue is usually addressed at document level (Yessenalina et al., 2010), in which the naive assumption is that each document expresses an overall sentiment. When dealing with social media contents coming from microblogs (like Facebook and Twitter), a lower granularity level could be more useful and informative (Jagtap and Pawar, 2013; Zhang et al., 2011). This new kind of virtual communication has led to new types of contents and diffusion models that need to be modeled explicitly starting from the language. The characteristics that distinguish well-formed contents (e.g. reviews) from microblogs messages relate to the use of canonical, coherent and at least paragraph-length pieces of text. However, sentiment analysis on social media leads towards new and more complex scenarios: the sentiment is conveyed in at most two sentence passages often with an informal linguistic register and with non-standard spelling (Eisenstein, 2013). These novel scenarios lead researchers to move from a traditional approach, which solves the sentiment analysis task by using machine learning models (Pang and Lee, 2008), to a communication-oriented paradigm.

The first expressive signals that have been considered in the literature to aid the detection of sentiment in a given message are concerned with lexical elements (e.g., adjectives, verbs, adverbs). Pak and Paroubek (2010) investigated the relationships of several part-of-speech (POS) tags with respect to the message subjectivity/objectivity. For instance, interjections and adjectives are relevant indicators of subjective texts, while objective messages contain more common and proper nouns. Once positive and negative texts have been annotated with their part-of-speech tags, the resulting corpus is used to train a sentiment classifier. A further approach that exploits the part-of-speech characteristics is presented in (Kouloumpis et al., 2011), where the combination of n-grams and POS tags shows a significant improvement in detecting the sentiment orientation of messages. In the context of social language processing, the use of emoticons has attracted machine learning researchers for the sentiment classification task (Hogenboom et al., 2013; Liu et al., 2012; Zhao et al., 2012). Emoticons are considered to be handy and reliable indicators of sentiment, and hence could be used either to automatically generate a training corpus or to act as evidence feature to enhance sentiment classification. With regard to expressive lengthening (e.g., “I loooooove you”), not much has been investigated for evaluating its contribution in polarity classification. An exception is (Brody and Diakopoulos, 2011), where the lengthening phenomenon in microblogs has been shown to be strongly associated with subjectivity and sentiment.

Inspired by the wide availability of emotional signals in social media and the promising results obtained by our previous contribution (Pozzi, Fersini, Messina and Blanc, 2013), in this paper we investigate the contribution of the most used expressive signals. To the best of our knowledge, no studies consider the combination of adjectives, initialisms for emphatic and onomatopoeic expressions, emoticons and word lengthening as possible additional features able to drive the detection of polarity in online social media. In this paper different contributions are given: (1) the analysis of three main language characteristics of social media language, (2) a text normalization procedure to better conform social media messages to a canonical language, (3) a feature expansion approach to improve polarity detection, (4) an analysis of the impact of the expressive signals studied both independently on each others and jointly in traditional learning models and (5) an analysis of the impact of the expressive signals in ensemble models, which have not been yet investigated in the state of the art. To the best of our knowledge, two main research papers (Fersini et al., 2014; Wang et al., 2014) deal with ensemble learning for sentiment analysis, but the focus is on the classification model instead of the impact of the expressive signals. Experimental results highlight not only the link between the characteristics of social media language with the polarity of the messages, but also their beneficial effects with respect to sentiment classification accuracy when jointly considered.

The paper is organized as follows. In Section 2 the main expressive forms in online social media are outlined. In Section 3 the text normalization procedure together with the proposed feature expansion approach are detailed. In Section 4 the baseline classifiers and ensemble methods, used to evaluate the impact of the proposed approach, are presented. In Section 5 the experimental investigation is detailed, while in Section 6 a detailed analysis about the behavior of the classifiers and the role of the expressive signals considered is reported. Finally, in Section 7 conclusions are derived.

To better capture the sentiment orientation of the messages, several valuable expressive forms should be taken into account when tackling polarity detection in online social environments. Although microblogs make available several expressive signals, most of them are platform-dependent. For example, Twitter has ‘hashtags’ (words prefixed with the symbol ‘#’) which allow users to easily specify topics and summarize the overall sentiment. Differently from Twitter where posts are plain texts, messages on Google+ and Tumblr can be characterized by formatted text. For instance, the bold style can be used to empathize the sentiment (e.g., ‘The iPhone is so beautiful!’) and the strikethrough (a typographical presentation of words with a horizontal line through their center) can be used to convey humor (i.e. the sentiment orientation can be reversed). People use the strikethrough to look like an edit, as if you were crossing something out on paper, but so it is still readable by people (e.g., That was kinda strikethrough …ehmmm..hilarious:)’).

In order to investigate expressive signals that are independent on the platform, this paper focuses on: (1) adjectives, (2) pragmatic particles, such as emoticon, emphatic and onomatopoeic expressions and (3) expressive lengthening. In our investigation, Twitter has been exploited thanks to its availability of data that are public by default: the percentage of public profiles available in Twitter is much higher than other social media. For example, in 2012, just over 11% of Twitter users were using the private profiles, compared to over 53% of Facebook (Dey et al., 2012). This turns Twitter into a gold mine of free data.


                     Adjectives. Adjectives are lexical components that operate on the substructure of a sentence to either describe or modify a given element. In this work, we argue that adjectives are strictly related to positive and negative opinions and therefore could contribute to better detect the sentiment of a given text message. Starting from the idea proposed in (Benamara et al., 2007), our paper is aimed at evaluating the spread of adjectives in online social media and their role in polarity prediction. To this purpose, a Part-Of-Speech tagging has been applied in order to tag each term of a message with respect to its verbal form. Canonical (J), comparative (JJR) and superlative (JJS) adjectives
                        1
                     
                     
                        1
                        The used tag set represents a standard for Part-Of-Speech tagging and it has been defined in the Penn Tree Bank Project (released through the Linguistic Data Consortium). See https://www.cis.upenn.edu/treebank/.
                      have been detected and considered as positive (or negative) according to one of the most used lexicon (Hu and Liu, 2004) known as DictHuLiu.
                        2
                     
                     
                        2
                        
                           http://www.cs.uic.edu/∼liub/FBS/sentiment-analysis.html.
                      The lexicon is composed of 4783 negative and 2006 positive words. Since online conversational text differs markedly from traditional written genres like newswire, we used a supervised POS tagger
                        3
                     
                     
                        3
                        
                           www.ark.cs.cmu.edu/TweetNLP/.
                      proposed by Owoputi et al. (2013) and trained on manually-annotated social media contents.


                     Pragmatic particles. Pragmatic particles, such as emoticons, emphatic and onomatopoeic expressions, represent those linguistic elements typically used on social media to elicit a given message. Emoticons are introduced as expressive, non-verbal components into the written language, mirroring the role played by facial expressions in speech (Walther and DAddario, 2001). Their role is mainly pragmatic: emoticons give a positive or negative sense to written sentences by a visual expression. According to this consideration, we formulate the hypothesis that there exists a relationship between the sentiment orientation of emoticons and messages. In order to corroborate this hypothesis (a descriptive analysis will be subsequently conducted), emoticons have been distinguished in two main categories, i.e. positive and negative. Instances of positive emoticons are ‘:-)’, ‘:)’, ‘=)’, ‘:D’, while examples of negative ones are ‘:-(’, ‘: (’, ‘=(’, ‘; (’.


                     Initialisms for emphatic expressions represent a further pragmatic element used in non-verbal communication in online social media. Although they act as constituent, these emphatic abbreviations play a similar role of emoticons: expressions such as ‘ROFL’ (Rolling On Floor Laughing) clearly represent positive expressions, while abbreviations as ‘BM’ (Bad Manner) denote negative statements.


                     Onomatopoeic expressions in online social media can help to convey emotions: some expressions such as ‘bleh’ and ‘wow’ are clear indicators of negative and positive emotional states and therefore can help to distinguish the polarity of a text message. In order to deal with onomatopoeic forms, a regular expression has been defined to map these text elements to the corresponding sentiment orientation dictionaries (positive and negative).

The complete list of pragmatic particles is available as supplementary material.


                     Expressive lengthening. In text-based social media, word styling (as bold, italic and underlining) is not always available and often replaced by some linguistic conventions. Moreover, the informal nature of expressions leads social media users to make use of orthographic styles that are actually close to the spoken language. In this paper, we claim that the commonly observed phenomenon of expressive lengthening (usually known as word lengthening or word stretching) is an indication of emphasis that is strongly associated with subjectivity and sentiment (Brody and Diakopoulos, 2011). These expressive forms, which are specific to informal social communication, are usually denoted by some orthographic conventions that mark important expressions used to help polarity detection.
                        
                           Example 1 [negative]: One. More. Source. C’mon google, just one more #
                              
                                 PLEAAASSEEEEE
                              
                           
                        
                     
                  

To better capture the positive or negative orientation of a message, also an expressive lengthening should be considered depending on the sentiment. However, in order to identify its polarity, the corresponding canonical (condensed) form need to be extracted. The main problem when addressing word lengthening is the selection of the correct root. For instance consider the term “gooood” that appears in the following two messages:
                        
                           Example 2 [positive]: Thanks to 
                           
                              gooood
                            
                           it’s Friday!!!!
                        
                     
                     
                        
                           Example 3 [positive]: The new Oreo cookies are really 
                           
                              gooood!
                           
                        
                     
                  

When dealing with sentiment analysis its fundamental to detect the correct root of the lengthened word, to subsequently identify the corresponding polarity. Although some approaches in the literature are aimed at tackling expressive lengthening, they are usually based on strict assumptions that make difficult (uncertain) the association of polarity to the original word. For instance, repeated characters are replaced with a single instance of that letter. With respect to the case reported above, both gooood will be replaced with god originating therefore an error in the subsequent polarity association. For this reason, we decided to consider only the presence of a lengthening without its potential sentiment orientation.

The hypothesis underlying this paper is that the main expressive signals mentioned above are strictly related to positive and negative opinions, and therefore could contribute to better detect the polarity of a given text message. In order to corroborate this hypothesis, a preliminary Bayesian analysis is conducted to clarify the relationships between these expressive forms and the sentiment orientation of messages. In order to support our hypothesis that the sentiment orientation of a given expressive signal agrees with the sentiment of the message, conditional probabilities have been computed.

In particular, given an expressive form e occurring in a message m and the polarity label set 
                        
                           Ω
                           =
                           {
                           +
                           ,
                           −
                           }
                        
                      (where + stands for positive and 
                        −
                      for negative), the conditional probability can be estimated as:
                        
                           (1)
                           
                              
                                 
                                    
                                       
                                          P
                                          
                                             (
                                             pol
                                             
                                                (
                                                e
                                                ∈
                                                m
                                                )
                                             
                                             =
                                             
                                                s
                                                e
                                             
                                             ∣
                                             pol
                                             
                                                (
                                                m
                                                )
                                             
                                             =
                                             
                                                s
                                                m
                                             
                                             )
                                          
                                          =
                                          
                                             
                                                I
                                                (
                                                pol
                                                
                                                   (
                                                   m
                                                   )
                                                
                                                =
                                                
                                                   s
                                                   m
                                                
                                                ∧
                                                pol
                                                
                                                   (
                                                   e
                                                   )
                                                
                                                =
                                                
                                                   s
                                                   e
                                                
                                                )
                                             
                                             
                                                I
                                                (
                                                pol
                                                
                                                   (
                                                   m
                                                   )
                                                
                                                =
                                                
                                                   s
                                                   m
                                                
                                                )
                                             
                                          
                                          ,
                                          
                                          
                                             s
                                             e
                                          
                                          ,
                                          
                                             s
                                             m
                                          
                                          ∈
                                          Ω
                                       
                                    
                                 
                              
                           
                        
                     where pol( · ) denotes the polarity label and I( · ) is the indicator function. The discussion about the descriptive analysis is based on two benchmarks presented in Section 5.

Unlike well-formed documents (e.g., reviews), the writing style and the lexicon of microblogging messages are widely varied. Moreover, messages are often highly ungrammatical, and filled with spelling errors. As reported in Eisenstein (2013), the non-standard spelling on the social media is mainly due to the fast writing of the users, length limits of messages in online microblogs and finally the spread of common illiteracies until they become “the norm”.

One approach to deal with language of social media consists in conforming the texts to a canonical language. For this purpose, we captured a set of patterns using dictionaries a priori defined and regular expressions (REGEX). The text normalization approach includes:
                           
                              •
                              
                                 URL removal: URLs do not provide valuable information for the sentiment analysis task. To this purpose, all the tokens matching the REGEX (https?
∣
ftp
∣
file)://[-a-zA-Z0-9+&@#/%?=∼_∣
!:,.;]∗[-a-zA-Z0-9+&@ #/%=∼_∣
] have been removed;


                                 Sharing symbols elimination: most of the social media provide specific tools that allow users to share as much as possible their messages (e.g., Hashtags, Mention and Retweet on Twitter). Analogously to URLs, all the symbols related to the sharing tools have been removed because ineffective with respect to polarity detection;


                                 Spell-Checker: messages in online social media are often highly ungrammatical and filled with spelling errors. In order to overcome this issue, misspelled tokens have been corrected using the Google’s Spell Checker API.
                                    4
                                 
                                 
                                    4
                                    
                                       https://code.google.com/p/google-api-spelling-java/.
                                  Since the Google’s algorithm takes the neighborhood (context) of a misspelled token into account in suggesting the correction, the whole previously filtered tweet is considered as a query rather than the single token
                                    5
                                 
                                 
                                    5
                                    The spell checking is performed later than the identification of expressive lengthening.
                                 ;


                                 Slang correction: in order to aggregate terms with the same meaning but represented with different slangs, a dictionary
                                    6
                                 
                                 
                                    6
                                    
                                       http://www.chatslang.com/terms/social_media.
                                  of a priori defined slang expressions with their meaning, such as ‘btw’ (by the way), ‘thx’ (thanks), ‘any1’ (anyone) and u (you) has been used (Balahur et al., 2014).

Most of the works on sentiment analysis have relied on machine learning approaches: by using a bag of words representation, the main goal is to learn a positive/negative classifier based on given weights associated to the words in the text. According to these strategies, the traditional feature vector representing a message m (used to train a given classifier) only includes terms that belong to a common vocabulary V of terms derived from a message collection:
                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             
                                                m
                                                →
                                             
                                             =
                                             
                                                (
                                                
                                                   w
                                                   
                                                      t
                                                      1
                                                   
                                                
                                                ,
                                                
                                                   w
                                                   
                                                      t
                                                      2
                                                   
                                                
                                                ,
                                                ⋯
                                                ,
                                                
                                                   w
                                                   
                                                      t
                                                      ∣
                                                      V
                                                      ∣
                                                   
                                                
                                                ,
                                                class
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where wti
                         denotes the weight of term i belonging to the message m.

Less effort has been devoted to enrich the feature space used by the learning machines: to the best of our knowledge no studies consider the combination of adjectives, initialisms for emphatic and onomatopoeic expressions, emoticons and word lengthening as a set of possible additional features used to improve polarity detection.

To this purpose, we propose to enhance the traditional feature vector by including indications about the expressive signals previously introduced. The novel feature vector of a message m is defined as:
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   m
                                                   →
                                                
                                                
                                                   new
                                                
                                             
                                             =
                                             
                                                (
                                                
                                                   w
                                                   
                                                      t
                                                      1
                                                   
                                                
                                                ,
                                                
                                                   w
                                                   
                                                      t
                                                      2
                                                   
                                                
                                                ,
                                                ⋯
                                                ,
                                                
                                                   w
                                                   
                                                      t
                                                      ∣
                                                      V
                                                      ∣
                                                   
                                                
                                                ,
                                                
                                                   p
                                                   +
                                                
                                                ,
                                                
                                                   p
                                                   −
                                                
                                                ,
                                                
                                                   a
                                                   +
                                                
                                                ,
                                                
                                                   a
                                                   −
                                                
                                                ,
                                                l
                                                ,
                                                class
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where ps
                         represents the pragmatic elements (emoticons, initialisms for emphatic and onomatopoeic expressions) with polarity 
                           
                              s
                              ∈
                              
                                 {
                                 +
                                 ,
                                 −
                                 }
                              
                              ,
                              
                                 a
                                 s
                              
                           
                         denotes the adjectives with polarity s, l denotes the expressive lengthening and class is the ground truth polarity.

In this section a brief overview of the traditional machine learning approaches used for polarity detection is given. In the following, Multinomial Naïve Bayes, Decision Tree, Support Vector Machines and Bayesian Networks are presented.


                        
                           Multinomial Naïve Bayes. Multinomial Naïve Bayes (MNB) is a classifier often used for text categorization. Given 
                           
                              
                                 x
                                 k
                              
                              ,
                              
                                 
                                    0.15
                                    e
                                    m
                                 
                                 
                                    0
                                    e
                                    x
                                 
                              
                              k
                              =
                              1
                              ,
                              2
                              ,
                              ⋯
                              ,
                              K
                           
                         be the k-th training vector and yk
                         is the corresponding label such that 
                           
                              
                                 y
                                 k
                              
                              ∈
                              
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 ⋯
                                 ,
                                 Y
                                 }
                              
                           
                        , its main goal is to compute the model probability as:
                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             P
                                             
                                                (
                                                
                                                   y
                                                   k
                                                
                                                ∣
                                                
                                                   x
                                                   
                                                      k
                                                   
                                                   1
                                                
                                                ,
                                                ⋯
                                                ,
                                                
                                                   x
                                                   
                                                      k
                                                   
                                                   n
                                                
                                                )
                                             
                                             =
                                             P
                                             
                                                (
                                                
                                                   H
                                                   i
                                                
                                                )
                                             
                                             P
                                             
                                                (
                                                
                                                   x
                                                   
                                                      k
                                                   
                                                   1
                                                
                                                ,
                                                ⋯
                                                ,
                                                
                                                   x
                                                   
                                                      k
                                                   
                                                   n
                                                
                                                ∣
                                                
                                                   y
                                                   k
                                                
                                                ,
                                                
                                                   ∑
                                                   j
                                                
                                                
                                                   x
                                                   
                                                      k
                                                   
                                                   j
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

This probability model is based on the assumption that the sample length and the class hypothesis are marginally independent.

Regarding polarity classification, the approach has been investigated in Pang et al. (2002), Pang and Lee (2004) and Go et al. (2009).


                        
                           Support Vector Machines
                        . Support Vector Machines (SVMs) are learning machines that try to find the optimal hyperplane discriminating samples of different classes (Cortes and Vapnik, 1995). A good separation is achieved by the hyperplane that has the largest distance to the nearest training data point of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. In this paper, we exploited the probabilistic extension of the original SVMs (Hastie and Tibshirani, 1998).

The main goal is to estimate
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             
                                                p
                                                y
                                             
                                             =
                                             P
                                             
                                                (
                                                
                                                   y
                                                   k
                                                
                                                =
                                                y
                                                ∣
                                                
                                                   x
                                                   k
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

We first estimate pairwise class probabilities following the setting of the one-against-one approach for multi-class classification
                           
                              (6)
                              
                                 
                                    
                                       
                                          
                                             
                                                τ
                                                
                                                   
                                                      yy
                                                   
                                                   ′
                                                
                                             
                                             ≈
                                             P
                                             
                                                (
                                                
                                                   y
                                                   k
                                                
                                                =
                                                y
                                                ∣
                                                
                                                   y
                                                   k
                                                
                                                =
                                                y
                                                
                                                   
                                                      0.35
                                                      e
                                                      m
                                                   
                                                   
                                                      0
                                                      e
                                                      x
                                                   
                                                
                                                
                                                   or
                                                
                                                
                                                   
                                                      0.35
                                                      e
                                                      m
                                                   
                                                   
                                                      0
                                                      e
                                                      x
                                                   
                                                
                                                
                                                   
                                                      y
                                                   
                                                   ′
                                                
                                                ,
                                                
                                                   x
                                                   k
                                                
                                                )
                                             
                                             ,
                                             
                                                
                                                   1
                                                   e
                                                   m
                                                
                                                
                                                   0
                                                   e
                                                   x
                                                
                                             
                                             
                                                
                                                   y
                                                
                                                ′
                                             
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             ⋯
                                             ,
                                             K
                                             ,
                                             
                                                
                                                   1
                                                   e
                                                   m
                                                
                                                
                                                   0
                                                   e
                                                   x
                                                
                                             
                                             y
                                             
                                                
                                                   0.25
                                                   e
                                                   m
                                                
                                                
                                                   0
                                                   e
                                                   x
                                                
                                             
                                             ≠
                                             
                                                
                                                   0.25
                                                   e
                                                   m
                                                
                                                
                                                   0
                                                   e
                                                   x
                                                
                                             
                                             
                                                
                                                   y
                                                
                                                ′
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

After collecting all pairwise 
                           
                              τ
                              
                                 
                                    yy
                                 
                                 ′
                              
                           
                         values, the following optimization problem has been solved:
                           
                              (7)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   APTARANORMAL
                                                   min
                                                
                                                p
                                             
                                             
                                                
                                                   1
                                                   e
                                                   m
                                                
                                                
                                                   0
                                                   e
                                                   x
                                                
                                             
                                             
                                                1
                                                2
                                             
                                             
                                                ∑
                                                
                                                   y
                                                   =
                                                   1
                                                
                                                Y
                                             
                                             
                                                ∑
                                                
                                                   
                                                      
                                                         y
                                                      
                                                      ′
                                                   
                                                   :
                                                   
                                                      
                                                         y
                                                      
                                                      ′
                                                   
                                                   ≠
                                                   y
                                                
                                             
                                             
                                                
                                                   (
                                                   
                                                      τ
                                                      
                                                         
                                                            
                                                               y
                                                            
                                                            ′
                                                         
                                                         y
                                                      
                                                   
                                                   
                                                      p
                                                      y
                                                   
                                                   −
                                                   
                                                      τ
                                                      
                                                         
                                                            yy
                                                         
                                                         ′
                                                      
                                                   
                                                   
                                                      p
                                                      
                                                         
                                                            y
                                                         
                                                         ′
                                                      
                                                   
                                                   )
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    
                                       
                                          
                                             subject
                                             
                                             to
                                             
                                                
                                                   1
                                                   e
                                                   m
                                                
                                                
                                                   0
                                                   e
                                                   x
                                                
                                             
                                             
                                                p
                                                y
                                             
                                             ≥
                                             0
                                             ,
                                             ∀
                                             y
                                             ,
                                             
                                                
                                                   1
                                                   e
                                                   m
                                                
                                                
                                                   0
                                                   e
                                                   x
                                                
                                             
                                             
                                                ∑
                                                
                                                   y
                                                   =
                                                   1
                                                
                                                Y
                                             
                                             
                                                p
                                                y
                                             
                                             =
                                             1
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Regarding polarity classification, Support Vector Machines have been investigated in (Go et al., 2009; Pang and Lee, 2004; Pang et al., 2002).


                        
                           Decision Trees
                        . Decision Trees (DT) are classifiers presented as binary tree-like structure, where each node corresponds to a variable and edges represents possible realization of that variable. Given a sample 
                           
                              
                                 x
                                 k
                              
                              =
                              
                                 (
                                 
                                    x
                                    
                                       k
                                    
                                    1
                                 
                                 ,
                                 ⋯
                                 ,
                                 
                                    x
                                    
                                       k
                                    
                                    n
                                 
                                 )
                              
                           
                        , leaf nodes correspond to the possible class hypothesis yk
                        . The main goal of Decision Trees is to build a model of the class hypotheses based on the observed attributes of training data. Since this classifier outputs a dichotomic decision tree, it can be used to determine the class label of unclassified sample by considering its descriptive attribute realizations. Building a decision tree model from a training dataset involves two phases. In the first phase, a splitting attribute and a split index are chosen. The second phase involves splitting the records among the child nodes based on the decision made in the first phase. For evaluating whether a node should be splitted or not, the Entropy Deviance (Aha et al., 1991) measure has been used.

Regarding polarity classification, Decision Trees have been investigated in Bifet and Frank (2010) and Jia et al. (2009).


                        
                           Bayesian Networks
                        . Bayesian Networks (BN) are probabilistic graphical models that compactly represent the joint probability distribution of n random variables. The main assumption, captured graphically by a dependency structure, is that each variable is directly influenced by only few others. A probability distribution is represented as a directed acyclic graph (DAG) whose nodes represent random variables and whose edges denote direct dependencies between a node 
                           
                              
                                 h
                                 k
                              
                              =
                              
                                 {
                                 
                                    x
                                    k
                                 
                                 ∪
                                 
                                    y
                                    k
                                 
                                 }
                              
                           
                         and its set of parents Pa(hk
                        ). Formally, a Bayesian Network asserts that each node is conditional independent of its non-descendants given its parents. Given n features, the joint probability distribution can be decomposed as:
                           
                              (9)
                              
                                 
                                    
                                       
                                          
                                             P
                                             
                                                (
                                                
                                                   h
                                                   
                                                      k
                                                   
                                                   1
                                                
                                                ,
                                                ⋯
                                                ,
                                                
                                                   h
                                                   
                                                      k
                                                   
                                                   n
                                                
                                                )
                                             
                                             =
                                             
                                                ∏
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                n
                                             
                                             P
                                             
                                                (
                                                
                                                   h
                                                   
                                                      k
                                                   
                                                   j
                                                
                                                ∣
                                                
                                                   h
                                                   
                                                      k
                                                   
                                                   1
                                                
                                                ,
                                                ⋯
                                                ,
                                                
                                                   h
                                                   
                                                      k
                                                   
                                                   
                                                      j
                                                      −
                                                      1
                                                   
                                                
                                                )
                                             
                                             =
                                             
                                                ∏
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                n
                                             
                                             P
                                             
                                                (
                                                
                                                   h
                                                   
                                                      k
                                                   
                                                   j
                                                
                                                ∣
                                                Pa
                                                
                                                   (
                                                   
                                                      h
                                                      
                                                         k
                                                      
                                                      j
                                                   
                                                   )
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              P
                              (
                              
                                 h
                                 
                                    k
                                 
                                 j
                              
                              ∣
                              Pa
                              
                                 (
                                 
                                    h
                                    
                                       k
                                    
                                    j
                                 
                                 )
                              
                              )
                           
                         is described by a conditional probability distribution (CPD).

Regarding polarity classification, Bayesian Networks have been investigated in Bai (2011) and Airoldi et al. (2006).

Given a space of possible models, classical statistical inference selects the single model with the highest likelihood given the training data and uses it to make predictions. This may lead to over-confident inferences and decisions that do not take into account the inherent uncertainty of the natural language in wider context as social media. Instead, the idea behind a ensemble mechanism is to exploit the characteristics of several independent classifiers by combining them in order to achieve higher performance than the best single classifier. In order to understand whether the typical social media expressive signals have an impact on several learning schemes, also the main ensemble approaches have been considered.

Regarding polarity classification, various ensamble approaches have been investigated in Whitehead and Yaeger (2010), Xia et al. (2011), Hassan et al. (2013) and Wang et al. (2014).


                        
                           Majority Voting
                        . One of the most popular ensemble system is Majority Voting (MV), which is characterized by a set of “experts” that classifies the message polarity by considering the vote of each classifier as equally important and determines the final polarity by selecting the most popular label prediction (Dietterich, 2002). Let C be a set of independent classifiers and poli
                        (m) the label assigned to a message m by classifier i ∈ C. Then, the optimal label pol
                        MV(m) is assigned as follows:
                           
                              (10)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   pol
                                                
                                                
                                                   MV
                                                
                                             
                                             
                                                (
                                                m
                                                )
                                             
                                             =
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     arg
                                                                     
                                                                     max
                                                                  
                                                                  
                                                                     s
                                                                     m
                                                                  
                                                               
                                                               
                                                                  ∑
                                                                  
                                                                     i
                                                                     ∈
                                                                     C
                                                                  
                                                               
                                                               I
                                                               
                                                                  (
                                                                  
                                                                     pol
                                                                     i
                                                                  
                                                                  
                                                                     (
                                                                     m
                                                                     )
                                                                  
                                                                  =
                                                                  
                                                                     s
                                                                     m
                                                                  
                                                                  )
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               if
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  ∑
                                                                  
                                                                     i
                                                                     ∈
                                                                     C
                                                                  
                                                               
                                                               I
                                                               
                                                                  (
                                                                  
                                                                     pol
                                                                     i
                                                                  
                                                                  
                                                                     (
                                                                     m
                                                                     )
                                                                  
                                                                  =
                                                                  
                                                                     s
                                                                     m
                                                                  
                                                                  )
                                                               
                                                               >
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  ∑
                                                                  
                                                                     i
                                                                     ∈
                                                                     C
                                                                  
                                                               
                                                               I
                                                               
                                                                  (
                                                                  
                                                                     pol
                                                                     i
                                                                  
                                                                  
                                                                     (
                                                                     m
                                                                     )
                                                                  
                                                                  =
                                                                  
                                                                     s
                                                                     
                                                                        m
                                                                     
                                                                     ′
                                                                  
                                                                  )
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               ∀
                                                               
                                                                  s
                                                                  
                                                                     m
                                                                  
                                                                  ′
                                                               
                                                               
                                                                  
                                                                     0.25
                                                                     e
                                                                     m
                                                                  
                                                                  
                                                                     0
                                                                     e
                                                                     x
                                                                  
                                                               
                                                               ≠
                                                               
                                                                  
                                                                     0.25
                                                                     e
                                                                     m
                                                                  
                                                                  
                                                                     0
                                                                     e
                                                                     x
                                                                  
                                                               
                                                               
                                                                  s
                                                                  m
                                                               
                                                               ∈
                                                               Ω
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  pol
                                                                  ^
                                                               
                                                               
                                                                  (
                                                                  m
                                                                  )
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            otherwise
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where I( · ) is the indicator function, Ω is the set of labels and 
                           
                              
                                 pol
                                 ^
                              
                              
                                 (
                                 m
                                 )
                              
                           
                         is the label assigned to m by the “most expert” classifier, i.e. the classifier that is able to ensure the highest accuracy.


                        
                           Bayesian Model Averaging
                        . The most important limit introduced by Majority Voting is that the models to be included in the ensemble have uniform distributed weights regardless their reliability. However, the uncertainty left by data and models can be filtered by considering the Bayesian paradigm. In particular, through Bayesian Model Averaging (BMA) all possible models in the hypothesis space could be used when making predictions, considering their marginal prediction capabilities and their reliabilities:
                           
                              (11)
                              
                                 
                                    
                                       
                                          
                                             P
                                             
                                                (
                                                pol
                                                
                                                   (
                                                   m
                                                   )
                                                
                                                ∣
                                                C
                                                ,
                                                D
                                                )
                                             
                                             =
                                             
                                                ∑
                                                
                                                   i
                                                   ∈
                                                   C
                                                
                                             
                                             P
                                             
                                                (
                                                pol
                                                
                                                   (
                                                   m
                                                   )
                                                
                                                ∣
                                                i
                                                ,
                                                D
                                                )
                                             
                                             P
                                             
                                                (
                                                i
                                                ∣
                                                D
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              P
                              (
                              pol
                              (
                              m
                              )
                              ∣
                              i
                              ,
                              D
                              )
                           
                         is the marginal distribution of the label predicted by classifier i and 
                           
                              P
                              (
                              i
                              ∣
                              D
                              )
                           
                         denotes the posterior probability of model i. The posterior 
                           
                              P
                              (
                              i
                              ∣
                              D
                              )
                           
                         can be computed as:
                           
                              (12)
                              
                                 
                                    
                                       
                                          
                                             P
                                             
                                                (
                                                i
                                                ∣
                                                D
                                                )
                                             
                                             =
                                             
                                                
                                                   P
                                                   (
                                                   D
                                                   ∣
                                                   i
                                                   )
                                                   P
                                                   (
                                                   i
                                                   )
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         j
                                                         ∈
                                                         C
                                                      
                                                   
                                                   
                                                      
                                                         0.12
                                                         e
                                                         m
                                                      
                                                      
                                                         0
                                                         e
                                                         x
                                                      
                                                   
                                                   P
                                                   
                                                      (
                                                      D
                                                      ∣
                                                      j
                                                      )
                                                   
                                                   P
                                                   
                                                      (
                                                      j
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where P(i) is the prior probability of i and 
                           
                              P
                              (
                              D
                              ∣
                              i
                              )
                           
                         is the model likelihood. In Eq. (12), 
                           
                              
                                 ∑
                                 
                                    j
                                    ∈
                                    C
                                 
                              
                              P
                              
                                 (
                                 D
                                 ∣
                                 j
                                 )
                              
                              P
                              
                                 (
                                 j
                                 )
                              
                           
                         is assumed to be a constant and therefore can be omitted. Therefore, BMA assigns the label pol
                        BMA(m) to m according to the following decision rule:
                           
                              (13)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   pol
                                                
                                                BMA
                                             
                                             
                                                (
                                                m
                                                )
                                             
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                arg
                                             
                                             
                                                
                                                   APTARANORMAL
                                                   max
                                                
                                                
                                                   pol
                                                   (
                                                   m
                                                   )
                                                
                                             
                                             P
                                             
                                                (
                                                pol
                                                
                                                   (
                                                   m
                                                   )
                                                
                                                ∣
                                                C
                                                ,
                                                D
                                                )
                                             
                                             =
                                             
                                                arg
                                             
                                             
                                                
                                                   APTARANORMAL
                                                   max
                                                
                                                
                                                   pol
                                                   (
                                                   m
                                                   )
                                                
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   ∈
                                                   C
                                                
                                             
                                             P
                                             
                                                (
                                                pol
                                                
                                                   (
                                                   m
                                                   )
                                                
                                                ∣
                                                i
                                                ,
                                                D
                                                )
                                             
                                             P
                                             
                                                (
                                                i
                                                ∣
                                                D
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                arg
                                             
                                             
                                                
                                                   APTARANORMAL
                                                   max
                                                
                                                
                                                   pol
                                                   (
                                                   m
                                                   )
                                                
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   ∈
                                                   C
                                                
                                             
                                             P
                                             
                                                (
                                                pol
                                                
                                                   (
                                                   m
                                                   )
                                                
                                                ∣
                                                i
                                                ,
                                                D
                                                )
                                             
                                             P
                                             
                                                (
                                                D
                                                ∣
                                                i
                                                )
                                             
                                             P
                                             
                                                (
                                                i
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The implicit measure 
                           
                              P
                              (
                              D
                              ∣
                              i
                              )
                           
                         can be easily replaced by an explicit estimate, known as F1-measure, obtained during a preliminary evaluation of the classifier i. In particular, by performing a cross validation, each classifier can produce an averaged measure stating how well a learning machine generalizes to unseen data. Considering ϕ-folds for cross validating a classifier i, the measure 
                           
                              P
                              (
                              D
                              ∣
                              i
                              )
                           
                         can be approximated as
                           
                              (14)
                              
                                 
                                    
                                       
                                          
                                             P
                                             
                                                (
                                                D
                                                ∣
                                                i
                                                )
                                             
                                             ≈
                                             
                                                1
                                                ι
                                             
                                             
                                                ∑
                                                
                                                   ι
                                                   =
                                                   1
                                                
                                                ϕ
                                             
                                             
                                                
                                                   2
                                                   ×
                                                   
                                                   
                                                      P
                                                      
                                                         i
                                                         ι
                                                      
                                                   
                                                   
                                                      (
                                                      D
                                                      )
                                                   
                                                   ×
                                                   
                                                   
                                                      R
                                                      
                                                         i
                                                         ι
                                                      
                                                   
                                                   
                                                      (
                                                      D
                                                      )
                                                   
                                                
                                                
                                                   
                                                      P
                                                      
                                                         i
                                                         ι
                                                      
                                                   
                                                   
                                                      (
                                                      D
                                                      )
                                                   
                                                   +
                                                   
                                                      R
                                                      
                                                         i
                                                         ι
                                                      
                                                   
                                                   
                                                      (
                                                      D
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 P
                                 
                                    i
                                    ι
                                 
                              
                              
                                 (
                                 D
                                 )
                              
                           
                         and 
                           
                              
                                 R
                                 
                                    i
                                    ι
                                 
                              
                              
                                 (
                                 D
                                 )
                              
                           
                         denotes precision and recall obtained by classifier i at fold ι. The measure 
                           
                              P
                              (
                              D
                              ∣
                              i
                              )
                           
                         can be estimated both for positive and negative polarities. In this way P(l(m)∣i, D) Eq. (13) is tuned according to the ability of the classifier to fit the training data. This approach allows the uncertainty of each classifier to be taken into account, avoiding over-confident inferences. For more details on BMA for polarity detection, please refer to (Pozzi, Fersini and Messina, 2013; Fersini et al., 2014).

In order to verify whether the proposed normalization techniques and feature expansion improve polarity detection, three benchmark datasets have been considered: Gold Standard Movie, Gold Standard Person (Chen et al., 2012) and SemEval 2013 - Task 2.
                           7
                        
                        
                           7
                           
                              http://www.cs.york.ac.uk/semeval-2013/task2/(Nakov et al., 2013).
                         Gold Standard Movie and Person contain respectively 1500 manually labeled Twitter data. Although the original dataset is composed of 3 different polarities (POS, NEG and NEU), a reduction of instances has been performed in order to deal only with positive and negative opinions. The resulting datasets are therefore unbalanced: Person is composed of 105 ( ≃ 26.44%) negative and 292 ( ≃ 73.56%) positive opinions, while Movie includes 96 ( ≃ 18.6%) negative and 420 ( ≃ 81.4%) positive orientations. The SemEval benchmark is composed of 4922 manually labeled tweets, 3474 ( ≃ 70.58%) positive and 1448 ( ≃ 29.41%) negative.

Concerning the traditional baseline classifiers (also enclosed in the ensembles), WEKA toolkit has been used, while BMA and MV have been developed from scratch. Regarding the classifier configurations, probabilistic SVMs have been trained with linear kernel (with cost parameter equal to 1.0 and tolerance to misclassification equal to 0.0010). K2 search algorithm has been exploited to learn the structure of the Bayesian Network. For Decision Trees, C4.5 (J48 in Weka) has been adopted while for Multinomial Naive Bayes no particular setting is required. In order to evaluate the performance achieved by the investigated approaches, a 10-folds cross validation has been adopted.

As performance evaluation, we employed the classical state-of-the-art measure for classification known as Precision (P), Recall (R) and F1-measure (Zaki and Meira, 2014). In particular, in order to directly compare the ensemble learning techniques with the baseline classifiers, we employed accuracy:
                           
                              (15)
                              
                                 
                                    
                                       
                                          
                                             Acc
                                             =
                                             
                                                
                                                   #
                                                   
                                                      
                                                         0.35
                                                         e
                                                         m
                                                      
                                                      
                                                         0
                                                         e
                                                         x
                                                      
                                                   
                                                   of
                                                   
                                                   messages
                                                   
                                                   successfully
                                                   
                                                   predicted
                                                
                                                
                                                   total
                                                   
                                                      
                                                         0.35
                                                         e
                                                         m
                                                      
                                                      
                                                         0
                                                         e
                                                         x
                                                      
                                                   
                                                   #
                                                   
                                                      
                                                         0.35
                                                         e
                                                         m
                                                      
                                                      
                                                         0
                                                         e
                                                         x
                                                      
                                                   
                                                   of
                                                   
                                                   messagges
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

@&#EXPERIMENTAL RESULTS@&#

In this section, computational results achieved on all the studied datasets are presented. First, results regarding the investigation on the single contribution of the studied features are shown. Finally, the features have been combined and the classification results are presented. In the experimental results, baseline classifiers, Majority Voting and BMA have been considered.

The first evaluation that has been conducted is mainly aimed at evaluating the spread of the adjectives in social media messages. To this purpose, the distribution of tweets with and without adjectives has been estimated and reported in Table 1
                           . Results highlight that this expressive signal is largely widespread on all the datasets.

Given the remark that adjectives are frequently used in social media contents, the subsequent analysis is focused on highlighting the relationship that exists between them and the sentiment orientation of the messages. In particular, the conditional probability distributions presented in Eq. (1) have been computed and reported in Table 2. If we focus on the first two rows of Tables 2a, we can highlight the relationship that exists between the presence/absence of an adjective and the sentiment orientation of the message (both for positive and negative messages). By analyzing the other two dataset (Tables 2b and c), we can note that while for positive tweets there is a good correspondence with the occurrence of adjectives, for negative tweets it is more probable to do not have any observation about their presence. If we consider the sentiment orientation of an adjective conditioned to the overall message polarity (row three and four), results show that positive messages have positive adjectives with a high probability as well as for negative adjectives in negative messages. A possible explanation of this polarity agreement is related to the characteristics of Twitter messages: due to the 140-characters limit, adjectives are usually used as powerful instruments to mark the essence of the opinion. A user does not have the possibility to express an articulated opinion, where potentially discordant adjectives (related to different aspects) can be used to express an overall polarity (such as reviews).
                              
                                 Example 4 [positive]: @user You’ll like Shutter Island it was really 
                                 
                                    
                                       good
                                    
                                  :D
                           
                           
                              
                                 Example 5 [positive]: […] star trek generations ========== trek fans may be more 
                                 
                                    
                                       forgiving
                                    
                                 , but for the rest of us, the 
                                 
                                    
                                       sluggish
                                    
                                  
                                 star trek generations is a mixed bag at best. the story is 
                                 
                                    
                                       interesting
                                    
                                 , but each scene goes too 
                                 
                                    
                                       long
                                    
                                 . the cast is 
                                 
                                    
                                       earnest
                                    
                                 , but the direction lacks. and so on. (the 
                                 
                                    
                                       best
                                    
                                  
                                 example of the latter is a klingon comeuppance that delivers none of the impact of a similar scene in star trek ii .) original enterprise captain james t. kirk appears on both ends of the story, though they cut the scene where shatner turns to the screen to plead “get a life”. remarkably 
                                 
                                    
                                       unremarkable
                                    
                                 . […]
                              
                           
                        

For instance, consider the examples reported above where two opinions are expressed on movies both by a tweet (Example 4, from Gold Standard Movie) and a review (Example 5, from the v2.0 polarity dataset
                              8
                           
                           
                              8
                              
                                 http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz.
                            (Pang and Lee, 2004)). Although both examples denote a positive sentiment orientation, in the first opinion the adjectives are clearly coherent with the overall message polarity, while in the second one the polarity of adjectives is characterized by a strong variability (3 positive adjectives vs 4 negative ones).

Regarding the classification results, Fig. 1
                            shows that including adjectives as additional features leads the most performing baseline classifier (SVM for all the datasets) to an accuracy improvement of 2.53% for Movie, 2.48% for Person and 0.49% for SemEval. Similar improvements can be noted for the ensemble methods (MV and BMA), highlighting the effective contribution of adjectives independently on the learning scheme. If we focus on a comparison between the best baseline classifier with no additional features and BMA
                              9
                           
                           
                              9
                              The best ensemble compositions are DT+BN for Movie, MNB+BN for Person and SVM+MNB+BN for SemEval.
                            with the inclusion of the expressive signal, we can see that a greater accuracy improvement of 4.09% for Movie, 3.73% for Person and 3.35% for SemEval has been achieved.

Unlike adjectives, which are frequent lexical functions in natural language, the pragmatic particles are expected to be less frequent. This characteristic is confirmed by the probabilities reported in Table 3
                           , where the distribution on the studied datasets is shown.

However, in order to verify that pragmatic particles could be an important source of information for polarity classification, a detailed analysis has been conducted by conditioning the presence/absence of a particle, as well as their polarity, with respect to the sentiment orientation of the message (Table 4
                           ).

Results for Movie (Table 4a) show that the polarity of pragmatic particles in the messages generally agrees with the message polarity: this leads a positive message to have positive particles, as well as negative messages with negative particles, with a high probability. Regarding the Person dataset (Table 4b), we can assert that while the occurrence of a positive particle is conditioned by the positive nature of the message, for the negative ones any conclusion can be drawn. In fact, a zero-probability event is not an event that never happens: the Person dataset does not enclose negative messages that contain negative pragmatic particles. Results for SemEval (Table 4c) show that while the occurrence of a positive particle is conditioned by the positive nature of the message, for the negative ones there is a very small difference that makes any conclusion statistically not significant.
                              
                                 Example 4 [negative]: Disappointing to see the UK Govt issueing reassuring messages to Tehran http://t.co/FEePWqPh Maybe they were influenced by @Nigel_Farage:)
                              
                           
                        

A possible explanation about 
                              
                                 P
                                 
                                    (
                                    
                                       p
                                       +
                                    
                                    ∈
                                    m
                                    ∣
                                    
                                       s
                                       m
                                    
                                    =
                                    −
                                    )
                                 
                                 >
                                 P
                                 
                                    (
                                    
                                       p
                                       −
                                    
                                    ∈
                                    m
                                    ∣
                                    
                                       s
                                       m
                                    
                                    =
                                    −
                                    )
                                 
                              
                            in Tables 4(b)–(c) is related to the use of mockery, humor and irony. Consider for instance Example 3, where a negative opinion has been extracted from the SemEval benchmark about politics. The judgment the UK Government is clearly negative although the positive emoticon “:)” is present introducing irony.

As far is concerned with the sentiment classification, in Fig. 2
                            the prediction accuracies of the considered learning machines have been reported. Considering pragmatic particle features with the most performing baseline classifier leads to an accuracy improvement of 1.77% for Movie (SVM), 0.72% for Person (MNB) and 0.24% for SemEval (SVM). A more significant improvement can be noted considering feature expansion with BMA
                              10
                           
                           
                              10
                              the best ensemble compositions are SVM+MNB for Movie, DT+MNB for Person and SVM+MNB+DT for SemEval.
                            against the best baseline with no additional features: BMA is able to provide an increment of 2.4% for Movie, 0.83% for Person and 3.18% for SemEval. Even though MV is significantly worst than the other approaches, the use of pragmatic particles leads to an accuracy improvement.

Since we argue that word lengthening could be a relevant sentiment indicator (although it rarely occurs), its presence has been conditioned to the message polarity. Table 5
                            shows that word lengthening has a greater correspondence with positive messages. This can be motivated by strongly positive emotional states related to joy. An instance is reported in Example 4, where a positive sentiment orientation has been extracted from the SemEval benchmark.
                              
                                 Example 5 [Positive]: 
                                 7:23 a young savage named suadonte wright was born, I love you baby may your soul rest < 3333 I miss you donnnn!
                              
                           
                        

Although the probability of expressing a stretched word is very low, this expressive form is able to provide a bit of information. In particular, if we focus on the classification results depicted in Fig. 3
                           , we can note that most of the classifier performance are improved when the expressive lengthening is considered. The most performing baseline classifier achieves an improvement of 1.18% for Movie (SVM), 0.21% for Person (MNB) and 0.05% for SemEval (SVM), an increment of 3.17% for Movie, a decrease of −2.42% for Person and an increment of 0.08% for SemEval by MV, and a gain of 0.43% for Movie, 0.2% for Person and 0.43% for SemEval ensured by BMA (SVM+MNB for Movie and Person, and SVM+MNB+BN for SemEval).

In the previous sections, the contribution of adjectives, pragmatic particles and expressive lengthening has been independently studied. In this section, the impact of all the expressive signals has been analyzed. Fig. 4
                            shows that including the considered additional features leads to a significant performance improvement for MV and the most performing baseline classifier and BMA ensemble. In particular, the feature expansion leads the best baseline classifier (SVM for all the datasets) to an accuracy improvement of 3.51% for Movie, 2.98% for Person and 1% for SemEval. The combination of BMA and all the expressive signals leads to an improvement of 4.32% for Movie (DT+BN), 5.02% for Person (DT+SVM+MNB) and 4.69% for SemEval (SVM+MNB+BN). An outstanding improvement can be noted when training a MV approach. In particular, all the expressive forms provide an improvement on Movie and Person of 14.80% and 5.94% respectively. These encouraging results suggest that not only words play an important role in sentiment classification on social media, but also that a mixture of expressive signals can significantly contribute to better discriminate between positive and negative opinions.

@&#DISCUSSION@&#

The results reported in the previous section need a deeper discussion about the behavior of the classifiers and the role of the expressive signals considered. The first consideration relates to the performance achieved by MV. If we focus on Figs. 1–4, we can easily note that MV has broadly reduced generalization abilities. For instance, on Movie and Person dataset, MV has decreasing performance than baseline in terms of accuracy of 12–13%. The main motivations of this behavior are related to two main aspects:
                        
                           1.
                           
                              Decision rule. The traditional MV approach exploits a democratic voting rule to assign a polarity to a given message, do not taking advantage of marginal distributions. For example consider a message m with negative polarity and a MV ensemble composed of three classifiers A, B and C. While A and B provide the same marginal distribution < 0.51;0.49 > for positive and negative labels respectively, C has < 0.05;0.95 > . This leads to a misclassification when considering the traditional voting rule, i.e. disregarding the marginal distributions the majority of classifiers originates a positive label prediction.

Therefore, the traditional voting rule is not able to take into account the indecision of the classifiers due to small gaps between positive and negative probabilities of the marginals. Alternative voting rules that could be able to overcome this limitation (experimentally investigated in the following) are:


                              Maximum rule. It selects the maximum a posteriori probability among the classifiers in the ensemble according to:
                                 
                                    (16)
                                    
                                       
                                          
                                             
                                                
                                                   P
                                                   
                                                      (
                                                      
                                                         
                                                            pol
                                                         
                                                         MV
                                                      
                                                      
                                                         (
                                                         m
                                                         )
                                                      
                                                      )
                                                   
                                                   =
                                                   
                                                      
                                                         APTARANORMAL
                                                         max
                                                      
                                                      i
                                                   
                                                   P
                                                   
                                                      (
                                                      pol
                                                      
                                                         (
                                                         m
                                                         )
                                                      
                                                      ∣
                                                      i
                                                      )
                                                   
                                                   ,
                                                   
                                                      
                                                         1
                                                         e
                                                         m
                                                      
                                                      
                                                         0
                                                         e
                                                         x
                                                      
                                                   
                                                   i
                                                   ∈
                                                   C
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           


                              Average rule. The decision is determined according to the mean of the a posteriori probabilities given by the classifiers:
                                 
                                    (17)
                                    
                                       
                                          
                                             
                                                
                                                   P
                                                   
                                                      (
                                                      
                                                         
                                                            pol
                                                         
                                                         MV
                                                      
                                                      
                                                         (
                                                         m
                                                         )
                                                      
                                                      =
                                                      
                                                         s
                                                         m
                                                      
                                                      )
                                                   
                                                   =
                                                   
                                                      1
                                                      
                                                         ∣
                                                         C
                                                         ∣
                                                      
                                                   
                                                   
                                                      ∑
                                                      
                                                         i
                                                         ∈
                                                         C
                                                      
                                                   
                                                   P
                                                   
                                                      (
                                                      pol
                                                      
                                                         (
                                                         m
                                                         )
                                                      
                                                      =
                                                      
                                                         s
                                                         m
                                                      
                                                      ∣
                                                      i
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           


                              Product rule. The decision is determined by the product of the posterior probabilities:
                                 
                                    (18)
                                    
                                       
                                          
                                             
                                                
                                                   P
                                                   
                                                      (
                                                      
                                                         
                                                            pol
                                                         
                                                         MV
                                                      
                                                      
                                                         (
                                                         m
                                                         )
                                                      
                                                      =
                                                      
                                                         s
                                                         m
                                                      
                                                      )
                                                   
                                                   =
                                                   
                                                      ∏
                                                      
                                                         i
                                                         ∈
                                                         C
                                                      
                                                   
                                                   P
                                                   
                                                      (
                                                      pol
                                                      
                                                         (
                                                         m
                                                         )
                                                      
                                                      =
                                                      
                                                         s
                                                         m
                                                      
                                                      ∣
                                                      i
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              In order to validate the hypothesis that the traditional democratic voting rule negatively affects the final prediction of MV, we report in Tables 6 and 7
                              
                              
                              a comparison with the above mentioned alternative decision rules on Movie Dataset. Independently on the expansion of features (Table 6), all the decision rules which take into account the marginal distributions outperform the democratic rule. When feature expansion is adopted (Table 7), the accuracy performance are generally higher and the gap between democratic and other decision rules tends to decrease. In addition, we can assert that Average decision rule is able to ensure better performance both when feature expansion is considered or not. All these conclusions are still valid when considering the other two datasets.
                                 11
                              
                              
                                 11
                                 The comparison of voting rules on Person and SemEval Datasets are provided as additional material.
                              
                           


                              Size of the dataset. The performance of MV on Movie and Person Dataset are lower than SemEval due to the small number of training instances available (see Fig. 4). In fact, the large volume of training examples of the SemEval dataset contributes to improve the learning abilities of baseline classifiers and therefore the generalization
                              
                               capacities of MV.

Obviously, any ensemble method have to deal with these two issues. However, BMA is less subjected to these concerns thanks to its paradigm (see Eq. (13)): the marginal distribution of the classifier predictions contributes to improve the decision rule, while the reliability of the models reduces the effect of the small volume of training data.

A further
                      discussion relates to the role of the expressive signals considered across domains and models. In order to evaluate the process independence across domains, we estimated for each dataset the accuracy gain when considering all features. This gain is estimated as the difference of accuracy considering all the features with respect to no feature expansion (see Table 8). All the features are able to ensure a gain in terms of accuracy for all the models across different domains, highlighting the whole process as independent on the dataset. In particular this gain ranges between a minimum of 1% and a maximum of 4.40%.

However, the entire process (feature expansion+learning models) is clearly dependent on the language used in the dataset. The proposed investigation focuses on social media messages characterized by an informal language style, where adjectives, pragmatic particles and expressive lengthening are commonly used. If the entire process was focused on social media messages characterized by a formal language style, the expected contribution would be driven only by adjectives.

In order to understand if there is an expressive signal that is more discriminative than others, we estimated for each dataset the accuracy gain when considering each feature in the optimal and worst model configuration with respect to no feature expansion (see Tables 9 and 10). When considering the best model configuration (Table 9), all the features ensure a positive gain on average. As expected, adjectives are more discriminative compared with pragmatic particles and lengthening. Focusing on the worst model configuration (Table 10), it emerges that while adjectives are still discriminative, programatic particles and expressive lengthening negatively affect the classifiers.

To better grasp the negative role of pragmatic particles and expressive lengthening, we report in Figs. 5–7
some learning curves related to the considered baseline classifiers. By analyzing the results, it emerges not only that the bag of word model without feature expansion is not able to perfectly encode the information provided by the adjectives (as well as the other features), but also that the error line when considering all the feature is almost asymptotic to the error line related to the them. This implies as general conclusion that only adjectives play a fundamental role as expressive signal, while pragmatic particles and expressive lengthening lead to the definition of erratic behaviors.

A final interesting issue relates to non-literal meaning, such irony and sarcasm, when dealing with polarity classification tasks. In order to understand if the considered expressive signals are discriminative when non-literal meaning is present, a preliminary experimental investigation has been conducted on the Evalita Dataset (Basile et al., 2014). The dataset is composed of Twitter messages (4513 training samples and 1935 test instances), for which subjectivity, polarity and irony annotations are available for each message. We report in Fig. 8
                      some experimental results on baseline classifiers. As expected, it emerges that adjectives contribute to reduce the misclassification error in non-ironic tweets (see Fig. 8(a)). On the opposite, we can easily point out that when dealing with ironic expressions, the investigated expressive signals are not able to provide any improvement with respect to the polarity classification error (see Fig. 8(b)).

@&#CONCLUSIONS@&#

In this paper, three valuable expressive signals have been explored for sentiment classification purposes. The role and impact of adjectives, pragmatic particles and expressive lengthening have been investigated on two benchmark datasets. The experimental results show that, although adjectives give the highest contribution in polarity detection, jointly considering all the proposed features leads to promising results. There are many potential future extensions of this work. It would be interesting to investigate the contributions of additional expressive signals that could be grasped from social media, such as repeated exclamation marks and the use of upper-words. Emoji and symbols related to sharing tools could be also useful sentiment information. Concerning emoji, which are platform/plugin dependent and implemented as their own character set, specific decoding processes (Suttles and Ide, 2013) will be implemented in order to deal with tweets that might contain Android/iPhone-specific emojis. In order to deal with this kind of information (e.g. hashtags), which are typically provided as a single token although they contain multiple words (e.g. #greatstart), the approach presented in Maynard and Greenwood (2014) could be exploited. A more challenging ongoing research relates to a communication-oriented paradigm to better model the user-generated contents in online social media. In addition to lexical information, cues about the language style (formal, informal and semi-formal) and the subjectivity/objectivity of the message should be considered as a gold mine. A final interesting aspect relates to the explicit modeling of non-literal meaning Reyes et al. (2013), such as irony and sarcasm, for a better modeling of polarity classification in real world environments. In this context, the role of most of the expressive signals needs to be deeply investigated.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.ipm.2015.04.004.


                     
                        
                           
                        
                     
                  


                     
                        
                           
                        
                     
                  


                     
                        
                           
                        
                     
                  


                     
                        
                           
                        
                     
                  

@&#REFERENCES@&#

