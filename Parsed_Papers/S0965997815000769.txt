@&#MAIN-TITLE@&#Extending parallelization of the self-organizing map by combining data and network partitioned methods

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Data partitioning and network partitioning parallelization methods are combined.


                        
                        
                           
                           Large performance gains are found on small networks.


                        
                        
                           
                           The network is parallelized to the level of a single node dimension.


                        
                        
                           
                           Parallelization is maximized to allow robustness for future hardware.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

GPU

Self-organizing map

Parallel computing

Neural network

Data visualization

High-dimensional data

@&#ABSTRACT@&#


               
               
                  High-dimensional data is pervasive in many fields such as engineering, geospatial, and medical. It is a constant challenge to build tools that help people in these fields understand the underlying complexities of their data. Many techniques perform dimensionality reduction or other “compression” to show views of data in either two or three dimensions, leaving the data analyst to infer relationships with remaining independent and dependent variables. Contextual self-organizing maps offer a way to represent and interact with all dimensions of a data set simultaneously. However, computational times needed to generate these representations limit their feasibility to realistic industry settings. Batch self-organizing maps provide a data-independent method that allows the training process to be parallelized and therefore sped up, saving time and money involved in processing data prior to analysis. This research parallelizes the batch self-organizing map by combining network partitioning and data partitioning methods with CUDA on the graphical processing unit to achieve significant training time reductions. Reductions in training times of up to twenty-five times were found while using map sizes where other implementations have shown weakness. The reduced training times open up the contextual self-organizing map as viable option for engineering data visualization.
               
            

@&#INTRODUCTION@&#

Many problems facing industry today are investigated using data acquired through a number of mediums (e.g., sensor recordings or simulation results). Data is becoming cheaper to acquire while at the same time becoming more accurate. Leveraging available data is increasingly important for remaining on industry’s cutting edge. One problem continuing to emerge is the shear amount and corresponding dimensional complexity of data being investigated. No longer can basic plotting methods (e.g., orthogonal plots, scatterplots, etc.) alone be relied upon to understand data characteristics. Researchers have realized this for a number of years and have developed many techniques in an effort to visualize growing levels of data complexity [1–4]. A number of current techniques map n-dimensional data sets into a two or three-dimensional representation that is directly interpretable by human visual perception. If, for example, a seven variable data set is to be visualized, it is common to set four or five of the variables to constant values and then plot the remaining two or three dimensions using a traditional graphing technique. This can be carried out multiple times using permutations of the dimensions held constant in an attempt to understand the complex relationship therein. This becomes impossible to understand as the number of permutations increase, leading to the necessity of new or further developed techniques.

Much research has been conducted on multidimensional data representations attempting to present pertinent data to a user [5]. Methods like parallel coordinates [6], graph morphing [7], and linking multiple visuals [1,8] lessen mental load on an investigator by limiting the visual complexity displayed in a single view. Each of the noted methods, however, remain difficult to interpret for high levels of dimensionality. Pixel-based [9] techniques approach the problem from a different direction by showing massive amounts of data while limiting focus only to overall data trends. In many cases a single choice from prior developed methods is not fully effective on its own. This has led to the development of tools that integrate multiple visualization techniques [3] into a suite of tools. Each of the methods noted, whether alone or combined, provide insights into the data under investigation but leave a large part of the complexity remaining for the investigator to make inferences from.

Kohonen’s Self-Organizing Map (SOM) [10] allows for low-dimensional visualization of a high-dimensional space while the data’s topology is preserved. It has been used extensively to avoid many of the issues noted with modern data visualization methods [11–14]. The Contextual SOM (CSOM) supplements the standard SOM with the addition of a contextual label on the individual nodes of the resulting SOM. Prior work showed that the added contextual information allows an investigator to identify characteristics of the data set that are otherwise extremely difficult to find [15].

However, the original SOM method is a serial method and trains the map one data point at a time. It can take hours to days for a single 100,000 data set to train on a modern desktop computer. This limitation has led to the development of a batch processing method of training SOMs called batch-SOM [16]. The batch method allows for faster convergence with large data sets by allowing the independent processing of each data point in parallel. Prior literature [17–19] has shown training time reductions using the batch-SOM in combination with the two parallelization techniques commonly used to breakup the SOM training process: network partitioning and data partitioning. These methods alone are useful and can reduce training times, but are limited to certain cases as will be described in the remaining sections. A new implementation of the batch-SOM is developed in this work by combining network partitioning and data partitioning and further parallelizing the network partitioning method traditionally used.

In 1982, Tuevo Kohonen modeled the human brain’s learning processes in the cerebral cortex using an artificial neural network [10]. The SOM uses an unsupervised learning strategy to train a lattice of neurons. Determining the structure of the original neuron lattice is done in a heuristic fashion by the investigator based on data set size and dimensionality, for example. Each neuron i in the lattice has its own weight vector w as shown in Eq. (1) with the same dimensionality as each data point x as shown in Eq. (2). This structure allows the SOM to scale to any dimensionality k as shown in Eqs. (1) and (2).
                           
                              (1)
                              
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 〈
                                 
                                    
                                       w
                                    
                                    
                                       i
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       i
                                       2
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       ik
                                    
                                 
                                 〉
                              
                           
                        
                        
                           
                              (2)
                              
                                 x
                                 =
                                 〈
                                 
                                    
                                       x
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       2
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       k
                                    
                                 
                                 〉
                              
                           
                        
                     

SOM training involves two phases, ordering and convergence, each containing many iterations through the data. The number of iterations for the method to run is decided upon by the investigator. To begin an iteration, a data point is randomly selected from the data set and is compared against each node in the map using the Euclidean distance metric shown in Eq. (3). The neuron found with the smallest Euclidean distance is determined to be the winner or “activated” neuron.
                           
                              (3)
                              
                                 Distance
                                 =
                                 
                                    
                                       
                                          
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   1
                                                
                                             
                                             -
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                   1
                                                
                                             
                                             )
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       
                                          
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   2
                                                
                                             
                                             -
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                   2
                                                
                                             
                                             )
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       …
                                       +
                                       
                                          
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   k
                                                
                                             
                                             -
                                             
                                                
                                                   w
                                                
                                                
                                                   ik
                                                
                                             
                                             )
                                          
                                          
                                             2
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

With the winning node decided, the weight vector w of each node in the surrounding neighborhood h of the map is “influenced” using Eq. (4) to have its values become more like the current data point x. The influence’s magnitude depends on the neuron’s position, ri, relative to the winning node’s position, rj
                        , in the map as well as the current number of training iterations n that have elapsed. η is a time-varying learning rate and defines the amount of the influence on neighboring nodes. The neighborhood influence h is a Gaussian-based influence factor that effects nodes closer to the winner more than those farther away. Learning rate and neighborhood influence are shown in Eqs. (5)–(7) respectively.
                           
                              (4)
                              
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 n
                                 +
                                 1
                                 )
                                 =
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 n
                                 )
                                 +
                                 η
                                 (
                                 n
                                 )
                                 
                                    
                                       h
                                    
                                    
                                       j
                                       ,
                                       i
                                    
                                 
                                 (
                                 n
                                 )
                                 (
                                 x
                                 (
                                 n
                                 )
                                 -
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 n
                                 )
                                 )
                              
                           
                        
                        
                           
                              (5)
                              
                                 η
                                 (
                                 n
                                 )
                                 =
                                 
                                    
                                       η
                                    
                                    
                                       0
                                    
                                 
                                 ∗
                                 
                                    
                                       exp
                                    
                                    
                                       -
                                       n
                                       /
                                       λ
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 σ
                                 (
                                 n
                                 )
                                 =
                                 
                                    
                                       σ
                                    
                                    
                                       0
                                    
                                 
                                 ∗
                                 
                                    
                                       exp
                                    
                                    
                                       -
                                       n
                                       /
                                       λ
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    
                                       h
                                    
                                    
                                       j
                                       ,
                                       i
                                    
                                 
                                 (
                                 n
                                 )
                                 =
                                 exp
                                 
                                    
                                       
                                          
                                             
                                                -
                                                ‖
                                                
                                                   
                                                      r
                                                   
                                                   
                                                      i
                                                   
                                                
                                                -
                                                
                                                   
                                                      r
                                                   
                                                   
                                                      j
                                                   
                                                
                                                
                                                   
                                                      ‖
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                             
                                                σ
                                                
                                                   
                                                      (
                                                      n
                                                      )
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Applying a label to each node of the map is referred to as the Contextual Self-organizing Map (CSOM). When using the CSOM, data points are passed into the SOM a final time to determine the closest node, again using Euclidean distance. In the contextual phase, the data point’s label is added to the activated neuron instead of updating the neighborhood. Fig. 1
                         
                        [20] shows an example from Haykin that trained a CSOM using animal attributes. The animal’s attributes make up the training data and the contextual label is the animal’s name. The trained map shows groups of animals with similar attributes. Zebras, horses, and cows were grouped together by the training. This group is noted by Haykin as peaceful, four-legged large mammals.

Using the traditional SOM as described above, the map node weights are updated after every data point. With the batch-SOM, however, all data points are first evaluated (for their winner) before updating the map [16]. Two types of parallelization become possible using the batch formulation. First, because node updates only occur once per iteration (as opposed to per data point), all winning node calculations can be performed in parallel. Secondly, the map itself can be parallelized because each node is only required to perform a summation of the influence of all other nodes. The formulation can be thought of as similar to a weighted average being performed across the map. Eq. (8) shows the batch-SOM equation for updating the weight vectors of each node following a single map iteration where t
                        0 and tf
                         represent the start and finish of the present iteration, respectively.
                           
                              (8)
                              
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 
                                    
                                       t
                                    
                                    
                                       f
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   t
                                                
                                                
                                                   ′
                                                
                                             
                                             =
                                             
                                                
                                                   t
                                                
                                                
                                                   0
                                                
                                             
                                          
                                          
                                             
                                                
                                                   t
                                                
                                                
                                                   ′
                                                
                                             
                                             =
                                             
                                                
                                                   t
                                                
                                                
                                                   f
                                                
                                             
                                          
                                       
                                       
                                          
                                             h
                                          
                                          
                                             ji
                                          
                                       
                                       (
                                       
                                          
                                             t
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                       x
                                       (
                                       
                                          
                                             t
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   t
                                                
                                                
                                                   ′
                                                
                                             
                                             =
                                             
                                                
                                                   t
                                                
                                                
                                                   0
                                                
                                             
                                          
                                          
                                             
                                                
                                                   t
                                                
                                                
                                                   ′
                                                
                                             
                                             =
                                             
                                                
                                                   t
                                                
                                                
                                                   f
                                                
                                             
                                          
                                       
                                       
                                          
                                             h
                                          
                                          
                                             ji
                                          
                                       
                                       (
                                       
                                          
                                             t
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

The standard personal computer today often has between two and eight cores that make up the CPU. The drive behind an increasing number of cores has been due to the hardware venders beginning to reach the limits of how much performance can be achieved from each core. Multi-core processing has thus become the standard route to achieve higher performance levels [21]. More recently, the use of graphics processing units (GPUs) containing upwards of 2000 cores have become popular. Up until the early 2000s, the primary us of a GPU was to generate graphics to be displayed on a screen. Improvements to the hardware in recent years have opened up the control over the graphics pipeline for software developers to use the GPU for general purpose computation. GPU research has shown large improvements in computational capability over that of a modern CPU [22]. Today there are multiple software packages to choose from that allow programming on a GPU in a similar way to modern C/C++ programming [23–25].

CUDA [25] is a programming language created by NVIDIA Corporation to take advantage of NVIDIA GPUs for general purpose computing. The CUDA libraries provide a large range of computational functionality that can be used for general scientific computing and lower the barrier to entry for GPU parallelization of algorithms.

One primary difference with writing software to be executed on a CPU is that on a GPU the software developer must choose the memory storage location manually. This makes understanding the layout of memory (see Fig. 2
                        ) on a GPU very important. As an example, accessing the global memory that is available to all threads in a kernel can take from 200 to 800 clock cycles while instead using shared memory located on the processor die and limited to a single block of threads can take ∼1 clock cycle [26]. This difference could entirely change a method’s implementation. The GPU model provides a lot of computational power, but also leads to limitations ranging from those similar to the memory limitation noted above to hardware limitations like limited double-precision floating point operations [22].

There are two commonly used techniques for parallelizing the SOM whether for CPU or GPU parallelization. The first technique breaks up the map itself, and is known as network partitioning. In this case the winning node calculation and node updates are all computed on separate threads. This method of parallelization is often used with the original Kohonen SOM [10] formulation because all data points can be trained sequentially with map updates meaning the original algorithm’s integrity is held. This method is most effective when implemented with large maps as using small maps causes a large amount of time to be spent simply communicating between threads and writing to memory rather than performing calculations [23,24].

Ozdzynski et al. implemented a network-partitioned algorithm [28] on the CPU and tested three different update kernels for the maps using from one to eight processors cores on varying sized maps from small (2×4), medium (20×30), and large (30×40) and found that the time taken to train on smaller maps increased when increasing the number of parallel computing threads. The larger the maps used, the smaller the parallelization training time required [29]. Rauber et al. developed an improved technique implementing a similar network partitioned approach with an increased focus on limiting communication and increasing the use of cache memory [30]. Testing was done by parsing and classifying a collection of 420 articles from Time magazine by topic. Up to a twenty-two times increase in performance using the parallel method with caching was found.

The second method of parallelizing SOM training is to divide up the data amongst the individual threads, known as data partitioning. This implementation requires the use of a modified version of a SOM called the batch-SOM [16] noted in Section 1.2. With the batch-SOM, map training does not depend on the order of data points being trained because the map update does not occur until the winning node is found for all data points. This allows the data set to be divided among the parallel threads for calculating the winning nodes in parallel.

Lawrence et al. developed a method with a focus on data partitioned batch-SOMs [29]. They trained against sample retail and census data using only CPU parallel processing. Results showed comparable clustering to the traditional SOM methods but with linear speed increases as the number of processor increased. Similar parallel methods have been developed using the CUDA [18] and OpenCL [17] GPU libraries to achieve up to 44 times speed-ups compared to previous serial implementations with large maps [18].

Parallel implementations of the SOM thus far have generally made use of either network partitioning or data partitioning, but not both. The network partitioned methods were limited in their benefit to the size of the maps and the data partitioned methods did not leverage the fully parallelizable nature of the SOM. This work attempts to leverage the benefits of both network and data partitioning in a similar manner to [31] while at the same time increasing the parallelization further by breaking the network not only into separate nodes like standard network partitioning implementations, but also separate dimensions as will be described in detail in the remaining sections.

@&#METHODS@&#

When working with the GPU, structuring threads for execution is an important step. The thread structure developed for the first portion of the find best matching unit (BMU) kernel is depicted in Fig. 3
                         in green. A single thread was allocated for each data point in the data set X of size N. Once the threads have been structured, they can then be used for computation on the GPU once they are scheduled. Thread scheduling is handled entirely by the GPU. The mobile GPU used for this work contained two streaming multiprocessors each having access to 192 graphics cores for scheduling. In total T (384 in this case) thread cores were theoretically available for performing work during each clock cycle. “Occupying” these threads is a goal of GPU methods.

The second phase of the kernel was the execution of the threads and is depicted in the figure in brown. A copy of the current weight values for each node in the SOM was first added to low-latency shared memory for each block of threads. Each thread then accessed this memory to determine the Euclidean distance of each node from its respective data point. Once through all nodes in the map, each thread then determined the BMU for its respective data point and stored this value to global memory where it could later be read for map update purposes in the update kernel.

The second kernel, the update weights kernel, is where the primary contribution of this research resides. Its description is broken into two sections. First, creating and structuring the threads is described in this section and is depicted in green in Fig. 4
                        . Second, execution of the threads is described in the following Section 2.3 and is depicted in brown in Fig. 5
                        .

The data set X was first broken into groups g, which were each designated to a single map m. The number of groups g to break the data set into is selectable by the investigator and depends on the size of the data set. For example, if training a large data set, one would want to leverage this data-parallelism and would use a larger number of groups. Each map m was broken into its respective nodes n. Each node n was then broken into its respective weight values w(k). A single CUDA thread was finally allocated and assigned to each of the K weights found within the C nodes of the M maps making for a total of M
                        ×
                        C
                        ×
                        K threads capable of executing in parallel. This demonstrates the potential this method has over the standard network partitioning technique using only C parallel threads.

The second portion of the update map kernel performs the actual update computation as shown in Eq. (8). A diagram of this using the previously created threads is shown in Fig. 5. Each thread must use its provided thread ID (a global variable provided by the CUDA library) to calculate the map m it is contained in as well as its position (row, column, and dimension) within the map. Once the map and position are known the update can be performed (Eq. (8)). The thread ID is also used to compute the portion of the data set in which each thread operates. This is done by considering the map m it was assigned to, the total number of maps M, and the size of the data set N. For example, a thread contained in the first map of a two map decomposition only needs to look at the first half of the data.

At this point, a single thread only needs to sum the influence on its designated dimension of the map over the portion of the data assigned to that map. This influence can be computed by first accessing the globally stored BMU for each respective data point, computing the distance of the current node from the BMU, then using the current decay level of the Gaussian neighborhood function to find the influence caused. Once a thread has iterated through each of its designated data points in g, it contains locally summed numerator and denominator values for the respective dimension of the map in which it was assigned. Local values from each map m are then added to the global total accumulated by the same dimension in all other maps.

The final step is performing the actual division operation of Eq. (8). Threads designated to the first map m
                        −1 were re-used to complete the division. It should be noted that the final step could be performed by any single map’s set of threads, as only C ×
                        K threads are required to do so.

The two described kernels are executed for the given number of iterations determined prior to training (1000 and 500∗
                        C were used in this work for ordering and convergence phases respectively) and result in a fully trained map.

@&#RESULTS@&#

Each test was performed with an NVIDIA K5000 graphics card running on a 2.40GHz Intel Xeon E5-2609 Windows 7 machine. The K5000 is a 706MHz professional grade graphics card built on the NVIDIA Kepler architecture. It drives 1536 CUDA cores with eight streaming multiprocessors (192 CUDA Cores/MP) and has 4GB of global graphics memory clocked at 2700MHz with a 256-bit memory bus.

Results are presented in this section for the developed method and are in each case compared with Kohonen’s original method executed in serial. It is important to look at variation of both size and dimensionality of data sets when analyzing any new self-organizing map or, more broadly, machine learning algorithm. In this case, data sets are varied between two, five, and ten dimensions and are varied between 1000, 10,000, and 20,000 data points each computed on a 10×10 neuron grid.

Results for the 1000 point data set are shown in Table 1
                        . The first noticeable result is the large decrease in time for the GPU parallel implementation. In the case of the two-dimensional, 1000 point data set, the serial algorithm took five times as long as that of the GPU method. In the worst case, results show that the serial method took approximately 43× as long as the GPU method. It is also shown that as dimensionality increases, the methods benefit does decrease, but remains much faster. These results, while not on the same order of performance increase as [18] showing up to 44× on a large map, were performed on a small map, noted in recent work by [27] as often causing performance loss rather than gain in comparison with the serial method. As was the case in [27], it is often the overhead due to memory transfers and map syncing that causes the slowdown for small maps. In the work of this paper, kernel analysis revealed that the limited gains observed were more likely due to the extent of parallelization being performed. Using NVIDIA’s NSight GPU profiler application revealed that thread processor occupancy was very high (near theoretical max), while overall thread throughput was relatively low. That is, the method’s highly parallel nature was likely overloading the GPU with threads forcing them to wait in a queue to be executed on the CUDA cores. In this case the highly parallel nature may have hindered resulting performance, but the same reasoning makes the method scalable for larger GPUs in the future. The method could also be extended to fine-tune based on GPU characteristics and would may avoid some of this problem. This was not done in order to keep each test’s setup consistent.

Results of the 10,000 point data set are shown in Table 2
                        . The benefits seen from the GPU method on the 10,000 point data set are larger than those found for the smaller 1000 point data set. In comparison with the two-dimensional, 1000 point data set, the benefit from the GPU went from a 5× reduction in time to a nearly 20× reduction in time in comparison with the serial CPU algorithm. As handling of large data sets was an initial goal, this added reduction was as desired.

Results for the final, 20,000 point data set are shown in Table 3
                        . The relative computational gain shown when increasing the size from 10,000 to 20,000 points is smaller than that between the 1000 point and 10,000 point data sets, showing a possible leveling off of the gain when increasing data set size. This effect could be reduced by tuning both the number of maps to be used M and the kernel launch parameters (threads per block, memory size limits, etc.) to handle data set size and dimensionality, but again, for purposes of keeping similarity between tests, was not done in this work.


                        Fig. 6
                         depicts a direct comparison of the three data set sizes as dimensionality was varied between two, five, and ten dimensions. In each case the method produced large reductions in training time. These results, when considering potential training times of hours or even days, amount to potentially very large time savings when training self-organizing maps. The chart additionally reveals that the method as a whole shows less relative benefit over the CPU when dimensionality of the data set increases, and that gains found by data set size diminish as sizes continue to grow. Reducing this impact will be the core of the future work on this method.

@&#DISCUSSION AND CONCLUSIONS@&#

In this article a method was developed for reducing the training time of self-organizing maps using CUDA on a modern graphical processing unit. Initial implementations using data partitioning and network partitioning separately were each effective at reducing training time, but were limited in their respective benefits.

The method developed in this work first broke up the thread structure into separate maps thereby leveraging data partitioning, but at the same time then broke up each of said maps all the way down into each respective weight value to leverage network partitioning in a way that had not been done before. The number of threads available for execution using the developed method is equal to the number of maps, M, multiplied by the total number of neurons in the map, C, multiplied by the number of weight values found within a neuron, K. The structure developed will allow for its continued use as GPUs continue to increase the number of threads they can process in a single clock cycle.

In the experiment we tested the method against for data set size and dimensionality. Results showed greater performance enhancements as data set sizes increased and in some cases saw a performance gain of nearly 20X. The method showed a diminishing benefit when increasing dimensionality of the data set. Further refinement of hardware resources used by the algorithm could likely achieve similar results for dimensionality as for data set size, but is left for future work.

With the growing number of threads available per GPU clock cycle in today’s hardware, the method developed will continue to grow in its performance benefit. It is important to again point out that each test was run on a small map (10×10 neurons), something many current parallel methods do not perform well on and often perform worse on in comparison with serial methods. A full analysis of the method’s scalability will be left for future work.

The growth of both size and complexity of data in industry continues to demand methods that make data usable to an investigator. The self-organizing map is often used for this purpose, but is computationally demanding and limits its feasibility for use in practice. Modern graphics processing units (GPUs) have developed as a promising means by which to reduce the impact of these computational demands. The method developed in this article combined a fully parallelized network partitioning method with prior literature in data partitioning. The developed method is fully scalable and lends itself to increasingly larger benefit as graphics cards continue to grow in their computational power in years to come.

@&#REFERENCES@&#

