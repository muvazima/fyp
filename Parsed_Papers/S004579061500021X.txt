@&#MAIN-TITLE@&#Design, implementation and evaluation of a smartphone position discovery service for accurate context sensing

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We analyze the impact of smartphone carry positions on readings provided by sensors.


                        
                        
                           
                           We use accelerometer, gyroscope and light sensors to detect position information.


                        
                        
                           
                           Two-stage method based on position is shown to enhance fall classification accuracy.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Smartphone

Position discovery

Sensing

Pervasive computing

@&#ABSTRACT@&#


               
               
                  Detecting user context with high accuracy using smartphone sensors is a difficult task. A key challenge is dealing with the impact of different smartphone positions on sensor values. Users carry their smartphones in different positions such as holding in their hand or keeping inside their pants or jacket pocket, and each of these smartphone positions affects various sensor values in different ways. This paper addresses the issue of poor accuracy in detecting user context due to varying smartphone positions. It describes the design and prototype development of a smartphone position discovery service that accurately detects a smartphone position, and then demonstrates that the accuracy of an existing context aware application is significantly enhanced when run in conjunction with this proposed smartphone position discovery service.
               
            

@&#INTRODUCTION@&#

Modern smartphones embody a large set of sensors that can be utilized to learn a wealth of information about a user’s surrounding environment. Researchers view the availability of such sensors as an opportunity for developing context-aware applications that can provide services tailored for each user’s context. Context-aware mobile computing is not a new research topic, for example, a survey paper [1] covering advances in this field was published more than a decade ago. Despite the concept being there for a while, a breakthrough for the number of context-aware applications offered in smartphones application markets (e.g., App Store for Apple iOS or Google Play for Android OS) is yet to happen. For the most part, the current context-aware applications do not meet users’ high expectations from technology.

A key problem with current context aware applications is that they typically provide low level of accuracy, particularly when used in an environment different from what was conceived at the application development stage. A major reason leading to low accuracy is the wide variety of ways a user may carry his/her smartphone, henceforth referred to as smartphone position. Users carry their smartphones in different positions, e.g. in hand, in purse, in pants pocket, in shirt pocket, etc. Sometimes, their smartphones are in covered positions, in purse or pockets, while uncovered at other times, watching a video or talking on the phone. Sensor values of different sensors naturally vary based on smartphone position, which in turn impacts the accuracy of the context derived from these values.

This article takes a bottom-up approach to derive a generic solution to the smartphone position problem. First, in this Introduction Section, we reflect to the reader the severity of the problem by providing motivating examples from the literature for context-aware applications that behave poorly when dealing with varying smartphone positions. Next, we analyze the impact of the smartphone position on raw sensor data. This is important since context derivation algorithms start from raw sensor data collected from smartphone sensors. To do this, we conducted a range of experiments that involved collecting sensor data from several different users carrying their smartphones in several different positions. Analysis of the raw sensor values collected from different smartphone position shows that the level of smartphone position impact on raw sensor data ranges from no impact at all for the case of GPS to a considerably high impact as in the case of gyroscope and accelerometer. The details of the analysis can be found in Section 3 of the article.

Based on the analysis of the impact of smartphone position on raw sensors data, we have designed, implemented and evaluated a smartphone position discovery service. This service utilizes the sensor values collected from some carefully chosen sensors and detects the smartphone position with very high accuracy. It runs orthogonal to any other context aware service or application. The article describe the design, implementation and a detailed evaluation of this service. The utility of the service was demonstrated by showing that the accuracy of an existing context-aware application is significantly improved when it following the two-stage classification technique.

To summarize, our contributions are as follows:
                        
                           1.
                           We present a detailed study for the impact of smartphone positions on raw data for sensors that are common in current commercially available smartphones.

Based on this study, we built a smartphone position discovery service that detects the smartphone position with high accuracy. This service can run in conjunction with context-aware applications to provide them with the current smartphone position.

We integrated the service with an existing context-aware application for fall classification and demonstrated improvement in accuracy for this application when using the service.

Before we indulge into the details of our solution, it is important to reflect to the reader the severity of the smartphone position problem by giving examples of context-aware applications that suffers from low accuracy when dealing with arbitrary carry positions.

Consider three different context-aware applications each utilizing different sensor(s). SurroundSense [2] performs logical localization such as detecting if the user is currently having coffee at Starbucks, partying and shopping at Wal-Mart. The application is able to achieve logical localization by harnessing online sensor data from camera, microphone, and accelerometer sensors and comparing them with previous knowledge about the place. Smartphone position is a major obstacle for SurroundSense. For instance, if the smartphone is in a covered position, e.g. Hip Pocket, Pants Pocket, or Jacket Pocket, the system will not be able to take the required image to perform the color fingerprinting for the location. We believe that applications like SurroundSense can consult the smartphone position discovery service in order to take the required image when the phone is in reliable positions. We also believe that the sensor data from microphone and accelerometer would benefit from smartphone position by excluding the disadvantageous positions for the corresponding measured context.

Next, lets consider an application that uses the microphone sensor. Researchers in [3] have developed an audio-based cough counting system running on a smartphone to monitor the health of patients with respiratory diseases. The application requires the patient to place the smartphone either in the shirt pocket or attached to a neck strap. In fact, the authors acknowledge that the chosen two positions do not represent an optimum choice in terms of patient comfort. However, the application needed to stick to these positions to achieve acceptable accuracy. We believe that smartphone position discovery service can play an important role if integrated with the cough counting system. Users can be given the chance to carry their smartphones freely and coughs will only be counted in suitable positions, i.e. when the smartphone is in the upper body region. In the data usage statistics of smartphones reported in [4], it was shown that user-smartphone interaction durations can be as high as 500min a day. We believe that with such long interaction durations between the user and the smartphone, the cough system would still have enough opportunity to capture audio in favored positions without restricting the users to place their smartphones in specific positions.

Finally, we take an example for a context-aware application that is based on the accelerometer sensor. Here the benefit of smartphone position knowledge is not just bound to taking a go/no-go decision to capture contexts as in the previous two examples. Some applications provide better accuracy if trained for a single position. The fall classification application in [5] detects the type of fall from four different fall categories namely forward trips, backward slips, left lateral falls, and right lateral falls. The output of this application can be used by experts in the field of elderly care to develop fall prevention mechanisms and to assist first responders in providing more customized emergency procedures. In order to detect the type of fall, the application uses supervised machine-learning classifier with training data collected beforehand. Despite the fact that the experiment used a smartphone for data collection, users were not given the chance to carry the smartphone freely. Rather, the used smartphone was attached to the backside of a belt and users were asked to wear this belt and simulate the different categories of falls. Restricting the smartphone position in the experiment surely results in higher accuracy since the unification of position in both training and test data reduces the variability in the data, thereby, putting fewer burdens on the classifier. Nevertheless, we believe that such restriction in terms of smartphone position limits the practicality of the application.

To overcome the problem caused by arbitrary smartphone positions, we propose a two steps approach for such context-aware applications. First, the offline training for the classifier can be done with different smartphone positions to generate a classifier trained for each position. Second, with the presence of the smartphone position discovery service, the application will know in advance the current smartphone position and choose the classifier corresponding to that specific position during the classification process. To demonstrate the potential improvement in accuracy that this approach can achieve, we have conducted two experiments to detect the above-mentioned four types of falls (similar to [5]). In the first experiment, both the training data and the test data were collected at arbitrary smartphone positions by allowing the user to put the smartphone either in pants pocket, hip pocket or jacket pocket. The confusion matrix for this experiment is shown in Table 1
                     . In the second experiment, three training files, each containing the four types of falls, were collected for three mentioned smartphone positions. Afterwards, the user was asked to simulate the required four types of falls and the classifier was pointed to the training file corresponding to the smartphone position, assuming the smartphone position is known in advance. The confusion matrix of the second experiment is shown in Table 2
                     .

We notice that with arbitrary position (Table 1), the classifier fails significantly to distinguish the left lateral fall from the right lateral fall. Also, slips are being confused with right lateral falls in many occasions. The overall accuracy for the arbitrary positions experiment is 72.22%. We now turn into evaluating the ideal solution of assuming a smartphone position known in advance (Table 2). The results show a significant improvement for all fall categories with an overall accuracy of 94.8%. We conclude from these two experiments that with the knowledge of the smartphone position, the accuracy of fall classification improves dramatically, and in general the accuracy of any context aware application is likely to improve. However, the assumption of a complete knowledge of smartphone position is not a valid one. Any service that provides smartphone position information to a context aware application will most likely be not 100% accurate. So, given that a smartphone position discovery service is not 100% accurate, the question we hope to answer is whether the accuracy of a context aware application based on that position discovery service would still be higher than when that application does not use the position discovery service. We address this for the same experiment in Section 4.3 after presenting the implementation details for the smartphone position discovery service. So far, we have provided specific detailed examples of context-aware applications that can benefit from the smartphone position service. However, we anticipate that the service can provide utility to emerging context-aware application fields such as smart shopping [6,7], smart health [8] and smart living [9].

The rest of the article is organized as follows. Next section provides literature review for existing body of works that tackled the smartphone position problem. Then, Section 3 provides an extensive analysis of the impact of different smartphone carry positions on raw sensors data. Section 4 describes the design, implementation and evaluation of the position discovery service. Finally, Section 5 concludes the paper.

@&#RELATED WORK@&#

We focus on studies aimed at providing generic solution for the smartphone position problem. The work in [10] anticipated the importance of body-position knowledge even before the popularity of sensor-equipped smartphones. Their analysis utilized wearable accelerometer sensor to differentiate between four body positions. Another early work [11], limited to on-table, in-hand, and inside-pocket positions, augmented smartphones with a 2D accelerometer and demonstrated an accuracy of 87%. Some preliminary work to distinguish between the in-pocket and out-of-pocket body-positions is provided in [12] for a smartphone based on the microphone sensor. Good accuracy level of 80% was achieved. However, this study is also limited considering the number of positions covered. A recent project [13] used accelerometer to detect nine body positions with an accuracy of 74.6%. It identifies 60 relevant features for body position discovery. We believe that this result can be integrated with our work to increase the number of positions covered and enhance the overall accuracy of position discovery.

The work in [14] suggested the use of a rotation-based approach to recognize four body-positions. The presented solution is based on accelerometer and gyroscope. Achieved accuracy using support vector machine SVM classification was 91.69%. We achieve a comparable accuracy while covering a larger set of seven positions. The work in [15] performed automatic position discovery for four body-positions as a necessary step for achieving accurate user’s facing direction estimation. Analysis of position discovery based on different machine learning classifier was proposed. We conclude this related work section by pointing the reader to a recent survey paper [16] describing an extensive list of studies targeting the smartphone position problem.

Since context derivation algorithms start from raw sensor data collected from smartphone sensors, it is important to understand the impact a smartphone position may have on the sensor values of different sensors. We have conducted a series of experiments to study the impact of smartphone positions on raw sensor values. The objective of the study is to answer two questions. First, what sensors are most influenced by smartphone positions? Second, for those sensors, what are the features that can best reveal the differences in the raw data corresponding to the smartphone position? These features can then be utilized by the smartphone position discovery service. We focused on the sensors that are commonplace in current smartphones: accelerometer, gyroscope, light sensor, microphone, GPS and magnetometer. We collected data for each type of sensor at six different smartphone positions [17]. Since our goal is to cover the common scenarios in the daily life of a user, we added an extra position for the typical situation of a smartphone placed on a table. Fig. 1
                      illustrates these smartphone positions. Overall, ten users participated in these experiments. The analysis covered the physical contexts of idle, walking, and running. Following sections demonstrates the results of the experiments of walking users. We devote a separate section for the other physical contexts due to the challenge they impose on the position discovery service. The on table position is also analyzed separately due to its specific nature of being the only off-body position from the positions under analysis. Note that the presented results in the study apply for all participating ten users. However, for brevity, we only include analysis for subset of the users to help convey the point to the reader.

Accelerometer, whether embedded in a smartphone or as a wearable sensor, has been widely used to analyze the physical activities of human beings [18] as well as to detect other contextual information linked to the physical activities in the surrounding environment [19]. A single accelerometer reading provides three values in x-axis, y-axis, and z-axis. It is typical to use the magnitude as a single value to reflect on the three values. We logged the data for the accelerometer at the rate of 10Hz (i.e. 10 readings per second), which appeared to be sufficient to capture potential repetitive behavior. Fig. 2
                         compares accelerometer magnitude values for two positions, hand holding and pants pocket. The plot reveals that the pants pocket position magnitude values exceed 15m/s2 very frequently and reaches 20m/s2 in many data points. Whereas, for the case of the hand holding position, the accelerometer magnitude spikes hardly reach the value of 15m/s2. The reason for this difference is that if the smartphone is in the lower body part region (i.e. pants pocket and hip pocket), it is going to be subject to more vibrations than if it is in the upper body part region. This observation is in harmony with many accelerometer analysis techniques in other research that required the smartphone to be placed near the pelvic region to better detect user physical activity [18].

Next, we are interested in comparing two positions from the lower body part namely the pants pocket and the hip pocket positions. Unlike the previous case, we did not observe any big gap in the peak values for the two positions. Hence, we resorted to statistical analysis to reveal the potential differences. The mean, variance and standard deviation have been widely used in prior work with the accelerometer sensor [18]. In order to calculate these statistical features, we begin by dividing the data into time frames. Our chosen time frame size is five seconds, as this period of time is enough to capture any likely repetitive behavior influenced by the smartphone position. We then calculated the mean, variance, and standard deviation for each time frame. Fig. 3
                         provides plots for the standard deviation of four participants for the pants pocket and hip pocket positions. Notice that the standard deviation of the pants pocket position is higher than the standard deviation for hip pocket positions for the first three users. However, for user no. 4 the opposite is true for most of the time frames except time frame numbers 4 and 6. We attribute such differences to the diversity in body movements’ styles, different clothing (e.g. pocket size or design) and other measures related to the environment of the experiment. Nonetheless, we believe that the similar behavior found in most of the 10 users, as represented by the 3 shown users, of having a higher standard deviation values for pants pocket position can be exploited by the smartphone position discovery service.

In the microphone experiment, our goal is to identify any differences in the sound recorded that can be linked to the smartphone position. The focus of a prior work with the microphone [12] was limited to determining whether the smartphone is inside or outside the user’s pocket. Analyzing the friction noise generated as a result of the smartphone being inside a pocket can be used to unveil such information. We requested users to record a sound file using the smartphone with different smartphone positions under analysis. The recording was made in a quiet room to avoid any background noise interference. Audio clips were recorded at a rate of 44.1kHz then split into five seconds frame segments to detect potential repetitive behaviors. We then carried out amplitude analysis, nearly similar to the analysis done in [2], on the sound frame segments to generate sound fingerprints related to each smartphone position. The fingerprints are generated as follows. First, the amplitude range between −1 and 1 is divided into 200 equal intervals with one hundred intervals in the negative range and another one hundred in the positive range. We note here that the −1 and 1 values represent a linear scale to measure the loudness of the captured sound. Readings near +/−1 represent capturing loud noise. Whereas, readings near zero represent a quieter audio. This type of analysis was suitable to us since we are only interested in the friction noise imposed by the position. Second, by counting the number of occurrences of amplitude values within each range we develop a histogram. Finally, the values corresponding to each interval in the histogram are divided by the total number of samples, which generates the percentage of amplitude occurrences within each interval. The acoustic fingerprints for different smartphone positions for a single user are illustrated in Fig. 4
                        . By looking at the resultant fingerprints, one can notice that the upper body positions of hand holding, watching a video, talking on phone and inside jacket pocket have almost similar fingerprints with the biggest percentage of amplitudes concentrated near the zero amplitude value. Since the recordings were done in a quiet room, this result indicates that the friction noise for these positions was minimal. In contrast, the sound fingerprints for the pants pocket and hip pocket positions have amplitude values scattered in the range of −0.5 and 0.5. Therefore, we can conclude that the sound signatures divide the positions into two groups that are distinguishable from each other. However, the signatures within each group share common theme making the task of differentiating amongst them quite hard.

The gyroscope sensor is becoming more popular in recent commercial smartphones. The main reason behind including this type of sensor in smartphones is to increase the smartphones sensitivity to motion, which is required especially by games applications for a better gaming experience. Meanwhile, researchers are also exploiting gyroscope’s recent popularity to improve accuracy of many context aware services including user motion. Similar to accelerometer, gyroscope has sensitivity to motion, and has been used in many context aware applications [20,21]. For example, some dead reckoning techniques [20] have used the gyroscope to detect the direction of the user step, which provides better indoor localization accuracy. The gyroscope has been also used in ubiquitous computing applications involving motions. For instance, the work in [21] introduced a system to provide notifications to the car driver about aggressive driving actions such as excessive speeding or aggressive turning. The system utilizes several sensors including the gyroscope, where the role of the gyroscope is to measure the smartphone rotations. We learned from these prior works that gyroscope has the potential for providing information to help detect smartphone position since it demonstrated high sensitivity to user motion. Gyroscope reading also consists of three components in the x-axis, y-axis, and z-axis. The three values represent instantaneous angular velocity of the smartphone in each direction. Our application for logging accelerometer values was also used to log gyroscope values at the same rate as that of accelerometer, 10Hz. We conducted the same analysis that we did with accelerometer data to uncover any potential repetitive behavior of gyroscope raw data that can be attributed to the smartphone position. Fig. 5
                         depicts the magnitude of gyroscope raw values for a user for two different smartphone positions, hand holding and pants pocket for 1min.

Notice that for the hand holding position, gyroscope magnitude values can hardly reach the 3rad/s, whereas, for the pants pocket position the magnitude values pass the 3rad/s a lot of times and reach 5rad/s on some occasions. This clearly indicates that the gyroscope sensor is analogous to accelerometer in having sensitivity to the smartphone position. In addition, Fig. 5 reveals the difference between the upper body part positions and the lower body part positions. We now turn into another experiment aiming at identifying the potential differences between smartphone positions within the same body part. Fig. 6
                         plots the standard deviation of the gyroscope data for four users in two different smartphone positions, pants pocket and hip pocket.

It is clear that similar to the accelerometer, the standard deviation values of pants pocket position are always higher than the hip pocket position. This is true for all users except user 4 who had the same behavior in the accelerometer standard deviation values.

We conducted experiments for reading GPS values from different smartphone positions. Our preliminary results revealed no noticeable differences in latitude, longitude and altitude values of different positions. However, other results obtained in a study for the impact of smartphone position on GPS data [22] revealed that there is an error introduced to GPS readings from smartphone positions. These errors are calculated by comparing the reading from different positions to a ground truth data obtained from an accurate GPS device. We refrain from doing deeper analysis similar to this work since the position discovery service cannot capitalize on those differences to calculate position due to the high energy cost of GPS readings.

The magnetic field sensor measures the strength of earth magnetic field in the three directions of x, y, and z. It has been used in indoor localization techniques in the absence of GPS signals [20]. Fig. 7
                         displays two plots of magnetometer readings magnitudes related to the positions of hand holding and pants pocket. We have used the magnitude to fuse the three directions values into a single reading for simpler comparison.

We would like to pinpoint two facts from the plot. First, the magnitude values for both positions are experiencing similar cycle that starts with the peak before 20s and ends with the peak before 40s. Second, the hand holding position curve is smoother than the pants pocket curve. This can be noticed from the more frequent small spikes in the pants pocket curve. The first point can be attributed to the fact that while performing the experiment, our users were repeating the same circular path. Therefore, we can see the same impact of direction changes on the two curves. The second point is actually related to the smartphone position. In the hand holding case, the smartphone experienced fewer vibrations due to body movements making the curve smoother. In contrast, for the pants pocket position, the vibrations were high resulting in the spikes. We also look at the standard deviation of four of the users in Fig. 8
                         to decide on the statistical features that can be used by the smartphone position discovery service.

One can notice from the figures that the standard deviations for the two body positions of pants pocket and hip pocket do not experience any pattern for the four users. This is due to the fact that the influence of direction changes is much higher than the influence of the smartphone positions vibrations. Since direction changes can take place arbitrarily, the smartphone position discovery service cannot depend on magnetometer to perform the required smartphone position distinction.

Light sensor provides a single reading in lumen (lm) representing the measured luminous flux. As expected, throughout our experiments, we have seen that the measured light intensity values are influenced by whether the smartphone is in covered or uncovered position. However, no patterns have been noticed that can be attributed to the specific smartphone position. Covered positions have average values nearly equal to the lowest light intensity value the smartphone can give. In contrast, uncovered positions produce much higher values that are dependent on the light intensity of the environment. This observation can be clearly spotted from Fig. 9
                        , which depicts the light intensity raw data for the two smartphone positions of hand holding and pants pocket.

All our experiments so far have been done for a walking user. Though walking context is popular, a user could be in other physical contexts such as idle and running. Notice that in the idle context, sensor values of accelerometer, gyroscope, magnetometer and GPS are unaffected by the smartphone position due to the absence of body movement patterns. On the other hand, all the sensors that are impacted by smartphone positions in walking context will also be impacted by smartphone positions in running context, although the nature and magnitude of the impact may be different. Since accelerometer demonstrated sensitivity to smartphone positions, we recorded accelerometer values for a running user. Fig. 10
                         illustrates accelerometer standard deviation of four running users for hand holding and pants pocket positions.

Unlike the walking context, the raw data for the running context does not reveal any patterns that can be exploited by the smartphone position service to distinguish between the smartphone positions. By looking at Fig. 10, we see that for user no. 1 and 4, the hand holding position has higher standard deviations for all frames when compared to the pants pocket position, whereas, the opposite is true for user no. 2. Also, for user no. 3, the standard deviation does not follow any pattern. We attribute this chaotic behavior to the range of different paces a running context might exhibit. When the user is asked to run, on some occasions the user would run fast, while in others the same user would run relatively slower. Therefore, running context represents a challenging environment for the smartphone position discovery service.

Analogous to an idle user carrying a smartphone, the smartphone at this position will not experience any movement patterns. However, a distinguishing factor between the two situations is the orientation of the smartphone. A smartphone placed on a table will typically have the gravity component appearing in its z-axis. This typical behavior can be exploited to detect with fair accuracy level if the smartphone is placed on a table.

@&#DISCUSSION@&#

It is clear that the sensor values of accelerometer, gyroscope, microphone, magnetometer and light sensor are affected by smartphone position. Thus, context aware applications that depend on one or more of these sensors have the potential to benefit from a smartphone position discovery service. On the other hand, GPS sensor values remain largely unaffected by smartphone positions. Sensor values of accelerometer, gyroscope and magnetometer are affected by the differences in vibrations at different smartphone positions while microphone values are affected by friction noise and light sensor values are affected by whether the phone is covered or uncovered. So, a context aware application that is based on accelerometer, gyroscope and/or magnetometer sensor values is likely to benefit from the knowledge of actual smartphone position such as hand holding, pants pocket, jacket pocket and hip pocket. On the other hand, a context aware application that is based on light sensor values is likely to benefit from the knowledge of whether the phone is covered or uncovered. Finally, a context aware application that is based on microphone sensor values is likely to benefit from the knowledge of whether the phone is in the upper body position (hand holding, jacket pocket, talking on the phone, or watching video) or lower body position (hip pocket or pants pocket).

Based on the observations made in Section 3, we have designed, implemented and evaluated a smartphone position discovery service that provides four types of information: (1) Is the user idle, walking or running? (2) Is the phone covered or uncovered? (3) Is the phone placed in upper body or lower body? (4) What is the actual smartphone position? This service is designed to be configurable, so that an application can choose to receive only one or two or all types of information.

The challenge in building this service is that it utilizes sensor data from specific sensors (e.g. accelerometer and gyroscope), whose values are dependent on the physical contexts of the user. It is possible that the data from a particular sensor under one smartphone position and user activity is indistinguishable from the data from the same sensor under a different smartphone position and user activity. We address this challenge by detecting user’s physical context (idle, walking or running) and utilizing data from multiple sensors. The key idea is that different sensors are affected differently by various user contexts, and we exploit these differences to accurately detect smartphone positions.

To detect whether the smartphone is in covered or uncovered position, the service compares the online captured light intensity data with a predefined threshold. The situation is more complex when it comes to the other finer granularity information. For both the upper-body/lower-body and the exact smartphone position decisions, the service uses machine-learning libraries to compare knowledge obtained from online sensor data with knowledge from labeled training data prepared offline. This classification process involves data from accelerometer or gyroscope, or both sensors based on the preference of the serviced context-aware application. Fig. 11
                         illustrates this design. It is worth noting that the complete solution runs on the smartphone. The smartphone position service can be utilized locally by other applications running on the same smartphone or remotely by collaborative sensing applications running on other smartphones.

There are three offline components: Framing, Feature extraction and Frame labeling. The Framing component aims at capturing the repetitive patterns in the raw sensor data by dividing the data stream, from accelerometer and gyroscope, into five-second frames. Our choice of five-second frame size is based on analysis presented in [17] on the effect of frame size on step detection accuracy. Their analysis revealed that a frame size larger than three seconds is sufficient enough to provide good step detection accuracy and favored the five-second frame as it gives more accurate results.

The features extraction component calculates statistical features for each frame. Frame features must be chosen smartly to reveal the different patterns induced by each smartphone position. Our frame features are subset from the features presented in [18]. Based on our observations from smartphone position impact on sensor data, we have chosen the mean, variance, and standard deviation over 50 data points (10 data points per second) for each frame to capture the variations in accelerometer and gyroscope data. We have also included two other features related to each axis (average for each axis and average absolute difference of each axis) so as to capture the different orientations a smartphone can take for each position. Average of each axis captures the variation in the data due to body motion at the axis level. In addition, it reveals the nature of the orientation the smartphone is experiencing for each body position. Average absolute difference of each axis is the sum of the differences between each axis data point and the mean of that axis divided by the number of data points. We include the average absolute difference to enhance the solution accuracy in capturing the information revealed by axis data points. A recent study [13] identified an extensive set of features for detecting smartphone positions. These features can be integrated to the position discovery service to expand covered set of positions and further enhance the accuracy. Finally, the Frame labeling component labels each frame with the corresponding smartphone position before loading the data to the training database. The labeling process was done manually. We asked our users to capture the data for the different smartphone positions while walking and labeled resultant frame segments with the practiced smartphone position during the experiment. Each frame record carries two labels: upper-body/lower-body position and the exact smartphone position.

There are seven online components: Training data, Machine learning libraries, Sensor values from accelerometer, gyroscope and/or light sensor, Framing, Feature extraction, Position classifier and Upload optimizer. The training data is the output from the offline components. Sensor data from ten users performing the same experiment for different smartphone positions was collected offline. After performing the (offline) Framing and Feature extraction processes, the resultant frame records constitute the knowledge database to be utilized for automatic discovery of smartphone position. We placed the training data on the external memory card of the smartphone to be utilized by the smartphone position discovery service.

We utilized Java language machine-learning libraries provided by the WEKA tool for Android [23]. The correctness of used classifiers was tested by performing a test experiment with the same training and test data on a desktop by normal WEKA and on Android device by WEKA for Android. Same results were obtained for the two experiments.

We chose three sensors (accelerometer, gyroscope and light sensor) for the service to operate on. Accelerometer is used for detecting physical context, accelerometer and/or gyroscope are used for detecting the actual phone position, and light sensor is used for detecting whether the phone is covered or uncovered. The use of two sensors for detecting actual phone position is subject to a trade-off between energy consumption and smartphone position detection accuracy. The Framing and Features extraction components have the same functionalities as in the offline case. The only addition is the capture of average light intensity per frame, which is not required for training the classifier. The position classifier component receives the gathered online frame data and uses it in three ways. First, it compares the standard deviation of accelerometer magnitude with predefined thresholds to determine idle/walking/running contexts. Second, it consults the machine learning classifier to detect smartphone position information. Third, it compares the light intensity average of the frame with a predefined threshold to determine covered/uncovered position. In the case of idle or running contexts, the position service provides the latest smartphone position discovered under walking context along with a timestamp and leave it to the consuming context-aware application to use this cached smartphone position based on its accuracy preferences. Our goal here is to exploit the fact that, in some situations the user might change their physical context but maintain the same smartphone position.

Finally, the upload optimizer is utilized only in case the position is required to be relayed over the network as part of a collaborative sensing solution. We developed this component because we envision the smartphone position discovery service to be an important part of collaborative sensing applications. The upload optimizer logic is based on optimization techniques discussed in [24]. The optimizer implements three alternative techniques for upload optimization: (1) Upload whenever a position change occurs; (2) upload when a position change persists for some period of time; and (3) upload the position with the highest number of occurrences within a window of given size. While the first technique is simple and provides most accurate results, it is subject to noise due to momentary smartphone position changes. The second technique eliminates this noise and reports only more permanent position changes. Finally, the third technique is suitable when there are frequent smartphone position changes. This technique tries to report the most commonly occurring smartphone position.

Assume a context-aware application that is interested in finding the exact position. Fig. 12
                            uses Business Process Model and Notation (BPMN) to demonstrate the flow of execution of the position discovery service. The start event happens when a context-aware application expresses intention to receive the smartphone position. First, the service calculates the variance of the accelerometer for the captured window and derives the physical context of the user based on it. If the smartphone is idle, the service calculates the average x, y and z accelerometer values to check the direction of the gravity. If the gravity appears in the z-direction, then the smartphone is likely to be placed on a table. If not, then the position is difficult to calculate and a cached position is returned. Also, a running context means that position cannot be detected and a cached value is returned. If the user is walking, the service will calculate window features based accelerometer and gyroscope and provide those to a 6-way classifier. The pre-trained classifier will use the features to classify the window into one of the six body positions and return the final result to the calling application.

We have implemented the proposed smartphone position discovery service on Samsung Galaxy Note device running Android version 4.0.3 (Ice Cream Sandwich). The device has a Dual-core 1.4GHz ARM Cortex-A9 processor and 1GB of RAM and is equipped with the accelerometer, gyroscope and light sensors required for the service. We collected data from ten different participants to train the smartphone position discovery classifier. Before conducting the experiment, an approval was obtained from the Institutional Review Board at the University of Colorado, Boulder. We asked each participant to carry the smartphone in the six smartphone positions. The experiment setup was kept as natural as possible. Participants were free to move at their own pace and place their smartphones at any orientation they liked. Next, we evaluate the accuracy of the service based on this collected training data.

To detect the physical context of a user, we calculate the standard deviation of the accelerometer magnitude and compare it to a predefined threshold. By observing the data we have chosen the threshold values of 0.5m/s2 and 5m/s2 to detect idle and running contexts respectively. These thresholds achieved near-perfect accuracy in our experiments.

We used a threshold of three luminous flux to detect a dimmed environment that results often from covered positions. This naive solution was satisfactory in our experiment. However, we believe that a more intelligent solution can be built in which the time of day and user location (i.e. indoor vs. outdoor) can be taken into account to calculate this context.

Smartphone positions covered in this research can be divided into two groups: upper-body group, including hand holding, talking on phone, watching a video, and jacket pocket; and lower-body group, including pants pocket and hip pocket. To detect the group that a smartphone is in, we trained the classifier with accelerometer data from 10 users and carried a 10-folds cross-validation test. The results of the classification process with different classifiers are shown in Table 3
                           . The achieved accuracy using accelerometer is fairly high for the simple logistic regression and J48 classifiers. Therefore, we conclude that the accelerometer is the best candidate to perform this classification task and exclude gyroscope from our analysis.

Both accelerometer and gyroscope have shown sensitivity to smartphone positions. Our goal here is to compare between the two sensors. In the beginning, we conducted a test experiment with the six smartphone positions and collected the data for both the accelerometer and gyroscope. Then, to evaluate a single sensor, we kept the data for that sensor and deleted the data for the other sensor. By doing so, we ensure fair comparison since the three results we show next are basically for the same experiment, but, with different sensors included. Here we also used data from 10 users and performed a 10-folds cross-validation test. The classifiers employed are NB: Naive Bayes, MLP: Multilayer Perceptron, LR: Logistic Regression, and J48. Table 4
                            illustrates the results of smartphone position classification using only accelerometer. We note that the J48 decision tree classifier achieves good accuracy of 88.5% with the accelerometer as the only input. On the other hand, the Naive Bayes classifier had the lowest accuracy of 66.7%. We also note that the source of confusion varies from one classifier to another for the same experiment. For example, in the multilayer perceptron experiment, the jacket-pocket position produced the lowest accuracy. On the other hand, with the logistic regression in use, the pants pocket position was the hardest position to classify.

Next, we evaluate the service when gyroscope is in use. Table 5
                            illustrates the results of smartphone position classification using only gyroscope. We note that all classifiers achieved lower total accuracy when compared to the use of accelerometer. This shows that the gyroscope is less sensitive to smartphone positions than accelerometer. Nevertheless, the accuracy achieved by the gyroscope is still at acceptable levels making the sensor worth considering for some cases. For example, some positions achieved accuracy level of above 80% for some classifiers. However, the overall accuracy remains less than the values achieved when the accelerometer is in use.

Now, we consider the situation where we use both accelerometer and gyroscope to detect the smartphone position. Table 6
                            provides the position discovery results when features from both sensors are used in the classification. As expected, mostly all the classifiers achieved a gain in accuracy when compared to the previous two single-sensor configurations. We also note that three out of the four classifiers achieved very high accuracy levels (above 80%). However, this improved accuracy comes at the cost of increased energy demands. When only accelerometer is employed the service drained 10% from the battery in 10h whereas, using both accelerometer and gyroscope drained more than 50% within the same period of time. Finally, notice that none of the classifiers consistently produced best results in all three cases. However, the Naive Bayes classifiers always produced worst results due to its assumption of feature independence, which does not hold for our set of features.

A smartphone is typically a personal device owned by a single user. Therefore the idea of each user (custom) training his/her position discovery classifier is worthwhile. We experimented with this idea, where a user trained his smartphone by performing the above-mentioned classifier training experiments. Next day, we collected sensor data from the same user and ran our smartphone position discovery service using the custom-trained classifier from the previous day. Table 7
                            shows the accuracy of smartphone position detection when both accelerometer and gyroscope data were used. We can see that the total accuracy for each classification algorithm has improved dramatically (compare the results with Table 6). One point to note is that the user wore similar clothing on both days in this experiment. We expect that the detection accuracy may be slightly lower for different style of clothing. One way to address this is to train the classifier with different clothing styles. The idea of training a classifier on smartphone by the user before application use has been used in [25]. However, the authors tried to keep the training period as minimum as possible as they believed that users might refrain from using applications requiring training beforehand. We share the same concern and believe that the position service can be installed with multiple users training data, which has shown acceptable accuracy levels, and the user is then given a choice for custom training.

Due to its special nature, the on table position is handled separately. We directly use the window features of average x-axis, average y-axis, and average z-axis to detect this position. The z-axis value will be nearly equal to the gravity pull value of 9.8m/s2 whereas the x-axis and y-axis will have the value of near zero. This approach detected this position with nearly perfect accuracy. However, we can think of rare situations that can confuse the approach such as placing the smartphone on a stand.

To analyze the energy demands of the proposed smartphone position discovery service, we measured the battery drain when each of the three configurations is employed. The results are shown in Fig. 13
                           . Note that the screen as well as all background services and applications were turned off to ensure that the reported battery drain corresponds to capturing data from the sensors.

Some context-aware applications are simply blocked by disadvantageous positions (e.g. camera-based application waiting for uncovered position). Other context-aware applications can operate in different smartphone positions, but with severe accuracy degradation when experiencing a smartphone position other than the one trained for. To address this issue, we propose a two-stages classification method. First, the offline training for the classifier is done with different smartphone positions to generate a separate classifier trained for each position. Second, with the presence of the smartphone position discovery service, the application first determines the current smartphone position and then chooses the classifier corresponding to that specific position during the classification process. To demonstrate the effectiveness of this approach, we have implemented a fall classification application [5] that was described in the introduction section. Recall from the results in this section that the application achieved a poor accuracy of 72.22% under arbitrary position and a high accuracy of 94.8% assuming known position as shown in Tables 1 and 2 respectively.

Admittedly, the assumption of a complete knowledge of smartphone position is not a valid one. Therefore, we integrated the fall classification application with the online position discovery service and followed the above-mentioned two-stage classification method. Fig. 14
                        (a) and (b) show the “per-fall” and “overall” accuracies for this case. We also included the results from Tables 1 and 2 to make it easier to grasp the effect of introducing the smartphone position discovery service. Notice from Fig. 14(a) that the accuracies of trips and left lateral falls detection have been improved. For the other two types of falls, introducing the position service to the scene did not improve the results but did not negatively impact them. Fig. 14(b) reflects the overall accuracy improvement. The improvement in the case of “known-position” proves the fact that by introducing the position service, context-aware applications will achieve better results. We also saw an improvement for the case of with-the-position service. However, the improvement was not as significant as in the optimal situation. We noticed that our position service provided the correct position in most situations, but it was the fall classification that is difficult to achieve. We attribute this difficulty to arbitrary after fall behaviors such as standing immediately after fall or remaining stationary.

@&#CONCLUSION@&#

Variation in smartphone carry position significantly impacts sensor readings, thereby negatively affecting the accuracy of context-aware applications that depend on those sensors. To address this challenge, we built a smartphone position detection solution that runs solely on the smartphone.The proposed solution can act as a service provider to context-aware applications running on the same smartphone by providing them with smartphone position information. The service can answer the following four questions. (1) Is the user idle, walking or running: (2) Is the smartphone covered or uncovered? (3) Is the smartphone attached to upper-body or lower-body? (4) What is the actual position of the smartphone? We evaluated the service by integrating it with an existing context-aware application for fall classification. Results show significant accuracy improvement proving the utility of the service.

@&#REFERENCES@&#

