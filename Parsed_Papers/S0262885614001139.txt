@&#MAIN-TITLE@&#Estimating layout of cluttered indoor scenes using trajectory-based priors

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Scene layout estimation using both trajectory data and image features


                        
                        
                           
                           No assumptions are considered about the structure of the scene.


                        
                        
                           
                           Efficient estimation of the scene structure in the presence of scene clutter


                        
                        
                           
                           We show that using line segments we obtain better surface normal.


                        
                        
                           
                           Our data and segmentation results will be publicly available for comparison.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Scene segmentation

Trajectory

Scene layout

Semantic context

Conditional random field

@&#ABSTRACT@&#


               
               
                  Given a surveillance video of a moving person, we present a novel method of estimating layout of a cluttered indoor scene. We propose an idea that trajectories of a moving person can be used to generate features to segment an indoor scene into different areas of interest. We assume a static uncalibrated camera. Using pixel-level color and perspective cues of the scene, each pixel is assigned to a particular class either a sitting place, the ground floor, or the static background areas like walls and ceiling. The pixel-level cues are locally integrated along global topological order of classes, such as sitting objects and background areas are above ground floor into a conditional random field by an ordering constraint. The proposed method yields very accurate segmentation results on challenging real world scenes. We focus on videos with people walking in the scene and show the effectiveness of our approach through quantitative and qualitative results. The proposed estimation method shows better estimation results as compared to the state of the art scene layout estimation methods. We are able to correctly segment 90.3% of background, 89.4% of sitting areas and 74.7% of the ground floor.
               
            

@&#INTRODUCTION@&#

Estimating layout or structure of an indoor scene is important for many tasks, such as activity analysis [1], robot navigation [2,3], scene understanding [4,5] and object placement [6–8]. Specifically for the analysis of elderly activity, scene layout provides a semantic context knowledge that is necessary for long-term observation. With the help of scene context, we can localize a person and monitor his daily behavior. Semantic context also benefits the unusual event prediction, such as fall detection [9,10]. Lying on the sofa has a different interpretation from lying on the floor. With semantic context information usual lying on sofa can be taken as usual activity.

Keeping these important aspects and applications in mind, different mechanisms have been proposed for indoor scene layout estimation. State of the art methods use either spatial image features [11–13] or trajectory-based temporal information [1,5] for this purpose. However, both types of features have their own problems and either of the features cannot be recommended individually to estimate the layout of indoor scene. A major challenge for spatial image-based feature techniques arises from the fact that most indoor scenes are cluttered by a lot of furniture and decorations [14]. They often obscure the geometric structure of the scene, and also occlude boundaries between walls and the floor. Appearances and layouts of clutters can vary drastically across different indoor scenes, so it is extremely difficult (if not impossible) to model them consistently. Similarly trajectory-based techniques normally cluster the trajectory data and model only the paths [5,15]. They do not take care of the clutter or resting places in the scene. The modeling of resting areas [1] has been done using stop points of a trajectory inside a resting place. This mechanism cannot be reliably used in case of noise in the trajectory data. A normal stop outside a resting area might be taken as a resting place.

Trajectory data either can be directly used or it can be used for action recognition. Action information in turn can be used for different purposes like for detecting action consequences and classifying videos of manipulation action according to action consequences [16]. Similarly another approach encodes the essential changes in visual scenery in a condensed way such that a robot can recognize and learn a manipulation without prior object knowledge [17].

Though image features and trajectory data are not self-sufficient for reliable scene layout estimation but they can be used together to achieve reliable indoor scene layout estimation. In this work, we propose a mechanism which learns the scene semantic context model using image segmentation mechanism in an unsupervised way. We do not use trajectories directly for scene layout estimation rather our segmentation mechanism uses both spatial image and trajectory-based features. We are also able to model the resting or sitting places in the scene. An overview of the approach is as follows. We assume a static and uncalibrated surveillance camera in the scene. Given a moving person in the scene; we first model the trajectory of moving person using a set of key-points on his silhouette. We identify or cluster the regions corresponding to feet locations of moving person as floor. Given a potential floor area, we define the relative height of each point relative to the floor. Similarly using lines and trajectories we define the orientation of each point in the scene. We now incorporate height, orientation and color information into a Conditional Random Field (CRF) to define relationship between different points in the scene. Fig. 5
                     
                     
                     
                     
                      gives an overview of the CRF-based image segmentation for unsupervised scene layout estimation procedure. A graphcut-based inference algorithm is run on our CRF to define the final scene segmentation or layout.

The key contribution of this work are as follows:
                           
                              (1)
                              Indoor scene layout estimation using both trajectory data of a moving person and image features. The estimation process is fully automatic and unsupervised. We do not use any training data. No assumptions are considered about the structure of the scene.

Efficient estimation of the scene structure in the presence of scene clutter. We classify scene clutter as either sitting areas or scene background. Modeling resting areas as a separate class improves overall scene layout estimation process.

We show that using line segments instead of voting-based straight lines we can obtain better orientation map or surface normal. Improvement in orientation map improves overall scene layout estimation by providing correct orientations for resting places like sofa and bed.

Experiments are performed on a new dataset of scene videos with moving person along with publicly available videos of the indoor scenes and better segmentation results are achieved. We show using quantitative and qualitative results that by combining trajectory information of moving persons with image attributes, we can obtain an accurate indoor scene layout, superior to geometric methods. We will make the data and segmentation results publicly available for comparison.

The rest of the paper is organized as follows. Related work and contributions of this paper are described in Section 2. The proposed scene layout estimation mechanism is elaborated in Section 3. Image segmentation mechanism used for scene layout estimation is explained in Section 4. The performance of the approaches are evaluated in Section 5. The proposed approach is compared with other approaches in this section followed by a conclusion in Section 6.

@&#RELATED WORK@&#

Layout of indoor scenes has been estimated in literature mainly using single-image segmentation [11–13,18–20]. Such techniques use the image attributes like line segments, geometric context and color to define 3D structure or geometry of the scene. However, recognition of scene structure using only spatial image features is challenging. In [21,12] different features are learned from an image database and then these learned features are used to train a classifier to segment a scene into different layers like ceiling, walls, and clutter. These features like color context are not discriminative enough for different classes. Due to color similarity a part of a wall might be detected as scene clutter or vice versa. Another set of approaches tries finding volumetric structures inside the scene to define different objects in the single images [22,11,23,19]. They also find cubic objects like beds in the scene image. They have high dependencies on straight lines in the scenes. In home environment it is difficult to detect all straights lines on objects due to cluttered scene and occlusions. Such approaches fail to detect objects like bed and sofa if they do not have enough straight lines and cubic constraint is not fulfilled.

Some other techniques use supplementary information to compensate the shortcomings of spatial image features. In the recent years features generated from laser range data [24], or Kinect-based 3D data [25–27] have been used for scene layout estimation. Similarly different areas in a scene are marked as suitable for sitting using 3D data by their ability to support a sitting action [28]. Structure from camera motion has also been used in the layout estimation of the indoor scenes [29].

Tracking information has also been used in literature to couple different actions with certain regions in the scene. Resting areas are modeled as a Gaussian mixture using minimum description length [1]. The image points where a person stopped in the scene are clustered as sitting or resting areas. Simply using tracking information is not sufficient enough for scene layout estimation while a person stopping outside a resting area might be taken as a resting area.

Some other trajectory-based approaches [30,31,23,28] detect different areas in the scene by object interactions. A chair or a sofa for example is detected when someone sits in a particular scene area. Motion or person interactions alone are not reliable enough for scene layout estimation. Motion-based feature like speed is affected by the errors in moving object detection mechanism. Similarly user interaction-based methods are dependent on user detection and posture classification. Any error in these two modules will propagate in scene layout estimation process.

In this work, we build on these efforts and take one step further to jointly segment scene and sitting places. We combine trajectory information of the moving persons along image attributes like color and perspective cues to segment indoor scene into activity areas like ground floor, inactivity areas and sitting places like bed, table, sofa and the remaining image area as background. We assume that sitting places are higher than floor and have orientation similar to floor. As objects like tables can also be used for sitting and poses similar attributes: i.e. they have surface orientation similar to a bed and are higher than floor, we also consider them as sitting or resting places.

In order to estimate layout of a given scene, we use trajectory of coarse body motion as a reliable low level feature. In our case a trajectory T is a sequence of K correspondences
                        
                           (1)
                           
                              T
                              =
                              
                                 
                                    
                                       T
                                       1
                                    
                                    ,
                                    …
                                    ,
                                    
                                       T
                                       t
                                    
                                    ,
                                    ....
                                    
                                       T
                                       K
                                    
                                 
                              
                           
                        
                     where vector Tt
                     
                     =[H(x,y),C(x,y),F(x,y)] compactly represents a person's location in terms of its corresponding key-points at time t. In order to find the key-points in each frame moving persons are segmented from the background scene using a combination of color and gradient-based background subtraction method [32,33]. We then use connected component analysis to find the center of mass C(x,y) and ellipse fitting to define head location H(x,y) [10,34]. Assuming the person in standing posture, feet location F(x,y) is defined by projecting a medial axis from head to the silhouette bottom. Each key-point is a 2D location in the image referred to as point (x,y) in coming text.

The correspondence vectors Tt
                      are monitored over time to record parameters from the movements of the persons. The scene areas through which feet or lower body centroid (in case of person to object occlusion) F pass are marked as floor areas B.


                     Fig. 2 shows the key-points extracted from a video and the unsupervised learning process for floor area B. The learned correspondence vectors Tt
                      are used to generate features that are used in the image segmentation process for scene layout estimation.

For the layout estimation we use three types of information, namely color, height map H and orientation map O. We consider the layout estimation as a segmentation problem and define a Conditional Random Field (CRF) [35] using our three features. We will explain this procedure in Section 4.

Height map describes the relative homogeneous height h of each pixel with respect to the floor B. Highly probable floor pixels B have height zero. In order to define the height for rest of the pixels in an image, we compute vanishing points [vx
                           , vy
                           , vz
                           ] in the scene. We use a combination of trajectory information [36] and lines for this purpose. In order to minimize the effects of noisy trajectories we follow a RANSAC-based method to classify inliers and outliers and in turn to find three orthogonal vanishing points. The vanishing line of the floor plane can then be found by the two vanishing points vx
                            and vy
                           : VL
                           
                           =
                           vx
                           
                           ×
                           vy
                           . The key-point information in the form of head to feet 
                              
                                 HF
                                 ¯
                              
                            and centroid to feet 
                              
                                 CF
                                 ¯
                              
                            correspondences serve as a basis for height computation. For height computation we need the projection of every point p on the floor plane. Thus to define the floor of point p, first a nearest known correspondence is found. Then using nearest correspondence and vanishing line VL
                            a line is projected to the ground plane to define the floor of point p. In order to find the floor for the current point p vanishing line VL
                            can be used. Fig. 1 (a) shows the process of finding the floor of a point. Initially a line connecting the top point T (on the medial axis of a nearest standing posture) and point p is intersected with the vanishing line VL
                           . From the point of intersection v a bottom line can be projected back to the known floor point F. The unknown floor point B is orthogonal to the current point p at the bottom line.

In order to find the homogeneous height of point p, we need a known standing posture Hr
                           , Br
                            as reference and the vanishing line VL
                            
                           [37]. We can then use basic trigonometry to find the height of a point p.
                              
                                 (2)
                                 
                                    
                                       H
                                       R
                                    
                                    =
                                    
                                       
                                          
                                             
                                                p
                                                −
                                                
                                                   B
                                                   r
                                                
                                             
                                          
                                          
                                             
                                                ∞
                                                −
                                                
                                                   H
                                                   r
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   H
                                                   r
                                                
                                                −
                                                
                                                   B
                                                   r
                                                
                                             
                                          
                                          
                                             
                                                ∞
                                                −
                                                
                                                   B
                                                   r
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        


                           Fig. 4 (c) shows the quantized height of each pixel. Floor points with lowest possible height are drawn in black. Pixels with very low height are drawn in red. Pixels that have height less than the average body centroid height are in the probable areas of inactivity zones, and are drawn in green. Pixels higher than body centroids are drawn in yellow, and pixels having height higher than head positions are drawn in white. The pixels whose height cannot be estimated due to lack of neighbors are drawn in blue.

An orientation map defines the major orientation of each pixel with respect to the world using vanishing points. In order to compute an orientation map, the orientation of each line in the image is defined. Orientation of the lines can be used to define the orientation of the surface lying between lines [38]. Orientation of a point is decided by the surface on which it is lying.

Orientation of a line segment can be computed by the vanishing point that lies on the extension of the line segment in the image. Common line detectors [39,40] like Hough transform try to find straight lines in an image using a voting-based strategy. If pixels more than a threshold vote for a line then this line is taken as valid in an image. We want to use the orientation map for modeling resting places. Resting places like a sofa and a bed may not have enough straight lines due to their irregular shape and irregular boundaries. Thus common line detectors fail to find sufficiently many lines. This fact can be seen in Fig. 3 (b) where a voting-based method [40] fails to detect enough lines on the small sofa and the bed, which in turn resulted in wrong orientations in these areas.

It can be observed in Fig. 3 (a) that resting places in the scene may not have enough straight lines instead they have many irregular line segments and curves that can be detected for the orientation map generation. Irregular lines do not follow the basic definition of a straight line and cannot be represented by the linear equation y
                           =
                           mx
                           +
                           b. Thus all the points on an irregular line might not have same slope m. As voting-based techniques follow a fixed slope m, hence they have problems with irregular lines and curves. Secondly, random alignments of pixels might generate wrong lines in cluttered indoor scene. Instead of searching for straight lines, we followed a method [41] to identify line segments in the scene. Rather than voting, pixels are merged to form lines. A curve is detected as a combination of a number of lines.

Let Lo
                           
                           =
                           l
                           
                              o,1, l
                           
                              o,2,…, l
                           
                              o,n
                            be the set of line segments of orientation o, where o ∈[x, y, z] denotes one of the three orientations. Orientation o of a line is determined by a parallelism check of a line with known vanishing points (vx
                           , vy
                           , vz
                           ). Fig. 4 (a) shows the orientations of lines, Lz
                            drawn in red, Ly
                            drawn in green, and Lx
                            drawn in blue while lines with unknown orientations are drawn in yellow.

For the pixel orientation a “sweep” of the lines in an image area toward the corresponding vanishing point is calculated. For example a sweep S(l
                           ,x,i
                            ,vy
                           , α) of a line l
                           ,x,i
                            toward a vanishing point vy
                            by amount α is the set of pixels that is supported by line l
                           ,x,i
                            to be orientation z 
                           [38].


                           Fig. 4 (b) shows orientations Ox
                           , Oy
                            and Oz
                            colored in red, green, and blue. Red color represents the horizontal surfaces with vertical orientation, while green and blue represent vertical surfaces with horizontal orientation.

Different areas in an indoor scene are modeled using joint conditional multi-class segmentation. Initially we segment a scene into homogeneous regions based on color and height information [42]. Using prior knowledge of the potential floor B in the scene and pixel-level information then each homogeneous region is assigned to either ground floor GF, one of the sitting places ST or static background BG. The global topological order of the classes such as resting places are above ground and background is also above ground or at same level as resting places, is locally integrated in to a CRF [35,43] by ordering constraints. Fig. 5 gives an overview of the CRF-based image segmentation for unsupervised scene layout estimation procedure. First column shows the original surveillance scene image, key-point trajectories of the moving persons and lines in the scene image. Second column shows the three features used to define unary class potentials i.e. color priors areas are used to define Gaussian distributions of different classes. Quantized height defines the relative height of each pixel in the image, and surface orientations define the normal orientations at each pixel. Color seed areas for different classes are selected using their known reliable properties. Binary potentials, i.e. ordering and smoothness constraints are added to define homogeneous regions–neighborhood relationship. Graphcut-based inference procedure is used to find the optimal scene segmentation by minimizing energy on CRF.

For a given image with N homogeneous regions, each homogeneous region and its corresponding pixels can take a unique label from a set L, where L
                     =
                     l
                     1,…,lN
                      represents the assignment of potential classes C
                     1,…,CJ
                      to each homogeneous region. The optimum labeling is computed by finding the minimum energy configuration of the CRF. The energy function characterizing the CRF used for the scene segmentation is as follows:
                        
                           (3)
                           
                              
                                 E
                                 I
                              
                              
                                 L
                                 f
                                 Θ
                              
                              =
                              
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    N
                                 
                                 
                                    Φ
                                    
                                       
                                          l
                                          i
                                       
                                       
                                          f
                                          i
                                       
                                       Θ
                                    
                                 
                              
                              +
                              
                                 
                                    ∑
                                    
                                       
                                          a
                                          b
                                       
                                       ∈
                                       B
                                    
                                 
                                 
                                    Ψ
                                    
                                       
                                          l
                                          a
                                       
                                       
                                          l
                                          b
                                       
                                       
                                          f
                                          a
                                       
                                       
                                          f
                                          b
                                       
                                       Θ
                                    
                                 
                              
                           
                        
                     where Φ denotes the set of unary potentials which is defined for all the classes using all the features. Ψ denotes a pairwise term which depends upon the labels of neighboring homogeneous regions a and b. The feature vector fi
                      defines the color, height and orientation of each homogeneous region, i.e. fi
                     
                     ={hi
                     , oi
                     , ci
                     }. The parameter set Θ includes the knowledge of color seed areas of the different classes and relative position of different classes in the scene. The color seed areas define prior colors for different sitting places ST
                     ={ST
                     1,…STM
                     }, floor GF and background BG in the scene. These areas are defined using some basic properties and feature set fi
                     . Sitting places ST are the areas or surfaces in the scene that have vertical orientation and they are higher than floor surface. Floor GF is the area with zero height and vertical orientation, while background BG are the areas with maximum heights and orientations other than vertical. These seed areas are only used to define the color potentials for different classes and do not impact other potentials. They are also not involved in the segmentation process directly.

Unary potentials capture the labeling preference for a single class. Each potential function is defined for all candidate classes Cj
                        . Each potential function has different discriminative criteria for different target classes. We use the following three unary potentials in our CRF model. Height potential encodes the relative quantized height Hi
                         of each homogeneous region in the scene image w.r.t. the ground floor. Ground floor is considered to have zero height, the background class has regions that are higher than any other class. While sitting places or areas are considered to be near floor and have lower height than the background.

The height potential is given by
                           
                              (4)
                              
                                 
                                    Φ
                                    1
                                    H
                                 
                                 
                                    
                                       
                                          l
                                          i
                                       
                                       =
                                       
                                          C
                                          j
                                       
                                       ,
                                       
                                          f
                                          i
                                       
                                       ,
                                       Θ
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             −
                                             logs
                                             
                                                
                                                   
                                                      H
                                                      i
                                                   
                                                   −
                                                   
                                                      H
                                                      min
                                                   
                                                   ,
                                                   
                                                      λ
                                                      gs
                                                   
                                                   ,
                                                   
                                                      k
                                                      gs
                                                      j
                                                   
                                                
                                             
                                             ,
                                          
                                          
                                             
                                                C
                                                j
                                             
                                             =
                                             GF
                                          
                                       
                                       
                                          
                                             −
                                             log
                                             Π
                                             
                                                
                                                   H
                                                   i
                                                
                                                
                                                   λ
                                                   st
                                                   j
                                                
                                                
                                                   H
                                                   min
                                                   ′
                                                
                                                
                                                   H
                                                   max
                                                   ′
                                                
                                                
                                                   k
                                                   st
                                                   j
                                                
                                             
                                             ,
                                          
                                          
                                             
                                                C
                                                j
                                             
                                             =
                                             S
                                             
                                                T
                                                m
                                             
                                          
                                       
                                       
                                          
                                             −
                                             logs
                                             
                                                
                                                   
                                                      H
                                                      i
                                                   
                                                   −
                                                   
                                                      H
                                                      max
                                                   
                                                   ,
                                                   
                                                      λ
                                                      bg
                                                   
                                                   ,
                                                   
                                                      k
                                                      bg
                                                      j
                                                   
                                                
                                             
                                             ,
                                          
                                          
                                             
                                                C
                                                j
                                             
                                             =
                                             BG
                                          
                                       
                                    
                                 
                              
                           
                        where minimum height Hmin
                        , H
                        min′ and maximum height Hmax
                        , H
                        max′ values are decided using known average key-point heights. s(x, λ, k) is one dimensional sigmoid function with width λ and turning point at x
                        =0, scaled to the range k with
                           
                              
                                 s
                                 
                                    x
                                    λ
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          k
                                          max
                                       
                                       −
                                       
                                          k
                                          min
                                       
                                    
                                 
                                 /
                                 
                                    
                                       1
                                       +
                                       exp
                                       
                                          
                                             −
                                             x
                                             /
                                             λ
                                          
                                       
                                    
                                 
                                 +
                                 
                                    k
                                    min
                                 
                                 .
                              
                           
                        
                     

Similarly Π is a gating function for input x that is composed of two opposite sigmoid functions with slope λ
                        
                           
                              (5)
                              
                                 Π
                                 
                                    x
                                    
                                       x
                                       min
                                    
                                    
                                       x
                                       max
                                    
                                    λ
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          k
                                          max
                                       
                                       −
                                       
                                          k
                                          min
                                       
                                    
                                 
                                 ⋅
                                 
                                    
                                       s
                                       
                                          
                                             x
                                             −
                                             
                                                x
                                                min
                                             
                                             ,
                                             λ
                                             ,
                                             0
                                             ,
                                             1
                                          
                                       
                                       −
                                       s
                                       
                                          
                                             x
                                             −
                                             
                                                x
                                                max
                                             
                                             ,
                                             λ
                                             ,
                                             0
                                             ,
                                             1
                                          
                                       
                                    
                                 
                                 +
                                 
                                    k
                                    min
                                 
                                 .
                              
                           
                        
                     

The orientation potential encodes the orientations or normal of homogeneous regions according to the orientation of the surfaces on which they lie. The ground surface GF has vertical orientation Ov
                        . The sitting places ST have both horizontal and vertical parts. Hence they have either horizontal Oh
                         or vertical orientation Ov
                        . The background areas BG should always have one of the two horizontal orientations Oh
                        . The orientation potential is given by
                           
                              (6)
                              
                                 
                                    Φ
                                    2
                                    O
                                 
                                 
                                    
                                       
                                          l
                                          i
                                       
                                       =
                                       
                                          C
                                          j
                                       
                                       ,
                                       
                                          f
                                          i
                                       
                                       ,
                                       Θ
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             −
                                             log
                                             d
                                             
                                                x
                                                
                                                   O
                                                   v
                                                
                                                
                                                   k
                                                   j
                                                
                                             
                                             ,
                                          
                                          
                                             
                                                C
                                                j
                                             
                                             =
                                             GF
                                          
                                       
                                       
                                          
                                             −
                                             log
                                             d
                                             
                                                x
                                                
                                                   O
                                                   v
                                                
                                                
                                                   k
                                                   j
                                                
                                             
                                             ∨
                                             −
                                             log
                                             d
                                             
                                                x
                                                
                                                   O
                                                   h
                                                
                                                
                                                   k
                                                   j
                                                
                                             
                                             ,
                                          
                                          
                                             
                                                C
                                                j
                                             
                                             =
                                             S
                                             
                                                T
                                                m
                                             
                                          
                                       
                                       
                                          
                                             −
                                             log
                                             d
                                             
                                                x
                                                
                                                   O
                                                   h
                                                
                                                
                                                   k
                                                   j
                                                
                                             
                                             ,
                                          
                                          
                                             
                                                C
                                                j
                                             
                                             =
                                             BG
                                          
                                       
                                    
                                 
                              
                           
                        where d is a customized delta function that returns higher potential in case of match and lower potential otherwise
                           
                              
                                 d
                                 
                                    x
                                    o
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          k
                                          max
                                       
                                       −
                                       
                                          k
                                          min
                                       
                                    
                                 
                                 ⋅
                                 δ
                                 
                                    x
                                    o
                                 
                                 +
                                 
                                    k
                                    min
                                 
                                 .
                              
                           
                        
                     

Color potentials are modeled for the color similarity in the potential class areas using modified Gaussian Mixture Models (GMM). We define a separate GMM for each seed area of a class. The parameters for each GMM are defined from color values in the seed area in the scene. The color potential is
                           
                              (7)
                              
                                 
                                    Φ
                                    3
                                    c
                                 
                                 
                                    
                                       
                                          l
                                          i
                                       
                                       =
                                       
                                          C
                                          j
                                       
                                       ,
                                       
                                          f
                                          i
                                       
                                       ,
                                       Θ
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             −
                                             log
                                             gmm
                                             
                                                c
                                                
                                                   Θ
                                                   gs
                                                
                                                
                                                   k
                                                   gs
                                                   j
                                                
                                             
                                             ,
                                          
                                          
                                             
                                                C
                                                j
                                             
                                             =
                                             GF
                                          
                                       
                                       
                                          
                                             −
                                             log
                                             gmm
                                             
                                                c
                                                
                                                   Θ
                                                   st
                                                
                                                
                                                   k
                                                   st
                                                   j
                                                
                                             
                                             ,
                                          
                                          
                                             
                                                C
                                                j
                                             
                                             =
                                             S
                                             
                                                T
                                                m
                                             
                                          
                                       
                                       
                                          
                                             −
                                             log
                                             gmm
                                             
                                                c
                                                
                                                   Θ
                                                   bg
                                                
                                                
                                                   k
                                                   bg
                                                   j
                                                
                                             
                                             ,
                                          
                                          
                                             
                                                C
                                                j
                                             
                                             =
                                             BG
                                          
                                       
                                    
                                 
                              
                           
                        where modified gmm is
                           
                              (8)
                              
                                 gmm
                                 
                                    x
                                    
                                       
                                          Ć
                                          x
                                          N
                                       
                                    
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          k
                                          max
                                       
                                       −
                                       
                                          k
                                          min
                                       
                                    
                                 
                                 ⋅
                                 
                                    
                                       
                                          ∑
                                          
                                             n
                                             =
                                             1
                                          
                                          N
                                       
                                       
                                          max
                                          
                                             
                                                g
                                                
                                                   x
                                                   
                                                      
                                                         Ć
                                                         x
                                                         n
                                                      
                                                   
                                                   k
                                                
                                             
                                          
                                       
                                    
                                 
                                 +
                                 
                                    k
                                    min
                                 
                              
                           
                        
                        g is a bell-shaped, zero-mean, multi-dimensional Gaussian function with covariance matrix Ć
                        
                           x
                        , defined as
                           
                              
                                 g
                                 
                                    x
                                    
                                       
                                          Ć
                                          x
                                       
                                    
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          k
                                          max
                                       
                                       −
                                       
                                          k
                                          min
                                       
                                    
                                 
                                 ⋅
                                 exp
                                 
                                    
                                       −
                                       1
                                       /
                                       2
                                       
                                          x
                                          T
                                       
                                       
                                          
                                             Ć
                                             x
                                             
                                                −
                                                1
                                             
                                          
                                       
                                       x
                                    
                                 
                                 +
                                 
                                    k
                                    min
                                 
                              
                           
                        where c is a mean RGB vector and x is the distance of vector c from mean vector of a particular Gaussian.

Binary or pairwise potentials Ψ define the preferences for labels over two neighboring homogeneous regions. Two neighboring regions with the same class should be assigned high likelihood τ
                        1 otherwise low likelihood τ
                        2. We also integrate the relative hierarchy of different classes within the binary potentials.

For two neighboring homogeneous regions on rows va
                         and vb
                         where va
                        
                        ≤
                        vb
                         the binary terms are given as follows:
                           
                              (9)
                              
                                 Ψ
                                 
                                    
                                       
                                          l
                                          a
                                       
                                       =
                                       
                                          C
                                          ja
                                       
                                       ,
                                       
                                          l
                                          b
                                       
                                       =
                                       
                                          C
                                          jb
                                       
                                       ,
                                       
                                          f
                                          a
                                       
                                       ,
                                       
                                          f
                                          b
                                       
                                       ,
                                       Θ
                                    
                                 
                                 =
                                 −
                                 log
                                 
                                    
                                       
                                          
                                             
                                                τ
                                                1
                                             
                                             ,
                                          
                                          
                                             
                                                j
                                                a
                                             
                                             =
                                             
                                                j
                                                b
                                             
                                          
                                       
                                       
                                          
                                             
                                                τ
                                                2
                                             
                                             ,
                                          
                                          
                                             
                                                j
                                                a
                                             
                                             ≠
                                             
                                                j
                                                b
                                             
                                             ∧
                                             
                                                
                                                   
                                                      j
                                                      a
                                                   
                                                   ≺
                                                   
                                                      j
                                                      b
                                                   
                                                   ∨
                                                   
                                                      
                                                         
                                                            v
                                                            a
                                                         
                                                         =
                                                         
                                                            v
                                                            b
                                                         
                                                         ∧
                                                         
                                                            o
                                                            a
                                                         
                                                         =
                                                         
                                                            o
                                                            h
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                τ
                                                3
                                             
                                             ,
                                          
                                          
                                             
                                                j
                                                a
                                             
                                             ≠
                                             
                                                j
                                                b
                                             
                                             ∧
                                             
                                                j
                                                a
                                             
                                             ⊀
                                             
                                                j
                                                b
                                             
                                             ∧
                                             
                                                v
                                                a
                                             
                                             <
                                             
                                                v
                                                b
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where j represents the relative ordering of two homogeneous regions. If we assume that the rows in image increase downward and the row above va
                         has orientation Oh
                         then we assign higher potential τ
                        2, because BG has only horizontal orientation and it should be above all.

The energy function E(L, f, Θ) defined using CRF is solved using a graphcut-based inference method. The inference method tries to find the optimal solution where total energy using potentials is at minimum for labeling [44]. It is assumed that all the individual nodes in graph share the same state (label) space. For the pairwise potentials, it is achieved that the energy for two labels taking similar values should be less than the energy for them taking different values. At pixel-level either 4 or 8 neighborhood can be considered to define pair-wise relationships. This predefined neighborhood does not exist in the graph for homogeneous regions, as number of neighbors varies for each homogeneous region. Hence we define pair-wise relationships for all neighbors of a homogeneous region.

We compared the proposed scene layout estimation method with state of the art methods [11,12] on several scenes. First we compare our layout estimation results with techniques that estimate indoor scene layout using single-image segmentation [11,12]. Later we shall show that the layout estimation results can be improved using depth information. Single-image segmentation techniques use the line segments to define 3D structure or geometry of the scene. We used the original softwares publicly provided by the authors on their websites [11,12]. Software from Lee et al. [11] generates only the bounding box for the room layout. The area behind the blue lines is the background and the area in front of blue lines is the floor. The resting areas are marked as cubes.

Similarly software from Hedau et al. [12] generates the room layout estimate where each wall and roof has a different color. As we are interested in the background area as one class, thus we have given all background areas like walls and roof a uniform blue color. In order to evaluate the layout estimation, we captured five indoor scene videos using standard video surveillance network cameras for our experiments. A person walked across the scene for about 2 to 5min. In Fig. 2 we have also shown that we can even use a surveillance video with multiple persons traversing the scene for learning purposes. Persons can perform any actions in the scene, but in this particular case we should perform tracking and action classification [30]. Additionally we used three video sequences from publicly available standard datasets [45]. We assume a perspective camera view of the scene with minimum possible tilt. For pixel-wise comparison of estimation results, we have generated the ground-truth for all the scenes. We marked spatial layout of the room along the resting places. All the background areas are colored as blue, all resting places as green and floor as black.


                     Figs. 7 and 9
                     
                     
                     
                      show the original images and layout estimation results for qualitative evaluation of these scenes. Our dataset represents different possible indoor scenarios like living room, bed room, office, working computer lab, and dining area. The videos in dataset are in different resolutions ranging from 352×288 to 1024×768. The scene images have been captured in varying lighting conditions like doors and windows open or closed, and artificial light on or off.

CRF-based segmentation normally depends on different parameters. We performed an offline parameter optimization using particle swarm optimization (PSO) [46]. In order to find the best parameter set, we tried the optimization mechanism indifferent combinations. Similar to cross-validation mechanism, we optimized the parameters on one scene data and then tested these parameters on the other N-1 scenes. This mechanism is repeated N times. Optimizing all the parameters together or in parts have given similar segmentation results and values. Optimizing all the parameters together took relatively more iterations.

The CRF parameters are not sensitive and follow the intuitive rules for different potentials. They can take a range of values between 0 and 1, while still giving the similar segmentation results. We use the following minimum and maximum unary potential levels k
                     
                        gs,bg,st
                     
                     
                        j
                     
                     =[0.1,0.9]. To accommodate the vertical parts in sitting areas in case of horizontal orientation we use k
                     
                        st
                     
                     
                        O
                     
                     =[0.1,0.5] in all our experiments. For binary terms we used the values τ
                     1
                     =0.85, τ
                     2
                     =0.15 and τ
                     3
                     =0.001. Similarly, λbg
                     
                     >0 λgs
                     
                     <0 and λst
                     
                     >0.

In first set of experiments we analyzed the role of different unary and binary potentials. Fig. 6 illustrates the importance of different potentials introduced in our approach. Our trajectory-based height feature plays an important role. It not only improves the scene layout but also benefits in the detection of resting places. Fig. 6 shows the segmentation results for an indoor home scene for different configurations. Original scene image is shown in Fig. 6 (a). The manually labeled ground truth image is shown in Fig. 6 (b). Background BG is drawn in blue, sitting places ST are drawn in green, while floor is drawn in black. Final scene segmentation using all of the unary and binary potentials for homogeneous regions is illustrated in Fig. 6 (c). It can be observed that proposed mechanism correctly identified all the inactivity zones in the scene. Some false segmentations also appear on walls that cannot be avoided at homogeneous regions level and shall be removed at object-level processing. In some areas vertical wall behind a sitting place got merged with the inactivity. Such vertical areas do not harm the objective of inactivity zones because they just extend the wall support area of an inactivity zone. In Fig. 6(d–f) either a unary or a binary potential is removed to judge its effect on the segmentation process. In the absence of orientation or color information background areas like wall are wrongly detected as inactivity zone. In the absence of height major parts of inactivity zones got detected as either background or floor. Fig. 6 (g) shows the segmentation using only the unary potentials. A number of wrong areas appear within the actual classes. Fig. 6 (e–f) shows the segmentations using pixel-based unary and binary potentials or only unary potentials. Segmentation at pixel-level cannot detect some resting areas properly. A large number of pixels belonging to resting areas are wrongly detected as either background or ground floor.

Along internal comparison we also compare our results qualitatively with state of the art methods. Figs. 7 and 9 show the scene layout estimation results for different indoor scenes using the proposed and reference methods. First row in Fig. 7 shows the original scene images from video sequences LivingLab, StudentLab and LivingRoom, while the first row in Fig. 9 shows the original scene images from video sequences SurfaceClean, Pace and OfficeW. The second row in both Figures shows the corresponding ground truths. The third rows in both Figures show the scene layouts estimated using the proposed method. The third rows show the scene layout estimation results using [11], while the last row in these Figures shows the results using [12]. The color represents the most probable class at a specific homogeneous region. Blue represents background, black represents floor and green represents the sitting places or scene clutter. In results from Lee et al. [11], background and floor areas are enclosed in a blue-colored box. Two upper portions in this box represent left and right walls, while lower portion represents the floor area. Sitting places are enclosed in cubic shapes. The method mainly uses the line segments in the scene to define a geometry or 3D structure of the scene and assume that clutter in the scene follows a volumetric constraint. It finds the room structure in the form of walls and ceiling well but it is unable to detect the majority of the sitting areas because they do not obey a volumetric constraint due to the absence of straight lines. Different rest areas like sofa and bed might have curvy surface and lack straight lines. The third column shows the scene layout estimation results using [12]. They use a training-based method and learn different features for different scene areas from a set of scene images. A large number of pixels belonging to sitting areas are wrongly detected as either background or ground floor due to their similarity in training data for wrong class.

In order to compare the proposed method quantitatively we have generated confusion matrices and pixel-wise accuracy graphs. Confusion matrix is used to analyze the percentage of correctly and wrongly classified pixels using standard parameter sets for the estimation methods. Pixel-wise accuracy graphs are used to analyze the accuracy of all the classes in a scene image while changing a particular parameter of the estimation method.


                           Table 1
                            shows a confusion matrix for the mean and standard deviation of segmentation results for different scenarios using the training-based mechanism proposed by Hedau et al. [12]. We compared the layout estimation result image with its ground-truth for each scene. The mean and standard deviation values are calculated using pixel-level comparison values for all the scenes. Both the ground floor GF and sitting places ST show lower number of true positives. This is mainly due to the similarity of features for different classes in the training data.


                           Table 2
                            shows a confusion matrix for the mean and standard deviation of segmentation results for different scenarios using the proposed method. As compared to reference method, we achieve better segmentation or estimation for background BG and sitting places ST. This is mainly due to the introduction of height feature in the segmentation process. Height classifies the different classes like resting places, floor and background even if they have very similar color and orientation. The ground floor GF shows lower number of true positives. This is mainly due to the unavailability of trajectory information in some floor regions.

We perform a second quantitative evaluation for scene layout using pixel-wise accuracy comparison between the segmentation results and the ground truth [47]. Pixel-wise accuracy represents the sum of accuracy for all classes in a scene image.
                              
                                 (10)
                                 
                                    Acc
                                    =
                                    
                                       
                                          T
                                          
                                             P
                                             BG
                                          
                                          +
                                          T
                                          
                                             P
                                             GF
                                          
                                          +
                                          T
                                          
                                             P
                                             ST
                                          
                                       
                                       N
                                    
                                    .
                                 
                              
                           
                        

This accuracy term is calculated for different values of a parameter. In this experiment, the parameter is basically number of homogeneous regions that are used as an initial input for final scene segmentation.


                           Fig. 8 shows pixel-wise accuracy curves for different indoor scenes using proposed and a reference method [12]. Different accuracy values are derived by changing number of homogeneous regions in the scene image. As both proposed and reference method [12] use homogeneous regions as an initial input to perform layout estimation. Hence, we used different number of homogeneous regions as a parameter to compare the performance of two methods. First row shows the graphs for OfficeD and SurfaceClean. The second row shows the curves for LivingLab and LivingRoom sequence. While the third row shows the graphs for StudentLab and OfficeW.

The proposed method shows better results for all the scenes except for the sequence OfficeD. For this particular sequence reference method achieves much better performance when number of homogeneous regions is low. This is mainly due to less amount of trajectory data available for this scene. Inaccurate height map is generated due to less trajectory information. As a result height information cannot improve homogeneous regions and image segmentation process. Reference method [12] also showed better performance when size of homogeneous regions was too large. This fact can be seen in curves for OfficeD, OfficeW and SurfaceClean. For too large homogeneous regions the reference method performed better due to better similarity in their training data. As we use neighborhood relationships for homogeneous regions in our segmentation approach, thus we get better results when number of homogeneous regions is higher and they are smaller in size. Similarly, feature values are uniform when we have smaller size of homogeneous regions.

The scene layout estimations by the proposed method can be further improved if we use RGB-D sensors. Depth information from the sensor not only improves the homogeneous regions but also depth produces better orientation maps. These improved features are then used in the CRF-based segmentation process to produce better scene layouts. Commonly used RGB-D sensors like Kinect do not deliver a complete depth map. Kinect particularly faces problems in the dark and shining areas in the scene. Similarly, depth for the areas farther than 4m might not be accurate. Fig. 10
                         (b) shows that we are unable to get any depth information in different areas in StudentLab scene. In order to correct the depth information we used two mechanisms in all scenes. Kinect delivers slightly varying depth at different times. We performed temporal averaging to improve depth information from multiple images. Then we performed an interpolation or in-painting step using cross-bilateral filter [48] to fill the rest of missing depth information from neighboring pixels. Fig. 10 (c) shows the inpainted depth for OfficeD (office from door) scene. The missing depth has been filled while maintaining the boundaries between different objects. In some severe cases depth inpainting mechanism fails to recover missing depth at object boundaries correctly (see Fig. 10(c)). This may happen when major part of an object is missing in the original depth map.

We used depth information from RGB-D sensor to improve the quality of homogeneous regions. Homogeneous regions are used as initial input in our CRF-based segmentation algorithm. Errors in homogeneous regions propagate throughout segmentation process and result in inaccurate scene layout. Fig. 12
                        
                         shows homogeneous regions for OfficeD scene image. Color information is not always enough to generate good homogeneous regions. Using only color information results in homogeneous regions that contain multiple regions wrongly combined together due to color similarity. This can be observed in chair areas in the scene shown in Fig. 12 (b). Part of one chair is wrongly combined with the floor, while in case of other chair a part is combined with the background. Fig. 12 (c) shows that using depth information we can solve these problems as chair and their nearby floor or background area are at different depths and lie at different distance from the camera.

We also used depth and depth-based orientation map in our CRF-based scene layout estimation mechanism. Fig. 13
                         (b) shows the orientation map generated using RGB-D information from Kinect sensor for OfficeD scene. Orientation map is based on the surface normal and defines the surface orientation for each point using its neighboring points [49] in the point cloud. The surface normal is defined by the eigenvectors (Principal Component Analysis) of the covariance matrix created from the nearest neighbors of a point. Orientation map being generated from inpainted depth might contain the areas where no single orientation is dominant. We refined the orientation map using homogeneous regions generated on the basis of color and depth information. We selected the dominant orientation for each homogeneous area.

In CRF-based segmentation mechanism depth information cannot be used as a unary feature. Depth information is not a discriminatory feature for different scene classes. Different classes might have same depth in different areas of a scene. Floor near to camera might have similar depth as a chair near to the camera. Though depth is not a discriminatory feature in the whole image but it is a discriminatory feature in a local neighborhood in the scene. A chair should have different depth from its nearby floor and background. Keeping this point in view we integrated depth only as a binary or pairwise potential. We restricted the areas to have higher binary potential value only if they have similar depth. Depth-based orientation map is used as replacement for the line-based orientation in unary potentials.


                        Fig. 11 shows the scene layout estimation results for different indoor scenes with RGB and RGB-D-based features using proposed method. Depth information clearly improves the boundaries between different scene classes. Sitting areas are well separated from the background and floor. First row shows the scene layout results for OfficeD scene. Without depth the parts of chair are segmented as floor due to strong color similarity with the nearby floor, similarly a number of background areas are merged with the sitting places to the similar height and color. These problems have been removed or minimized using depth information. The second and fourth rows show the scene layout results for OfficeW (office from window) and LivingRoom scenes, a part of floor and background have been wrongly detected as sitting area in RGB only results. This problem has been reduced using the depth-based features. Third row shows scene layout results for StudentLab scene. Depth-based features do not show any improvement in estimation results. This is mainly due to the missing or wrong depth information from the Kinect sensor. Fig. 10 (c) shows that even interpolation cannot recover the missing depth information in some scene areas completely. We are unable to get any depth information in the chair area while it has some darker part. Boundaries in the chair are mixed with both floor and table in the unpainted depth image. Fig. 14
                         shows pixel-wise accuracy for different indoor scenes by changing the number of homogeneous regions using proposed method with and without depth information. Note that different numbers of homogeneous regions were output with and without depth information. We plotted the graphs where similar homogeneous regions were available. As we skip a number of accuracy values from Fig. 8, hence plots for the proposed method may change in Fig. 14. First row shows the graphs for OfficeD and LivingRoom sequence. While the second row shows the graphs for StudentLab and OfficeW. It can be observed that introducing depth resulted in better pixel-wise accuracy. Especially depth improved performance when we have low number of homogeneous regions. Different objects or areas merge when less number of homogeneous regions are output. Errors in homogeneous regions are propagated in scene segmentation process. We can avoid or minimize this problem by using height and by maintaining the number of homogeneous regions high when no depth information is available. This fact is also evident from the graphs. Curves for higher of number of homogeneous regions are comparative. We can achieve approximate scene layout estimate without depth, but for better accuracy we can also include depth information. Depth improves pixel-wise accuracy from 1 to 10%. Note that in the case of StudentLab sequence depth does not bring any advantage, while input depth image is highly erroneous. It shows that segmentation results are very much dependent on quality of depth information. Missing or wrongly interpolated depth information may not result in improvement rather it might lower the segmentation quality.

@&#CONCLUSION@&#

In this paper, we present an algorithm using the trajectories and image features to estimate the layout of indoor scenes captured with a static and uncalibrated 2-D surveillance camera. We develop a relationship between the moving person and the scene layout. By incorporating trajectory information along line segments into the same scene segmentation framework we show that we can obtain a more accurate estimate of scene layout. The proposed method yields very accurate segmentation results on challenging real world scenes. We focus on videos with people walking in the scene and show the effectiveness of our approach through quantitative and qualitative results. We are able to correctly segment 90% of background, 89% of sitting areas and 75% of the floor(2). The ground floor shows lower true positive. This is mainly due to unavailability of information in some floor regions. Publicly available software from [12] was able to segment 83% of background, 75% of sitting areas and 75% of the floor. The publicly available software from [11] finds the room structure well but is unable to detect the majority of the sitting areas because they do not obey a volumetric constraint due to the absence of straight lines. The scene layout information will be extremely helpful for activity analysis, navigation and other applications.

@&#REFERENCES@&#

