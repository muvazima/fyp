@&#MAIN-TITLE@&#Upright orientation of 3D shapes with Convolutional Networks

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present a data-driven method for 3D object upright orientation estimation using 3D Convolutional Networks.


                        
                        
                           
                           General objects, including asymmetric ones, can be handled by this approach thanks to the learning ability of ConvNets.


                        
                        
                           
                           The proposed method is at least 30 times faster than existing methods.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Upright orientation

Data-driven shape analysis

Voxelization

Convolutional Networks

@&#ABSTRACT@&#


               
               
                  Posing objects in their upright orientations is the very first step of 3D shape analysis. However, 3D models in existing repositories may be far from their right orientations due to various reasons. In this paper, we present a data-driven method for 3D object upright orientation estimation using 3D Convolutional Networks (ConvNets), and the method is designed in the style of divide-and-conquer due to the interference effect. Thanks to the public big 3D datasets and the feature learning ability of ConvNets, our method can handle not only man-made objects but also natural ones. Besides, without any regularity assumptions, our method can deal with asymmetric and several other failure cases of existing approaches. Furthermore, a distance based clustering technique is proposed to reduce the memory cost and a test-time augmentation procedure is used to improve the accuracy. Its efficiency and effectiveness are demonstrated in the experimental results.
               
            

@&#INTRODUCTION@&#

Most objects are usually posed in their upright orientations, which makes them easily recognizable. Also, it is the very first step to pose the given 3D shapes in their upright orientations (Fig. 1) in many graphics and robotics tasks, such as matching [2], retrieval [13,28], shape analysis [34] and placement planning [17]. Moreover, it can be used to generate recognizable object thumbnails, helping the management of 3D shape repositories. Due to various reasons such as modeling platforms or scanning systems, many models in existing databases are not in their upright orientation. Therefore, a number of approaches have been proposed to handle this problem. However, these methods are usually limited to shapes with some regularity and take several seconds to process each shape. Thus more efficient and effective methods are needed.

In this paper, we present a learning based method to predict the upright orientation using 3D Convolutional Networks (ConvNets). Given voxel representations of 3D shapes and corresponding orientation vectors, this prediction task can be formulated as a regression problem. Leveraging the learning ability of deep neural networks, general categories of 3D shapes can be handled without making any assumptions such as symmetry or parallelism. Besides mesh models, the proposed method can deal with shapes represented in other types that can be voxelized, such as implicit surfaces and point clouds, without surface reconstruction [7].

Compared with the ConvNets based approach, existing methods are limited by their predefined rules. For example, the method proposed by Fu et al. [8] is based on the observation that man-made object should have a supporting base on which it can be steadily positioned. Nevertheless, this observation is not applicable to all shapes, especially natural ones. Thus learning based methods are appreciated to deal with general objects. Although the idea of data-driven is adopted in Fu et al. [8], the learning procedure is based on the hand-crafted features such as stability, visibility and parallelism, which fall into the field of feature engineering. In one word, it is hard to define a universal rule to upright general 3D shapes effectively. By contrast, neural networks work in the style of end-to-end learning. High-level knowledges can be captured from raw data, without relying on object’s regularity such as explicit symmetry.

However, a single ConvNet does not work well for all types of shapes. The key challenge is that each shape category exhibits particular characteristic on the upright orientation, for example, cars tend to be horizontal while bicycles are likely to be vertical. This is referred to as interference effect [14] which will lead to poor generalization. In other words, different strategies should be taken to handle diverse categories. Thus a divide-and-conquer scheme is used in our system. Each shape is first classified by a network and then fed into one of the orientation regression networks that are trained on each of the categories. Furthermore, a distance based clustering method is proposed to reduce the number of networks and a novel test-time augmentation procedure is used to improve the accuracy.

The efficiency and effectiveness of this approach are demonstrated by extensive experiments. Our system achieved the accuracy of more than 90% on the test data and showed the generalization capability of inferring upright orientations for shapes not belonging to the training categories. Also experimental results showed that our system is able to handle several cases that other methods fail. Moreover, estimation for each shape took no more than 0.15 s on average, which is much faster than existing approaches, thus applicable to robotics tasks in which immediate feedback is required.

The main contributions of our approach are summarized in the following.
                        
                           •
                           General objects can be handled by this approach thanks to the learning ability of ConvNets, including asymmetric shapes.

The proposed method is at least 30 times faster than existing methods.

The remainder of this paper is structured as follows. Section 2 briefly reviews several related works. In Section 3 our network system is specified. The experimental results and comparisons with related works are demonstrated in Section 4. Finally, Section 5 presents our conclusions and directions of future work to improve our method.

@&#RELATED WORK@&#


                     Orientation of images. Images may differ from their correct orientations by 0°, 90°, 180°, or 270° [3,23,24,32]. Therefore, the image orientation detection problem can be formulated as a four-class classification problem. Most of the existing approaches extract high dimensional feature vector in each possible orientation and then train support vector machines (SVM) [23,32] or other classifiers [3] on feature vectors to detect correct orientation. However, it is difficult to reduce the two-dimensional orientation space to a few candidates for general 3D objects. Thus we formulate the upright orientation estimation of 3D models as a regression problem.


                     Upright orientation of 3D models. In computer graphics, several methods have been proposed to estimate upright orientation or align the given models. One commonly used method is the principal component analysis (PCA) [19] which is inaccurate and not robust for many models, especially asymmetric ones. In Fu et al. [8] and Lin and Tai [22], upright orientation is estimated using supporting base candidates on which a 3D model can stand upright. These methods work well for most of the man-made models while not applicable to natural objects whose supporting bases are not well defined. Another type of method is based on the observation that the coordinate matrix of the 3D object with upright orientation should have reduced rank. Inspired by [37], Jin et al. [18] present an algorithm in which a 3D shape is aligned with axes by iterative rectification of axis-aligned projections as low-rank matrices independently. In Wang et al. [31], a method is proposed by minimizing the tensor rank of the 3D shape’s voxel representation. Both methods can handle shapes that have some kinds of symmetries. We can see that none of the above methods is able to deal with general objects.


                     Viewpoint selection. Representative viewpoint provides the most informative and intuitive view of a 3D shape, which benefits many geometry processing applications like shape retrieval. Most approaches select representative viewpoints using geometric information of the 3D models, such as number of visible polygons [25] and silhouette contours [1]. Some works are based on information theory, such as viewpoint entropy [29], multi-scale entropy [30], and viewpoint mutual information [6]. It will be much easier to select the representative views for 3D models if they are posed at the upright orientation by our method.


                     3D shape matching, retrieval and registration. 3D shape retrieval [13,28] and matching [2] techniques attempt to find the similar shapes from databases with queries. 3D shape registration techniques [36] make efforts to find corresponding parts of multiple models. These methods are trying to design a robust and efficient method for measuring the similarity between two shapes or parts over the space of all transformations [19]. To address this issue, most techniques pre-align the models into a common coordinate frame, typically using PCA alignment. Since our orientation estimation approach predicts a consistent upright orientation for models, it is able to reduce the orientation alignment problem from two to one degree of freedom.


                     Deep neural networks. For computer vision tasks, deep neural networks, especially convolutional networks, have demonstrated excellent performance, by taking 2D images (RGB or RGBD) as input [9,20]. It is only very recent that a few works attempt to tackle 3D shapes related problems via deep learning methods, such as classification, recognition and retrieval. However, most of the works treat 3D shape as a series of multi-view color/depth images [5,27,38], discarding the 3D relationship between different frames. To the best of our knowledge, Wu et al. [33] is the first paper that take volumetric data as input of neural networks, which propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network (CDBN), obtaining good results on shape classification. Another type of 3D Convolutional Network is proposed by [15] for human action recognition in videos, treating time as the third dimension.

Taking n classes of 3D shapes 
                           
                              
                                 C
                                 i
                              
                              
                              
                                 (
                                 i
                                 =
                                 1
                                 ,
                                 ⋯
                                 ,
                                 n
                                 )
                              
                           
                         into account, the problem of upright orientation estimation is formulated as a regression task. Given a quantity of voxel representations V of 3D shapes and corresponding unit vectors u of upright orientation, a function u ≈ f
                        
                           
                              β
                           
                        (V) with unknown parameters 
                           β
                         should be estimated to fit the data.

3D ConvNets can be straightforwardly applied onto this problem. However, due to different shape categories exhibit particular characteristics on their upright orientations, strong interference effects occur that lead to poor generalization [14]. It is difficult to train a universal network which works well for all the n shape categories. Therefore, this task should be accomplished in the style of divide-and-conquer, namely, training different networks on different shape categories. Naturally, n regression networks can be trained separately. Moreover, a classification network should be trained to work as a gate by predicting which regression network should be applied onto the input shape. Fig. 2
                         shows the test stage of the system.

We use the standard architecture of ConvNets for regression and classification.

The regression network takes voxel representations of 3D shapes as input, and the 3D vectors of predicted upright orientation as the output. As illustrated in Fig. 3
                        , the regression network consists of a number of 3D convolution layers and fully-connected layers, each of them is followed by a layer of activation units. The hyperbolic tangent tanh (·) is chosen as the activation function in the output layer. To avoid slow learning when output values are close to 1 and 
                           
                              −
                              1
                              ,
                           
                         the orientation vectors u are rescaled by 0.5. In the other layers, we choose rectifier [21]
                        
                           
                              
                                 
                                    
                                       ReLU
                                    
                                    (
                                    x
                                    )
                                    =
                                    max
                                    (
                                    0
                                    ,
                                    x
                                    )
                                 
                              
                           
                        as the activation function. In the end, this network is trained to minimize the Euclidean loss function, using mini-batch gradient descent with batch size N.
                           
                              
                                 
                                    
                                       Loss
                                    
                                    =
                                    
                                       1
                                       
                                          2
                                          N
                                       
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       
                                          ∥
                                          
                                             
                                                u
                                                ^
                                             
                                             i
                                          
                                          −
                                          
                                             u
                                             i
                                          
                                          ∥
                                       
                                       2
                                       2
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              
                                 u
                                 ^
                              
                              i
                           
                         is the ground-truth three dimensional orientation vector and u
                        
                           i
                         is the corresponding regression value.

The classification network shares a similar architecture with the regression networks. The differences are that the output of the last fully-connected layer is fed to a n-way softmax which produces a distribution over the n class labels, and a multinomial logistic loss layer is used.

Although different shape categories exhibit particular characteristics, some categories, such as chair and table, may be handled by similar strategy to find their upright orientations. Those categories can be clustered together and processed with the same regression network. As a result, redundant networks can be removed and then the memory cost can be reduced.

However, it is nontrivial to determine which categories are consistent and which are not. If a network is trained on inconsistent categories, the sacrifice of accuracy would be dramatic compared to the networks trained on each category separately. We propose a clustering strategy based on the distance measure defined by the error rate of each regression network on the other shape categories.

To define the distance measure, we first evaluate the n regression networks on all of the n categories to get a square matrix E in which E(i, j) is the error (i.e., 
                           
                              ∠
                              (
                              u
                              ,
                              
                                 u
                                 ^
                              
                              )
                           
                         is larger than some threshold) rate of regression network 
                           
                              R
                              i
                           
                         on shape category 
                           
                              C
                              j
                           
                        . Then we get the symmetric matrix 
                           
                              D
                              =
                              (
                              E
                              +
                              
                                 E
                                 T
                              
                              )
                              /
                              2
                           
                         in which D(i, j) measures the distance between shape categories 
                           
                              C
                              i
                           
                         and 
                           
                              C
                              j
                           
                        . The shorter between two object categories, the more likely their upright orientations can be estimated with the same network.

Once we obtain the distance matrix, a hierarchical agglomerative clustering algorithm [11] is performed, after which a cluster tree is constructed. Then, we should determine where to cut the hierarchical tree into a number of clusters. At last, new regression networks should be trained on agglomerated shape category clusters while those nets for categories left in their own cluster can be kept. Also, the classification network do not need to be retrained.

For classification tasks, test-time augmentation (TTA) has been proposed to improve the accuracy by taking average of the output from many virtual samples in [4]. From the points of prediction error in Fig. 5
                        , we find some outliers from the results of shape’s different input poses. Therefore it would be helpful to take test-time augmentation and average the results in someway robust to outliers, such as taking their median, i.e., 1-norm average.

Given a test shape 
                           
                              S
                              ,
                           
                         we augment it by transforming with randomly generated rotation matrices 
                           
                              
                                 R
                                 i
                              
                              
                              
                                 (
                                 i
                                 =
                                 1
                                 ,
                                 ⋯
                                 ,
                                 m
                                 )
                              
                           
                        . Then m correspondence voxel representations V
                        
                           i
                         are fed into the network system. We classify them into the same class by majority voting of m predicted labels and put them into the same regression network. After getting m regression predictions 
                           
                              
                                 
                                    u
                                    ˜
                                 
                                 i
                              
                              ,
                           
                         we map them back into the coordinate frame of 
                           S
                         as 
                           
                              
                                 u
                                 i
                              
                              =
                              
                                 R
                                 i
                                 
                                    −
                                    1
                                 
                              
                              
                                 
                                    u
                                    ˜
                                 
                                 i
                              
                           
                        . By minimizing the objective function defined below,
                           
                              
                                 
                                    
                                       u
                                       *
                                    
                                    =
                                    
                                       argmin
                                       
                                          ∥
                                          u
                                          ∥
                                          =
                                          1
                                       
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    ∠
                                    
                                       (
                                       u
                                       ,
                                       
                                          u
                                          i
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        a better prediction u
                        * is expected to be obtained. In existing works, Weiszfeld algorithm [10] was proposed to solve this optimization problem in an iterative manner. However, we replace it with the following reduced version, which is much simpler to solve and works well.
                           
                              
                                 
                                    
                                       u
                                       *
                                    
                                    =
                                    
                                       argmin
                                       
                                          
                                             u
                                             j
                                          
                                          ,
                                          
                                          j
                                          =
                                          1
                                          ,
                                          ⋯
                                          ,
                                          m
                                       
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    ∠
                                    
                                       (
                                       
                                          u
                                          j
                                       
                                       ,
                                       
                                          u
                                          i
                                       
                                       )
                                    
                                    .
                                 
                              
                           
                        
                     

@&#EXPERIMENTS@&#

@&#IMPLEMENTATION@&#

We chose 10 common object categories with unambiguous upright orientation from Princeton ModelNet [33]. Each category contained 100 shapes and was split into training set and test set randomly. The training shapes were rotated 100 times for data augmentation. The test data was also rotated 20 times to study the robustness of this approach to the pose of input shape. Fig. 6
                         displays some objects sampled from our test set. All experimental results presented in this paper have been tested on a desktop with an Intel(R) Core(TM) i5-4570 CPU @ 3.20 GHz, 8 GB RAM and an NVIDIA GeForce GTX 760 GPU.

A 3D shape is represented as a 24 × 24 × 24 voxel grid. The architecture of our regression network is briefly illustrated in Fig. 3. First we place three convolutional layers, and each of them is followed by a layer of rectified linear units (ReLU). Then two more fully-connected layers are appended. Dropout [26] is applied on the first fully-connected layer. The last layer has 3 output units corresponding to the 3-dimensional orientation label. Such a networks contains 10.6 million floating point parameters, costing 42.6 MB memory. The classification network shares a similar architecture. The details of the designed networks are listed in Tables 1
                         and 2
                        . We select the network architectures experimentally. Results in different architectures are presented in the supplementary material. The networks were implemented with the deep learning framework Caffe [16].

After training the networks for classification and regression, the distance matrix on the shape categories was computed. Based on the distance measurement, we performed the agglomerative clustering algorithm. As a result, 10 shape categories were partitioned into five clusters. (We cut the cluster tree into five clusters empirically. Intuitively, the fewer clusters are left, the stronger interference effect would arise.) Then new regression networks were trained with the same architecture and half of the parameters for regression were saved. The four-legged/wheeled object categories (i.e., car, chair, dog and table) were collected into the same cluster, while the cup-shaped shape categories (i.e., bathtub and cup) were collected into another cluster (this cluster also contains airplane). As each regression network costs 42.6 MB memory and five networks were used instead of 10, about 213 MB memory was reduced. The threshold used for distance measurement was 15°, which should be enough for most graphics and robotics tasks. The distance matrix and result of clustering are shown in Fig. 4.

The classification network was trained eight epochs and achieved the accuracy of 95.6%. Each regression network was trained around 30 epochs. The final accuracy tested on each category of the entire system, which was combined with the classification network and the regression networks, is listed in Table 3
                        . Error distribution in degrees is presented in Fig. 5. The results of test-time augmentation are also shown in Table 3. We rotated each input shape 10 times and the accuracy was improved about 6%. Moreover, TTA would help to give a reasonable result if the regression network’s output of some pose degenerates, i.e., producing zero vector (although it had never arisen through our experiments).


                        Interference effects. To demonstrate the effect of interference, we compared the training of regression networks on two groups of shape categories using the same architecture as before (Table 1). The first group (group A) contains two shape categories: airplane and person. The second group (group B) contains four categories: car, chair, dog and table, which were clustered together by our method. The learning process is plotted in Fig. 7
                        . The final training loss of the two groups are similar while the testing loss of group A is apparently higher than that of group B. From the perspective of accuracy, we got 0.713 from the test set of group A and 0.861 from group B, while they were expected to be comparable based on the first row (Nets-10) in Table 3. As a consequence, strong interference effects made the network for group A hard to generalize while its impact on the group clustered by our method is significantly lower. In other words, it is nontrivial to determine the clustering criterion.


                        Network visualization. Inspired by [35], we visualize the network’s response to different portions of the voxel grid. We hollowed out a 7 × 7 × 7 cube around each voxel, and computed the angle error between the prediction for the disturbed data and the ground truth to measure the network’s sensitivity to the hollowed region. As shown in Fig. 8
                        , the regression network for the person category always responds strongly to the torsos of the human models while shows insensitivity to arms, legs and objects held in hands. These examples demonstrate that this ConvNet has strong ability to learn orientation covariant and posture invariant high-level features. Although upright orientation is the only supervision information, our system is able to locate shape parts with semantic meanings. If more specific labels are available, more semantic and representative features could be learned. A similar example is shown in Fig. 9
                        .


                        Generalization capability. Finally, we present an illustration on how the data-driven method can be used to predict upright orientation for shapes not shown in the training dataset, thus illustrating the generalization ability of the proposed method. As for the examples in Fig. 10
                        , our system classified bird as airplane, piano as table, bed as bathtub and house as cup. The first three cases are predicted correctly while the last one is failed. Our system has generalization ability to some degree, while it would be much better to train new networks for unseen shape categories.

Compared with existing approaches, our system can handle more general object categories. The method proposed in Fu et al. [8] is based on the observation that a man-made object should have a supporting base on which it can be steadily positioned and the supporting polygons correspond to faces of the object’s convex hull. Nevertheless, this observation would fail on some objects, especially natural ones. Several such examples handled by our method are presented in Fig. 11
                        , demonstrating the advantage of feature learning over feature engineering. The tensor rank minimization approach Wang et al. [31] is not able to deal with shapes with large part not aligned with its upright orientation, as illustrated in Fig. 12
                        . Thanks to the learning ability of ConvNets, these objects can be handled by our method correctly.

Our approach also has an advantage of efficiency over other techniques. The method proposed by Fu et al. [8] contains two main steps: convex hull computation for candidate supporting bases selection and feature extraction for candidates evaluation. These two steps took 5 s on average for each object. In Wang et al. [31], the tensor rank minimization problem is highly nonlinear and hard to optimize. Therefore a genetic algorithm is adopted which took about 1–2 min for each shape. In contrast, our method reached a much more fast speed due to the parallel nature of ConvNets which is match for GPU acceleration. Furthermore, a batch of data can be processed simultaneously. The detailed timing results are listed in Table 4
                        , from which we can conclude that our method is at least 30 times faster than existing approaches.

@&#CONCLUSIONS AND FUTURE WORK@&#

We proposed a data-driven method for 3D object upright orientation estimation using 3D Convolutional Networks. Thanks to the feature learning ability of ConvNets, not only man-made objects but also natural ones can be handled. In addition, a distance based clustering technique was proposed to reduce the memory costs and a test-time augmentation procedure was proposed to further improve the accuracy. The experimental results demonstrate the efficiency and effectiveness of our approach. Besides this, the visualization results indicate that ConvNets are able to capture more semantic features if more informative labels are provided. At last, our method is extremely efficient, so it can be used as preprocessing to speed up several geometry processing tasks, such as 3D shape retrieval, matching and registration.

On the other hand, our method can still be improved in several directions. First, this approach is not as accurate as geometric methods. We consider improving the performance by geometric technique such as finding supporting bases (if available) around our result. Second, techniques of committee machines [12] should be considered to optimize the entire system all together, other than training the networks for classification and regression independently. Third, further visualization [35] works should be done to gain more insights from the trained networks and to answer several mysterious questions, such as: Why and how do the networks work? Why does the network trained on airplane work well on bathtub and vice versa? Last and not least, we would like to adopt our system to take range image as input for robotics tasks.

@&#ACKNOWLEDGMENTS@&#

We would like to acknowledge Hao Li from University of Science and Technology of China and Hongxuan Zhang from the Pennsylvania State University for their helpful suggestions on this paper. This work was supported by the NSF of China (nos. 61303148, 61222206, 11526212, 11426236), NSF of Anhui Province, China (no. 1408085QF119), Specialized Research Fund for the Doctoral Program of Higher Education under contract (no. 20133402120002) and the Hundred Talents Program of the Chinese Academy of Sciences.

Supplementary material associated with this article can be found, in the online version, at 10.1016/j.gmod.2016.03.001.


                     
                        
                           Supplementary Data S1
                           
                              Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/
                              
                           
                           
                        
                     
                  

@&#REFERENCES@&#

