@&#MAIN-TITLE@&#Feature selection using data envelopment analysis

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Feature selection is regarded as a multi-index evaluation process.


                        
                        
                           
                           A novel feature selection framework based on data envelopment analysis is proposed.


                        
                        
                           
                           The framework evaluates features from a perspective of “efficient frontier” without parameter setting.


                        
                        
                           
                           A simple feature selection method is proposed based on super-efficiency DEA.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Feature selection

Data envelopment analysis

Super-efficiency

Relevance

Redundancy

@&#ABSTRACT@&#


               
               
                  Feature selection has been attracting increasing attention in recent years for its advantages in improving the predictive efficiency and reducing the cost of feature acquisition. In this paper, we regard feature selection as an efficiency evaluation process with multiple evaluation indices, and propose a novel feature selection framework based on Data Envelopment Analysis (DEA). The most significant advantages of this framework are that it can make a trade-off among several feature properties or evaluation criteria and evaluate features from a perspective of “efficient frontier” without parameter setting. We then propose a simple feature selection method based on the framework to effectively search “efficient” features with high class-relevance and low conditional independence. Super-efficiency DEA is employed in our method to fully rank features according to their efficiency scores. Experimental results for twelve well-known datasets indicate that proposed method is effective and outperforms several representative feature selection methods in most cases. The results also show the feasibility of proposed DEA-based feature selection framework.
               
            

@&#INTRODUCTION@&#

In many data mining applications, identifying the most characterizing features (or attributes, variables, hereafter they will be used interchangeably) of the observed data is critical to optimize the classification result. Tremendous new computer and internet applications, e.g. the prevalent use of social media, generate large amounts of data at an exponential rate in the world. Massive irrelevant and redundant features existing in the feature space deteriorate the performance of machine learning algorithms, and thus present challenges to feature selection.

Feature selection is desirable and essential for a number of reasons, such as reducing the complexity of training a classifier and the cost of collecting features, improving the quality of the data, and even resulting in an improvement in classification accuracy [1,2]. Roughly, there are three types of feature selection methods [3,4]: Embedded methods, filters and wrappers. As for embedded methods in C4.5 [5] or SVM-RFE [6], the process of selecting features is integrated into the learning algorithm [5]. Wrappers rely on performance estimated by a specific learning method to evaluate and select features. The drawbacks of embedded methods and wrappers are their expensive computational complexity in learning and poor generalization to other learning methods as they are tightly coupled with specified ones. In contrast, filters assess features based on some classifier-agnostic criteria (e.g. Fisher score [7], 
                        
                           
                              
                                 χ
                              
                              
                                 2
                              
                           
                        
                     -test [3,8], mutual information [9–11], symmetrical uncertainty [1], Hilbert–Schmidt operator [12], etc.) and select features by focusing only on the intrinsic properties of the data. Developing efficient and effective filter methods has attracted great attention during past years [1,9,13,14].

Feature ranking and feature subset selection are two typical categories of feature selection regarding the output style. The former outputs ranked features weighted by their predictive power [15–17] while the latter evaluates feature subsets and searches for the best one [18,1,19–21]. Since finding an optimal subset is usually intractable and many problems related to feature selection have been shown to be NP-hard [22,23], a trade-off between result optimality and computational efficiency has been taken under consideration in literature. Heuristic methods with various feature evaluation criteria have thus been proposed [17,24,19,1]. These criteria mainly focus on the measurement of feature relevance, redundancy, conditional independence, inter-dependence, etc., and the combination of such criteria (e.g. relevance analysis with mutual information+redundancy analysis with conditional mutual information) brings about diversity of feature selection methods. Nevertheless, most of the combinations evaluate features with either prior arguments or constant coefficients, and the relative importance (weight) of each feature property such as relevance or redundancy usually cannot be identified. For example, MIFS [17] applies two information-theoretic metrics to respectively measure feature dependence (D) and redundancy (R), and uses 
                        
                           max
                           (
                           D
                           -
                           β
                           R
                           )
                        
                      to evaluate the quality of selected features. Parameter 
                        
                           β
                        
                      plays a role of mediating the weight of measured redundancy and thus any changes to it may influence the quality of the finally-selected features. Owing to more than one feature property or criterion to be considered and utilized, feature selection can thus be categorized as a multi-index evaluation process.

Data Envelopment Analysis (DEA) is an effective nonparametric method for efficiency evaluation and has been widely applied in many industries. It employs linear programming to evaluate and rank the Decision Making Units (DMUs) when the production process presents a structure of multiple inputs and outputs. Inspired by this, we regard feature selection as evaluation process with multiple inputs and outputs, and introduce in this paper a novel DEA-based feature selection framework. An effective feature selection method based on this framework is then proposed and evaluated. The remainder of the paper is organized as follows: Section 2 briefly reviews related works. Section 3 introduces some related information-theoretic metrics and Section 4 introduces a novel feature selection framework based on DEA. Then Section 5 proposes a feature selection method based on this framework. In Section 6, experimental results are given to evaluate the effectiveness of proposed method comparing with the representative feature selection methods on twelve well-known datasets, and some discussions are presented. Section 7 finally summarizes the concluding remarks and points out the future work.

@&#RELATED WORK@&#

Various aspects of feature selection have been studied for years. One of the key aspects is to measure the quality of selected features. John, Kohavi, and Pfleger classified features into three categories, i.e. strongly relevant, weakly relevant, and irrelevant ones [25], and feature selection research at that time mainly focused on searching for relevant features [26]. However, since the existence and effect of feature redundancy were pointed out [27,28,9,10], how to effectively select more relevant and less redundant features has been a hot issue in literature [18,29,1,19,30,17,24,10,31]. Although other characteristics like conditional independence [32,20,21,33] are revealed and studied, they are all variants of the basic concepts of feature relevance and redundancy. In the following text, we mainly review and analyze related work from the viewpoint of redundancy analysis.

Regarding the relationship between the class and the features, feature redundancy analysis can be divided into two categories: One only measures the correlation among features without considering the effect of the class [18,17,29,9,24,1,10,34] while the other considers such effect [27,19,30,35,32,20,21,33,12]. In other words, the former considers redundancy in an unsupervised manner, while the latter is more consistent with supervised learning scheme (Hereafter we call them unsupervised redundancy analysis and supervised redundancy analysis, respectively.).
                        1
                        In [36], they are also called “redundancy” and “conditional redundancy”, respectively.
                     
                     
                        1
                      Correlation-based Feature Selection (CFS) method [18] is a typical algorithm that handles redundancy by unsupervised redundancy analysis. A correlation-based metric 
                        
                           cor
                           =
                           
                              
                                 kr
                              
                              
                                 ic
                              
                           
                           /
                           
                              
                                 k
                                 +
                                 k
                                 (
                                 k
                                 -
                                 1
                                 )
                                 
                                    
                                       r
                                    
                                    
                                       ij
                                    
                                 
                              
                           
                        
                      (where k is the number of the currently-selected features, 
                        
                           
                              
                                 r
                              
                              
                                 ic
                              
                           
                        
                      is the average correlation between the features and the class, 
                        
                           
                              
                                 r
                              
                              
                                 ij
                              
                           
                        
                      is the average inter-correlation of the features) is applied by CFS and the subset maximizing its cor value will be chosen as the final selected one. A series of representative feature selection methods with minimal Redundancy and Maximal Relevance (mRMR) criterion [17,29,9,24] also apply unsupervised redundancy analysis to measure redundancy.
                        2
                        For the sake of convenience, here we regard MIFS [17] as a general form of mRMR.
                     
                     
                        2
                      They generally apply 
                        
                           D
                           =
                           α
                           ·
                           
                              
                                 ∑
                              
                              
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                                 ∈
                                 S
                              
                           
                           I
                           (
                           
                              
                                 F
                              
                              
                                 i
                              
                           
                           ,
                           C
                           )
                        
                      to measure relevance and 
                        
                           R
                           =
                           β
                           ·
                           
                              
                                 ∑
                              
                              
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       F
                                    
                                    
                                       j
                                    
                                 
                                 ∈
                                 S
                              
                           
                           I
                           (
                           
                              
                                 F
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 F
                              
                              
                                 j
                              
                           
                           )
                        
                      to measure redundancy, where I denotes mutual information and 
                        
                           α
                        
                      and 
                        
                           β
                        
                      are importance weights corresponding to D and R, respectively, and apply 
                        
                           max
                           (
                           D
                           -
                           R
                           )
                        
                      or 
                        
                           max
                           (
                           D
                           /
                           R
                           )
                        
                      to evaluate and select features. Since unsupervised redundancy analysis only refers to the inter-dependence among features, it is not enough to effectively identify redundancy when the class concept is considered: When the redundancy score between two features is large, it is even unable to determine which one is redundant. Most of the feature selection methods with unsupervised redundancy analysis implicitly eliminate redundancy with the help of their relevance analysis in order to get more relevant features while keeping them independent to each other, whereas exceptions also exist in literature. For example, FCBF [1] and QMIFS-p [10] handle redundancy with unsupervised redundancy analysis in a more explicit manner. FCBF sorts features in a descending order according to their relevance scores and measures the inner-correlation between any pairs of features. Then it removes redundant features via an approximate Markov blanket criterion: If 
                        
                           SU
                           (
                           
                              
                                 F
                              
                              
                                 i
                              
                           
                           ,
                           C
                           )
                           >
                           SU
                           (
                           
                              
                                 F
                              
                              
                                 j
                              
                           
                           ,
                           C
                           )
                        
                      and 
                        
                           SU
                           (
                           
                              
                                 F
                              
                              
                                 j
                              
                           
                           ,
                           C
                           )
                           <
                           SU
                           (
                           
                              
                                 F
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 F
                              
                              
                                 j
                              
                           
                           )
                        
                      (where SU is symmetrical uncertainty and 
                        
                           
                              
                                 F
                              
                              
                                 i
                              
                           
                           ,
                           
                           
                              
                                 F
                              
                              
                                 j
                              
                           
                        
                      and C are candidate features and the class, respectively), then 
                        
                           
                              
                                 F
                              
                              
                                 j
                              
                           
                        
                      is determined as a redundant feature which should be removed. Recently, this method is extended to remove redundancy from a viewpoint of feature subsets with a cluster-based search strategy [34]. QMIFS-p applies the so-called MISF criterion to explicitly measure the similarity between the candidate feature (F) and the currently-selected features (
                        
                           
                              
                                 F
                              
                              
                                 j
                              
                           
                           ∈
                           S
                        
                     ). Since criterion MISF 
                        
                           (
                           F
                           )
                           =
                           
                              
                                 argmax
                              
                              
                                 
                                    
                                       F
                                    
                                    
                                       j
                                    
                                 
                                 ∈
                                 S
                              
                           
                           (
                           I
                           (
                           F
                           ,
                           
                              
                                 F
                              
                              
                                 j
                              
                           
                           )
                           /
                           H
                           (
                           
                              
                                 F
                              
                              
                                 j
                              
                           
                           )
                           )
                        
                      estimates redundancy without considering the effect of the class, it is an unsupervised redundancy metric. Dissimilar from FCBF, QMIFS-p identifies relevance and redundancy during every iteration, which enhances the ability for identifying potential redundant features during the search process.

All of the above representative methods with unsupervised redundancy analysis measure the inter-dependence between pairs of features instead of that among the feature subsets, and hence neglect the complementarity which may provide a significant performance improvement among two or more features [3]. In addition, inter-independence may no longer exist when the class is considered. Features with less inter-dependence may be conditional dependent on each other given the distribution of the class, and hence redundancy may still exist under this circumstance. To untie the knots, a series of feature selection methods with supervised redundancy analysis are proposed [27,19,30,35,32,20,21,33,12]. Conditional Mutual Information (CMI) is an important information-theoretic metric referring to supervised redundancy analysis. It measures conditional dependence between two variables with respect to the class concept: A very small value of 
                        
                           I
                           (
                           F
                           ;
                           C
                           |
                           S
                           )
                        
                      (which denotes the CMI between feature F and the class C given the feature subset 
                        
                           S
                        
                     ) implies that F carries little additional information about C given the subset 
                        
                           S
                        
                     , namely (a) F is redundant to 
                        
                           S
                        
                      or (b) F is irrelevant to C. The advantage of paying attention to both relevance and redundancy makes CMI a frequently used metric in related work [19,30,35,33,34]. The algorithms with Conditional Mutual Information Maximization (CMIM) criterion [19,30] harness CMI to conduct supervised redundancy analysis. To avoid the difficulty of joint distribution estimation on insufficient samples, CMIM only uses 
                        
                           
                              
                                 F
                              
                              
                                 ∼
                              
                           
                           =
                           arg
                           
                              
                                 min
                              
                              
                                 
                                    
                                       F
                                    
                                    
                                       ∼
                                    
                                 
                                 ∈
                                 S
                              
                           
                           I
                           (
                           F
                           ;
                           C
                           |
                           
                              
                                 F
                              
                              
                                 ∼
                              
                           
                           )
                        
                      as the conditioning feature on behalf of 
                        
                           S
                        
                     , and selects F satisfying 
                        
                           
                              
                                 max
                              
                              
                                 F
                                 ∈
                                 F
                              
                           
                           I
                           (
                           F
                           ;
                           C
                           |
                           
                              
                                 F
                              
                              
                                 ∼
                              
                           
                           )
                        
                     , where 
                        
                           F
                        
                      is the original feature set. The algorithm with Joint Mutual Information (JMI) [26] also applies similar approximation and selects F satisfying 
                        
                           
                              
                                 max
                              
                              
                                 F
                              
                           
                           
                              
                                 ∑
                              
                              
                                 
                                    
                                       F
                                    
                                    
                                       ∼
                                    
                                 
                              
                           
                           I
                           (
                           F
                           
                              
                                 F
                              
                              
                                 ∼
                              
                           
                           ;
                           C
                           )
                        
                     , where 
                        
                           I
                           (
                           F
                           
                              
                                 F
                              
                              
                                 ∼
                              
                           
                           ;
                           C
                           )
                        
                      is the mutual information between the class and the feature subset 
                        
                           {
                           F
                           
                              
                                 F
                              
                              
                                 ∼
                              
                           
                           }
                        
                     . The approximation applied in CMIM and JMI is called in [36] “first-order utility”-approximation and makes them so efficient on large-scale data while inevitably ignore the dependency among three or larger families of features in the feature subset 
                        
                           S
                        
                     . The algorithm with minimal Relevant Redundancy (mRR) criterion [35] utilizes clustering approach with a CMI-based distance function to conduct supervised redundancy analysis. It clusters features into several segments and then gathers the centroid of each segment together as the selected features. The motivation is that less 
                        
                           
                              
                                 ∑
                              
                              
                                 F
                                 ∈
                                 
                                    
                                       S
                                    
                                    
                                       i
                                    
                                 
                              
                           
                           I
                           (
                           F
                           ;
                           C
                           |
                           
                              
                                 F
                              
                              
                                 o
                              
                           
                           )
                        
                      implies 
                        
                           
                              
                                 F
                              
                              
                                 o
                              
                           
                        
                      a better representative (which is more relevant to the class and less redundant to other features in the same segment) of segment 
                        
                           
                              
                                 S
                              
                              
                                 i
                              
                           
                        
                     . Analogous to CMIM and JMI, mRR does not consider the dependency among three or more features. Worse still, the key proposition of mRR has been shown to be erroneous recently [37]. IAMB algorithm and its variants [32,38–40,20] and other algorithms like OSFS [21] and CCM [33] are proposed to search for the optimal feature subset taking into account of the dependency among three or more features. They argue that salient features should have strong discriminative power and the power would not be impaired given other features. And these two aspects can be achieved simultaneously by conditional independence test. In other words, neither irrelevant nor redundant features can pass the conditional independence test. Nevertheless, conditional independence test may suffer from inaccurate estimation of joint distribution on insufficient samples, and it is often inefficient in high dimensional feature space arising from real-world datasets [35,33]. Thus its application is restricted in many fields to a great extent.

In this section, we introduce some essential information-theoretic metrics that will be used in our method. In information theory, Mutual Information (MI) is a basic metric of information and has been widely used for quantifying the mutual dependence of random variables. Mutual information between two random variables X and Y, denoted as 
                        
                           I
                           (
                           X
                           ;
                           Y
                           )
                        
                     , is formed as
                        
                           
                              I
                              (
                              X
                              ;
                              Y
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       x
                                       ∈
                                       X
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       y
                                       ∈
                                       Y
                                    
                                 
                              
                              p
                              (
                              xy
                              )
                              log
                              
                                 
                                    p
                                    (
                                    xy
                                    )
                                 
                                 
                                    p
                                    (
                                    x
                                    )
                                    p
                                    (
                                    y
                                    )
                                 
                              
                              ,
                           
                        
                     where 
                        
                           x
                           ∈
                           X
                        
                      and 
                        
                           y
                           ∈
                           Y
                        
                      are the possible value assignments of X and Y, respectively, and the logarithm to base 2 is used.
                        3
                        For the sake of convenience, the notation log is used instead of 
                              
                                 
                                    
                                       log
                                    
                                    
                                       2
                                    
                                 
                              
                            throughout this paper.
                     
                     
                        3
                      MI can also be interpreted as the amount of information shared by two variables. According to this proposition, MI is one of the most frequently used metrics in feature selection algorithms and is usually applied to measure the relevance between features and the class.

Conditional Mutual Information (CMI) is an extension of MI for measuring the conditional dependence between two random variables given the third. The CMI between X and Y given a variable Z is defined as
                        
                           
                              I
                              (
                              X
                              ;
                              Y
                              |
                              Z
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       z
                                       ∈
                                       Z
                                    
                                 
                              
                              p
                              (
                              z
                              )
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       x
                                       ∈
                                       X
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       y
                                       ∈
                                       Y
                                    
                                 
                              
                              p
                              (
                              xy
                              |
                              z
                              )
                              log
                              
                                 
                                    p
                                    (
                                    xy
                                    |
                                    z
                                    )
                                 
                                 
                                    p
                                    (
                                    x
                                    |
                                    z
                                    )
                                    p
                                    (
                                    y
                                    |
                                    z
                                    )
                                 
                              
                              .
                           
                        
                     
                     
                        
                           I
                           (
                           X
                           ;
                           Y
                           |
                           Z
                           )
                        
                      can be interpreted as the quantity of information that X provides about Y which Z cannot. If 
                        
                           I
                           (
                           X
                           ;
                           Y
                           |
                           Z
                           )
                           =
                           0
                           ,
                           
                           X
                        
                      and Y are conditional independent given Z, i.e. no additional information X can provide about Y when the distribution of Z is given.

MI and CMI can also be expressed with entropies as follows
                        
                           
                              I
                              (
                              X
                              ;
                              Y
                              )
                              =
                              H
                              (
                              X
                              )
                              -
                              H
                              (
                              X
                              |
                              Y
                              )
                           
                        
                     and
                        
                           
                              I
                              (
                              X
                              ;
                              Y
                              |
                              Z
                              )
                              =
                              H
                              (
                              X
                              |
                              Z
                              )
                              -
                              H
                              (
                              X
                              |
                              Y
                              ,
                              Z
                              )
                              .
                           
                        
                     The above formulations are often used for the implementation of MI and CMI. Note that both MI and CMI are nonnegative 
                        
                           (
                           ∀
                           X
                           ,
                           Y
                           ,
                           Z
                           ,
                           I
                           (
                           X
                           ;
                           Y
                           )
                           ⩾
                           0
                           
                           and
                           
                           I
                           (
                           X
                           ;
                           Y
                           |
                           Z
                           )
                           ⩾
                           0
                           )
                        
                      and symmetric 
                        
                           (
                           ∀
                           X
                           ,
                           Y
                           ,
                           Z
                           ,
                           I
                           (
                           X
                           ;
                           Y
                           )
                           ≡
                           I
                           (
                           Y
                           ;
                           X
                           )
                           
                           and
                           
                           I
                           (
                           X
                           ;
                           Y
                           |
                           Z
                           )
                           ≡
                           I
                           (
                           Y
                           ;
                           X
                           |
                           Z
                           )
                           )
                        
                     .

Symmetric Uncertainty (SU) [1] is a well-known and widely used metric that normalizes MI and CMI to the range 
                        
                           [
                           0
                           ,
                           1
                           ]
                        
                      with their corresponding entropies. It is defined as
                        
                           (1)
                           
                              SU
                              (
                              X
                              ;
                              Y
                              )
                              =
                              
                                 
                                    2
                                    ·
                                    MI
                                    (
                                    X
                                    ;
                                    Y
                                    )
                                 
                                 
                                    H
                                    (
                                    X
                                    )
                                    +
                                    H
                                    (
                                    Y
                                    )
                                 
                              
                              ,
                           
                        
                     or
                        
                           (2)
                           
                              SU
                              (
                              X
                              ;
                              Y
                              |
                              Z
                              )
                              =
                              
                                 
                                    2
                                    ·
                                    CMI
                                    (
                                    X
                                    ;
                                    Y
                                    |
                                    Z
                                    )
                                 
                                 
                                    H
                                    (
                                    X
                                    |
                                    Z
                                    )
                                    +
                                    H
                                    (
                                    Y
                                    |
                                    Z
                                    )
                                 
                              
                              .
                           
                        
                     SU compensates for MI or CMI’s bias toward features with more values. 
                        
                           SU
                           (
                           X
                           ;
                           Y
                           )
                           =
                           1
                        
                      (
                        
                           SU
                           (
                           X
                           ;
                           Y
                           |
                           Z
                           )
                           =
                           1
                        
                     ) indicates that knowing the values of either X or Y completely predicts the values of the other (given Z); 
                        
                           SU
                           (
                           X
                           ;
                           Y
                           )
                           =
                           0
                        
                      (
                        
                           SU
                           (
                           X
                           ;
                           Y
                           |
                           Z
                           )
                           =
                           0
                        
                     ) indicates that X and Y are (conditional) independent with each other (given Z).

As introduced previously, DEA is a powerful and widely used linear programming technique for the assessment of the relative efficiency of a set of DMUs. The basic DEA model originated by Charnes, Cooper and Rhodes is called CCR model [41]. Consider n DMUs that use m inputs to produce s outputs. Let 
                           
                              
                                 
                                    x
                                 
                                 
                                    ij
                                 
                              
                           
                         
                        
                           
                              (
                              i
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              m
                              )
                           
                         represents the ith input and 
                           
                              
                                 
                                    y
                                 
                                 
                                    rj
                                 
                              
                           
                         
                        
                           
                              (
                              r
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              s
                              )
                           
                         the rth output of 
                           
                              
                                 
                                    DMU
                                 
                                 
                                    j
                                 
                              
                              
                              (
                              j
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              n
                              )
                           
                        , the relative efficiency of 
                           
                              
                                 
                                    DMU
                                 
                                 
                                    p
                                 
                              
                           
                         under CCR model can be achieved by solving the following linear optimization problem
                           
                              (3)
                              
                                 
                                    min
                                 
                                 
                                 
                                    
                                       θ
                                    
                                    
                                       c
                                    
                                    
                                       p
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    
                                       
                                          
                                             s
                                             .
                                             t
                                             .
                                          
                                       
                                       
                                          
                                             
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   
                                                      n
                                                   
                                                
                                             
                                             
                                                
                                                   λ
                                                
                                                
                                                   j
                                                
                                             
                                             
                                                
                                                   x
                                                
                                                
                                                   ij
                                                
                                             
                                             +
                                             
                                                
                                                   s
                                                
                                                
                                                   i
                                                
                                                
                                                   -
                                                
                                             
                                             =
                                             
                                                
                                                   θ
                                                
                                                
                                                   c
                                                
                                                
                                                   p
                                                
                                             
                                             
                                                
                                                   x
                                                
                                                
                                                   ip
                                                
                                             
                                             ,
                                             
                                             i
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             m
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   
                                                      n
                                                   
                                                
                                             
                                             
                                                
                                                   λ
                                                
                                                
                                                   j
                                                
                                             
                                             
                                                
                                                   y
                                                
                                                
                                                   rj
                                                
                                             
                                             -
                                             
                                                
                                                   s
                                                
                                                
                                                   r
                                                
                                                
                                                   +
                                                
                                             
                                             =
                                             
                                                
                                                   y
                                                
                                                
                                                   rp
                                                
                                             
                                             ,
                                             
                                             r
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             s
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   λ
                                                
                                                
                                                   j
                                                
                                             
                                             ⩾
                                             0
                                             ,
                                             
                                             j
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             n
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   i
                                                
                                                
                                                   -
                                                
                                             
                                             ⩾
                                             0
                                             ,
                                             
                                             i
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             m
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   r
                                                
                                                
                                                   +
                                                
                                             
                                             ⩾
                                             0
                                             ,
                                             
                                             r
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             s
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    λ
                                 
                                 
                                    j
                                 
                              
                           
                         is the weight assigned to 
                           
                              
                                 
                                    DMU
                                 
                                 
                                    j
                                 
                              
                              ;
                              
                              
                                 
                                    s
                                 
                                 
                                    i
                                 
                                 
                                    -
                                 
                              
                           
                         and 
                           
                              
                                 
                                    s
                                 
                                 
                                    r
                                 
                                 
                                    +
                                 
                              
                           
                         are the slack variables for the ith input and the rth output of the target 
                           
                              
                                 
                                    DMU
                                 
                                 
                                    p
                                 
                              
                           
                        , respectively. 
                           
                              
                                 
                                    DMU
                                 
                                 
                                    p
                                 
                              
                           
                         is determined to be efficient, i.e. locates on the efficient frontier if and only if 
                           
                              
                                 
                                    
                                       
                                          θ
                                       
                                       
                                          c
                                       
                                       
                                          p
                                       
                                    
                                 
                                 
                                    ∗
                                 
                              
                              =
                              1
                           
                         (hereafter 
                           
                              
                                 
                                    θ
                                 
                                 
                                    ∗
                                 
                              
                           
                         denotes the optimal value); otherwise, it is referred to as non-efficient (under the efficient frontier). However, CCR model can only separate DMUs into efficient ones (with the optimal value equal to 1) and inefficient ones (with the optimal value less than 1). Super-efficiency DEA model is one of the typical models that can fully rank DMUs. The super-efficiency modification to the standard CCR model involves excluding the column of the current DMU being scored from the coefficient matrix, which can be shown as
                           
                              (4)
                              
                                 
                                    min
                                 
                                 
                                 
                                    
                                       θ
                                    
                                    
                                       s
                                    
                                    
                                       p
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    
                                       
                                          
                                             s
                                             .
                                             t
                                             .
                                          
                                       
                                       
                                          
                                             
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      j
                                                      =
                                                      1
                                                      ,
                                                      j
                                                      ≠
                                                      p
                                                   
                                                   
                                                      n
                                                   
                                                
                                             
                                             
                                                
                                                   λ
                                                
                                                
                                                   j
                                                
                                             
                                             
                                                
                                                   x
                                                
                                                
                                                   ij
                                                
                                             
                                             +
                                             
                                                
                                                   s
                                                
                                                
                                                   i
                                                
                                                
                                                   -
                                                
                                             
                                             =
                                             
                                                
                                                   θ
                                                
                                                
                                                   s
                                                
                                                
                                                   p
                                                
                                             
                                             
                                                
                                                   x
                                                
                                                
                                                   ip
                                                
                                             
                                             ,
                                             
                                             i
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             m
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      j
                                                      =
                                                      1
                                                      ,
                                                      j
                                                      ≠
                                                      p
                                                   
                                                   
                                                      n
                                                   
                                                
                                             
                                             
                                                
                                                   λ
                                                
                                                
                                                   j
                                                
                                             
                                             
                                                
                                                   y
                                                
                                                
                                                   rj
                                                
                                             
                                             -
                                             
                                                
                                                   s
                                                
                                                
                                                   r
                                                
                                                
                                                   +
                                                
                                             
                                             =
                                             
                                                
                                                   y
                                                
                                                
                                                   rp
                                                
                                             
                                             ,
                                             
                                             r
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             s
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   λ
                                                
                                                
                                                   j
                                                
                                             
                                             ⩾
                                             0
                                             ,
                                             
                                             j
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             n
                                             ,
                                             
                                             j
                                             
                                             ≠
                                             
                                             p
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   i
                                                
                                                
                                                   -
                                                
                                             
                                             ⩾
                                             0
                                             ,
                                             
                                             i
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             m
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   r
                                                
                                                
                                                   +
                                                
                                             
                                             ⩾
                                             0
                                             ,
                                             
                                             r
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             s
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        It is noted that 
                           
                              
                                 
                                    
                                       
                                          θ
                                       
                                       
                                          s
                                       
                                       
                                          p
                                       
                                    
                                 
                                 
                                    ∗
                                 
                              
                              ⩾
                              
                                 
                                    
                                       
                                          θ
                                       
                                       
                                          c
                                       
                                       
                                          p
                                       
                                    
                                 
                                 
                                    ∗
                                 
                              
                           
                         holds for the optimal solutions from model (4) are always feasible to model (3). This makes super-efficiency model may score efficient DMUs more than one and hence is able to rank all the DMUs according to their 
                           
                              
                                 
                                    θ
                                 
                                 
                                    s
                                 
                                 
                                    ∗
                                 
                              
                           
                        . Here we do not plan to further discuss the principle of super-efficiency model. Readers interested in a detailed description of the model are referred to [42,43].

As introduced in Section 2, two essential properties, i.e. class-relevance and redundancy, have always attracted much attention when evaluating candidate feature(s). How to properly measure them and combine them together to finally evaluate features is always a hot issue and results in various feature evaluation criteria such as MIFS, mRMR, CMIM, JMI which have been previously discussed. Unfortunately, most of the criteria evaluate features with either prior arguments (e.g. 
                           
                              β
                           
                         in MIFS) or constant coefficients (e.g. 
                           
                              
                                 
                                    1
                                 
                                 
                                    n
                                    -
                                    1
                                 
                              
                           
                         in an equivalent and more general form of JMI introduced in [36]) and the relative importance (weight) of each feature property such as relevance and redundancy is completely not able to be identified by such criteria themselves. This motivates us to introduce a nonparametric evaluation method into feature selection.

According to the evaluation and ranking ability of DEA, it is natural that one applies it to rank features if he/she regards feature selection as an evaluation system with multiple inputs and outputs, i.e. each feature in the original feature set is seen as a DMU, and some of its properties are selected to be the inputs and some the outputs. The system then evaluates the efficiency of the feature according to its inputs and outputs (Fig. 1
                        ). An “efficient” feature is expected to be the one that using relatively small input values to “produce” relatively large output values.

The most significant advantage of this framework is that it can make a trade-off among several feature properties or evaluation criteria from a perspective of “efficient frontier” without parameter setting. Obviously, the evaluation and ranking results may vary a lot and this diversity is strongly related to the selection of evaluation criteria and indices, i.e. (a) which kind of feature properties (e.g. relevance, redundancy, conditional independence, etc.) should be considered and selected to establish the evaluation system and (b) which properties should be used as the inputs and the outputs. Note that the framework shown in Fig. 1 is specified to neither unsupervised learning scheme nor supervised one, and whether the framework is unsupervised-learning-oriented or supervised-learning-oriented is determined by the inputs and the outputs of the system. For example, if the relevance between feature and the class is chosen to be one of the outputs, the framework may be more supervised-learning-oriented. As the aim of our study is to propose a feature selection method for classification, we hence propose in the next section a simple and effective feature selection method based on a supervised-learning-oriented framework.

In this section, we propose a super-efficiency-DEA-based feature selection method. Unlike most of the existing feature selection methods, our method focuses on the entire redundancy distribution for each candidate feature, i.e. redundancy between the candidate feature and every other feature in the feature space. Since salient features should have strong discriminative power and the power would not be impaired given other features, we measure the conditional independence between the currently-evaluated feature and the class given every other feature in the feature space, and finally get the conditional independence distribution. For feature 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                        , we define its conditional independence score given 
                           
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                           
                         
                        
                           
                              (
                              i
                              
                              ≠
                              
                              j
                              )
                           
                         as
                           
                              (5)
                              
                                 
                                    
                                       CI
                                    
                                    
                                       C
                                    
                                 
                                 (
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 
                                    
                                       F
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 1
                                 -
                                 SU
                                 (
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                                 ;
                                 C
                                 |
                                 
                                    
                                       F
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 ,
                              
                           
                        and define the relevance score of 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                         as
                           
                              (6)
                              
                                 
                                    
                                       R
                                    
                                    
                                       C
                                    
                                 
                                 (
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 SU
                                 (
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                                 ;
                                 C
                                 )
                                 .
                              
                           
                        According to Eq. (5), if 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                         is highly conditional independent (dependent) to the class given 
                           
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                              ,
                              
                              SU
                              (
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                              ;
                              C
                              |
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         would be small (large) and hence 
                           
                              
                                 
                                    CI
                                 
                                 
                                    C
                                 
                              
                              (
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                              |
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         would be large (small). Since qualified features are always with high class-relevance and low redundancy, we expect higher-ranking features F with smaller 
                           
                              
                                 
                                    CI
                                 
                                 
                                    C
                                 
                              
                              (
                              F
                              |
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         and larger 
                           
                              
                                 
                                    CI
                                 
                                 
                                    C
                                 
                              
                              (
                              F
                              )
                           
                        , i.e. using smaller 
                           
                              
                                 
                                    CI
                                 
                                 
                                    C
                                 
                              
                              (
                              F
                              |
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         to “produce” larger 
                           
                              
                                 
                                    CI
                                 
                                 
                                    C
                                 
                              
                              (
                              F
                              )
                           
                        . However, the redundancy of F varies according to the changes of the conditioning feature 
                           
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                           
                        , and the relative importance of each redundancy value is difficult to identify. For a feature F, if there are many features given which F is significantly conditional independent to (conditional dependent on) the class, it seems to be redundant or irrelevant (salient) and thus a low- (high-) ranking feature; if F cannot be definitely identified as a redundant or irrelevant feature via its conditional independence distribution, one may simply add up its conditional independence scores to identify this (this is similar to the redundancy measurement of MIFS and mRMR). However, this manner neglects the diversity of the distributions, and thus may result in similar redundancy scores of F under different distributions. For example, for two features 
                           
                              
                                 
                                    F
                                 
                                 
                                    1
                                 
                              
                           
                         and 
                           
                              
                                 
                                    F
                                 
                                 
                                    2
                                 
                              
                           
                        , if 
                           
                              
                                 
                                    F
                                 
                                 
                                    1
                                 
                              
                           
                         and 
                           
                              
                                 
                                    F
                                 
                                 
                                    2
                                 
                              
                           
                         are always significantly conditional independent to the class given the actually-non-salient and actually-salient features respectively, and significantly conditional dependent on the class given the actually-salient and actually-non-salient ones respectively, 
                           
                              
                                 
                                    F
                                 
                                 
                                    1
                                 
                              
                           
                         is then more salient than 
                           
                              
                                 
                                    F
                                 
                                 
                                    2
                                 
                              
                           
                         in actual. Under this circumstance, the operation that simply adds up 
                           
                              
                                 
                                    CI
                                 
                                 
                                    C
                                 
                              
                              (
                              
                                 
                                    F
                                 
                                 
                                    1
                                 
                              
                              |
                              F
                              )
                           
                         for every 
                           
                              F
                              ∈
                              F
                           
                         (
                           
                              F
                              
                              ≠
                              
                              
                                 
                                    F
                                 
                                 
                                    1
                                 
                              
                           
                        ) and 
                           
                              
                                 
                                    CI
                                 
                                 
                                    C
                                 
                              
                              (
                              
                                 
                                    F
                                 
                                 
                                    2
                                 
                              
                              |
                              F
                              )
                           
                         for every 
                           
                              F
                              ∈
                              F
                           
                         (
                           
                              F
                              
                              ≠
                              
                              
                                 
                                    F
                                 
                                 
                                    2
                                 
                              
                           
                        ) neglects the difference between the distributions of 
                           
                              
                                 
                                    CI
                                 
                                 
                                    C
                                 
                              
                              (
                              
                                 
                                    F
                                 
                                 
                                    1
                                 
                              
                              |
                              F
                              )
                           
                         and 
                           
                              
                                 
                                    CI
                                 
                                 
                                    C
                                 
                              
                              (
                              
                                 
                                    F
                                 
                                 
                                    2
                                 
                              
                              |
                              F
                              )
                           
                        , and thus possibly lead to wrong identification if the adding-up values of 
                           
                              
                                 
                                    F
                                 
                                 
                                    1
                                 
                              
                           
                         and 
                           
                              
                                 
                                    F
                                 
                                 
                                    2
                                 
                              
                           
                         are similar to each other. Consequently, the diversity of the distributions is necessary to be taken into account to further identify redundant and irrelevant features.

To this end, we apply the DEA-based framework to conduct feature selection task. For 
                           
                              
                                 
                                    F
                                 
                                 
                                    p
                                 
                              
                           
                        , we regard the distribution of its conditional independence as inputs where each 
                           
                              
                                 
                                    CI
                                 
                                 
                                    C
                                 
                              
                              (
                              
                                 
                                    F
                                 
                                 
                                    p
                                 
                              
                              |
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                              )
                           
                         
                        
                           
                              (
                              j
                              =
                              1
                              ,
                              …
                              ,
                              |
                              F
                              |
                              ;
                              
                              j
                              
                              ≠
                              
                              p
                              )
                           
                         is taken as the input, and the relevance between 
                           
                              
                                 
                                    F
                                 
                                 
                                    p
                                 
                              
                           
                         and the class i.e. 
                           
                              
                                 
                                    R
                                 
                                 
                                    C
                                 
                              
                              (
                              
                                 
                                    F
                                 
                                 
                                    p
                                 
                              
                              )
                           
                         as the output. We thus obtain a feature evaluation system with 
                           
                              |
                              F
                              |
                              -
                              1
                           
                         inputs related to the conditional independence and one output related to the class-relevance. We apply this evaluation system to search “efficient” features, which is shown in Fig. 2
                        .

To fully rank features according to their efficiency scores 
                           
                              
                                 
                                    
                                       
                                          θ
                                       
                                       
                                          s
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    ∗
                                 
                              
                              
                              (
                              i
                              =
                              1
                              ,
                              …
                              ,
                              |
                              F
                              |
                              )
                           
                        , we apply a super-efficiency DEA model to achieve the evaluation task for the system, which is shown as model (7).
                           
                              (7)
                              
                                 
                                    min
                                 
                                 
                                 
                                    
                                       θ
                                    
                                    
                                       s
                                    
                                    
                                       p
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    
                                       
                                          
                                             s
                                             .
                                             t
                                             .
                                          
                                       
                                       
                                          
                                             
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      j
                                                      =
                                                      1
                                                      ,
                                                      j
                                                      ≠
                                                      p
                                                   
                                                   
                                                      |
                                                      F
                                                      |
                                                   
                                                
                                             
                                             
                                                
                                                   λ
                                                
                                                
                                                   j
                                                
                                             
                                             ·
                                             
                                                
                                                   R
                                                
                                                
                                                   C
                                                
                                             
                                             (
                                             
                                                
                                                   F
                                                
                                                
                                                   j
                                                
                                             
                                             )
                                             -
                                             
                                                
                                                   s
                                                
                                                
                                                   +
                                                
                                             
                                             =
                                             
                                                
                                                   R
                                                
                                                
                                                   C
                                                
                                             
                                             (
                                             
                                                
                                                   F
                                                
                                                
                                                   p
                                                
                                             
                                             )
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      j
                                                      =
                                                      1
                                                      ,
                                                      j
                                                      ≠
                                                      r
                                                      ,
                                                      j
                                                      ≠
                                                      p
                                                   
                                                   
                                                      |
                                                      F
                                                      |
                                                   
                                                
                                             
                                             
                                                
                                                   λ
                                                
                                                
                                                   j
                                                
                                             
                                             ·
                                             
                                                
                                                   CI
                                                
                                                
                                                   C
                                                
                                             
                                             (
                                             
                                                
                                                   F
                                                
                                                
                                                   j
                                                
                                             
                                             |
                                             
                                                
                                                   F
                                                
                                                
                                                   r
                                                
                                             
                                             )
                                             +
                                             
                                                
                                                   s
                                                
                                                
                                                   r
                                                
                                                
                                                   -
                                                
                                             
                                             =
                                             
                                                
                                                   θ
                                                
                                                
                                                   s
                                                
                                                
                                                   p
                                                
                                             
                                             ·
                                             
                                                
                                                   CI
                                                
                                                
                                                   C
                                                
                                             
                                             (
                                             
                                                
                                                   F
                                                
                                                
                                                   p
                                                
                                             
                                             |
                                             
                                                
                                                   F
                                                
                                                
                                                   r
                                                
                                             
                                             )
                                             ,
                                             
                                             r
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             |
                                             F
                                             |
                                             ,
                                             
                                             r
                                             
                                             ≠
                                             
                                             p
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   λ
                                                
                                                
                                                   j
                                                
                                             
                                             ⩾
                                             0
                                             ,
                                             
                                             j
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             |
                                             F
                                             |
                                             ,
                                             
                                             j
                                             
                                             ≠
                                             
                                             p
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   +
                                                
                                             
                                             ⩾
                                             0
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   r
                                                
                                                
                                                   -
                                                
                                             
                                             ⩾
                                             0
                                             ,
                                             
                                             r
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             |
                                             F
                                             |
                                             ,
                                             
                                             r
                                             
                                             ≠
                                             
                                             p
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        
                     

It is noted that the “gap” between the inputs and the outputs in DEA rather than the absolute value of the outputs (recall that in our model the output is the value of the class-relevance) is the essential fact that determines the relative efficiency and thus the rankings of DMUs. Therefore, feature with higher relevance score is possibly less efficient and then sorted behind the one with lower relevance score if the conditional independence scores of the former are in general higher than those of the latter. Additionally, the conditional independence score of the “efficient” feature 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                         also cares the pairwise complementarity which may lead to a performance improvement between 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                         and any other “efficient” feature 
                           
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                           
                        , for the “efficient” features are likely to be conditional dependent on each other in general, and 
                           
                              
                                 
                                    CI
                                 
                                 
                                    C
                                 
                              
                              (
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                              |
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                              )
                              ≪
                              
                                 
                                    R
                                 
                                 
                                    C
                                 
                              
                              (
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         possibly derives 
                           
                              SU
                              (
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                              ;
                              C
                              |
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                              )
                              >
                              SU
                              (
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                              ;
                              C
                              )
                           
                         and hence implies that 
                           
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                           
                         has the ability to increase the relevance between 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                         and the class.
                           Algorithm 1
                           DEAFS: DEA-based Feature Selection algorithm
                                 
                                    
                                 
                              
                           

We show our DEA-based Feature Selection method (DEAFS) in Algorithm 1. The pseudo code of DEAFS can be divided into two parts: For loops part (lines 2–13, 5–10) and while loop part (lines 14–19). The first for loop (lines 2–13) makes DEAFS totally calculate SU 
                           
                              |
                              F
                              |
                           
                         times, and the second for loop (lines 5–10) makes it calculate SU 
                           
                              |
                              F
                              |
                              -
                              1
                           
                         times at each iteration of the first for loop. Hence the total number of the calculation of SU is 
                           
                              |
                              F
                              
                                 
                                    |
                                 
                                 
                                    2
                                 
                              
                           
                        . For while loop part, it is straightforward that DEAFS takes 
                           
                              δ
                              |
                              F
                              |
                           
                         iterations to get the top 
                           
                              δ
                           
                         features. And if 
                           
                              δ
                           
                         is not so small, we may first order 
                           
                              F
                           
                         in descending 
                           
                              
                                 
                                    θ
                                 
                                 
                                    s
                                 
                                 
                                    ∗
                                 
                              
                           
                         value with the time complexity of 
                           
                              O
                              (
                              |
                              F
                              |
                              log
                              |
                              F
                              |
                              )
                           
                         and then directly select the top 
                           
                              δ
                           
                         features (
                           
                              O
                              (
                              1
                              )
                           
                        ). Hence, the iteration complexity of DEAFS is 
                           
                              O
                              (
                              |
                              F
                              
                                 
                                    |
                                 
                                 
                                    2
                                 
                              
                              +
                              δ
                              |
                              F
                              |
                              )
                           
                         or 
                           
                              O
                              (
                              |
                              F
                              
                                 
                                    |
                                 
                                 
                                    2
                                 
                              
                              +
                              |
                              F
                              |
                              log
                              |
                              F
                              |
                              )
                           
                        , namely 
                           
                              O
                              (
                              |
                              F
                              
                                 
                                    |
                                 
                                 
                                    2
                                 
                              
                              )
                           
                        .

The computational complexity of DEAFS refers to the implementation of SU as well as Linear Programming (LP) solving. According to [33], MI and Information Entropy (IE) which take the forms of 
                           
                              I
                              (
                              X
                              ;
                              Y
                              )
                           
                         and 
                           
                              H
                              (
                              X
                              )
                           
                         (
                           
                              H
                              (
                              X
                              |
                              Y
                              )
                           
                        ) respectively can all be implemented within linear time; CMI which takes form of 
                           
                              I
                              (
                              X
                              ;
                              Y
                              |
                              Z
                              )
                           
                         (where 
                           
                              X
                              ,
                              Y
                              ,
                              Z
                           
                         are discrete features rather than feature sets) can be implemented with the time complexity of 
                           
                              O
                              (
                              
                                 
                                    r
                                 
                                 
                                    ¯
                                 
                              
                              N
                              )
                           
                        , where 
                           
                              
                                 
                                    r
                                 
                                 
                                    ¯
                                 
                              
                           
                         is the upper bound of the number of value assignments of each feature and N is the number of samples in the dataset. And if 
                           
                              
                                 
                                    r
                                 
                                 
                                    ¯
                                 
                              
                           
                         is treated as a constant, it is reduced to linear time complexity. Since SU is made up of MI (CMI) and IE (Eqs. (1) and (2)), the time complexity for the implementation of SU is 
                           
                              O
                              (
                              
                                 
                                    r
                                 
                                 
                                    ¯
                                 
                              
                              N
                              )
                           
                        . For LP solving, the interior algorithm can be applied in DEAFS to guarantee polynomial time complexity, e.g. Karmarkar’s famous algorithm [44] with the computational complexity of 
                           
                              O
                              (
                              
                                 
                                    n
                                 
                                 
                                    3.5
                                 
                              
                              L
                              )
                           
                        , where n is the dimension of variables and L is the bi-length of data and in DEAFS we have 
                           
                              n
                              =
                              |
                              F
                              |
                           
                        . Since the LP solver will be called 
                           
                              |
                              F
                              |
                           
                         times during the first for loop, the worst-case computational complexity of DEAFS is 
                           
                              O
                              (
                              |
                              F
                              
                                 
                                    |
                                 
                                 
                                    4.5
                                 
                              
                              L
                              +
                              |
                              F
                              
                                 
                                    |
                                 
                                 
                                    2
                                 
                              
                              N
                              
                                 
                                    r
                                 
                                 
                                    ¯
                                 
                              
                              )
                           
                        .

In this section, we empirically evaluate the performance of proposed method by comparing it with the most representative and well-performed feature ranking methods (CMIM [19], mRMR [9], and ReliefF [45]) and feature subset selection methods (CFS [18] and FCBF [1]). ReliefF is a well-known distance-based feature ranking method that searches nearest neighbors of samples for each class label and then weights features in terms of how well they differentiate samples for different class labels. For ReliefF, we use 5 neighbors and 30 instances throughout the experiments as suggested by [45]. For CFS, we apply the default search approach in Weka, i.e. Best-First approach to search candidate features. For FCBF, we set the predefined threshold 
                        
                           γ
                           =
                           0
                        
                      as suggested by [1]. We choose Weka (Waikato environment for knowledge analysis) [46] as the classification platform. Since CFS, FCBF, and ReliefF have already been integrated in Weka, we directly call them in Weka to generate datasets with their selected features before classification. CMIM and mRMR are implemented in MATLAB and C++ from a toolbox called FEAST [36], so we first run them in MATLAB to get the ranking lists of features and then we write transformation and data-generation programs that can be integrated in Weka and apply them to generate datasets with their selected features. Proposed method DEAFS is implemented in MATLAB and C++ and the process for the acquisition of datasets with its selected features is similar to CMIM and mRMR. All experiments are conducted on a 2.40GHz CPU, 2GB RAM personal computer.

Twelve well-known datasets
                           4
                           
                              http://archive.ics.uci.edu/ml/.
                        
                        
                           4
                         are selected in our experiments for performance validation. Table 1
                         summarizes general information about them. Note that there are some datasets only containing continuous features and some containing both continuous and discretized ones. For these datasets, we take a supervised discretization method called MDL [47] to convert continuous features to discretized ones before feature selection and classification.

Classification performance test is an effective and reliable way for feature selection validation. And classification accuracy is one of the most frequently used metrics applied in the test. It is defined as
                           
                              
                                 Acc
                                 .
                                 =
                                 
                                    
                                       #
                                       samples
                                       
                                       correctly
                                       
                                       classified
                                    
                                    
                                       #
                                       all
                                       
                                       samples
                                    
                                 
                                 .
                              
                           
                        In our experiments, four famous and most frequently used classifiers – Naïve Bayesian Classifier (NBC) [46], k-Nearest Neighbor (kNN) [48], C4.5 decision tree [5], and Support Vector Machine (SVM) [49] are adopted to generate classification accuracy on the datasets with selected features preprocessed by different feature selection methods. As for parameter settings, we apply default parameters for each classifier in Weka, i.e. 
                           
                              k
                              =
                              1
                           
                         for kNN, 
                           
                              C
                              =
                              0.25
                              ,
                              
                              M
                              =
                              2
                           
                         for C4.5, and Gaussian RBF kernel with 
                           
                              C
                              =
                              1
                           
                         and 
                           
                              γ
                              =
                              0
                           
                         for SVM.

In order to conduct comparison among feature ranking methods and feature subset selectors, we take the top 
                           
                              1
                              ,
                              …
                              ,
                              m
                           
                         selected features as the feature subsets respectively, and report the best classification result for each feature ranking methods. We directly report the result on the selected subset for each feature subset selector. The upper bound of m in our experiments is set to be
                           
                              
                                 
                                    
                                       
                                          
                                             m
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       1
                                    
                                 
                                 =
                                 
                                    min
                                 
                                 
                                    
                                       
                                          50
                                          ,
                                          
                                             
                                                
                                                   
                                                      
                                                         |
                                                         F
                                                         |
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        Recall that 
                           
                              F
                           
                         is the original feature set of the dataset. For example, for audiology with 69 features, 
                           
                              
                                 
                                    
                                       
                                          m
                                       
                                       
                                          ¯
                                       
                                    
                                 
                                 
                                    1
                                 
                              
                              =
                              min
                              {
                              50
                              ,
                              34
                              }
                              =
                              34
                           
                         and the selected features for feature ranking methods should be no more than 34; for musk2 with 166 features, 
                           
                              
                                 
                                    
                                       
                                          m
                                       
                                       
                                          ¯
                                       
                                    
                                 
                                 
                                    1
                                 
                              
                              =
                              min
                              {
                              50
                              ,
                              83
                              }
                              =
                              50
                           
                        . It is noted that the comparison between feature ranking and feature subset selection methods is somewhat unfair, because for feature ranking methods, the subset with the best classification result can be manually chosen with respect to the rankings, whereas for feature subset selectors, final subset is automatically chosen without consulting the classifier. Consequently, this comparison is only conducted to show the feasibility of our proposed method. 10-fold cross validation is conducted ten times and the average classification result is used to increase the statistical significance as well as to achieve impartial result. Similar to [1], we use paired two-tailed t-test to determine the statistical significance of the difference of the results. The significance level of 0.05 is applied throughout our experiments.

In addition, we also conduct experiments only for feature ranking methods to compare and analysis classification results on different selected features with the upper bound of m defined as
                           
                              
                                 
                                    
                                       
                                          
                                             m
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       2
                                    
                                 
                                 =
                                 
                                    min
                                 
                                 {
                                 60
                                 ,
                                 |
                                 F
                                 |
                                 }
                                 .
                              
                           
                        We only conduct 10-fold cross validation one time and report the average results in this part of experiments.

@&#RESULTS AND DISCUSSION@&#


                        Tables 2–5
                        
                        
                        
                         show the average accuracy of NBC, kNN, C4.5, and SVM on twelve datasets over ten-time 10-fold cross validation, respectively. Notation“
                           
                              •
                           
                        ” (“
                           
                              ∘
                           
                        ”) denotes that the classification result with current feature selector is significantly better (worse) than proposed method (the corresponding “DEAFS” column) in paired two-tailed t-test with significant level 0.05. Bold value in each row shows the best classification result among six feature selection methods. The last row “W/T/L” records the wins/ties/losses in accuracy comparing current feature selector with proposed method over all datasets.

Results in Table 2 show the performance of Naïve Bayesian classifier after the process of six feature selection methods. It is clear that all of the selectors except ReliefF maintain or improve the accuracy of NBC in most cases. In addition, we can see that DEAFS surpasses other five selectors in general. As an illustration, the W/T/L of the two feature subset selection methods CFS and FCBF is 1/1/10, where they are significantly inferior to DEAFS on eight datasets. Although this is partially due to the unfair comparison between feature ranking methods and feature subset selection methods as mentioned previously, it still shows that our proposed method is feasible and acceptable. For feature ranking methods, ReliefF performs worst with a W/T/L of 0/1/11, and is significantly inferior to DEAFS on seven datasets. CMIM and mRMR perform a little bit better than ReliefF, but obtain lower accuracies than DEAFS on six and eight datasets, respectively, and both obtain significantly lower accuracies on five datasets. For DEAFS, it achieves most cases with best performance and the highest average accuracy (89.36%). Results in Tables 3–5 similar to Table 2 show that the superiority of DEAFS is pronounced, since it always achieves more cases with best performance and higher average classification accuracy than any other selector. We also observe from the tables that the average numbers of selected features of DEAFS (for NBC 17.3, kNN 15.9, C4.5 16.5, and SVM 15.8) are all at a mean-level among six selectors. In summary, DEAFS is superior to other five selectors and is proved to be a sound feature selection method according to their best selected features.

It is noted that Azadeh et al. [50] have also paid attention to the combination of feature selection and DEA. They select attributes (features) of personnel efficiency based on rough set theory and Artificial Neural Network (ANN) technique and choose the best attribute subset (feature subset) as the evaluation indices of personnel efficiency. They argue that using one error measure alone may not be appropriate in ranking these subsets, so they utilize multi-index evaluation method to achieve the selection task: They apply a DEA method with several error measures as the inputs and a constant as the output to select the most “efficient” subset as the final evaluation indices of personnel efficiency. Obviously, the feature selection method they employ is a feature subset selection method because it generates (searches) candidate feature subsets based on rough set theory. In addition, it is also a wrapper method since it is strongly related to the classifier (predictor) it uses (in their work the classifier (predictor) is ANN). Compared with their method, our proposed method is a feature ranking method and a filter one since it employs MI as the metric rather than a specific classifier. Thus, our method tends to be more general as it is classifier-irrelevant.

To illustrate the effect of changes of the performance with respect to an incremental scale of selected features, we show the classification results of NBC, kNN, C4.5 and SVM on molecular, lymphography, flags, mfeat-fourier, optdigits, and musk2 preprocessed by four selected feature ranking methods, namely CMIM, mRMR, ReliefF and proposed DEAFS. Since Tables 2–5 show that classification performances of NBC, kNN, C4.5, and SVM vary a lot on a certain dataset with the selected features, we report for each feature ranking method the average accuracy of the above four classifications on each dataset, in such a way as to reduce the possible bias caused by specific classifiers. These results are shown in Figs. 3–8
                        
                        
                        
                        
                        
                        , where the X axis depicts the consecutive numbers of selected features, and Y axis depicts the average classification accuracy of the four classifiers.

Results shown in Figs. 3–8 illustrate that proposed DEAFS is effective in most cases. Specifically, DEAFS performs better particularly on flags and musk2, and is comparable to CMIM, mRMR, and ReliefF on other datasets. ReliefF performs worst among the four selectors in most cases. CMIM and mRMR perform better than ReliefF since they consider feature redundancy. But in some cases, e.g. on flags, musk2, they seem to be not so effective. For CMIM, the possible reason is that it only considers pairwise rather than k-wise (
                           
                              k
                              ⩾
                              3
                           
                        ) relation among features. For mRMR, it may be due to the shortages of unsupervised redundancy analysis. We also find that performance of DEAFS is not outstanding and sometimes inferior to CMIM and mRMR with the first few selected features in some cases. This possibly lies in that DEAFS is very likely not to select the most relevant features at the beginning since the relative efficiency measured by DEA only cares the ‘gap’ between the inputs and the outputs of the evaluation system. In addition, unreasonable solutions from super-efficiency method may also cause this phenomenon: Optimal value 
                           
                              
                                 
                                    
                                       
                                          θ
                                       
                                       
                                          s
                                       
                                       
                                          p
                                       
                                    
                                 
                                 
                                    ∗
                                 
                              
                           
                         from super-efficiency model may correspond to multiple optimal solutions from its duality model (DEA multiplier model), and thus correspond to different rankings. Some unreasonable solutions (e.g. zero solution) may obstacle the observation of redundancy distribution and hence influence the effectiveness of redundancy analysis. To partially avoid unreasonable solutions, idea of cross-efficiency [51] may be introduced to refine DEAFS in future work. In addition, CCR model that DEAFS applies to implement super-efficiency evaluation evaluates DMUs in a Debreu–Farrell input-oriented manner and thus neglects slacks for both inputs and outputs. It is a kind of radial measures and features may be inaccurately evaluated for it never considers trade-offs between inputs or between outputs.

@&#CONCLUSIONS AND FUTURE WORK@&#

In this paper, we regard feature selection as an evaluation system with multiple inputs and outputs and propose a DEA-based feature selection framework: Properties of features such as relevance and redundancy are taken as the inputs and the outputs of the evaluation system and a DEA method is then applied to evaluate the relative efficiency of the features and then to achieve feature rankings according to the efficiency values. To illustrate the effectiveness of this framework, we propose a feature selection method with super-efficiency DEA, which employs two important feature properties namely conditional independence and relevance to establish the evaluation system. For salient features should have strong discriminative power which would not be impaired given other features, we take the conditional independence between the currently-evaluated feature 
                        
                           
                              
                                 F
                              
                              
                                 p
                              
                           
                        
                      (the DMU) and the class given every other feature in the feature space as the input and take the relevance between 
                        
                           
                              
                                 F
                              
                              
                                 p
                              
                           
                        
                      and the class as the output of the evaluation system, and apply super-efficiency DEA technique to get feature rankings. We evaluate our method by comparing it with the most representative feature ranking and subset selection methods through several classification experiments. It can be verified by the results that proposed method is feasible and superior to other feature selection methods in most cases.

However, there still exist some issues in our feature selection method needed to be improved. Since the LP solver integrated in MATLAB runs very slowly, it is really time-consuming for our method to deal with an even medium-sized dataset. Advanced optimizers such as Gurobi and CPLEX which solve large-scale LP problems more efficiently will be considered in our future work. It is also noted that a rule-of-thumb in DEA [52] suggests that the number of DMUs should be at least three times the total number of inputs plus outputs used in the models. In model (4), this rule can be shown as 
                        
                           n
                           ⩾
                           3
                           ×
                           (
                           m
                           +
                           s
                           )
                        
                     . However in our method, the number of inputs (
                        
                           |
                           F
                           |
                           -
                           1
                        
                     ) is nearly the same to that of DMUs (
                        
                           |
                           F
                           |
                        
                     ) and hence the results may not perform well in some cases. Meanwhile, alternative non-radial DEA measures that estimate technical inefficiency a la Pareto–Koopmans (e.g. the Enhanced Russell Graph [53], RAM [54], and BAM [55]) are possible to be applied in our feature selection algorithms to make trade-offs between inputs or between outputs. In addition, excessively focusing on the ‘gap’ between the inputs and the outputs and unreasonable solutions of super-efficiency methods are also needed to be improved in our future work.

@&#ACKNOWLEDGEMENTS@&#

The authors are grateful to the editor and three anonymous referees for their valuable and constructive comments. The first author would like to thank Dr. Xinyue Tong for her insightful comments on this paper. This work is partially supported by the National Natural Science Foundation of China (71320107001), the Fundamental Research Funds for the Central Universities, HUST (CXY12Q044 and CXY13Q035), and the Graduates’ Innovation Fund of Huazhong University of Science & Technology (HF-11-20-2013).

@&#REFERENCES@&#

