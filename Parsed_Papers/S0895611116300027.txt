@&#MAIN-TITLE@&#Curvelet initialized level set cell segmentation for touching cells in low contrast images

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A method to segment touching cells in very low contrast cell images has been proposed.


                        
                        
                           
                           To improve contrast multiscale top-hat transform and h-maxima has been proposed.


                        
                        
                           
                           Curvelet initialized modified Chan–Vese model has been proposed for segmentation.


                        
                        
                           
                           The enhancement results of the proposed method have been verified using PSNR.


                        
                        
                           
                           Accuracy, precision and sensitivity validate the proposed segmentation method.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Cell segmentation

Multiscale top hat transform




                     h-maxima

Curvelets

Level sets

@&#ABSTRACT@&#


               
               
                  Cell segmentation is an important element of automatic cell analysis. This paper proposes a method to extract the cell nuclei and the cell boundaries of touching cells in low contrast images. First, the contrast of the low contrast cell images is improved by a combination of multiscale top hat filter and h-maxima. Then, a curvelet initialized level set method has been proposed to detect the cell nuclei and the boundaries. The image enhancement results have been verified using PSNR (Peak Signal to noise ratio) and the segmentation results have been verified using accuracy, sensitivity and precision metrics. The results show improved values of the performance metrics with the proposed method.
               
            

@&#INTRODUCTION@&#

The biological cell studies rely on the analysis of large cell clusters with the help of microscopy imaging [1,2]. When the goal is to study phenomena of the live cells, fluorescence microscopy is commonly used as it allows biologists to experiment on the live cells with high sensitivity and specificity. Cell image analysis provides information about the cell characteristics and the dynamic behavior of the cells. The complexity of performing the cell analysis increases in touching cell images. Manual processing of such data is time consuming and error prone, creating a demand for automated techniques [3,4]. Thus, automation of cell structure analysis is becoming a necessity for describing biological processes. Cell segmentation is a crucial and basic processing step in many biological image applications. Automatic segmentation of live cells is more time efficient as compared to time consuming manual processes. To segment different kinds of cell images perfectly, many segmentation methods have been reported in literature. In recent years, active contour model has gained much importance for image segmentation. Active contour model was first proposed in [5], in which the energy guided by the internal and external image forces is minimized. The limitation of this model is that it fails to give good results when the topology of the test structure changes.

An extension of the active contour method was introduced to overcome its limitations. The improvement led to the level set method for capturing moving fronts [6], in which the interface is represented implicitly by its introduction into a domain of one higher dimension level set function. In the level set method, contours or surfaces are represented as the zero level set of a higher dimensional function, called a level set function. In image processing applications, the level set method was introduced independently by Caselles et al. [5] and Malladi et al. [6]. After the work by Osher and Sethian in [7], the level set method became well known and since then has been widely used in many applications. A desirable advantage of level set method is that the changes in topology of a surface can be tracked naturally. Another advantage is that there is no need to parametrize the points on the contour and the numerical computations can be performed on a fixed cartesian grid.

Level set methods are broadly divided into two classes: edge based [5] and region based [7]. Region based level set methods have outperformed edge based models in terms of factors such as robustness to initial conditions and boundary leakage problems in images with weak edges [7]. But it is difficult to accurately initialize level sets. Liasis et al. [8] proposed K-means presegmentation algorithm to develop the initial contours. But the method incorporates subject knowledge prior to the application of Chan and Vese (CV) model [9]. Hence this algorithm is not time efficient. Ouyang et al. [10] proposed a fast marching based labeling approach and a modification of the signature function to eliminate the numerical instability. But this approach is computationally efficient only in a small range around the boundary data. Savelonas et al. [11] proposed an initialization of the level set by introducing a level-set surface of cones centered at regional intensity maxima positions but this initialization process used histogram equalization and morphological processing to further refine the results. Renbo et al. [12] proposed a fast and accurate initialization algorithm based on the vector distance transform, by propagating a vector with the coordinates of the nearest pixel of the object. A new labeling method, based on the flood fill, was proposed to separate the inside and outside of the 2D closed active contour. Inspired by Padfield et al. [13] Yang et al. [14] used wavelet initialized level set segmentation. As wavelets extract only horizontal, vertical and diagonal information of an image, these three directions were unable to capture image information in other directions. The motivation behind the proposed work is that as curvelets are capable of detecting image information along curves, extracting the curvelet coefficients at the desired scales and directions will help to initialize the level set model of the cell image more accurately.

Most of the cell segmentation methods reported in literature give good results in images with no cell crowding, but fail to separate touching cell boundaries. Marker controlled watershed algorithm [15] used rule based criterion to merge oversegmented cell regions but human intervention is required to derive a generalized rule for merging the oversegmented regions. To segment clustered cells, a double threshold-based watershed was proposed in [16]. The algorithm also introduced a feedback loop to correct errors. This algorithm is based on the assumption that all cells are of same type. Kothari et al. [17] proposed a semiautomatic method for touching cell segmentation by introducing concavity detection at the cluster edges to find the overlapping region between two nuclei. As ellipse-fitting algorithm was also used to refine the results but the algorithm failed to accurately detect the cells of other shapes. Chen et al. [18] proposed a geodesic distance based clustering algorithm to segment densely touching cells which gives improved segmentation accuracy of the cell boundaries. But the algorithm incorporates an adaptive learning scheme to adjust the clustering centers.

The proposed work attempts to segment touching cells using a curvelet initialized modified Chan Vese model.

@&#MOTIVATION@&#

The curvelet transform was introduced to provide a multiscale representation in images. It was an improvement over the wavelet and ridgelet transforms to tackle with the unusual phenomena occurring along curved edges in images [19]. It is a generalization of the wavelet transform and aids in representing images at different scales and orientations (angles). Curvelets have frequency and time properties of wavelets but are capable of showing very high degree of directionality and anisotropy. Moreover, the singularities can be well approximated by very few curvelet coefficients. Hence, the curvelet transform was introduced by Candes et al. [19] to overcome the existing limitations of the traditional multiresolution techniques. The curvelet transform represents the curved edges of images much more efficiently as compared to the wavelets and ridgelets.

The curvelet transform also allows the sparse representation of objects optimally with C
                        2 singularities. The best n−term approximation i
                        
                           n
                         for a smooth object i with C
                        2 discontinuities along a generic smooth curve by wavelet thresholding obeys
                           
                              (1)
                              
                                 
                                    
                                       
                                          ∥
                                          i
                                          −
                                          
                                             i
                                             n
                                          
                                          ∥
                                       
                                       2
                                       2
                                    
                                    ≈
                                    
                                       n
                                       
                                          −
                                          1
                                       
                                    
                                 
                                 ,
                              
                           
                        whereas curvelet approximation 
                           
                              i
                              n
                              c
                           
                         achieves
                           
                              (2)
                              
                                 
                                    
                                       
                                          ∥
                                          i
                                          −
                                          
                                             i
                                             n
                                          
                                          ∥
                                       
                                       2
                                       2
                                    
                                    ≈
                                    
                                       Cn
                                       
                                          −
                                          2
                                       
                                    
                                    
                                       
                                          (
                                          log
                                          
                                          n
                                          )
                                       
                                       3
                                    
                                 
                                 .
                              
                           
                        where the symbol 
                           
                              
                                 ∥
                                 ∥
                              
                              2
                              2
                           
                         represents the squared l
                        2 norm and ≈ represents the approximation. The above equation provides the optimal solution as no other multiscale representation can represent a system with the same number of terms. [19].

The curvelet transform has undergone two major revisions. It was first introduced in [19] by Candes and Donoho in 2000. That version performed the ridgelet analysis of the radon transform of an image. The performance of that method was very slow; hence a new version of the curvelet transform was developed in [20]. This version implements a tight frame expansion and has a much lower redundancy. Unlike the first version, this implementation does not use ridgelets leading to a faster algorithm.

The improved version of curvelet transform is known as Fast Discrete Curvelet Transform (FDCT). The FDCT is faster, simpler and less redundant than the curvelet transform. Candes et al. [20] introduced FDCT in two forms, the wrapping version and the unequally spaced FFT (USFFT) version. The two versions of the FDCT differ by the selection of the spatial grid used to translate curvelets at each scale and orientation. The USFFT version is only approximately invertible whereas the wrapping version is much faster and invertible upto numerical position. Hence we have used the wrapping version in this paper. The Discrete Fourier Transform of an image with intensity values given by the function I(x, y), x
                        =0, 1, …, N
                        1
                        −1, y
                        =0, 1, …, N
                        2
                        −1 is given by
                           
                              (3)
                              
                                 I
                                 (
                                 
                                    n
                                    1
                                 
                                 ,
                                 
                                    n
                                    2
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       y
                                       =
                                       0
                                    
                                    
                                       
                                          N
                                          2
                                       
                                       −
                                       1
                                    
                                 
                                 
                                    ∑
                                    
                                       x
                                       =
                                       0
                                    
                                    
                                       
                                          N
                                          1
                                       
                                       −
                                       1
                                    
                                 
                                 I
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 
                                    e
                                    
                                       −
                                       2
                                       π
                                       i
                                       (
                                       
                                          n
                                          1
                                       
                                       x
                                       /
                                       
                                          N
                                          1
                                       
                                       +
                                       
                                          n
                                          2
                                       
                                       y
                                       /
                                       
                                          N
                                          2
                                       
                                       )
                                    
                                 
                              
                           
                        The discrete curvelet transform is a decomposition of the image into the curvelet coefficients c
                        
                           jlk
                        , such that
                           
                              (4)
                              
                                 I
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    J
                                 
                                 
                                    ∑
                                    
                                       l
                                       =
                                       0
                                    
                                    
                                       
                                          L
                                          j
                                       
                                       −
                                       1
                                    
                                 
                                 
                                    ∑
                                    
                                       
                                          k
                                          1
                                       
                                       =
                                       0
                                    
                                    
                                       
                                          K
                                          
                                             jl
                                             ,
                                             1
                                          
                                       
                                       −
                                       1
                                    
                                 
                                 
                                    ∑
                                    
                                       
                                          k
                                          2
                                       
                                       =
                                       0
                                    
                                    
                                       
                                          K
                                          
                                             jl
                                             ,
                                             2
                                          
                                       
                                       −
                                       1
                                    
                                 
                                 
                                    c
                                    jlk
                                 
                                 
                                    φ
                                    jlk
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                              
                           
                        where k
                        =(k
                        1, k
                        2) and φ
                        
                           jlk
                         is the curvelet on level j with direction l and spatial shift k. The discrete curvelet transform thus provides a decomposition of the image I into J detail levels, with L
                        
                           j
                         directions on each level, and K
                        
                           jl,1
                        ×
                        K
                        
                           jl,2 spatial shifts for each of these directions. The curvelet coefficients c
                        
                           jlk
                         are computed as
                           
                              (5)
                              
                                 
                                    c
                                    jlk
                                 
                                 =
                                 〈
                                 I
                                 ,
                                 
                                    φ
                                    jlk
                                 
                                 〉
                              
                           
                        where the symbol 〈 〉 represents the inner product and φ
                        
                           jlk
                         is a discrete curvelet function. Wrapping based curvelet transform is a multiscale pyramid with several subbands at different scales with different orientations and positions in the frequency domain. At higher scale, curvelets look like a needle shaped element. At lower scales, curvelets are non directional coarse elements. Fig. 1
                         shows the spatial and frequency response of a curvelet.

One of the most widely used region-based level set method is the active contour without edges (CV model) [9]. Rather than finding the gradients on the boundaries Chan and Vese proposed this model using the region information of an image. Recently, a geometric active contour model was introduced in [21], which was derived from the mean curvature motion with a region-based external force. To minimize the effect of intensity inhomogeneity to image segmentation, a region-based level set (RBLS) model was proposed in [22]. It used both global and local image information.

Region-based level set methods have many advantages over edge-based methods. As region based models use the internal and external areas of the contour to control the curve evolution, which is less sensitive to noise, they obtain better results for images with weak edges or without edges. They are also less sensitive to the position of the initial contour, and can detect the exterior and interior boundaries simultaneously [23]. Thus, though the foreground and background may be heterogeneous in some images, accurate segmentation results can be achieved [24]. Therefore, in this paper the region based model has been used. CV model [25] is based on the total squared difference of intensities of the points inside and outside of the contour. Hence, the boundaries are always easily detected in case of noisy images. However, CV model has some limitations also. It is highly influenced by the initial position of the contour and it always falls into a local solution due to an improper initialization [26]. As a result, some low contrast images cannot be segmented out successfully by an improper initial position. As initialization is a major challenge in CV model, a new level set initialization method has been proposed in this paper.

@&#PROPOSED METHOD@&#

As the cell images are very low contrast images, the first aim is to improve the contrast of the cell image. In order to locate the nuclei of the cells, an assumption is made that the cell nuclei is the brightest feature in the image. However, because of intensity inhomogeneities in the cell image, the cell nuclei may appear darker than the other areas of the cell image. A basic idea is to enlarge the contrast between the white and black regions of the cell image. This is achieved by summing all the bright image regions of the cell image to the original image and subtracting the dark image regions from the result obtained [27] as given below.
                           
                              (6)
                              
                                 
                                    I
                                    en
                                 
                                 =
                                 I
                                 +
                                 WTH
                                 −
                                 BTH
                              
                           
                        where I
                        
                           en
                         is the enhanced image of the original image I. WTH and BTH are the white top hat transform and black top hat transform respectively given as:
                           
                              (7)
                              
                                 
                                    WTH
                                    
                                       (
                                       x
                                       ,
                                       y
                                       )
                                    
                                 
                                 =
                                 I
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 −
                                 I
                                 ∘
                                 
                                    B
                                    
                                       (
                                       x
                                       ,
                                       y
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    
                                       BTH
                                       
                                          (
                                          x
                                          ,
                                          y
                                          )
                                       
                                    
                                    =
                                    I
                                    •
                                    
                                       B
                                       
                                          (
                                          x
                                          ,
                                          y
                                          )
                                       
                                    
                                    −
                                    I
                                    (
                                    x
                                    ,
                                    y
                                    )
                                 
                                 ,
                              
                           
                        where B
                        (x,y) is the structuring element and
                           
                              (9)
                              
                                 I
                                 ∘
                                 
                                    B
                                    
                                       (
                                       x
                                       ,
                                       y
                                       )
                                    
                                 
                                 =
                                 (
                                 I
                                 ⊖
                                 
                                    B
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                                 )
                                 ⊕
                                 B
                              
                           
                        
                        
                           
                              (10)
                              
                                 I
                                 •
                                 
                                    B
                                    
                                       (
                                       x
                                       ,
                                       y
                                       )
                                    
                                 
                                 =
                                 (
                                 I
                                 ⊕
                                 
                                    B
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                                 )
                                 ⊖
                                 B
                              
                           
                        represent the opening and closing operations respectively. i
                        ⊖
                        B represents the erosion of I by structuring element B which is the set of all points z such that B translated by z is contained in A. I
                        ⊕
                        B represents the dilation of I by structuring element B which is the set of all displacements, z such that the reflection of B about its origin and A overlap by at least one element. As opening smooths the white regions of image corresponding to the size of the used structuring element, WTH is usually used to extract white image regions. Similarly, closing smooths the black regions of image corresponding to the size of the used structuring element, BTH is usually used to extract black image regions [28]. According to the definition of WTH and BTH, the top hat filter only enlarges the contrast between the white and black regions of the image at the scale corresponding to the size of the used structuring element in WTH and BTH. The white and black image features may exist in different scales of image and therefore can be extracted using different structuring elements. If the white and black features of the image are extracted using different structuring elements, and used in expression of the top hat filter, the original image can be enhanced more efficiently [29]. To extract the regions of interest at all the scales, multiscale structuring elements are used for enhancing the low contrast cell images. At each scale, the extracted white image regions are brighter than other image regions, which means the gray values of the extracted white image regions should be large. So, the real white image regions of all scales should be the large gray values of all scales. Then, the extracted white multi scale image regions at all scales could be the maximum gray values of all scales as follows:
                           
                              (11)
                              
                                 
                                    I
                                    c
                                    w
                                 
                                 =
                                 
                                    max
                                    
                                       1
                                       ≤
                                       i
                                       ≤
                                       n
                                    
                                 
                                 
                                    
                                       WTH
                                       i
                                    
                                 
                              
                           
                        Similarly the black image regions are extracted at all the scales
                           
                              (12)
                              
                                 
                                    I
                                    c
                                    b
                                 
                                 =
                                 
                                    max
                                    
                                       1
                                       ≤
                                       i
                                       ≤
                                       n
                                    
                                 
                                 
                                    
                                       BTH
                                       i
                                    
                                 
                              
                           
                        The low contrast cell image is enhanced by introducing the WTH and BTH at all the scales in the top hat transform as follows:
                           
                              (13)
                              
                                 
                                    I
                                    en
                                 
                                 =
                                 I
                                 +
                                 
                                    I
                                    c
                                    w
                                 
                                 −
                                 
                                    I
                                    c
                                    b
                                 
                              
                           
                        The above operation produces an image with the cell nuclei region highlighted which improves the contrast of the image. To further separate the cell nuclei and the cell boundaries, a combination of multiscale top hat transform and the h- maxima transform has been proposed. Hence, after performing multiscale top hat filtering on the image, h-maxima transform is applied on the image. The h-maxima transformation suppresses all the region maxima whose height is less than h. This is achieved by reconstructing the image obtained after tophat filtering A
                        
                           t
                         using dilation from the image (A
                        
                           t
                        
                        −
                        h). The h-maxima transformed image is obtained as
                           
                              (14)
                              
                                 
                                    
                                       A
                                       h
                                    
                                    =
                                    
                                       δ
                                       B
                                    
                                    (
                                    
                                       A
                                       t
                                    
                                    −
                                    h
                                    )
                                 
                                 ,
                              
                           
                        where δ
                        
                           B
                         is the l times dilation on A
                        
                           t
                        
                        −
                        h using structuring element B with l
                        ≥1. The image obtained after the preprocessing operation has better contrast and minimizes the detection of false positives (FP) from the cell image background.

The algorithm operates by decomposing the enhanced image into different frequency bands. The Discrete Curvelet Transform (DCUT) is a multi scale directional transform, which allows a sparse representation of objects in an image. Bright objects in the images correspond to large coefficients in the curvelet domain. High frequencies correspond to noise and low frequencies correspond to less varying background. The unwanted details of the image at different scales are ignored keeping only the details at desirable scales. The cell nuclei are detected at a fine scale. The detection of the cell nuclei helps to decide the accurate number of touching cells in the cell image. The cell boundaries are detected at a coarser scale as there is very low intensity difference between the cell boundary and the background due to low contrast of the image. The reconstructed image using curvelet coefficients at the desirable scales yields results that approximately cover the cell locations providing a good initialization to the level set. The level set implementation further improves the segmentation results. After detecting the approximate boundaries of the cells and initilializing the level set with that approximation, the near accurate boundaries and nuclei of the cells are detected (Fig. 2
                        ).

The Discrete Curvelet Transform decomposes an image into a set of frequency components (J) and directional cis omponents (L
                        
                           j
                        ). The region of interest in our case is the cell centre (nucleus) and the cell boundaries. The test images are cervical cell images of touching cells. The nuclei of the cells are extracted at fine scale whereas the boundary of the touching cells is extracted at a coarser scale. In the present experiments, six scale curvelet transform was applied. As cell nuclei are small structures with sharp edges, they are directly located at the finest scale by reducing the coefficients at all the other scales to zero. The nuclei boundaries are extracted using canny edge detector [30]. The boundaries of the three touching cells are extracted at coarser scales as the cell boundary information is present at coarser scales due to low intensity difference between the cell boundaries and the background of the cell images. The boundaries extracted with the help of the curvelet transform are used to initialize the level set method.

As discussed in the previous section, a significant challenge in level sets is the initialization. Accurate initialization is important for level set evolution because the level set algorithm competes with the background and other regions of no interest by using the local intensities of the initial regions.

As the cell centres were accurately detected using curvelet coefficients at a particular scale, the segmentation of the touching cells was performed using the Chan and Vese model [31]. CV model with a prevention term has been used to separate the touching cell boundaries [32]. The energy term E for cell segmentation can be expressed as follows:
                           
                              (15)
                              
                                 E
                                 =
                                 
                                    λ
                                    0
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    N
                                 
                                 ∫
                                 
                                    
                                       |
                                       F
                                       −
                                       
                                          m
                                          i
                                       
                                       |
                                    
                                    2
                                 
                                 
                                 dx
                                 
                                 dy
                                 +
                                 
                                    λ
                                    b
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    N
                                 
                                 ∫
                                 
                                    
                                       |
                                       F
                                       −
                                       
                                          m
                                          b
                                       
                                       |
                                    
                                    2
                                 
                                 
                                 dx
                                 
                                 dy
                                 +
                                 α
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    N
                                 
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    N
                                 
                                 
                                    
                                       R
                                       i
                                    
                                    ∩
                                    
                                       R
                                       j
                                    
                                 
                              
                           
                        where F is the reconstructed images using curvelet coefficients at scales 3 and 4. m
                        
                           i
                         is the mean intensity of cell region and m
                        
                           b
                         is the mean intensity of the background region. λ
                        0 and λ
                        
                           b
                         and α are the weighting parameters. R
                        
                           i
                         and R
                        
                           j
                         represent the regions of cell i and j respectively. Segmentation of the touching cells is achieved by minimizing the energy function E using the evolution of the level set. Using the heaviside function H
                        
                           
                              (16)
                              
                                 
                                    H
                                    ξ
                                 
                                 (
                                 x
                                 )
                                 =
                                 
                                    1
                                    2
                                 
                                 
                                    
                                       
                                          1
                                          +
                                          
                                             2
                                             π
                                          
                                       
                                    
                                 
                                 arctan
                                 
                                 
                                    
                                       
                                          
                                             x
                                             ξ
                                          
                                       
                                    
                                 
                              
                           
                        to express the energy function, the energy function can be minimized by applying the gradient descent method iteratively. Here, ξ is the regulation parameter of the Heaviside function. The evolution equation for the energy function Φ is obtained by deriving the associated Euler–Lagrange equation as
                           
                              (17)
                              
                                 
                                    
                                       ∂
                                       Φ
                                    
                                    
                                       ∂
                                       t
                                    
                                 
                                 =
                                 δ
                                 (
                                 
                                    Φ
                                    i
                                 
                                 )
                                 
                                    
                                       λ
                                       0
                                    
                                    
                                       
                                          |
                                          F
                                          −
                                          
                                             m
                                             i
                                          
                                          |
                                       
                                       2
                                    
                                 
                                 −
                                 
                                    
                                       λ
                                       b
                                    
                                    
                                       
                                          |
                                          F
                                          −
                                          
                                             m
                                             b
                                          
                                          |
                                       
                                       2
                                    
                                 
                                 
                                    ∏
                                    
                                       j
                                       =
                                       1
                                       ,
                                       j
                                       ≠
                                       i
                                    
                                    N
                                 
                                 H
                                 (
                                 
                                    Φ
                                    j
                                 
                                 )
                                 +
                                 α
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                       ,
                                       j
                                       ≠
                                       i
                                    
                                    M
                                 
                                 (
                                 1
                                 −
                                 H
                                 (
                                 
                                    Φ
                                    j
                                 
                                 )
                                 )
                              
                           
                        where t is an artificial (evolution) time parameter. After evolving the contours, the mean intensities of the cell region and the background regions are iteratively updated.

The proposed method has been applied on touching cell images taken from publicly available Herlev database [33]. The Herlev database contains single cell images and touching cell images collected by the Department of Pathology at Herlev University Hospital and the Department of Automation at Technical University of Denmark [34]. The images were acquired at a magnification of 0.201m/pixel. Average image size is 156×140 pixels. Cyto-technicians and doctors manually classified each cell into one of the 7 classes, namely superficial squamous, intermediate squamous, columnar, mild dysplasia, moderate dysplasia, severe dysplasia, and carcinoma in situ. The first three classes correspond to normal cells and the last four classes correspond to abnormal cells. Each cell image also has an associated ground truth of nucleus and cytoplasm regions. The test images are touching cervical cell images which have been acquired from the healthy and cancer effected smears by means of a digital camera and a microscope.

@&#IMPLEMENTATION@&#

The aim in this paper is to segment the nuclei and the outer boundaries of the very low contrast touching cells. To improve the contrast of the input images, a combination of multiscale top hat transform and h− maxima has been applied. Further, as curvelets provide a multiscale representation of an image, it helps to extract the required image information at few selective scales and ignoring the information at other scales. A six scale FDCT via wrapping has been applied on the test images. Firstly, the centres of the touching cells are detected at scale 1. The curvelet coefficients at all the other scales are reduced to zero. The advantage of detecting the cell nuclei is that the count of the cells is known before detecting the cell boundaries as the detection of boundaries in low contrast images is quite challenging. The boundaries of the nuclei of the touching cells are detected by applying the canny edge detector to the curvelet reconstructed image at scale 1. Scale 3 and 4 give better performance than scale 2, 5 and 6 to detect the cell boundaries. This is because the boundary features are present at these scales. The reconstructed images using the curvelet coefficients at scales 3 and 4 are used to initialize the level set function to extract the cell boundaries. As the shape of the cells is irregular in all the test images, the curvelet coefficients at all the scales are computed for all the directions. This helps to efficiently detect any shape of the cell. The parameters of the proposed algorithm were selected as λ
                        0
                        =1, λ
                        
                           b
                        
                        =0.5 and α
                        =0.7. The proposed contrast enhancement has been tested on ten images taken from the Herlev dataset and the segmentation algorithm has been tested on three images taken from the same dataset.

@&#EVALUATION@&#

To evaluate the contrast enhancement using multiscale top hat filtering and the h− maxima transform, Peak signal to noise ratio (PSNR) has been used. [35]. PSNR determines the changes in pixel intensities of the original and the enhanced image and is given by
                              
                                 (18)
                                 
                                    PSNR
                                    =
                                    10
                                    
                                    
                                       log
                                       10
                                    
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      255
                                                      2
                                                   
                                                
                                                MSE
                                             
                                          
                                       
                                    
                                 
                              
                           where MSE is the mean squared error and is given by:
                              
                                 (19)
                                 
                                    MSE
                                    =
                                    
                                       1
                                       pq
                                    
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       p
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       q
                                    
                                    ‖
                                    
                                       
                                          I
                                          o
                                       
                                       (
                                       i
                                       ,
                                       j
                                       )
                                       −
                                       
                                          I
                                          e
                                       
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                    
                                       ‖
                                       2
                                    
                                 
                              
                           where I
                           
                              o
                            and I
                           
                              e
                            are the original and enhanced images, respectively. Table 1
                            shows the result of applying the multiscale top hat filter and the h−maxima transform on the Herlev Dataset images using PSNR. The PSNR is evaluated between the original images shown in 3(a), 4(a), 5(a) and enhanced images shown in 3(b), 4(b) and 5(b) respectively. These are the images 1, 2 and 3 in Table 1. For the validation of results the evaluation has been done for other images also taken from the same dataset. The values of the PSNR for the test images show that there is an improvement in contrast of the images as compared to the original images.

To evaluate the segmentation results of the proposed algorithm, three measures have been used: Accuracy, precision and sensitivity [36] as described below:
                              
                                 (20)
                                 
                                    Accuracy
                                    =
                                    (
                                    TP
                                    +
                                    TN
                                    )
                                    /
                                    (
                                    TP
                                    +
                                    FN
                                    +
                                    FP
                                    +
                                    TN
                                    )
                                 
                              
                           
                           
                              
                                 (21)
                                 
                                    Precision
                                    =
                                    TP
                                    /
                                    (
                                    FP
                                    +
                                    TP
                                    )
                                 
                              
                           
                           
                              
                                 (22)
                                 
                                    
                                       Sensitivity
                                       =
                                       TP
                                       /
                                       (
                                       TP
                                       +
                                       FN
                                       )
                                    
                                    ,
                                 
                              
                           where TP (True Positive) show the cell pixels correctly detected. TN show the background pixels correctly detected. FP (False Positive) shows the pixels not belonging to a cell but are detected as cell pixels. FN (False Negative) shows the pixels belonging to a cell but are detected as background pixels.


                           Figs. 3–5
                           
                           
                            show the cell nuclei and cell boundary detection results for dataset 1-A, 1-B and 1-C respectively. The results of the proposed method are compared with the results of the canny edge detector [30], marker controlled watershed algorithm [38], CellProfiler [37] and the wavelet initialized level set method [14]. The canny edge detector thresholds are selected so as to optimize the detection of the cell boundaries. Figs. 6–8
                           
                           
                            compare the output of the proposed method with these algorithms for the test images. It is clear that the proposed method detects the cell nuclei and cell boundaries better than the other state of the art methods. The cell nuclei and cell boundaries are well defined in curvelet initialized level set method due to better directional sensitivity of the curvelet transform. The canny edge detector is unable to detect the cell boundaries in such very low contrast images as shown in Figs. 6(b), 7(b) and 8(b). The marker controlled watershed algorithm leads to oversegmentation of the low contrast cell images. As the watershed algorithm has poor results for touching cell images, it has not been considered in the performance metrics evaluation. The wavelet initialized level set method gives better performance than the Canny edge detector, watershed algorithm and the CellProfiler but the segmentation evaluation metrics show better performance of the proposed method over all these algorithms. As the number of true positives improve with the proposed method, there is an improvement in the values of accuracy, precision and sensitivity metrics. Tables 2–4
                           
                           
                            show improved performance metric results of the proposed method as compared to the state of the art methods.

@&#CONCLUSION@&#

In this paper, a new method for the cell nuclei and cell boundary detection of low contrast touching cell images has been presented. Specific characteristics of low contrast cell images make the cell boundary detection more challenging. A combination of multiscale top hat filter and h-maxima transform has been proposed to improve the contrast of the cell images. A curvelet initialized level set cell segmentation using modified CV model for detecting the touching cell nuclei and cell boundaries has been proposed. As curvelet transform is a multi scale transform with better directional sensitivity, it helps to extract the image detail at various scales and directions according to the features of interest to be extracted. The nuclei of the touching cells are detected at a finer scale and the boundaries of the cells at a coarser scale. The segmentation results show improved accuracy, precision and sensitivity values of the curvelet initialized level set method as compared to the canny edge detector and wavelet initialized level set method. However, there is a need for more contrast enhancement for such type of very low contrast cell images to find the true cell pixels while avoiding false cell pixels so that the segmentation results can be improved.

@&#ACKNOWLEDGMENTS@&#

The authors would like to thank The Management and Decision Laboratory, University of the Aegean for making the HERLEV dataset publicly available.

@&#REFERENCES@&#

