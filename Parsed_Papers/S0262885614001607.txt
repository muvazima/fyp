@&#MAIN-TITLE@&#Robust tracking with interest points: A sparse representation approach

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The proposed tracker combines the flexibility of interest points and robustness of sparse representation.


                        
                        
                           
                           Proposed a robust matching criteria for reliable tracking via L1 minimization


                        
                        
                           
                           The proposed tracker is computationally efficient and provides real-time performance.


                        
                        
                           
                           The tracker is benchmarked with many publicly available complex video sequences.


                        
                        
                           
                           Performance is compared with many recent state of the art trackers using 50 benchmark video dataset.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Visual tracking




                     l




                     1 minimization

Interest points

Harris corner

Sparse representation

@&#ABSTRACT@&#


               
               
                  Visual tracking is an important task in various computer vision applications including visual surveillance, human computer interaction, event detection, video indexing and retrieval. Recent state of the art sparse representation (SR) based trackers show better robustness than many of the other existing trackers. One of the issues with these SR trackers is low execution speed. The particle filter framework is one of the major aspects responsible for slow execution, and is common to most of the existing SR trackers. In this paper,
                        1
                     
                     
                        1
                        An earlier brief version of the paper has appeared in ICIP'13 (R. Venkatesh Babu and P. Priti, “Interest points based object tracking via sparse representation”, in proceedings of International Conference on Image Processing (ICIP), Melbourne, Australia, 2013).
                      we propose a robust interest point based tracker in l
                     1 minimization framework that runs at real-time with performance comparable to the state of the art trackers. In the proposed tracker, the target dictionary is obtained from the patches around target interest points. Next, the interest points from the candidate window of the current frame are obtained. The correspondence between target and candidate points is obtained via solving the proposed l
                     1 minimization problem.
                  In order to prune the noisy matches, a robust matching criterion is proposed, where only the reliable candidate points that mutually match with target and candidate dictionary elements are considered for tracking. The object is localized by measuring the displacement of these interest points. The reliable candidate patches are used for updating the target dictionary. The performance and accuracy of the proposed tracker is benchmarked with several complex video sequences. The tracker is found to be considerably fast as compared to the reported state of the art trackers. The proposed tracker is further evaluated for various local patch sizes, number of interest points and regularization parameters. The performance of the tracker for various challenges including illumination change, occlusion, and background clutter has been quantified with a benchmark dataset containing 50 videos.
               
            

@&#INTRODUCTION@&#

Visual tracking has been one of the key research areas in computer vision community for the past few decades. Tracking is a crucial module for video analysis, surveillance and monitoring, human behavior analysis, human computer interaction and video indexing/retrieval etc. Major challenges for tracking algorithms that arise in real life scenarios are due to both intrinsic and extrinsic factors. Intrinsic factors include pose, appearance and scale changes, and common extrinsic factors are illumination variation, occlusion and clutter.

There have been several proposals for object tracking algorithms in the literature. Based on object modeling, a majority of the trackers can be brought under the following two themes: i) Global object model and ii) Local object model. In the global approach, the object is typically modeled using all the pixels corresponding to the object region or some global property of the object. The simple template based SSD (sum of the squared distance) tracker, color histogram based meanshift tracker [1] and probabilistic tracker [2] are examples of global trackers. Traditional Lucas–Kanade tracker [3], fragment based approaches [4,5] and many bag-of-words model based trackers are some examples of local object trackers.

In global modeling, the features representing the global properties of the object are utilized for modeling. They could be simple template based models or histogram based models or shape based models. The template models carry appearance information of the object from a single view. These models are good for tracking objects whose appearances do not change much over time and not suitable for tracking objects undergoing significant appearance changes, which need frequent model updates. Since image intensity based template models are sensitive to illumination changes, image gradients have been used as a feature [6]. Template matching approach is computationally very expensive due to the brute force search. Efficient template matching methods have been proposed in the literature [7,8]. On the other hand, Comaniciu et al. [1] use the kernel weighted color histogram for modeling the object. Though the spatial information is lost in this model, it is suitable for applying iterative meanshift procedure. This meanshift tracker maximizes the similarity between the target and candidate models by iteratively seeking the mode of the underlying similarity space. Since the meanshift tracker performs gradient ascent over similarity space, it quickly converges to the mode in a couple of iterations and delivers real-time tracking performance. Unlike the template based object model, the histogram based object model provides better robustness to appearance change, since it captures the color configuration of the object rather than the spatial structure. These global object modeling approaches are sensitive to partial occlusion, illumination and scale changes since the model depends on the attributes of entire object region.

On the other hand, local object model uses the information from object parts for tracking. Most of these trackers use bag-of-words model based approaches [9–12,5]. The proposed interest point based tracker falls under the local object model based tracking. Shi and Tomasi showed [13] that the corner-like points are more suitable for reliable tracking due to its stability and robustness to various distortions like rotation, scaling, and illumination. Though the interest-point based trackers show more robustness to various factors like rotation, scale and partial occlusion, the major issues surface from description and matching of interest points between the successive frames. For example, Kloihofer and Kampel [14] use SURF [15] descriptors of interest points as feature descriptors. The object is tracked by matching the object points of the previous frame with candidate points in the current frame. The displacement vectors of these points are utilized for localizing the object in the current frame [16]. A detailed survey on various object tracking methods can be found in [17–19]. Matching the features of interest points between two frames is a crucial step in estimating the correct motion of the object and typically Euclidean distance is used for matching. The recently proposed vision algorithms in sparse representation framework clearly illustrate its superior discriminative ability even with very low dimensional data [20,21]. This motivated us to examine the matching ability of sparse representation approach for interest points.

In this paper, we have proposed a robust interest point based tracker in sparse representation framework. The interest points of the object are obtained from the initial frame by Harris corner detector [22] and a dictionary is constructed from the small image patch surrounding these corner points. The candidate corner points obtained from the search window of the current frame are matched with object points (dictionary) by sparsely representing the candidate corner patches in terms of dictionary patches. The correspondence between the target and candidate interest points is established via the maximum value of the sparse coefficients. A ‘robust matching’ criterion has been proposed for pruning the noisy matches by checking the mutual match between candidate and target patches. The displacement of these matched candidate points indicates the location of the object. Since the dictionary elements are obtained from a very small patch surrounding these corner points, the proposed approach is robust and computationally very efficient compared to the particle filter based l
                     1 trackers [23–25].

The rest of the paper is organized as follows: Section 2 briefly reviews related works. Section 3 explains the proposed tracker in sparse representation framework. Section 4 discusses the results and concluding remarks are given in Section 5.

The concept of sparse representation recently attracted the computer vision community due to its discriminative nature [20]. Sparse representation has been applied to various computer vision tasks including face recognition [20], image video restoration [26], motion segmentation [27], image denoising [28], image compression [29], action recognition [30], super resolution [31], tracking [21] and background modeling [32].

Wright et al. [20] exploited the discriminative nature of sparse representation for face recognition. In place of generic dictionaries, they have used the overcomplete dictionary with the training samples as its base elements. Given sufficient number of training samples for each class, the test sample can be sparsely represented using the training elements of the same class via l
                     1 minimization. The same concept can be effectively used for object tracking, since the correct candidate can be sparsely represented using target dictionary.

The l
                     1 minimization tracker proposed by Mei and Ling [33] uses the low resolution target image along with trivial templates as dictionary elements. The candidate patches can be represented as a sparse linear combination of the dictionary elements. To localize the object in the future frames, the authors use the particle filter framework. Here, each particle is an image patch obtained from the spatial neighborhood of previous object center. The particle that minimizes the projection error indicates the location of object in the current frame. Typically, hundreds of particles are used for localizing the object. The performance of the tracker relies on the number of particles used. This tracker is computationally expensive and not suitable for real-time tracking. Faster version of the above work was proposed by Bao et al. [24], here the l
                     2 norm regularization over trivial templates is added to the l
                     1 minimization problem. However, our experiments show that the speed of the above algorithm is achieved at the cost of its tracking performance. Another faster version was proposed by Li and Shen [34]. Jia et al. [12] used structural local sparse appearance model with an alignment-pooling method for tracking the object. All the above approaches are again implemented in the particle filter framework and run at low frame rate, hence not suitable for real-time tracking. A real-time tracker proposed by Zhang et al. [35] uses a sparse measurement matrix to reduce the dimension of foreground and background samples and uses naive Bayes classifier for classifying object and background.

Feature detection and matching has been an essential component of computer vision for various applications such as image stitching [36] and 3D model construction. The features used for such applications are typically obtained from specific locations of image known as interest points. The interest points (often called as corner points) indicate the salient locations of the image, which are least affected by various deformations and noise. The features extracted from these interest points are often used in various computer vision applications including visual tracking.

Most of the recently developed sparse trackers model the object as templates [23] or local patches [12] without considering the importance of salient locations of the target. Since many densely sampled object patches contain least information, using them for tracking will potentially drift the tracker over time. Further, these non-salient patches will increase the computational cost many times without much benefit.

In this paper, we propose a tracker that utilizes the flexibility of the interest points and robustness of sparse representation for matching these interest points across frames. The overview of the proposed tracker is illustrated in Fig. 1
                     . The proposed tracker models the object as a set of interest points. The interest points are represented by a small image patch surrounding it. The vectorized image patches are l
                     2 normalized to form the columns of the target dictionary (T). In order to localize the object in the current frame, the interest points are obtained from the predefined search window in the current frame. The vectorized image patches of candidate interest points form the candidate dictionary (Y). Each target interest point (object dictionary element) is represented as a sparse linear combination of the candidate dictionary atoms. The discriminative property of sparse representation enables matching between target and candidate patches. For each target patch, the candidate patch whose corresponding coefficient is maximum, is chosen as the match. The green patches shown in Fig. 1 indicate the matched candidate points with the target points (called as ‘1-way’ matched points). The noisy matches are pruned via the ‘robust matching’ criterion by checking the mutual match between candidate and target patches. The mutual correspondence is confirmed by sparsely representing the candidate interest points in terms of target interest points. Only the points that mutually agree with each other were considered for object localization. The red patches shown in Fig. 1 indicate the matched target points with the candidate points selected from the previous step (called as ‘2-way’ matched points). The object location in the current frame is estimated as the median displacement of the candidate interest points. Since the proposed tracker is a point tracker, it is robust to global appearance, scale and illumination changes. Further, the proposed tracker uses only a small patch around the interest points as dictionary elements, making it computationally very efficient.

Let T
                        ={t
                        
                           i
                        } be the set of target patches corresponding to the target interest points, represented as l
                        2 normalized vectors. The candidate interest points, represented as the vectorized image patches y
                        i, obtained within a search window of the current frame could be represented by a sparse linear combination of the target dictionary elements t
                        
                           i
                        .
                           
                              (1)
                              
                                 
                                    y
                                    i
                                 
                                 ≈
                                 T
                                 
                                    a
                                    i
                                 
                                 =
                                 
                                    a
                                    
                                       i
                                       1
                                    
                                 
                                 
                                    t
                                    1
                                 
                                 +
                                 
                                    a
                                    
                                       i
                                       2
                                    
                                 
                                 
                                    t
                                    2
                                 
                                 +
                                 ,
                                 …
                                 ,
                                 +
                                 
                                    a
                                    
                                       i
                                       k
                                    
                                 
                                 
                                    t
                                    k
                                 
                              
                           
                        where, {t
                        1,
                        t
                        2,…,
                        t
                        
                           k
                        } are the target bases and {a
                        
                           i1,
                        a
                        
                           i2,…,
                        a
                        
                           ik
                        } are the corresponding target-coefficient vectors. The coefficient a
                        
                           ij
                         indicates the affinity between the ith candidate and the jth target interest point based on the discriminative property of sparse representation.

The coefficient vector a
                        
                           i
                        
                        ={a
                        
                           i1,
                        a
                        
                           i2,…
                        a
                        
                           ik
                        } is obtained as a solution of
                           
                              (2)
                              
                                 
                                    min
                                    
                                       
                                          a
                                          i
                                       
                                       ∈
                                       
                                          R
                                          k
                                       
                                    
                                 
                                 ∥
                                 
                                    a
                                    i
                                 
                                 
                                    ∥
                                    1
                                 
                                 
                                 s
                                 .
                                 t
                                 .
                                 
                                 ∥
                                 
                                    y
                                    i
                                 
                                 −
                                 T
                                 
                                    a
                                    i
                                 
                                 
                                    ∥
                                    2
                                    2
                                 
                                 ≤
                                 λ
                              
                           
                        where, λ is the reconstruction error tolerance in mean square sense. Considering the effect of occlusion and noise, Eq. (1) can be written as,
                           
                              (3)
                              
                                 
                                    y
                                    i
                                 
                                 =
                                 T
                                 
                                    a
                                    i
                                 
                                 +
                                 ϵ
                                 .
                              
                           
                        
                     

The nonzero entries of the error vector ϵ indicate noisy or occluded pixels in the candidate patch y
                        
                           i
                        . Following the strategy in [20], we use trivial templates I
                        ={[i
                        1,…,
                        i
                        
                           n
                        ]∈
                        R
                        
                           n
                           ×
                           n
                        } to capture the noise and occlusion.
                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             y
                                             i
                                          
                                          =
                                          T
                                          
                                             a
                                             i
                                          
                                          +
                                          I
                                          
                                             c
                                             i
                                          
                                       
                                    
                                    
                                       
                                          =
                                          
                                             a
                                             
                                                i
                                                1
                                             
                                          
                                          
                                             t
                                             1
                                          
                                          +
                                          ,
                                          …
                                          ,
                                          +
                                          
                                             a
                                             
                                                i
                                                k
                                             
                                          
                                          
                                             t
                                             k
                                          
                                          +
                                          
                                             c
                                             
                                                i
                                                1
                                             
                                          
                                          
                                             i
                                             1
                                          
                                          ,
                                          …
                                          ,
                                          +
                                          
                                             c
                                             in
                                          
                                          
                                             i
                                             n
                                          
                                       
                                    
                                 
                              
                           
                        where, each of the trivial templates {i
                        1,…,
                        i
                        
                           n
                        }, is a vector having nonzero entry only at one location and {c
                        
                           i1,…,
                        c
                        
                           in
                        } is the corresponding coefficient vector. A nonzero trivial coefficient indicates the reconstruction error at the corresponding pixel location, possibly due to noise or occlusion.

The object is tracked by finding the correspondence between the target and candidate interest points in successive frames. Only the reliable candidate interest points that mutually match with each other are considered for estimating the object location. Let l be the number of interest points in the current frame within the search window (all points shown in Fig. 2(b)). Only k interest points out of l points that match with the dictionary atoms will be considered for object localization (all green+red points shown in Fig. 2(b)).

In order to find the candidate points that match with the target points, each target interest point t
                        
                           j
                         is represented as a sparse linear combination of candidate points (y
                        
                           i
                        ) and trivial bases.
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             t
                                             j
                                          
                                          =
                                          Y
                                          ·
                                          
                                             
                                                b
                                                j
                                             
                                          
                                          +
                                          ϵ
                                       
                                    
                                    
                                       
                                          =
                                          
                                             b
                                             
                                                j
                                                1
                                             
                                          
                                          
                                             y
                                             1
                                          
                                          +
                                          ,
                                          …
                                          ,
                                          +
                                          
                                             b
                                             
                                                j
                                                l
                                             
                                          
                                          
                                             y
                                             l
                                          
                                          +
                                          
                                             c
                                             
                                                j
                                                1
                                             
                                          
                                          
                                             i
                                             1
                                          
                                          ,
                                          …
                                          ,
                                          +
                                          
                                             c
                                             
                                                j
                                                n
                                             
                                          
                                          
                                             i
                                             n
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

Utilizing the discriminative property of sparse representation, the matching candidate point for the given target point t
                        
                           j
                         is determined by the maximum value of the candidate dictionary coefficient (b
                        
                           j
                        ). Let the coefficient b
                        
                           ji
                         corresponding to the candidate dictionary element y
                        
                           i
                         be the maximum value. Then the matching candidate point for the target point t
                        
                           j
                         is declared as y
                        
                           i
                        .

Mathematically, 
                           
                              b
                              
                                 j
                                 i
                              
                           
                           =
                           
                              max
                              r
                           
                           
                              
                                 b
                                 
                                    j
                                    r
                                 
                              
                           
                           ,
                           
                           r
                           ∈
                           
                              1
                              2
                              …
                              l
                           
                        , indicates the match between jth target point and ith candidate point. All the matched candidate points corresponding to each target dictionary element are shown by green and red patches in Fig. 2(b). If two or more target points match with same candidate point, only the candidate patch with higher coefficient value will be considered. Hence, the number of candidate matching points is less than or equal to the number of target dictionary elements.

Let k
                        1 (k
                        1
                        ≤
                        k) be the number of matched candidate interest points (all red and green points shown in Fig. 2(b)). In order to prune the noisy matches, now each of these candidate interest points (y
                        
                           i
                        ) is represented as a sparse linear combination of target points (t
                        
                           j
                        ) and trivial bases as in Eq. (4). Let 
                           
                              a
                              
                                 i
                                 j
                              
                           
                           =
                           
                              max
                              r
                           
                           
                              
                                 a
                                 
                                    i
                                    r
                                 
                              
                           
                           ,
                           
                           r
                           ∈
                           
                              1
                              …
                              k
                           
                         indicate the match between ith candidate point and jth target point. The ith candidate point is selected if:
                           
                              (6)
                              
                                 
                                    
                                       
                                          
                                          
                                             max
                                             r
                                          
                                          
                                             
                                                b
                                                
                                                   j
                                                   r
                                                
                                             
                                          
                                          
                                          =
                                          
                                             b
                                             
                                                j
                                                i
                                             
                                          
                                          ,
                                          
                                          r
                                          ∈
                                          
                                             1
                                             …
                                             l
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                             max
                                             r
                                          
                                          
                                             
                                                a
                                                
                                                   j
                                                   r
                                                
                                             
                                          
                                          
                                          =
                                          
                                             a
                                             
                                                j
                                                i
                                             
                                          
                                          ,
                                          
                                          r
                                          ∈
                                          
                                             1
                                             …
                                             k
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

All the candidate points that agree with the above robust matching criterion were considered for object localization (all red points shown in Fig. 2(b)). Let k
                        2 (k
                        2
                        ≤
                        k
                        1) be the candidate points located at [x
                        
                           i
                        
                        
                           c
                        ,
                        y
                        
                           i
                        
                        
                           c
                        ],
                        i
                        ∈{1,…,
                        k
                        2} with respect to object window center (x,
                        y). Let [x
                        
                           i
                        
                        
                           t
                        ,
                        y
                        
                           i
                        
                        
                           t
                        ],
                        i
                        ∈{1,…,
                        k
                        2} be the corresponding locations of target points with respect to the same object window center (x
                        
                           o
                        ,
                        y
                        
                           o
                        ). Now the displacement of object is measured as median displacement of candidate points with respect to its matching target points. Let dx
                        
                           i
                         and dy
                        
                           i
                         be the displacement of ith candidate point measured as:
                           
                              (7)
                              
                                 
                                    
                                       
                                          d
                                          
                                             x
                                             i
                                          
                                          =
                                          
                                             x
                                             i
                                             c
                                          
                                          −
                                          
                                             x
                                             i
                                             t
                                          
                                          ,
                                          
                                          i
                                          ∈
                                          
                                             1
                                             2
                                             …
                                             
                                                k
                                                2
                                             
                                          
                                       
                                    
                                    
                                       
                                          d
                                          
                                             y
                                             i
                                          
                                          =
                                          
                                             y
                                             i
                                             c
                                          
                                          −
                                          
                                             y
                                             i
                                             t
                                          
                                          ,
                                          
                                          i
                                          ∈
                                          
                                             1
                                             2
                                             …
                                             
                                                k
                                                2
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

The object location in the current frame is obtained using the median displacement of each coordinate.
                           
                              (8)
                              
                                 
                                    
                                       
                                          
                                             x
                                             o
                                          
                                          =
                                          
                                             x
                                             o
                                          
                                          +
                                          median
                                          
                                             
                                                d
                                                
                                                   x
                                                   i
                                                
                                             
                                          
                                          ,
                                          
                                          i
                                          ∈
                                          
                                             1
                                             2
                                             …
                                             
                                                k
                                                2
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             y
                                             o
                                          
                                          =
                                          
                                             y
                                             o
                                          
                                          +
                                          median
                                          
                                             
                                                d
                                                
                                                   y
                                                   i
                                                
                                             
                                          
                                          ,
                                          
                                          i
                                          ∈
                                          
                                             1
                                             2
                                             …
                                             
                                                k
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Since the object appearance and illumination change over time, it is necessary to update the target dictionary for reliable tracking. We deploy a simple update strategy based on the confidence of matching. The top 10% of matched candidate points are added to the dictionary if the matched sparse coefficient is above certain threshold. The same number of unmatched points from the target dictionary are removed to accommodate these new candidate points.
                           Algorithm 1
                           Proposed Tracking


                              
                                 
                              
                           

@&#RESULTS AND DISCUSSION@&#

The proposed tracker is implemented in MATLAB and experimented on complex video sequences with various challenging scenarios such as illumination change, partial occlusion and appearance change. These videos were shot with different sensors such as color, gray-scale, and infrared and were recorded in indoor and outdoor environments. To solve l
                     1 minimization problem we have used the SPAMS package [37] with the regularization constant λ set at 0.1 for all the experiments. For interest point detection, we have used Harris corner detector obtained from [38] with Noble's corner measure [39]. Since the number of interest points is typically proportional to the size of the object, we have set the parameters of Harris corner detector depending on the size of the target. For all video sequences we have used σ
                     =1 and threshold=1. For target size less than 50×50 we have used radius=0.5 and radius=2 for size greater than 50×50. For each interest point, a 5×5 image patch centered around that point is used for creating dictionary atoms. For evaluating the performance of the proposed tracker, we have compared the results with l
                     1 tracker (L1) proposed by Mei and Ling [33], using 300 particles, meanshift tracker (MS) [1], fragment-based tracking (Frag) [4], sparse collaborative model based tracker (SCM) [40], local sparse appearance tracker (VT) [12], accelerated proximal gradient tracker (APG) [24], track-learn-detect tracker (TLD) [41], structure preserving object tracking (SPOT) [42] and compressive tracking (CT) [35]. The performance of the proposed method without (1-way) and with robust matching (2-way) is also provided. We have shown results for the following publicly available seven different video sequences: pktest02 (515 frames), panda (900 frames), face (899 frames), Dudek (1145 frames), trellis (560 frames), car4 (431 frames) and Pets (156 frames).

The first video sequence pktest02 is obtained from the VIVID benchmark dataset available at ‘http://vision.cse.psu.edu/data/vividEval/datasets/PETS2005/PkTest02/index.html’. It is an infrared (IR) image sequence. Fig. 3
                      shows the tracking results for this video sequence. In this sequence, the car undergoes partial occlusion when it goes beneath the roadside trees and its appearance also changes as the car turns at the end of the sequence. Proposed tracker tracks the object throughout the video even without robust matching criterion, while the vehicle is partially occluded by trees and is also immune to the illumination change caused by the shadows of the trees. Fig. 10(a)
                     
                     
                     
                     
                     
                      shows the position error plot for all the trackers considered. The error plots and RMSE (Table 1
                     ) show similar performance by the proposed tracker and VT tracker. The l
                     1 and SCM trackers failed after 350 frames, when the object undergoes a severe occlusion. The meanshift tracker fails when the object (car) gets occluded by the tree around 150th frame and starts tracking another car. The FragTrack fails after 20 frames when the object goes below the shade of the tree. The proposed approach is much faster than l
                     1 and VT trackers (Table 2
                     ). The other reported fast trackers could not handle partial occlusion and failed to track the object. The TLD tracker fails to track at 164th frame, when the car is briefly occluded by tree, it starts tracking another car. Whereas the SPOT tracker failed at the beginning of sequence itself.

In the second video sequence (obtained from http://info.ee.surrey.ac.uk/Personal/Z.Kalal/TLD/TLD_dataset.ZIP), the animal (panda) is moving around a tree and the object undergoes severe appearance/pose variation throughout the sequence.

The tracking results for this sequence are shown in Fig. 4. The proposed 2-way tracker performs well throughout the video with minimal deviation from ground truth trajectory (Figs. 4 and 10(c)).

The l
                     1 tracker could not track the object properly after 600 frames. MS, FragTrack, VT and the proposed (2-way) trackers could track the object till the end with almost similar tracking performance. Here, again the faster trackers such as CT and APG, TLD failed to track the objects till the end. The TLD tracker fails around 190th frame when the panda goes behind the tree. Again the SPOT tracker failed at the start of the video. This could be attributed to the small size of the object.

The third video sequence is obtained from ‘http://www.cs.technion.ac.il/~amita/fragtrack/fragtrack.htm’ which is specifically shot to check the robustness of the tracking algorithms against partial occlusion (Fig. 5). In this video, the face of a person undergoes partial occlusion from various sides. A better tracker should not deviate from the location of the face when it is being partially occluded by another object (a book, in this video). Since FragTrack [4] is designed to handle such partial occlusion, it could track the face properly throughout the video. Along with FragTrack, l
                     1, SCM, TLD, SPOT and the proposed trackers could track the object properly during the partial occlusions. The meanshift tracker consistently exhibits large deviation from the face during partial occlusions as shown in Fig. 10(c). Due to the global color-histogram model, MS tracker uses only the visible object region for computing the object location. The proposed approach could track the object properly till the end since it relies on the interest points that are not occluded. Fig. 10(d) and Table 1 show trajectory error with respect to ground truth.

The fourth video is obtained from ‘http://www.cs.toronto.edu/vis/projects/dudekfaceSequence.html’ named as Dudek. This indoor sequence has more than 1100 frames and contains severe appearance changes and partial occlusions. The proposed tracker could track the face throughout the video. The tracking results for this sequence are given in Fig. 6. Both proposed tracker and APG trackers could track the face when it is occluded by the hand around frame number 207. The FragTrack and l
                     1 trackers fail around frame 600, when the person gets up swiftly. The SCM tracker fails to track the face after frame number 773 when it undergoes rapid appearance and illumination changes. This failure is mostly attributed to wrong template adaption.

The meanshift tracker drifts away when the face is occluded by the hand and when the person removes his spectacles. Further, meanshift tracker deviates from the object due to motion blur when the person stands up suddenly and gets back to chair swiftly. The proposed 1-way tracker (without robust matching criterion) fails to track between frames 750 and 805 due to change in object scale and illumination. It fails again after frame number 970. The tracking error of the proposed approach without robust matching criterion is due to rapid motion of the face and the blurriness caused due to this motion around frame numbers 574 and 955. The proposed tracker incorporating robust matching (2-way) and APG tracker could track the face throughout the video with minimal position error. The TLD tracker fails to track when the face was occluded briefly by the hand. Table 1 and Fig. 10(d) show the average error and the error plots for this video.

The fifth video sequence, trellis is obtained from ‘http://www.cs.toronto.edu/~dross/ivt/’, which is shot outdoors. This sequence is very challenging due to severe illumination changes and moderate appearance and scale changes. In this video, a person is walking under some kind of open roof and his face undergoes severe illumination change ranging between dark shade and bright sunlight as shown in Fig. 7. The tracking results for this sequence are given in Fig. 7. Meanshift tracker drifts away from the face from 100th frame and fails near 200th frame due to significant illumination change. The proposed (2-way) tracker, SPOT and visual tracker (VT) could faithfully track the object. The proposed approach without robust matching (1-way) could track the face up to frame number 520, but with slightly higher position error compared to robust matching approach. All other trackers failed at some point of time during sudden illumination change or appearance change.

The error plot for this sequence is given in Fig. 10(e) illustrating the accurate tracking of the proposed tracker. The trajectory RMS errors with respect to ground truth for various approaches are given in Table 1. For this video, the proposed tracker runs at real-time with a frame rate of 45 frames per second as shown in Table 2.

The ‘car’ video sequence is obtained from ‘http://www.cs.toronto.edu/~dross/ivt/’. This is outdoor video sequence. In this video the car is moving under drastic illumination changes and there is blurriness. The proposed tracker along with VT, l
                     1 and APG trackers could track the object properly throughout the video. The performance of the proposed tracker (2-way) is very close to the best performance of VT. The meanshift tracker, FragTrack, TLD, SPOT and compressive tracker (CT) fail when the car goes below the bridge creating a drastic illumination change. The proposed 1-way tracker slightly deviates from the object location during sudden change in illumination. Whereas, the proposed 2-way tracker with robust matching tracks the object seamlessly throughout the sequence. Fig. 8 shows the tracking result for this sequence for various approaches, the corresponding error plot and the average RMSE are given in Fig. 10(f) and Table 1, respectively. The proposed approach is much faster compared to VT and l
                     1 trackers as shown in Table 2.

For the last video sequence,
                        2
                     
                     
                        2
                        Link: ‘http://www.hitech-projects.com/euprojects/cantata/datasets_cantata/dataset.html’.
                      almost all the trackers, except APG tracker, could track the object close to the end with reasonable error (refer Fig. 10(g)). The SPOT tracker failed to track the object. The tracking results for this sequence are illustrated in Fig. 9.


                     Tables 1 and 2 summarize the performance of the trackers with respect to accuracy (pixel deviation from ground truth) and execution speed (frames/second). It can be observed that the proposed tracker achieves real time performance with better tracking accuracy compared to many recent state of the art trackers such as VT, CT, SCM, TLD, SPOT and APG. The robust matching criterion improves the tracker performance significantly with negligible computational overhead. Like many of the non-particle filter trackers, the proposed tracker currently does not estimate the scale of the object, which needs to be addressed separately in the future. The videos corresponding to the results presented between Figs. 3 and 9 are available at: http://www.serc.iisc.ernet.in/~venky/IPTresults.

We have also used the benchmark dataset and evaluation techniques given in [43] for evaluating our tracker. Authors of [43] use a set of 50 videos, each labeled with the challenging aspect that it poses to a tracking algorithm. We have evaluated our tracker with respect to illumination variation, in-plane rotation, low resolution, out-of-plane rotation, occlusion, motion blur, deformation and background clutter, and compared with sparse trackers [40,12,44,24], as well as other well known trackers [41,45,4,46,47,1,35]. Fig. 11
                      shows the success plots obtained using the TRE evaluation technique [43]. The numbers in bracket in the legend show area under the curve, which is a measure of how robust the tracker is.

We can observe that although the proposed tracker is not the best among all the trackers for all the challenging aspects, it is among the top trackers in many of the challenges. In case of videos with ‘Motion blur’, the proposed tracker has reduced robustness. This is because corners do not appear prominently when there is blur. However, our tracker is better than the other sparse trackers SCM, ASLA(VT), L1APG and MTT.

The proposed tracker is most likely to fail when the object of interest goes out of view. The tracker always searches for the object in the immediate vicinity of the previous object location, but when once the object goes out of view, it may not re-appear from the vicinity of the previous known location. Increasing the search space to the entire image not only reduces the execution speed, but also decreases the robustness of the tracker due to increased false matches of the interest points.

For further comparison of our tracker with the other existing sparse trackers with respect to execution speed, we have measured the execution speed (in fps) of the other sparse trackers for different numbers of particles used in the particle filter framework. We have used a 360×240 video with 17×50 sized initial object window, and executed the trackers on a 2.3GHz system. Our tracker was observed to run at 44.71fps. Fig. 12
                      shows plot of execution speed vs number of particles for the sparse trackers in the literature. The typical value used for number of particles is 600 (also used for benchmarking in [43]) for the trackers to work well. It can be observed from the plot that even with 8-fold decrease in the number of particles (75 particles), none of the existing trackers are able to reach the execution speed of our tracker (44.71fps).

The proposed approach has been experimented with different patch sizes and better results were obtained for patch size of 5×5. Though not much difference in performance is noted for smaller patch size (3×3), the performance for bigger patch size (7×7) came down for ‘trellis’ and ‘car4’ video sequences. The objects in these two sequences undergo drastic illumination change. The performance reduction could be attributed to some background information that is captured by the bigger window. Table 3
                      shows the tracker performance for different patch sizes. We have compared the performance of the proposed tracker with ‘nearest neighbor’ (NN) tracker, where the patch correspondences between the frames are obtained via Euclidean norm (l
                     2 distance). The performance of the NN matching based approach is found to be inferior compared to the proposed l
                     1 matching. The comparison is given in Table 4
                     .

Further, our experiments indicate only a marginal improvement in tracking performance due to dictionary update. This could be attributed to the point features, which are already robust to appearance, scale and illumination changes. Hence dictionary update does not seem to affect the tracking performance much. The comparison is provided in Table 5
                     . The performance of the proposed tracker is observed for various values of regularization constant (λ) and the results are given in Table 6
                     . No significant difference in tracking performance has been noted when the λ value is changed in the range 0.05 to 0.15.

In order to assess the performance of the proposed tracker for various numbers of interest points used, we have conducted experiments with reduced number of interest points. The performance for different fractions of interest points was given in Table 7
                     . When 75% of the selected interest points were used, the tracker performance is almost similar except for car4 video, in which the vehicle undergoes a sudden, severe illumination change. When only half of the selected points were used, the performance of the tracker degraded for ‘trellis’ and ‘car4’ sequences. Both these sequences contain severe illumination changes. Hence, it is suggested to use all the selected points for tracking challenging sequences with severe illumination changes.

The effect of noise model via trivial dictionary is of limited use in the proposed tracker. The noise model is important when the whole object is modeled by single template, since the model can handle the partially occluded object regions. Whereas the trivial dictionary offers limited help for the proposed tracker that models the object as collection of interest points. In our experiments, the effect of noise model is not noticeable for all videos except ‘trellis’, which is one of the most challenging sequences containing severe illumination changes. The above observation indicates that trivial dictionary based noise model is really useful in the proposed approach for sequences containing significant illumination changes. Table 8
                      shows the effect of noise modeling with trivial dictionary.

Since the proposed tracker is an interest point based tracker, the performance of the tracker depends on the number of interest points available in the region of interest. All interest point trackers will face problems if the number of interest points in the region is not enough. Objects with homogeneous smooth regions are not good candidates for the proposed tracker due to lack of interest points. But in most of the real-world videos, we find required number of interest points by reducing the threshold of corner strength. Object with repeated structures is another challenging scenario, since there could be confusion while matching the interest points. In order to show the tracker performance in such scenario, we have tried the proposed tracker on objects with homogeneous regions and object with repeated patterns taken in a laboratory set-up. The results illustrate that the proposed tracker could satisfactorily track these objects. This result indicates that l
                     1 matching could discriminate the interest points better even in a challenging setup. The snapshots of the results are illustrated in Fig. 13
                     .

@&#CONCLUSION@&#

In this paper, we have proposed a computationally efficient interest point tracker in sparse representation framework. The interest point based object modeling along with the proposed robust matching criterion in sparse representation framework leads to an efficient tracker. The proposed tracker runs at real-time and shows robustness to various challenges including partial occlusion, illumination change and appearance change. The performance of the tracker has been bench-marked with various publicly available complex video sequences. The proposed tracker is compared with many recent state of the art trackers including VT, CT, SCM, APG, TLD, SPOT and l
                     1 trackers. Robustness of the tracker towards various challenges including illumination change, occlusion, and background clutter has been quantified using a benchmark database. The overall performance of the proposed tracker is found to be comparable to the state of the art trackers in terms of accuracy. The tracker is found to execute much faster than the state of the art sparse trackers.

@&#ACKNOWLEDGMENT@&#

The authors wish to express grateful thanks to the referees for their useful comments and suggestions to improve the presentation of this paper.

@&#REFERENCES@&#

