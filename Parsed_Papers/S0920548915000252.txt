@&#MAIN-TITLE@&#Enabling policy making processes by unifying and reconciling corporate names in public procurement data. The CORFU technique

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We outline the problem and necessity of unifying corporate names in public procurement meta-data.


                        
                        
                           
                           We design and implement a stepwise method, CORFU, to unify corporate names using NLP techniques.


                        
                        
                           
                           We present the Public Spending initiative as a client of the CORFU technique.


                        
                        
                           
                           We evaluate the precision, recall and robustness of the CORFU technique using datasets of corporate names coming from Australia, United States, United Kingdom and the CrocTail project.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Name disambiguation

Public spending

Linked Data

@&#ABSTRACT@&#


               
               
                  This paper introduces the design, implementation and evaluation of the CORFU technique to deal with corporate name ambiguities and heterogeneities in the context of public procurement meta-data. This technique is applied to the “PublicSpending.net” initiative to show how the unification of corporate names is the cornerstone to provide a visualization service that can serve policy-makers to detect and prevent upcoming necessities. Furthermore, a research study to evaluate the precision, recall and robustness of the proposed technique is conducted using more than 40 million of names extracted from public procurement datasets (Australia, United States and United Kingdom) and the CrocTail project.
               
            

@&#INTRODUCTION@&#

Public bodies are continuously publishing procurement opportunities in which valuable meta-data is available. Depending on the stage of the process, new data pieces such as the supplier name that has been rewarded with the public contract arise. In this context, the extraction of statistics on how many contracts have been rewarded to the same company is a relevant indicator to evaluate the transparency of the whole process. Although companies that want to tender for a public contract must be officially registered and have a unique identification number, the reality is that in most of rewarded contracts the supplier is only identified by a name or a string literal typed by a civil-servant. In this sense, there is not usually a connection between the official company registry and the process of rewarding contracts implying different interoperability issues [14] such as naming problems and data inconsistencies that are spread to further stages hindering future activities such as reporting.

In the case of the type of contract and location, there are already standardized product scheme classifications [[48,47]] such as the Common Procurement Vocabulary (2003 and 2008), the Combined Nomenclature (2012), the Central Product Classification by the European Union, the International Standard Industrial Classification of All Economic Activities (Rev. 4) by the United Nations or the North American Industry Classification System (2007 and 2012) by the Government of United States that are currently used with different objectives such as statistics, tagging or information retrieval. Geo-located information can be also found in different common datasets and nomenclatures such as the Nomenclature of Territorial Units for Statistics (NUTS) by the European Union, the Geonames dataset,
                        1
                     
                     
                        1
                        
                           http://www.geonames.org/.
                      the GeoLinkedData initiative [[28,16]] or the traditional list of countries and ISO-codes.

However, corporate, organization, firm, company or institution names (hereafter, these names will be used to refer to the same entity) and structure are not yet standardized at global scope and only some classifications of economic activities or company identifiers such as the TARIC database (On-line customs tariff database) can be found. Thus, the simple task of grouping contracts by a supplier is not a mere process of searching by the same literal. Technical issues such as hyphenation, use of abbreviations or acronyms and transliteration are common problems that must be addressed in order to provide a final corporate name. Existing works in the field of Name Entity Recognition [[38,20,31]] (NER) or name entity disambiguation [[49,22,12]] have already addressed these issues. Nevertheless, the problem that is being tackled in these approaches lies in the identification of organization names in a raw text while in the e-Procurement sector the string literal identifying a supplier is already known.

In the particular case of the Australian e-Procurement domain, the supplier name seems to be introduced by typing a string literal without any assistance or auto-complete method. Obviously, a variety of errors and variants for the same company, see Table 6 in the Appendix I, can be found: misspelling errors [[40,25]], name and acronym mismatches [[53,45]] or context-aware data that is already known when the dataset is processed, e.g. country or year. Furthermore, it is also well-known that a large company can be divided into several divisions or departments but from a statistical point of view grouping data by a supplier name should take into account all rewarded contracts regardless the structure of the company.

On the other hand, the application of semantic technologies and the Linking Open Data initiative (hereafter LOD) [[4,15]] in several fields like e-Government (e.g. the Open Government Data effort) tries to improve the knowledge about a specific area providing common data models and formats to share information and data between agents. More specifically, in the European e-Procurement context [8], there is an increasing commitment to boost the use of electronic communications and transactions processing by government institutions and other public sector organizations in order to provide added-value services [44] with special focus on SMEs (Small and Medium Enterprises).

In this context, the LOD initiative seeks for creating a public and open data repository in which one of the principles of this initiative that lies in the unique identification or resources through URIs can become real. Thus, entity reconciliation techniques [[3,29]] coming from the ontology mapping and alignment areas or algorithms based on Natural Language Processing (hereafter NLP) have been designed to link similar resources already available in different vocabularies, datasets or databases such as DBPedia or Freebase.

Nevertheless, the issue of unifying supplier names as a human would do faces new problems that have been tackled in other research works [10] to extract statistics of performance in bibliographic databases. The main objective is not just a mere reconciliation process to link to existing resources but to create a unique name or link (n string literals→1 company→1 URI). For instance, the string literals “Oracle” and “Oracle University” could be respectively aligned to the entity<Oracle_Corporation>and<Oracle_University>but the problem of grouping by an unique (Big) name, identifier or resource still remains. That is why, a context-aware method based on NLP techniques combined with semantics has been designed, customized and implemented trying to exploit the naming convention of a specific dataset with the aim of grouping n string literals→1 company and, thus, easing the next natural process of entity reconciliation.

The remainder of this paper is structured as follows. Section 2 presents a literature review. Next section outlines main mismatches in corporate names and presents the CORFU approach to unify corporate names. Afterwards the possibilities of using public procurement data as policy-making tool and, more specifically the Public Spending initiative, are presented as a client of the CORFU technique. Last section exposes and discusses the experimentation carried out to test the presented approach using as a dataset the rewarded contracts of Australia in the period 2004–2012. Finally the main outcomes of this work, conclusions and some open issues are also outlined.

@&#RELATED WORK@&#

According to the previous section, some relevant works can be found and grouped by the topics covered in this paper.
                        
                           •
                           Natural language processing and computational linguistics. In these research areas common works dealing with the aforementioned data heterogeneities such as misspelling errors [[40,25]] and name/acronym mismatches [[53,45]], in the lexical, syntactic and semantic levels can be found. These approaches can be applied to solve general problems and usually follow a traditional approach of text normalization, lexical analysis, pos-tagging word according to a grammar and semantic analysis to filter or provide some kind of service such as information/knowledge extraction, reporting, sentiment analysis or opinion mining. Well-established APIs such as NLTK [27] for Python, Lingpipe [5], OpenNLP [24] or Gate [6] for Java, WEKA [46] (a data mining library with NLP capabilities), the Apache Lucene and Solr search engines provide the proper building blocks to build natural-language based applications. Recent times have also seen how the analysis of social networks [[36,33]] such as Twitter [[26,13]], the extraction of clinical terms [52] for electronic health records, the creation of bibliometrics [[10,35,30]], the identification of gene names [[23,11]] or the suggestion of knowledge pieces [42] to name a few have tackled the problem of entity recognition and extraction from raw sources. Other supervised techniques [37] have also been used to train data mining-based algorithms with the aim of creating multi-label classifiers [21].

Semantic web. More specifically, in the LOD initiative [4], the use of entity reconciliation techniques to uniquely identify resources is being currently explored. Thus, an entity reconciliation process can be briefly defined as the method for looking and mapping [[19]] two different concepts or entities under a certain threshold. There are a lot of works presenting solutions about concept mapping, entity reconciliation, etc. most of them are focused on the previous NLP techniques [[29,3,39,18]] (if two concepts have similar literal descriptions then they should be similar) and others (ontology-based) that also exploit the semantic information (hierarchy, number and type of relations) to establish a potential mapping (if two concepts share similar properties and similar super classes then these concepts should be similar). Apart from that, there are also machine learning techniques to deal with these mismatches in descriptions using statistical approaches. Recent times, this process has been widely studied and applied to the field of linking entities in the LOD realm, for instance using the DBPedia [32]. Although, there is no way of automatically creating a mapping with a 100% of confidence (without human validation) a mapping under a certain percentage of confidence can be enough for most of user-based services such as visualization. However, in case of using these techniques as previous step of a reasoning or a formal verification process this ambiguity can lead us to infer incorrect facts and must be avoided without a previous human validation.

On the other hand, the use of semantics is also being applied to model organizational structures. In this case the notion of corporate is presented in several vocabularies and ontologies as Dave Reynolds (Epimorphics Ltd.) reports.
                                 2
                              
                              
                                 2
                                 
                                    http://www.epimorphics.com/web/wiki/organization-ontology-survey.
                               Currently the main effort is focused on the designed of the Organizations Vocabulary (a W3C Recommendation) in which the structure and relationships of companies are being modeled. This proposal is especially relevant because of the next aspects.
                                 
                                    1.
                                    To unify existing models to provide a common specification

To apply semantic web technologies and the Linked Data approach to enrich and publish the relevant corporate information

To provide access to the information via standard protocols

To offer new services that can exploit this information to trace the evolution and behavior of the organization over time.

Corporate databases. Although corporate information such as identifier, name, economic activity, contact person, address or financial status is usually publicly available in the official government registries, the access to this valuable information can be tedious due to different formats, query languages, etc. That is why, other companies have emerged trying to index and exploit these public repositories; selling reporting services that contain an aggregated version of the corporate information. Taking as an example the Spanish realm, the Spanish Chambers of Commerce,
                                 3
                              
                              
                                 3
                                 
                                    http://www.camerdata.es/php/eng/fichero_empresas.php.
                               
                              Empresia.es
                              
                                 4
                              
                              
                                 4
                                 
                                    http://empresia.es.
                               or Axesor.es
                              
                                 5
                              
                              
                                 5
                                 
                                    http://www.axexor.es.
                               manage a database of companies and individual entrepreneurs. This situation can be also transposed to the international scope, for instance Forbes keeps a list of the most representative companies in different sectors. The underlying problems rely on the lack of unique identification, same company data in more than a source, name standardization, etc. and, as a consequence, difficulty of tracking company activity. In order to tackle these problems some initiatives applying the LOD principles such as the Orgpedia
                                 6
                              
                              
                                 6
                                 
                                    http://tw.rpi.edu/orgpedia/.
                               in United States or “The Open Database Of The Corporate World”
                                 7
                              
                              
                                 7
                                 
                                    http://opencorporates.com/.
                               have scrapped and published the information about companies creating a large database containing (76,197,263 of companies in July 2014) with high-valuable information like the company identifier.

Apart from that, reconciliation services have also been provided but the problem of mapping (n string literals→1 company→1 URI, as a human would do and the previous section has presented) still remains. Finally, public web sites and major social networks such as Google Places, Google Maps, Foursquare, Linkedin Companies or Facebook provide APIs and information managed by the own companies that are expected to be specially relevant to enrich existing corporate data once a company is uniquely identified.

According to [[10,35]], institutional name variations can be classified into two different groups:
                        
                           1.
                           Non-acceptable variations (affect to the meaning) due to misspelling or translation errors.

Acceptable variations (do not affect to the meaning) that correspond to different syntax forms such as abbreviations, use of acronyms or contextual information like country, sub-organization, etc.

In order to address these potential variations the CORFU (Company, ORganization and Firm Unifier) approach seeks for providing a stepwise method to unify corporate names using NLP and semantic-based techniques as a previous step to perform an entity reconciliation process. The execution of CORFU comprises several common but customized steps in natural language processing applications such as:
                        
                           1.
                           Text normalization,

filtering,

comparison and clusterization and

linking to an existing information resource.

The CORFU unifier makes an intensive use of the Python NLTK API and other packages for querying REST services or string comparison. Finally and due to the fact that the corporate name can change in each step the initial raw name must be saved as well as contextual information such as dates, acronyms or locations. Thus, common contextual information can be added to create the final unified name.
                        
                           1.
                           Normalize raw text and remove duplicates. This step is comprised of: 1) remove strange characters and punctuation marks but keeping those that are part of a word avoiding potential changes in abbreviations or acronyms; 2) lowercase the raw text (although some semantics can be lost, previous works and empirical tests show that this is the best approach); 3) remove duplicates and 4) lemmatize the corporate name. The implementation of this step to clean the corporate name has been performed using a combination of the aforementioned API and the Unix scripting tools AWK and SED. In this case, Fig. 1
                               presents a snippet of code for cleaning the name and making a basic word normalization.

Filter the basic set of common stop-words in English. A common practice in NLP relies in the construction of stop-words sets that can filter some non-relevant words. Nevertheless, the use of this technique must consider two key-points:
                                 
                                    •
                                    There is a common set of stop-words for any language than can be often used as a filter.

Depending on the context, the set of stop-words should change to avoid filtering relevant words. In this particular case, a common and minimal set of stop-words in English provided by NLTK has been used.

Thus, the normalized corporate name is transformed into a new set of words. Fig. 2
                               presents the function for removing a set of words given another set, it can also be applied to other stages that require filtering capabilities.

Filter the expanded set of most common words in the dataset. Taking into account the aforementioned step, this stage is based on the construction of a customized stop-words set for corporate names that is also expanded with Wordnet (ver. 3.0) synonyms with the aim of exploiting semantic relationships. In order to create this set, two strategies, as Fig. 3
                               partially shows, have been followed:
                                 
                                    •
                                    Handmade creation of the stop-words set (accurate but very time-consuming);

Extract automatically the set of “most common words” from the working dataset and make a handmade validation (less accurate and time-consuming).

Dictionary-based expansion of common acronyms and filtering. A dictionary of common acronyms in corporate names such as “PTY”, “LTD” or “PL” and their variants has been created in order to be able to extract, expand and filter acronyms.

Identification of contextual information and filtering. Corporate names can mainly contain nationalities or place names that, in most of cases, only add noise to the real corporate name. In this case, the use of external services such as Geonames, Google Places or Google Maps can ease the identification of these words and their filtering. In order to implement this functionality, the Geonames REST service has been selected due to its capabilities to align text to locations.

Spell checking (optional). This stage seeks for providing a method for fixing misspelling errors. It is based on the well-known speller of Peter Norvig [40] that uses a train dataset for creating a classifier. Although the accuracy of this algorithm is pretty good for relevant words in corporate names, the empirical and unit tests with a working dataset have demonstrated that spell checking of non-relevant words is more efficient and accurate using a stop-words set/dictionary (this set has been built with words that are not in the set of “most common words”, step 2, and exist in the Wordnet database). Furthermore, some spelling corrections are not completely adequate for corporate names due to the fact that words could change and, therefore, a non-acceptable variant of the name could be accidentally included. That is why, this stage is marked as optional and must be configured and performed with extreme care.

Pos-tagging parts of speech according to a grammar and filtering the non-relevant ones. The objective of this stage lies in “classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging” [27]. In order to perform this task both a lemmatizer based on Wordnet and a grammar for corporate names (“NN”-nouns and “JJ”-adjectives connected with articles and prepositions) have been designed, see Fig. 4
                              . Once words are correctly tagged, next step consists in filtering non-relevant categories in corporate names keeping nouns and adjectives, as an example Fig. 4 also shows how to walk and filter nodes in the parsed tree.

Cluster corporate names. This task is in charge of grouping names by similarity applying a string comparison function. Thus, if the clustering function is applied n times any name will be grouped by “the most probably/used name” according to a threshold generated by the comparison function. To do so, the CORFU technique has been configured to use the WRatio function to compare strings (available in the Levenshtein Python package) and a customized clustering function.

Validate and reconcile the generated corporate name via an existing reconcile service (optional). This last step has been included with the objective of linking the final corporate name with an existing information resource and adding new alternative labels. The OpenCorporates and DBPedia reconciliation services have been used in order to retrieve an URI for the new corporate name. As a consequence, the CORFU unifier is partially supporting one of the main principles of the LOD initiative such as unique identification.

In order to enable a policy-making process based on public procurement data different pieces of information should be considered: geographical information, data and time, amount, type of contract, payer and payee profiles [50], etc. As the related work section has outlined, there are already approaches to deliver such information in a standard way easing the access to this valuable data and enabling a better way of exploiting information through the extraction of statistics. Thus, standardized geographical information can be found in some classifications such as NUTS or the product scheme classifications that have been already unified [47] under a common and shared data model (the Resource Description Framework—RDF).

In the context of this research work, corporate names or, more specifically, payers and payee names are being processed with the CORFU technique to deliver a common name that can be re-used in the “PublicSpending” initiative [1,2].
                        8
                     
                     
                        8
                        
                           http://publicspending.net.
                      Once relevant public procurement data is unified in different datasets, network analysis techniques can be used to compare variables such as location, amount, time or type of contract in public spending data. As first step, public spending data is represented as a graph to create a payment network in which interesting relations among underlying agents can be easily captured, see Fig. 5
                     . This graph is created by the payments coming from payers (public institutions) to payees (mainly private organizations). Secondly, to interpret the graph every node is either the payer or the payee that is linked through a payment, which is characterized by its amount, time and type of contract. Moreover, measures of centrality, degree centrality (in-out degree, weighted degree) and measures relative to the rest of the network such as betweenness centrality are used to understand the public spending graph [[17,41]]. Finally, as a detailed example, the process of promoting public spending data and unifying corporate names with the CORFU technique has been applied to the next geographical entities: United States, United Kingdom, Australia, Greece, the State of Alaska and Massachusetts and the city of Chicago, see also Table 1
                     . Thus, a policy-maker is now able to graphically take the most of public spending data in a certain area and the purposed process also enables the possibility of preventing new necessities (products or services) or ensuring transparency to name a few.
                        
                           •
                           United States: there is one major node (payer) in the graph (“Department of Defense”), dispersing almost all (99%) the total budget (weight) of the graph. Obviously, defense has the lion's stake in subprime awards. Furthermore, most of the money (92%) is received by CTA Inc., which is solely connected to this department (no connections to other nodes). The dispersion of the public budget is made through 42 agents to the contractors. There are either payer or payee nodes in the graph (no mixed mode both payer and payee-except Smithsonian Inst.), consequently there are no brokers in the network resulting the diameter to equal 1 and the modularity fairly low at 0.022. Due to the above characteristics, there are mainly corporates of the deference-military sector (only US companies are eligible to become vendors due to legal restrictions) coupled by some major global enterprises with less weighted degree as they do not awarded purely defense contracts.

United Kingdom: is characterized by five major nodes (payers): health, family, education, business innovation and skills, local government disperse 88% of the total budget. The major payees are local authorities or funds responsible for the proper exploitation of the funds received. There are also private companies receiving money for goods and services mainly information technology, telecommunications and consulting. The dispersion of the public budget is made through 26 agents (payers).

Australia: there are two major nodes (payers) in the graph (Department of Defense and Defense Materiel Org.), dispersing almost half of the total budget (weight) of the graph. These two nodes are also the top out-degree nodes in the graph (22% of the payment links). This indicates that “Defense” is a major factor in the Australian economy sustaining a network of enterprises that selling goods and services suited for the defense needs of the state. The 35% of the budget is spent by institutions related to education, immigration, health and social security, taxation, public order and telecommunication. This reflects, in general, the priorities and major concerns of the Australian state. The dispersion of the public budget is made in a balanced way as there are no private enterprises receiving excessive amounts of money (except FMS and Central Office).

Greece: there two major projects in Greece: 1) the subway in Athens and Thessaloniki and 2) Egnatia Runway in North Greece. The Ministry of Public Order was involved in significant construction works in regard to other ministries or regional authorities. Universities and research institutes are important hubs in the network maintaining a wide network of payees that offer a variety of services for them and are significant contributors to the dispersion of funds. This also applies to Regional Authorities in local scale. Pension Funds, Labor Office and other social security institutes have a significant amount of payments for services and are important agents in the network presented, independently of their actual spending for pensions or social security allowances.

State of Alaska: there are distinct characteristics originating from the special conditions that apply to the region's low population, vast areas of natural resources and ecosystems, weather conditions, native (indigenous) population and distance from global markets. All the above result to a payment network where funds are allocated smoothly to local companies and authorities where health, education, environment, natural resource management, transportation and construction have the lead.

State of Massachusetts: the dispersion of the public budget is made in a balanced way through 157 payers to a network of local institutes and authorities. There are major global players as well but the amounts receiving are smaller due to the bigger amounts that are targeted to health, education and legal institutions and to local authorities. The graph diameter is 1 as there are payer/payee only nodes and modularity is 0.76 indicating the local structure of the payment network. It is worth noticing that there is great variety in the services offered, there are many companies present for every sector (competition) and the balanced value of the in degree indicates a mature market. Massachusetts is famous for its health and educational institutions and this fact is validated from the output data.

City of Chicago is the only city examined (compared to countries or states) but the volume of data ranging from 1993 to 2013 offer a total amount for examination fairly comparable to a state or country. There are distinct characteristics originating from the fact that a city has different needs and priorities from a state/country and of course many resemblances to one (e.g. no need for defense/border safeguarding expenses). The dispersion of the public budget is made in a balanced way through 52 agents (payers) to a network of local companies and authorities and there are also major global players as well. The fact that the city is a transportation hub and resides to Lake Michigan is pictured on the graph as major nodes both payers and payees are present and belong to transportation, water management and public utilities sector.

@&#EVALUATION@&#

Since the CORFU approach has been successfully designed and implemented,
                           9
                        
                        
                           9
                           
                              https://github.com/chemaar/corfu.
                         it is necessary to establish a method to assess quantitatively the quality of results. To do so, the following steps have been carried out.
                           
                              1.
                              Configure the CORFU technique, see Table 2
                                 .

Execute the algorithm taking as a parameter the file containing the whole dataset of company names.

Validate (manually) the dump of unified names.

Calculate measures of precision, see Eq. (1), recall, see Eq. (2), and F1 score (the harmonic mean of precision and recall), see Eq. (3), according to the values of tp (true positive), fp (false positive), tn (true negative) and fn (false negative).

In particular, this evaluation considers the precision of the algorithm as “the number of supplier names that have been correctly unified under the same name” while recall is “the number of supplier names that have not been correctly classified under a proper name”. More specifically, tp is “the number of corporate names properly unified”, fp is “the number of corporate names wrongly unified”, tn is “the number of corporate names properly non-unified” and fn is “the number of corporate names wrongly non-unified”.

As previous sections have introduced, there is an increasing interest and commitment in public bodies to create a real transparent public administration. In this sense, public administrations are continuously releasing relevant data in different domains such as tourism, health or public procurement with the aim of easing the implementation of new added-value services and improve their efficiency and transparency. In the particular case of public procurement, main and large administrations have already made publicly available the information with regard to public procurement processes. In this case of study, public procurement data coming from the Australia government are used to test and validate the CORFU unifier. More specifically, a dataset of supplier names in Australia in the period 2004–2012 containing 430,188 full names and 77,526 unique names has been selected. The experiment has been carried out executing the aforementioned steps in the whole dataset to finally generate a dump containing for every supplier the raw name and the unified name. On the other hand, the CORFU stepwise method has been customized to deal with the heterogeneities of this large dataset as Table 2 summarizes.

@&#RESULTS AND DISCUSSION@&#

According to the results presented in Table 3
                        , the precision and recall of the CORFU technique are consider acceptable for the whole dataset due to the fact that a 48% (77,526−40,278=37,248) of the supplier names have been unified with a precision of 0.762 and a recall of 0.311 (best values must be close to 1). The precision is pretty good but the recall presents a low value because some corporate names were not unified under a proper name; some of the filters must therefore be improved in terms of accuracy.

In order to improve the results for relevant companies, the experiment has also been performed and evaluated for the first 100 companies in the Forbes list, actually 68 companies were found in the dataset. In this case, results show a better performance in terms of precision, 0.926, and recall, 0.926, and all these supplier names, 299 in the whole dataset, were unified by a common correct name. The explanation of this result can be found due to the fact that some of the parameters of the CORFU technique were specially selected for unifying these names because of their relevance in world economic activities.

On the other hand, it is important to emphasize that the last step of linking these names with existing web information resources using the reconciliation service of OpenCorporates or DBPedia in Open Refine can generate 37,248∗0.762=28,383 correct links (36.61%) instead of the initial 8% that was reached in the first mapping process (without name unification). Thus, the initial problem of linking (n string literals→1 company→1 URI) has been substantially improved.

Finally, the frequency distribution of supplier and number of appearances are depicted in Fig. 6
                         with the objective of presenting how the cloud of points (appearances) that initially were only one per supplier has emerged due to the unification of names, for instance in the case of “Oracle” 75 apparitions can now be shown. On the other hand and due to the unique identification of supplier names, new RDF instances are generated, see Fig. 7
                        , and can be querying via SPARQL to make summary reports of the number of rewarding contracts by company, see Fig. 8
                        .

To illustrate the robustness of the presented approach to unify corporate names, a second experiment has been carried out as an extension of the previous one. This robustness experiment is necessary to ensure that results are creditable and it is based on similar studies that have been performed in the field of social network analysis [[7,34,43]] when natural language processing techniques are used. In this case and due to the fact that human-validation is not completely possible, a test-campaign (23 tests) based on random walk techniques [9] has been designed to measure again the precision and recall of the CORFU technique. Since the unification process generates a pair (corporate name, unified corporate name), it is possible to design a search process that taking as input a dataset of company names and a query (other corporate name), will match all relevant corporate names. Moreover, if we execute a query against a dataset which names have not been unified and compare the results to the ones generated using the same query against a dataset which names have been unified (expected results), we can extract metrics of precision, recall and the F1 score. In order to design this experiment, the following steps have been carried out creating a set of test cases.
                           
                              1.
                              Select a set of datasets of company names, 
                                    C
                                    =
                                    
                                       
                                          
                                             C
                                             1
                                          
                                          ,
                                          
                                             C
                                             2
                                          
                                          ,
                                          ..
                                          .,
                                          
                                             C
                                             k
                                          
                                          ,
                                          ..
                                          .,
                                          
                                             C
                                             n
                                          
                                       
                                    
                                 , where every element 
                                    
                                       C
                                       k
                                    
                                  is a dataset of company names. In this case, three different groups of datasets, see Table 4
                                 , have been downloaded and processed to extract corporate names.
                                    
                                       •
                                       The “Public Spending Data” from the United States of America (USA) Spending Data portal.
                                             10
                                          
                                          
                                             10
                                             
                                                http://www.usaspending.gov/data.
                                           In this case, the vendor names of every public contract between 2000 and 2015 have been selected as corporate names.

The “Basic Company Data” from the United Kingdom (UK) data portal.
                                             11
                                          
                                          
                                             11
                                             
                                                http://data.gov.uk/dataset/basic-company-data.
                                           The company name has been selected as corporate name.

The data dump of corporate information from “The CrocTail project”.
                                             12
                                          
                                          
                                             12
                                             
                                                http://croctail.corpwatch.org/.
                                           The company name has been also selected as corporate name.

For every company name dataset 
                                    
                                       C
                                       k
                                    
                                    ∈
                                    C
                                  applies the CORFU technique generating a new company name dataset 
                                    
                                       C
                                       CORFU
                                       k
                                    
                                 . The configuration of the technique has been the same as the one presented in Table 2.

Create a common set of queries, 
                                    Q
                                    =
                                    
                                       
                                          
                                             q
                                             1
                                          
                                          ,
                                          
                                             q
                                             2
                                          
                                          ,
                                          ..
                                          .,
                                          
                                             q
                                             k
                                          
                                          ,
                                          …
                                          
                                             q
                                             n
                                          
                                       
                                    
                                 . In the previous experiment, the list of the first 100 companies in the Forbes list was used to apply the CORFU technique. In this case, the list of the world's biggest public companies (2000) in the Forbes web site
                                    13
                                 
                                 
                                    13
                                    
                                       http://www.forbes.com/global2000/list/#page:20_sort:0_direction:asc_search:_filter:All%20industries_filter:All%20countries_filter:All%20states.
                                  has been extracted and processed to create a set of queries for every company name in the list.

Design and implement a search process. To do so, a program on top of the Apache Lucene and Solr search engines has been implemented to index any dataset of corporate names and to provide a search engine. This engine has been configured using the standard filters and a RAM-stored index.

Run the search process taking as parameters every company name dataset in 
                                    
                                       C
                                       CORFU
                                       k
                                    
                                  and the set of queries 
                                    Q
                                  to generate for every test case a set of expected results.

Run the search process taking as parameters every company name dataset in 
                                    C
                                  and the set of queries 
                                    Q
                                  to generate for every test case a set of real results.

Extract measures of precision, recall and F1 score by comparing the expected and real results. A Python program has been implemented to automatically process all results generated by all test cases.

@&#RESULTS AND DISCUSSION@&#


                           Table 5
                            shows the aggregated metrics of precision, recall and the F1 score after a total execution of 92,000 queries, 2000 target corporate names∗23 datasets∗2 types of corporate names (unified and non-unified). According to the results, the average precision of the CORFU technique is closed to 0.5 for most of cases, see Fig. 9
                           . It also seems that the number of company names has a direct impact on the precision. The main reason of this behavior is due to the fact that as much company names are available as more contextual information can be extracted and, thus, corporate names can be easily unified. However, it is also clear that there is a decrease in the precision regarding the first experiment. Since the number of company names has been dramatically increased, it is possible that more variants for the same name have been also included implying the necessity of refining the CORFU technique to cover a broad scope of names. Furthermore, the contextual information of the experiment could be carefully revised to ensure higher precision values.

Some key limitations of the presented work must be outlined. The first one relies on the sample size; our research study has been conducted in a closed world and, more specifically, using corporate names that have been extracted from a set of public sources. That is why, results in a broad or real scope could change, in terms of precision, since more complex names and contextual information could be found. For instance, we have evaluated the possibility of gathering corporate names from the public API of OpenCorporates/OpenLEIs or DBPedia but due to the restrictions on the use of the APIs we have preferred to download existing data dumps. However, the research methodology, the design of experiments and the creation of a kind of benchmark for testing the CORFU technique have been demonstrated to be representative and creditable.

On the other hand, we have automatically generated test cases from real data to avoid the necessity of human validation. In this case, we have focused on the creation and publication of set of datasets of corporate names for testing unification name processes due to the fact that the handmade creation of mappings between corporate names requires a great effort with a high probability of losing robustness (the same company can be named in a different way depending on the users and domain discourses). However, we consider that the precision and recall metrics are helpful to make a first estimation of the advantages of applying the CORFU technique to unify corporate names.

Building on the previous comment, we cannot either figure out the internal budget, methodologies, vocabularies, experience and background of specific sites to gather and create corporate information. We merely observe and re-use existing public and on-line knowledge sources to provide an accurate name unification process. Finally, we have also identified the necessity of re-designing the CORFU technique to scale up and to support large datasets since the performance of the algorithm also decreases depending on the number of corporate names.

@&#CONCLUSIONS AND FUTURE WORK@&#

A technique for unifying corporate names in the e-Procurement sector has been presented as a step towards the unique identification of organizations with the aim of accomplishing one of the most important LOD principles and easing the execution of reconciliation processes. The main conclusion of this work lies in the design of a stepwise method to prepare raw corporate names in a specific context, e.g. Australia supplier names, before performing a reconciliation process. Although the percentage of potential right links to existing datasets has been dramatically improved, it is clear that human-validation is also required to ensure the correct unification of names. As a consequence, the main application of CORFU can be found when reporting or tracking activity of organizations is required. However, this first effort has also implied, on the one hand, the validation of the stepwise method and, on the other hand, the creation of a sample dataset that can serve as input for more advanced algorithms based on machine learning techniques such as classifiers. Although the precision of the CORFU technique decreases when processing large datasets, it has been demonstrated to be creditable in a broad scope. From public administrations' point of view, this technique also enables a greater transparency providing a simple way to unify corporate names and boosting the comparison of rewarded contracts.

Finally, further steps in this work consist in the extension of the stop-words sets for corporate names, a better acronym detection and expansion algorithm, other techniques to make string comparisons such as n
                     −
                     grams 
                     [51] and the creation of a new final step to enhance the current implementation with a classifier that can automatically learn new classes of corporate names or automatically infer grammars for representing any type of corporate name. Furthermore, the technique must be reported to the international “Public Spending” initiative, as supporting tool, to be applied over other datasets to correlate and exploit public contracts meta-data.

@&#ACKNOWLEDGMENTS@&#

This work is part of the “PublicSpending.net” effort carried out in cooperation with Marios Meimaris ("Athena" Research and Innovation Center), Giorgos Vafeiadis (Technical University of Athens), Giannis Xidias (University of the Aegean) and Michalis Klonaras (OTE S.A.).


                     
                  

@&#REFERENCES@&#

