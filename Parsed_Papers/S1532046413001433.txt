@&#MAIN-TITLE@&#A biological continuum based approach for efficient clinical classification

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A significant speedup of 4.73-fold was achieved with the employment of BCEN.


                        
                        
                           
                           BCEN provides a reusable framework for clinical feature pre-selection.


                        
                        
                           
                           Efficient redevelopment of up-to-date clinical classification models is possible with BCEN.


                        
                        
                           
                           Risk factors identified in BCEN highly overlap those found in previous clinical studies.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Classification

Dimensionality reduction

Etiological network

Feature selection

Genetic algorithm

Support vector machine

@&#ABSTRACT@&#


               
               
                  Clinical feature selection problem is the task of selecting and identifying a subset of informative clinical features that are useful for promoting accurate clinical diagnosis. This is a significant task of pragmatic value in the clinical settings as each clinical test is associated with a different financial cost, diagnostic value, and risk for obtaining the measurement. Moreover, with continual introduction of new clinical features, the need to repeat the feature selection task can be very time consuming. Therefore to address this issue, we propose a novel feature selection technique for diagnosis of myocardial infarction – one of the leading causes of morbidity and mortality in many high-income countries. This method adopts the conceptual framework of biological continuum, the optimization capability of genetic algorithm for performing feature selection and the classification ability of support vector machine. Together, a network of clinical risk factors, called the biological continuum based etiological network (BCEN), was constructed. Evaluation of the proposed methods was carried out using the cardiovascular heart study (CHS) dataset. Results demonstrate a significant speedup of 4.73-fold can be achieved for the development of MI classification model. The key advantage of this methodology is the provision of a reusable (feature subset) paradigm for efficient development of up-to-date and efficacious clinical classification models.
               
            

@&#INTRODUCTION@&#

The efficient development of accurate clinical classification models has been a challenge for many reasons. One problem that is commonly encountered is the ‘curse of dimensionality’ [1], where the linear growth of clinical features (i.e. predicators) results in an exponential growth in the search space. This inevitably hinders the development of classification models as it becomes computationally expensive to investigate a plethora of clinical features simultaneously using search heuristics that analyze features in combinations (particularly, when performing multivariate analysis based on wrapper approach). This situation is exacerbated by the fact that up-to-date and sophisticated clinical classification models need to be constantly developed in order to continually improve the quality of clinical diagnosis. Specifically, the clinical classification models need to be rebuilt whenever new clinical risk factors that could potentially ameliorate the performance of the classification model are introduced. An example of such clinical effort is the perpetual studies of different types of clinical risk factors and approaches that could improve the ability to identify events of myocardial infarction (MI) [2,3]. This is of paramount importance as MI is a leading cause of morbidity and mortality in many developed countries, such as the United States (US) and the United Kingdom (UK) [4–6]. Despite considerable advances in medicine, MI approximately occurs every 34s in the US and about 15% who experience MI will die from it [4]. Moreover, MI is difficult to ascertain in patients presenting to the emergency department with anterior chest pain [2]. This advocates for the need of an efficient approach to develop up-to-date MI classification models for performing accurate diagnosis.

Furthermore, investigation of the association between a range of clinical observations (e.g. medical history, chemotherapy, stage of disease, gene, etc.) and the disease at the human population level is important as it has demonstrated promising potential for improving disease classification performance [7,8]. However, when such an investigation is carried out on a larger scale, this would involve a large amount of clinical features, making analysis challenging and even computationally infeasible. Additionally, it also hinders the ability for any machine learning method to perform accurate disease classification. One approach to mitigate the aforementioned problems is through dimensionality reduction – where significant clinical risk factors are identified, reducing the total number of predicators that need to be analyzed.

In this paper, we introduce a novel clinical feature selection methodology for the development of MI classification model. This approach utilizes on the conceptual framework of biological continuum (BC) [9,10], the optimization capability of genetic algorithm (GA) [11] for performing feature selection and the classification ability of support vector machine (SVM) [12–14] for dichotomizing patients experiencing a phenotypic manifestation from healthy individuals. The BC is the hierarchy of the human organism comprising body, systems, viscera, tissue, cells, proteins and genes. In this study, it provided the biological paradigm necessary for segregating a range of available clinical features; offering the advantage of reducing the number of clinical features that needs to be analyzed concurrently. A GA based wrapper approach using SVM, which selects significant clinical features capable of dichotomizing patients experiencing a phenotypic manifestation from healthy individuals, was implemented. This hybrid algorithm (called GA-SVM) was used to identify important clinical features at each level of the BC and incrementally built a network of clinical risk factors, called the biological continuum based etiological network (BCEN). The primary advantage of BCEN used for the construction of up-to-date clinical classification model is that it allows new clinical features to be considered for incorporation into the classification model without the need for a total reanalysis from scratch.

The reliability of the constructed BCEN was assessed by comparing the set of identified risk factors found in the (obesity-system) sub-network, with the risk factors found in previous clinical studies. Promising results were obtained from this analysis. An MI classification model was subsequently developed based on the clinical features identified and present in the BCEN. Significant reduction in the computational time required to develop the classification model was achieved. It is noteworthy that comparable classification accuracy was obtained between the proposed method (i.e. pre-selection of clinical features using BCEN) and the baseline approach (i.e. no pre-selection was performed). The Cardiovascular Health Study (CHS) [15] dataset was analyzed in this study.

The rest of the paper is organized as follows. Section 2 provides the background information on feature selection. In section 3, the experimental methodology involved in the development of the clinical feature selection technique and the clinical classification model is presented. The experimental results are presented in Section 4 and discussed in Section 5. Finally, conclusions are drawn in Section 6.

@&#BACKGROUND@&#

Conventionally, clinical predictions which provide the disease diagnosis for an individual are based on expert knowledge. However, with the exponential growth of clinical data generated in healthcare industries, this approach has become more and more difficult and costly. An approach to mitigate this challenge is to process and analyze the large amount of clinical data, extracting knowledge that enables support for cost-containment and decision making [16]. Machine learning is one method that has been proposed to address this issue. It provides the techniques necessary for the analysis of the data, discovery of hidden patterns and provides healthcare professionals with an additional source of knowledge for decision making. In the parlance of literature, machine learning is defined as a branch of artificial intelligence that postulates a set of computer-based methods for automatic analysis of information and recognition of patterns through repeated learning from the training data [17], and is a more powerful and sophisticated descendant of traditional statistical models. It is generally model-free and is capable of efficiently detecting and modeling the non-linear interactions in high dimensional datasets. Additionally, the associations or patterns detected by machine learning methods tend to be logical and can be identified by human experts if they analyze the problem carefully enough [18]. Clearly, this entails that machine learning is capable of saving both the time and effort necessary for the discovery of underlying patterns.

Clinical prediction (e.g. diagnosis of cardiovascular disease) based on machine learning approaches has gained popularity over the years [2,16,19–24] and shown to be an extremely useful tool in medical innovation [21]. It is often based on the patient’s unique clinical, genetic and environmental characteristics and plays a significant role in healthcare decision making and planni ng. Since each clinical feature collected is associated with a different financial cost, diagnostic value and risk [25], it is highly desirable to reduce the number of clinical tests that need to be taken by a patient. This would inevitably reduce the financial cost, and the time incurred on both the analysts and patients. One approach commonly adopted by machine learning techniques to reduce the number of clinical features while improving the diagnostic/classification accuracy is feature selection.

Feature selection is the process of selecting a subset of relevant features for model construction and provides better insights into the target concept of a real-world problem [21]. It differs from other dimensionality reduction techniques like project and compression where their original representation of the variables is modified. Therefore, feature selection has the advantage of preserving the original semantics of the features which enables domain experts to interpret the selected features. Furthermore, it has shifted from being an illustrative example to one of real prerequisite for developing classification models [26]. This is, in part, because of the exponential increase in the dimensionality of the data (e.g. in clinical and bioinformatics domains), the fact that most classifiers were originally not designed to handle plethora of irrelevant features, and the need to generate more accurate classifiers efficiently. In general, feature selection aims to identify a parsimonious subset of useful features (from a large set of features) that (1) does not decrease the classification accuracy, (2) reduces the computational time needed to learn a sufficiently accurate classification model, (3) does not acutely changes the class distribution while adequately representative for descripting the target concept, and (4) reduces the amount of examples that need to be collected in order to develop a classification model with the desired accuracy [27,28].

Feature selection algorithms typically fall under 4 categories depending on how it is performed in relation to the classification algorithm. They include (1) selection based on expert knowledge, (2) filter approach, (3) wrapper approach, and (4) embedded approach. Each has its own competitive advantages and drawbacks. Selection based on expert knowledge (e.g. human domain expert or referencing the scientific literature) offers a set of features with high interpretability in relation to the target concept. However, its major drawbacks are that it can be time consuming and human expert is required to perform the task. An illustration of this approach is demonstrated in [25], where the number of interaction tests that need to be performed can be limited with the use of experimental knowledge of the biological network. More specifically, knowledge extracted from protein interaction databases reduces the number of interaction tests from 1.25×1011 to 7.1×104, allowing more efficient analysis of genome-wide studies to be carried out.

Filter methods, on the other hand, evaluate the relevance of each feature by assessing only the intrinsic characteristics of the data. Although this approach does not need a domain expert to intervene, is simple, efficient and can easily scale to very high-dimensional datasets, it does not always guarantee improved performance [29] as it ignores the inductive bias associated with the classifier [30]. Examples of filter techniques include chi-square test, t-test, information gain, correlation-based feature selection and Markov blanket filter.

Wrapper methods embed the inductive bias associated with the classifier within the feature selection process. In this case, subsets of features are generated and their performance is assessed by training and testing them on a specific classification algorithm. The advantages of this approach are: (1) the freedom to choose the desired classification algorithm, (2) allowing interaction between feature selection and model selection, and (3) ensuring that feature dependencies are taken into consideration (i.e. the need to add or remove more than 1 feature at the same time in order to improve the performance [25]). Consideration of feature dependencies is important, especially in the medical field, as it has become evident that multiple genes collectively contribute to the etiology and clinical manifestation of human diseases [31]. Hence, important genotypic factors might be missed if they have been examined in isolation or in a linear fashion – without allowing for potential interactions. This situation would be exacerbated when performing genome-wide association studies where hundreds of thousands of single nucleotide polymorphisms (SNPs) need to be analyzed. Wrapper approach, on the downside, becomes computationally intensive when the number of features grows exponentially. This is because every feature subsets generated need to be executed on the selected learning algorithm. Moreover, it has a higher risk of over-fitting the classifier than filter approach. Examples of this technique include sequential forward selection, sequential backward selection, simulated annealing, genetic algorithm and estimation of distribution algorithm.

Finally, embedded approach integrates the process of identifying the optimal subset of features within the learning algorithm. Based on this mechanism, it has the advantage of being more computationally efficient (compared to wrapper approach) while maintaining interaction with the classifier. Examples include decision trees and weighted naïve Bayes.

@&#METHODOLOGY@&#

The CHS dataset, as described in [15], is an epidemiology study of the elderly (defined as adults aged 65 and older). It comprises of elderly subjects from four US communities, namely Forsyth County, North Carolina; Sacramento County, California; Washington County, Maryland; and Pittsburgh, Pennsylvania. A total of 5888 individuals from urban and rural areas form the baseline cohort of CHS. Eligible individuals were sampled from Medicare eligibility lists in each area. Eligible participants included all individuals sampled from the Health Care Financing Administration (HCFA) sampling frame – they were 65years or older at the time of examination, non-institutionalized, expected to remain in the area for the next 3years, able to give informed consent and do not require a proxy respondent at baseline. Individuals who were wheelchair-bound at home at baseline, receiving hospice treatment, radiation therapy or chemotherapy for cancer were excluded. The eligible individuals were examined yearly from 1989 to 1999. Extensive physical and laboratory evaluations were carried out to identify the presence and severity of cardiovascular disease (CVD) risk factors – such as hypertension; hypercholesterolemia and glucose intolerance; subclinical disease, such as carotid artery atherosclerosis; left ventricular enlargement; and transient ischemia. Criteria for identification of MI events include: observation of evolving Q-wave, cardiac pain and abnormal enzymes together with an evolving ST-T pattern or new left bundle branch block. A total of 355 clinical features related to the individual’s health status were selected from the CHS dataset for this study.

The dataset was chosen because of (1) the relatively high prevalence of coronary heart disease (CHD) among the elderly, (2) worldwide demographic aging, (3) paucity of information regarding risk factors for CHD among elderly, and (4) the changing clinical characteristics of CHD with advancing age [4,15,32,33].

Several steps were taken to construct the BCEN for MI with the canonical flow illustrated in Fig. 1
                        . A succinct description of the key steps taken is given below while we dedicate separate sections for the discussion of the details:
                           
                              1.
                              Sparse records were removed and missing entries in the dataset were imputed to ensure good quality data is used to model the risk factors associated with MI. This was performed with the K-nearest neighbor (KNN) algorithm [34] – it calculates the missing value by taking the K nearest training set vectors (based on Euclidean distance) into consideration.

Healthy individuals, forming a large proportion of the dataset in relation to the number of patient records, were sampled to avoid jeopardizing the ability of SVM to learn and generalize. This is carried out with Kohonen Self-Organizing Map (SOM) [35], where a representative subset of the majority class (i.e. healthy individuals) present in the CHS dataset was selected, a process known as under-sampling.

Clinical features, such as blood pressure, electrocardiography (EKG) readings, ultrasound data, hematology data, etc., were segregated along the BC – the hierarchy of the human organism. It comprises 7 levels, namely the body, system, viscera, tissue, cell, protein and gene.

GA-SVM, a hybrid algorithm used to identify significant clinical features, was implemented. It is used repeatedly at each level of the BC to identify significant risk factors that are related to the different phenotypic manifestations, and ultimately MI.

With the significant risk factors identified at the different levels of the BC, they were consolidated to construct a consensus network, known as the BCEN in this work. These risk factors, in turn, were used to perform MI classification using the GA-SVM algorithm.

As with many datasets collected from real subjects and patients, missing data is unavoidable. This may be due to various factors, e.g. the refusal of respondents, malfunction of equipment, data not entered correctly and the death of patients [36]. Moreover, since the quality of the results is largely determined by the quality of the data used in the analysis, detailed consideration was given before using the CHS dataset. It was found that the CHS dataset contains a significant percentage of missing information. Hence, data imputation was first conducted.

Data imputation, the process of substituting missing values in a dataset with plausible values, was performed using KNN. KNN imputation was used because of its excellent performance in estimating missing values [37–40] and its ability to estimate both qualitative and quantitative attributes. This makes it highly suitable for extrapolating the missing entries in the CHS dataset.

Firstly, individuals with unknown MI status were removed from the analysis. Next, to foster more accurate data imputation, individuals and clinical features with high percentage of missing entries were removed. It is important to have low percentage of missing values because the accuracy of the imputed result would suffer if too little complete entries were available for KNN to reference when estimating the missing values [37,40,41]. Hence, individuals and clinical features with more than 20% and 4.5% missing entries, respectively, were removed. Consequently, the resultant dataset was normalized to unit variance before data imputation was performed using KNN. This is important as it ensures that variables with large scale do not dominate the (Euclidean) distance measure [42].

The optimal value of K for each clinical feature was determined by 10-fold cross-validation. After the value of K for each clinical feature had been determined, data imputation for each missing attribute was performed. The type of replacement method used depends on the type of data present in each clinical feature. For instance, if the data is categorical, a reliable choice is to use the mode of the K nearest neighbors to assign the value for the missing entries [34,38]. On the other hand, if the data is continuous, the weighted-mean of the K nearest neighbor is used instead to calculate the missing value. Weighted-mean estimation has been demonstrated in [37,43] to be robust and accurate.

The class imbalance data problem is not uncommon in medical datasets where the data is predominated by the healthy subjects (i.e. controls), with only a small number of disease-affected subjects (i.e. cases). Consequently, this limited the effectiveness ability of standard machine learning algorithms – where the algorithms tend to be overwhelmed by the major class and ignore the minor one. This, in turn, hinders performance [44,45]. This class imbalance data problem prevails in the CHS dataset as well. Therefore, data balancing was performed before deploying the data to GA-SVM.

SOM, an unsupervised (neural network) learning algorithm, was employed to under-sample the major class. This algorithm was chosen because it is capable of generating high quality samples that are representative of the original dataset [35] and it has been shown in [46] that SOM outperforms random selection. Once the imputed dataset was obtained, the SOM was trained in two phases; namely, the ordering phase and the tuning phase. Two key adaptive parameters, neighborhood size and learning rate, were used when training the SOM. Neighborhood size defines the number of neurons that surround the winning neuron (i.e. most stimulated neuron) at each epoch, while the learning rate controls the degree of change for the adapting neurons.

During the ordering phase, large initial neighborhood size (i.e. 10) and learning rates (i.e. 0.9) were used. Conversely, small neighborhood size (i.e. 1) and learning rates (i.e. 0.02) were used during the tuning phase – where the neighborhood size will shrink progressively to 1. This is to allow the SOM to adjust quickly to the input pattern during the ordering phase and to stabilize the feature map during the tuning phase [35]. The following value for the SOM parameters was determined experimentally and used in this study: number of neurons: 21 by 21; topology function: hexagon; distance function: Euclidean; epoch: 1000; ordering phase learning rate: 0.9; tuning phase learning rate: 0.02; initial neighborhood size: 10; final neighborhood size: 1. The reason for using these values is because they have shown to provide reasonable performance.

The Biological Continuum was central to the development of the BCEN. It was utilized in this case to provide the necessary biological paradigm to relate the disease mechanisms to the clinical manifestations at various levels of the biological continuum. Upon analyzing the clinical features, it was found that these features fall under 4 key levels along the BC, namely: body, system, viscera and protein level. Clinical features related to medication were removed from the study as it was difficult to adjudicate to which level of the BC they belong. Categorization of the rest of the clinical features, in relation to the levels of the BC, was undertaken using the following guidelines:
                              
                                 •
                                 
                                    Body level – Contains clinical features related to individuals’ personal statistics (e.g. age, weight), lifestyle (e.g. smoking status, exercise intensity) and cardiovascular events which that individual is experiencing.


                                    System level – Consists of clinical features related to individuals’ medical history (e.g. arthritis, diabetes), symptoms (e.g. hearing/vision problems) that the individual is experiencing and blood pressure measurements.


                                    Visceral level – Clinical measurements, e.g. EKG, ultrasound data and treatment specific to an organ were classified under this level.


                                    Protein level – Clinical features related to hematology were grouped under this level.

GA-SVM, a hybrid algorithm that comprises of (1) SVM that models the statistical properties necessary to distinguish healthy individuals from patients experiencing a clinical phenotype, and (2) GA that selects the significant features that contribute to the construction of an accurate SVM model, was implemented. In this work, SVM uses radial basis function (RBF) as its kernel function and is defined as:
                              
                                 (1)
                                 
                                    K
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    =
                                    exp
                                    (
                                    -
                                    γ
                                    |
                                    |
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    -
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                    |
                                    
                                       
                                          |
                                       
                                       
                                          2
                                       
                                    
                                    )
                                 
                              
                           where γ is a variable used to adjust the width of the Gaussian functions of the kernel. RBF is used due to its ability to solve non-linearly separable problems, low complexity involved during model selection and excellent performance. Two parameters, namely the regularization cost and gamma (used in RBF) parameters, were tuned over the recommended range [2−5, 213] and [2−15, 23] respectively [47]. Optimization of SVM parameters were performed by evaluating a set of cost-gamma combinations defined using uniform design (UD) method [48]. UD is a technique that scatters a set of points uniformly across the cost-gamma landscape, proposed to alleviate the computational loads associated with the search for the optimal cost-gamma pair [49]. This search process begins by initializing a 30-points UD (global) search across the defined cost-gamma landscape. Next, it identifies the top 5 most accurate (global) cost-gamma pairs, where they form the centroid for 10-points UD (fine) search. If improved accuracy was achieved, the points will form the centroid for another 10-points UD search. This process repeats until no further improvement is achieved. Fig. 2
                            provides an illustration of this method.


                           Fig. 3
                            provides the schematic illustration of GA-SVM algorithm. The flow of the algorithm is as follow: GA first (randomly) initializes a pool of clinical feature subsets (Fig. 3 – chromosome 1 to N) from the CHS dataset (consisting of M clinical features). Each bit in the chromosome is assigned with a value of either ‘1’ or ‘0’, indicating whether that feature is selected or eliminated from consideration by the classifier, respectively. This produces a pool of chromosomes representing different input features. Consequently, each chromosome was evaluated by SVM (where optimization of SVM parameters was performed independently for each chromosome) in an attempt to determine how informative and discriminative the clinical features are in relation to the associated clinical or subclinical manifestation. This evaluation is conducted by performing a 10-fold stratified cross-validation. Subsequently, these subsets of clinical features undergo natural selection, crossover and mutation phases postulated by GA. The process repeats until GA converges or the maximum number of generations has been reached. GA is considered to have converged if the maximum fitness value (i.e. balanced accuracy – the average of sensitivity and specificity) does not improve after 20 consecutive generations. Upon termination, the subset of clinical features that yielded the highest balanced accuracy will be selected and considered as significant risk factors. A consensus network was constructed if several combinations of clinical feature subset yielded the same fitness performance. The reason for doing this is to build a parsimonious model that maximizes the likelihood of the clinical features that are most influential to the development of the phenotypic manifestation. It was derived by identifying clinical features that existed in more than 75% of the highest-performing clinical feature combinations. The parameters value used by GA are as follow: population size: 250; maximum generation: 300; natural selection: stochastic universal sampling; crossover type: uniform crossover; crossover probability: 0.8; mutation probability: 0.01. These values were chosen because they provided satisfactory result when experimented over a range of values. The algorithm was written in Matlab (MathWorks Inc., Natick, MA) and executed in parallel using a high performance computer (HPC) cluster.

The underlying cause of MI is multifactorial and subtle, with nonlinear causal dynamics. Moreover, with the plethora of clinical predicators available, analysis of all of them becomes computationally impractical. In view of such challenges, GA-SVM, together with the conceptual framework of the BC, were used to construct the BCEN for MI.

Firstly, by segregating the clinical features into various levels along the BC, the number of clinical features to be analyzed is effectively reduced to the number of clinical features present at each level (i.e. dimensionality reduction). Secondly, with the employment of GA, which is capable of performing global heuristic searches both effectively and efficiently, the computational burden of discovering significant risk factors is alleviated. Finally, facilitated by SVM, which outperforms popular technique like multifactor dimensionality reduction (MDR) [50], it ensures that accurate estimation of the association between the clinical features at adjacent levels of the BC is being carried out.

At onset, clinical features grouped under the “body level” of the BC were input into GA-SVM for investigation. This step aims to identify clinical features that contribute significantly to the development of an accurate inference model for MI. Consequently, significant risk factors, defined in this work as risk factors that can potentially contribute to the manifestation of a clinical or subclinical risk, were identified - forming the top level of the BCEN. If any of these identified risk factors are continuous, it is discretized based on the extended χ
                           2 algorithm [51]. The reason for performing this step was to alleviate the associated computational complexity when analysis was performed with SVM.

Next, clinical features categorized under the “system level” of the BC were input into GA-SVM for investigation. This, similar to the earlier step, aims to identify clinical features that have a significantly impact to the inference of the phenotypic manifestation previously identified at the “body level”. The resultant output from this step forms the “system level” of BCEN. This procedure is repeated for the rest of the levels along the BC, constructing a probabilistic tree-structured BCEN at the end of this propagation. The resultant BCEN is capable of scrutinizing how, for instance, clinical features at the visceral level are associated with those at the system level and, in turn, how these features at the system level are associated with those at the body level. This concept is graphically illustrated in Fig. 4
                           .

After the construction of BCEN for MI, the distinct risk factors present in the network were used to develop an MI classification model. The performance (both classification accuracy and computational time) yielded with this approach was compared with an MI classification model that uses all clinical features present in the CHS dataset. GA-SVM was used as the classification algorithm for both the postulated approaches; hence, any benefits or drawbacks of using this classifier would prevail in both approaches.

@&#EXPERIMENTAL RESULTS@&#

Records and clinical features with considerable missing entries were removed. In addition, only records with known MI status were selected. This resulted in a dataset comprising of 4612 instances and 272 clinical features, with less than 1% of missing values (with respect to the entire dataset) and 40.8% of records with complete entries. The training and query datasets thus have 1881 and 2731 instances (both with 272 features), respectively. Subsequently, the K neighbor value for each clinical feature was determined based on the normalized training dataset. This yielded an average K value of 9.80, with standard deviation of 9.38. Data imputation was next performed to impute the missing entries found in the query dataset.

The imputed dataset obtained has a high fraction of controls (i.e. without MI – 4200 instances) and a relatively small portion of cases (i.e. with MI – 412 instances). SOM was thus employed to resolve this class data imbalanced problem. Under-sampling was performed on the major class (i.e. controls), yielding 441 instances. The final dataset produced has 853 instances and 272 clinical features.

The construction of a BCEN involved the segregation of the clinical features (173 diagnostic measurements and 1 MI status) along the BC. These 173 clinical features (after excluding medication) satisfied the characteristics of only 4 levels of the BC; namely, body, system, viscera and protein. Among these clinical features, 38, 74, 41 and 20 belong to the body, system, viscera and protein levels, respectively. A description of the segregated clinical features is provided online as an Appendix at http://www.bg.ic.ac.uk/jtay/web/chs_appendix.html. Readers may refer to the CHS data dictionary made available at the Biologic Specimen and Data Repository Information Coordinating Center (BioLINCC) website for more information (https://biolincc.nhlbi.nih.gov/studies/chs/).

Clinical features at the body level were first deployed to GA-SVM to determine the set of risk factors that were highly correlated to MI (root node). A total of 11 risk factors, namely ANGBASE (angina status at baseline), CHFBASE (congestive heart failure at baseline), STRKBASE (stroke status at baseline), CBD (self-reported stroke, transient ischemic attack (TIA) and cardiac endarterectomy), SCORE03 (social support score), AMOUNT (cigarettes smoked per day), WGTEEN (teenage weight category), OVRWT120 (obesity>120% ideal), EDUC (education level), WAIST (waist circumference – cm) and ALCOH (number of alcoholic beverages per week) were identified at the body level (note that these modifiable risk factors are also identified in earlier reported clinical studies [52,53]).

When extending the network, only clinical feature subsets (child nodes) that yielded a balanced accuracy of at least 0.7 were considered. This threshold was imposed to reflect only child nodes that are highly correlated to their parent node. This resulted in 5 inner nodes at the body level – namely ANGBASE, CHFBASE, STRKBASE, CBD and OVRWT120. This criterion was applied to the rest of the levels of the BC.

The resultant inner nodes identified at the system level include ANBLMOD (angina modified at baseline status), CLBLMOD (claudication modified baseline status), SUPPUL16 (supine reading: 30s heart rate), CHSTPN (chest pain) and VISPROB (vision problem). Table 1
                         provides the details of the best-performing clinical feature subsets that satisfy the aforementioned criteria. Note that none of the clinical features at the protein level correlated well with those at the visceral level. The authors believe that this could be due to the discontinuity in continuum along the BC (i.e. missing data at the tissue and cell levels) when estimating the association between the clinical features and phenotypic manifestation that resulted in the low performance.

The resultant BCEN consists of 111 distinct nodes (Body level: 11; System Level: 63; Viscera Level: 37) in total, accounting for 64.1% of the original number of clinical features analyzed. The complete BCEN for MI (created using prefuse toolkit [54]) is illustrated in Web Fig. 1 – available at http://www.bg.ic.ac.uk/jtay/web/chsBCENFull.html. The BCEN provides a visual and interactive etiological network for the user to visualize and comprehend the relationship among the different risk factors along the BC for MI. For our discussion here, a sub-network of the BCEN was analyzed because of its complexity and numerous interrelated risk factors present in the complete network. This sub-network is presented in Fig. 5
                        .

Referring to Fig. 5, it can be seen that obesity (OVRWT120), a risk factor of MI, has 34 risk factors at the system level that are highly correlated with it. These risk factors are related to rheumatology, physical function, oncology, pulmonology, thromboembolism, sleep disorder, ophthalmology, otolaryngology, cognitive function and endocrinology. They account for 45.9% of the clinical features analyzed at the system level. This suggests that not all clinical features at the system level are good predictors of obesity and it could be more fruitful to focus investigations on significantly contributing clinical features.

MI classification, with GA-SVM algorithm, was next performed with the 111 clinical features that were present in BCEN. Baseline comparison was made with the original set of 173 clinical features present in the imputed CHS dataset. Results, as shown in Table 2
                        , were obtained from averaging 3 runs of GA-SVM. For each method, the best-performing clinical feature subset for the different runs is the same. Comparable classification performance was achieved for both the methods. However, the computational time required by the proposed method (i.e. deploying only risk factors present in the BCEN to GA-SVM algorithm) to develop the MI classification model was much lower (approximately 14.7h).

@&#DISCUSSION@&#

To develop MI classification models efficiently in high dimensional datasets, we introduced a novel methodology for the reduction of clinical features to be analyzed without compromising the performance of the classification model. Classification (without feature selection) conducted on a large number of clinical risk factors often produced low-performing classification models, as the performance is often jeopardized by the present of irrelevant or redundant predicators. On the other hand, the development of classification models with feature selection (e.g. the baseline method used in this work) conducted on a large number of clinical risk factors is usually computationally expensive. Therefore, pre-selection of clinical risk factors is vital to mitigate this problem contributed by the ‘curse of dimensionality’. This was performed by segregating the clinical features along the various levels of the BC. The segregation process effectively reduces the data dimension, where its size is dependent on the number of clinical features categorized under each level of the BC. In this study, for example, analysis performed at the “body level” requires only 38 clinical features to be considered at a time. This, in contrast to the initial 173 clinical features, offers a reduction of 4.55-fold in the data dimension. Having to analyze a smaller number of clinical features inevitably reduces the amount of computational time required to develop the classification model. Moreover, if prior knowledge is available the data dimension can be further restricted. For instance, Emily et al. [29] utilize knowledge from protein databases to reduce the search of SNPs to gene pairs that are known to interact and reference. A similar concept can be applied to other levels of the BC to alleviate the search effort required.

Although effort is required to construct the BCEN, the resultant network has several advantages. Firstly, with the introduction of new clinical risk factors the entire BCEN need not be reconstructed. It provides a reusable framework where only the level of the BC, at which the new clinical risk factor belong to, need to be redeveloped. If the newly introduced clinical risk factor is identified as an etiological factor (i.e. risk factor contributing to the cause of the disease), then starting with that clinical risk factor as the root node, the network is extended for levels of the BC that is below that of the newly inserted etiological factor. This approach thus provides a significant reduction in the time and effort required to build up-to-date clinical classification models. Secondly, the BCEN provides an excellent paradigm for the illustration of the potential biological pathways that underpin the different phenotypic manifestations and has the significant advantage of analyzing only clinical risk factors that are biologically plausible. This not only allows the identification of significant risk factors that can be used for efficient development of accurate classification models, but, also, (1) reveals relationships that are not readily apparent from the study of individual disorders, (2) provide a global perspective of the different risk factors and etiologic pathways associated with the disease, and (3) identify new risk factors that could pave the way to the development of novel diagnostic, preventive or therapeutic strategies. Therefore, BCEN may be a simple etiological network, but it has the potential to provide significant insights into the mechanisms of a disease.

The constructed BCEN was validated by comparing the identified inter-relationship among different risk factors with those reported in previous clinical studies. All risk factors found at the body level of BCEN were also identified in previous clinical studies. Further, comparisons of a sub-network of BCEN (i.e. obesity-system sub-network) have shown that there is a large overlap (of 82.4%) between the identified relationships and those found in previous work. A possible reason for the identification of the additional inter-relationships is the employment of machine learning techniques. Since previous clinical studies tend to use linear statistical models to perform the analysis, non-trivial and non-linear relationships may go undetected. Therefore, the use of machine learning techniques in this work could potentially identify the non-trivial, non-linear and interacting etiological factors. This enables one to better understand the underlying causes of the disease, allowing more appropriate and focus interventions to be recommended to the patients. Table 3
                      lists the risk factors found to be highly associated with obesity and their presence in the clinical literature.

Arthritis, for instance, has been reported previously to be more prevalent among obese patients [55,56]. This is primarily due to the presence of excess biomechanical stress, inducing deleterious effect on the joints. Similarly, obese individuals have a higher risk of cancer related to endometrium, prostate, colon, esophagus and stomach [57,58]. Previously reported investigations have also shown association between obesity and bronchitis, pneumonia, emphysema, deep vein thrombosis, intermittent claudication, duration of sleep, blindness, hearing impairment, activities of daily living, pulmonary embolism, ankle-arm index, loss of balance, walking capacity, cognitive function, unstable angina, stroke, transient ischemic attack, hypertension and diabetes [59–75].

This suggests that the BCEN is feasible and effective in characterizing a disease and identifying the possible etiological factors. It is noteworthy that analysis of the obesity-system sub-network identified 6 new clinical features that were not previously identified in previous work. This could indicate that these clinical features are potential etiological factors of MI where further investigations could improve the understanding and treatment of the disease. We hypothesize that the reconstruction of the etiologic pathways is of major importance in healthcare as it would allow a more proactive approach for providing medical interventions to eradicate or delay the onset of a disease. This differs from the traditional reactive approach where individuals visit a physician only when they are sick or in pain, which sometimes results in a situation where treatment is too late to achieve complete recovery. Early medical interventions can be realized with BCEN by monitoring and controlling the risk factors (especially at the lower levels of the BC) that contribute to the development of a disease (e.g. MI).

The employment of BCEN to reduce the number of clinical features to be analyzed significantly alleviated the computational demands. Without acutely compromising the classification performance, a speedup of approximately 4.73-fold was achieved. This was possible due to the earlier convergence of GA, suggesting that significant risk factors are already identified and present in BCEN. This facilitates the identification of risk factors that contribute significantly to the modeling of accurate MI classification model.

This study has a few limitations. Firstly, only a single dataset (i.e. CHS dataset) was used to build the etiological network for MI. This inevitably limits the power to detect all the associated risks and conclusively state that the BCEN has described the complete etiology of MI. Additionally, it limits the ability to state that the proposed method provides efficiency for all clinical classification problems. Nonetheless, it does shed some light to a novel approach for investigating the etiology of MI and efficient clinical classification. Secondly, only a single classification algorithm (i.e. SVM) has been used to identify the association between the clinical features and for developing MI classification model. This may hinder the discovery of the underlying associations and the performance of the classification model, as no single machine learning technique or statistical model is optimal for every problem. The reason for this is because each method would have its own inductive bias [76]. Hence, it is suggested in [18] that comparison between multiple machine learning techniques, traditional statistical models and expert-based schemes should be conducted in order to assess the suitability of each method for a particular problem. Finally, the CHS dataset only contains risk factors that fall under the body, system, visceral and protein levels. This hinders the construction of a complete BCEN, limiting the ability to provide a more comprehensive illustration of the underlying etiology of a disease and the development of a more accurate classification model.

Nevertheless, the constructed BCEN is potentially capable of presenting the etiology of a disease in a biologically-structured manner that could facilitate the understanding and management of a disease. Moreover, it offers an effective and efficient approach for the development of MI classification model.

@&#CONCLUSIONS@&#

In view of the high prevalence of MI worldwide, better ability to characterize and classify the disease is both appropriate and necessary. In this paper we have presented an integrated approach to build a single probabilistic network (i.e. BCEN which identifies and relates the etiological factors associated with MI) that aims to provide an efficient approach for the development of MI classification model.

Validation of the constructed BCEN was conducted and our results indicate that the network is reliable and capable of identifying significant etiological factors. There is a large overlap between the relationships identified by our approach and those found in previous work. Out of the 34 clinical features identified at the obesity-system level, 28 (82.4%) of them were found in the previous clinical studies. However, 6 new clinical features, that had not been identified previously, were found to be associated with obesity in this study. These new clinical features could be probable risk factors for MI. They indicate the need for further clinical investigations to improve the understanding and treatment of the disease.

Based on the distinct risk factors identified and present in BCEN, a classification model for MI was developed. The classification model obtained demonstrated high balanced accuracy of 0.933. It was developed at a rate of 4.73-fold faster than its counterpart that does not adopt any pre-selection strategy. This suggests that BCEN may be a desirable approach for developing clinical classification models when a large number of clinical features need to be considered.

Although further validation of this methodology is necessary, this approach may be valuable in exploring and identifying risk factors that underpin a disease. To conclude, the BCEN is an etiological network that is simply built but profoundly useful. It has the potential to provide insights, from a novel perspective, into the characteristics of (current/new) diseases - allowing more efficient and effective understanding, analysis, management and classification to be undertaken. We look forward to a more comprehensive understanding of the disease etiology and eventually, towards personalized medicine.

@&#ACKNOWLEDGMENTS@&#

Darwin Tay would like to express his sincere gratitude for his scholarship funding provided by the Nanyang Technological University-Imperial College London Joint PhD programme.

The authors would like to thank the National Heart, Lung and Blood Institute (NHLBI) for providing the CHS dataset.

Professor Richard Kitney and Dr Carolyn Goh wish to acknowledge the support of The Engineering and Physical Science Research Council (EPSRC) in this study.

Darwin Tay would also wish to thank EPSRC for partial support at Imperial College.

@&#REFERENCES@&#

