@&#MAIN-TITLE@&#A low lighting or contrast ratio visible iris recognition using iso-contrast limited adaptive histogram equalization

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Low lighting iris image will reduce the iris segmentation accuracy.


                        
                        
                           
                           Low contrast ratio between the iris and the pupil caused error pupillary detection.


                        
                        
                           
                           CLAHE is used to limit contrast amplification of sub-regions.


                        
                        
                           
                           ISO is used to provide sub-regions with linear distribution intensities.


                        
                        
                           
                           A fusion of iso-CLAHE improved the eye image from a low lighting or contrast ratio.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Iris recognition

Color contrast enhancement

iso-CLAHE

Low lighting

Low contrast ratio

@&#ABSTRACT@&#


               
               
                  Condition of eye images with a low lighting or low contrast ratio between the iris and pupil is one of the challenges for iris recognition in a non-cooperative environment and under visible wavelength illumination. Incorrect iris localization can affect the performance of the iris recognition system. Iso-contrast limited adaptive histogram equalization is proposed to overcome this challenge and increase the performance of iris localization. The eye image is partitioned into the contextual sub-region; then, the proposed method transfers the pixel intensity by referring to a local intensity histogram and a newly suggested cumulative distribution function. This research was tested on 1000 eye images from the UBIRIS.v2 dataset. The results showed that the proposed method performed better than existing methods when dealing with a low lighting or low contrast ratio between the iris and pupil in the eye image.
               
            

@&#INTRODUCTION@&#

The current iris recognition system has changed from focusing on capturing an eye image at a short-range distance, at a fixed position and under controlled lighting conditions (cooperative environment) to capturing an eye image at a long-range distance, in motion and under lighting variations (non-cooperative environment) [14,18,20,25–27]. In addition, rather than using near infrared wavelength illumination, a visible wavelength illumination is used to capture the eye image due to concerns that excessive levels of near infrared wavelength illumination could endanger the eye [14,18,26]. The use of inflexible image capturing conditions and procedures has produced data with very low quality [15,17] due to motion-blurring [27], occlusion by eyelids [1,19,29], eyelashes [1,29], reflections [1] or low lighting [17]. According to Roy et al. [22], the accuracy of iris recognition strongly depends on the iris segmentation accuracies. Eye images with low lighting have caused incorrect limbic and pupillary localizations, leading to a reduction in iris segmentation accuracy [15]. Further, according to Santos and Hoyle [25] and Chen et al. [3], in eye images that are captured using visible wavelength illumination, the contrast ratio between the iris and pupil in the eye image can be very low, especially for a dark pigmented iris (see Table 1
                     ). Hence, the segmentation sometimes leads to divergence from the correct pupillary localization [3,9,15,23,25]. Proenca and Alexandre [15] also noted that one of the errors in iris segmentation is the pupillary localization. Sahmoud and Abuhaiba [23] found that, in order to make the pupil more visible, a contrast enhancement method is needed. In the iris recognition system, the implementation of the contrast enhancement method is usually performed at either the segmentation or normalization stage. At the normalization stage, the contrast enhancement method is applied to the normalized iris in order to enhance the iris pattern in low lighting [22,26].

Three issues need to be addressed in order to increase the accuracy of iris segmentation: (i) eye images with low lighting; (ii) eye images with low contrast ratio between the iris and pupil; and (iii) eye images with both (i) and (ii). However, the extant research has focused only on resolving the first issue [25–27] and no specific research has been conducted to address the second or third issues. This is because the research to date has focused on eye images that were captured in cooperative environments. Thus, the eye images already have a better contrast ratio between the iris and pupil. The existing contrast enhancement methods in iris recognition can be distinguished into two categories: (i) spatial domain methods and (ii) frequency domain methods. The spatial domain methods, such as histogram-based techniques, deal directly with the image pixels while the frequency domain methods such as retinex [26] and homomorphic filtering [30], operate on the image transform. Santos and Hoyle [25] proposed the histogram equalization (HE) algorithm which is applied to the local area of an eye image with low lighting in order to obtain pupil boundaries in the search area. However, the HE method is limited to increasing the contrast in an eye image that has a low contrast ratio between the iris and pupil, especially for dark pigmented irises. Shin et al. [26] applied the retinex algorithm on the whole eye image in the segmentation stage and only on the iris region in the normalization stage. The execution of the contrast enhancement in the segmentation stage is designed to enhance the distinctiveness of the image so that it is possible to classify the left and right eye images using the measurement of eyelash distribution and detection of the specular reflection. In the normalization stage, the contrast enhancement is applied in order to increase the distinctiveness of the iris pattern so that the accuracy of iris recognition can be improved. Shukri et al. [27] proposed a combination of the homomorphic filtering and multiscale retinex [26] to deal with lighting variations and shadow removal. However, this combined method requires predefined parameters for homomorphic filtering and three filter scales for the multiscale retinex and different eye images might require different parameters and scales in order to get a good result. Most of the contrast enhancement techniques used in iris recognition systems is implemented on grayscale images. Nevertheless, according to Saleem et al. [24], the evolution of photography has increased the interest in color imaging and consequently the interest in color contrast enhancement methods. The goal of color contrast enhancement in general is to produce an appealing image or video with vivid colors and clarity of detail intimately related to different attributes of perception and visual sensation. For this reason, and because iris recognition systems have now changed to capturing eye images using the visible wavelength illumination, the present study uses color channel intensity to perform the contrast enhancement.

In this study, a contrast enhancement method, called iso-contrast limited adaptive histogram equalization (iso-CLAHE), is proposed to address the three issues in visible iris recognition. An advantage of the proposed method is that it manipulates the color intensity distribution in the image sub-regions using a new definition of the cumulative distribution function (CDF) of color from the iso-luminance plane. In addition, an entropy-based method is suggested in order to select an appropriate clip parameter for the iso-CLAHE. The proposed contrast enhancement process starts with the execution of a root mean square (RMS) computation at the preprocessing stage. This measurement determines the contrast level of the eye image and determines whether or not the process of contrast enhancement is required. Next, the eye image is partitioned into contextual sub-regions and iso-CLAHE is applied to each sub-region. An entropy-based clip parameter is used and a new formulation of the CDF is applied. The iris recognition processes are achieved after all the image sub-regions are enhanced. Reflections can also be observed in the eye image with low lighting after the process of contrast enhancement. Therefore, a combination of the line intensity profile and support vector machine (LIPSVM) proposed by Raffei et al. [21] is applied to the eye image to remove the reflections and obtain the correct iris boundaries. A summary of the proposed contrast enhancement process is illustrated in Fig. 1
                     . To explain the proposed method in detail, this paper comprises of several sections. Section 2 introduces the dataset used in this study, namely, the UBIRIS.v2. Section 3 explicates the use of the iso-CLAHE method and the experimental results are discussed in Section 4. The conclusions of the study and recommendations for future work are presented in Section 5.

Many eye image datasets can be publicly accessed for iris recognition research [2,4,11–13,18] and are divided into two types of light wavelengths: (i) near-infrared and (ii) visible. In the present study, the near-infrared datasets were not used because the research focus is on the visible eye image. Examples of visible eye image datasets include the University of Olomuc (UPOL) [4] dataset, versions one of the University of Beira Interior (UBIRIS.v1) [16] dataset and version two of University of Beira Interior (UBIRIS.v2) [18] dataset. The UBIRIS.v2 was selected because compared to the UPOL and UBIRIS.v1 datasets, it contains more realistic noise factors (such as low lighting, reflections, off-focus, and occlusions caused by hair and spectacles) and the images were captured at varying distances [18]. The UBIRIS.v2 dataset was created by the University of Beira Interior and downloaded from http://iris.di.ubi.pt/ubiris2.html. Three categories of iris pigmentation are classified in the UBIRIS.v2: (i) light, (ii) medium, and (iii) heavy. The light pigmentation category contains the blue and light green irises (18.3%), the medium pigmentation category contains the light and medium brown and dark green irises (42.6%), and the heavy pigmentation category contains the dark brown and almost black irises (39.1%). Thus, most of the eye images in the dataset have a low contrast ratio between the iris and pupil. The eye images were obtained using a Canon EOS 5D, and the distances ranged from four to eight meters. All the eye images have a resolution of 400×300 pixels, a standard RGB (sRGB) color representation, and are in the Tagged Image File Format (TIFF).

A total of 1000 frontal eye images with a low lighting and low contrast ratio between the iris and pupil were selected for use in this study. The eye images were categorized according to the distances and three types of noisy effects: occlusion by spectacles, occlusion by hair and off-focus (see Table 2
                     ). In addition, if the eye images had low lighting, an RMS measurement was performed to measure the distribution of the local contrast [5]. From this measurement, the eye images were characterized according to three contrast levels: (i) high, (ii) medium, and (iii) low. The images with high level contrast had RMS values of more than 0.4, while the images with medium level contrast had RMS values between 0.25 and 0.4, and the images with low level contrast had RMS values below 0.25. It should be noted that the RMS measurement was performed after the process of reflection removal. This is because the RMS value can be affected by the high intensity of reflections.

The intelligent iris recognition system proposed in this study is shown in Fig. 2
                      which consists of five stages. The system starts with the preprocessing stage, followed by segmentation, normalization, feature extraction and lastly, template matching. The reflection removal and proposed contrast enhancement methods are implemented in the preprocessing stage. Description for each of the stages can be found in Raffei et al. [21] except for the contrast enhancement. As discussed in the introduction, the three issues that reduce iris recognition accuracy are the eye image with a low lighting, the eye image with a low contrast ratio between the iris and pupil, and the eye image with both low lighting and low contrast ratio between the iris and pupil. To overcome these problems, a contrast enhancement method is required and currently, HE is the most popular method for enhancing the contrast of an image [6,10]. The basic idea of HE is to remap the scale of the input image so that the histogram of the output image approximates that of the uniform distribution, resulting in the enhancement of the quality of the output image. CLAHE is an extension method of HE which can address the noise amplification problem. Unlike the other contrast enhancement methods, CLAHE operates on a small region (called a sub-region) in the image rather than on the entire image. This makes the CLAHE method advantageous because for the image that has non-uniform illumination distribution, different areas in the image that have different intensity distribution can be enhanced specifically into uniform distribution. Considering this advantage, this study used the CLAHE method and added the following improvements in order to achieve the research objectives:
                        
                           i.
                           A default size (8×8) for the sub-region was used.

An entropy-based method [10] was used to determine the clip limit parameter of amplification for each sub-region, with the current parameter determination heuristically chosen by the researcher. According to Min et al. [10], the quality of the image relies on the clip limit rather than the sub-region’s size. Several clip limit comparisons were completed (see Table 3
                              ) in order to show the appropriate selection of parameters and the range of the clip parameters [0,0.1] was taken from the study in [10]. It was concluded that the eye image became brighter when the clip limit was increased. The entropy of the eye image also increased.

A new color CDF computation [6] was introduced to give a uniform distribution by reference to the iso-luminance plane.

The output for the color contrast enhancement is presented in Table 4
                      and the overall process of the proposed contrast enhancement method is illustrated in Fig. 3
                     . The color contrast enhancement was performed as follows:
                        
                           
                              Step 1: Partition the eye image into 8×8 sub-regions.


                              Step 2: Let the original input image be denoted as Iin
                              , the histogram equalized image as Iout
                              , the probability distribution function (PDF) as PDFin
                              , the corresponding (CDF) as CDFin
                              , and the intensity of the resulting output image as n
                              0.


                              Step 3: Calculate the gray level, ni
                               using the formula of iso-luminance: 
                                 
                                    (1)
                                    
                                       
                                          
                                             n
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       (
                                       
                                          
                                             R
                                          
                                          
                                             i
                                          
                                       
                                       +
                                       
                                          
                                             G
                                          
                                          
                                             i
                                          
                                       
                                       +
                                       
                                          
                                             B
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       /
                                       3
                                       ,
                                    
                                 
                               where R
                              =red, G
                              =green and B
                              =blue, and i
                              =1,2,3,…is the total number of pixels.


                              Step 4: Calculate the gray PDF as follows: 
                                 
                                    (2)
                                    
                                       
                                          
                                             PDF
                                          
                                          
                                             in
                                          
                                       
                                       (
                                       
                                          
                                             n
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       =
                                       P
                                       (
                                       in
                                       =
                                       
                                          
                                             n
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       ,
                                       
                                       0
                                       ⩽
                                       i
                                       <
                                       L
                                       -
                                       1
                                       ,
                                    
                                 
                               where L is the total number of gray levels in the image (typically 256), and n is the total number of pixels in the eye image.


                              Step 5: Specify the contrast enhancement limit using 
                                 
                                    (3)
                                    
                                       E
                                       (
                                       x
                                       )
                                       =
                                       -
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                N
                                             
                                          
                                       
                                       
                                          
                                             PDF
                                          
                                          
                                             in
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      n
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                       
                                       ∗
                                       
                                       
                                          
                                             log
                                          
                                          
                                             2
                                          
                                       
                                       
                                          
                                             PDF
                                          
                                          
                                             in
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      n
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                       ,
                                    
                                 
                              
                           


                              Step 6: Calculate the CDF as follows: 
                                 
                                    (4)
                                    
                                       
                                          
                                             CDF
                                          
                                          
                                             in
                                          
                                       
                                       (
                                       
                                          
                                             R
                                          
                                          
                                             i
                                          
                                       
                                       +
                                       
                                          
                                             G
                                          
                                          
                                             i
                                          
                                       
                                       +
                                       
                                          
                                             B
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       =
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                R
                                                +
                                                G
                                                +
                                                B
                                                ⩽
                                                3
                                                ni
                                             
                                          
                                       
                                       
                                          
                                             PDF
                                          
                                          
                                             in
                                          
                                       
                                       (
                                       n
                                       )
                                       ,
                                    
                                 
                              
                           


                              Step 7: Obtain the output brightness as follows: 
                                 
                                    (5)
                                    
                                       
                                          
                                             n
                                          
                                          
                                             0
                                          
                                       
                                       =
                                       L
                                       
                                       ∗
                                       
                                       
                                          
                                             CDF
                                          
                                          
                                             in
                                          
                                       
                                       (
                                       
                                          
                                             R
                                          
                                          
                                             i
                                          
                                       
                                       +
                                       
                                          
                                             G
                                          
                                          
                                             i
                                          
                                       
                                       +
                                       
                                          
                                             B
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       ,
                                    
                                 
                              
                           


                              Step 8: Repeat steps 1–7 for all sub-regions.

Several evaluations and analyses on the 1000 eye images were completed in order to compare the performance of the proposed iris recognition system and the existing methods proposed by Raffei et al. [21], Santos and Hoyle [25], Shin et al. [26], and Shukri et al. [27]. It should be noted that for satisfactory comparisons, the outputs of the reflection removal eye images from the method developed by Raffei et al. [21] were used as the input images. The most common measures of image quality in image processing: intensity, namely peak signal-to noise ratio (PSNR), mean square error (MSE) and absolute mean brightness error (AMBE) were used to compare the performance of the contrast enhancement methods [8]. To analyze the performance of the iris recognition systems, measurements of the accuracy of iris localization, accuracy of iris recognition and decidability index were required.

The intensity of the contrast enhancement methods was observed using a line of interest (LOI) graph and measuring the RMS values. The higher the RMS value, the better the contrast image. Tables 5–9 present the results of the contrast enhancement analyses for different categories of eye images, methods, contrast levels and distances. As seen in the results, the intensity of the eye images from the Raffei et al. [21] method was increased, as well as the RMS values, following the implementation of the contrast enhancement methods. For example, the RMS values for the eye images obtained from the Raffei et al. [21] method at a distance of 4m (Table 5
                     ) increased from the low level of contrast to the medium level of contrast and from the medium level of contrast to the high level of contrast. However, for the Shukri et al. [27] and the proposed methods, the RMS values only increased from the levels of low and medium contrast to the high level of contrast. Although the RMS values for the Shukri et al. [27] method were nearly equal to the RMS values for the proposed method, the contrast ratio between the iris and pupil for the Shukri et al. [27] method was slightly different. This is because this method deals with the overall area compared to the proposed method which deals directly with the pixel intensities in the small regions. Results of the intensity and RMS values from Tables 6–9 are shown in Appendix A.

The PSNR is commonly used as a measure of image quality reconstruction. It is the ratio between the maximum possible power and the corrupting noise that affects the representation of an image. A formula for the PSNR of a color image is different from the PSNR of a grayscale image [28] and is defined as follows:
                        
                           (6)
                           
                              PSNR
                              =
                              10
                              
                              
                                 
                                    log
                                 
                                 
                                    10
                                 
                              
                              
                                 
                                    
                                       
                                          255
                                       
                                       
                                          2
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   MSE
                                                
                                                
                                                   R
                                                
                                             
                                             +
                                             
                                                
                                                   MSE
                                                
                                                
                                                   G
                                                
                                             
                                             +
                                             
                                                
                                                   MSE
                                                
                                                
                                                   B
                                                
                                             
                                          
                                       
                                    
                                    /
                                    3
                                 
                              
                              ,
                           
                        
                     where MSER
                     , MSEG
                     , MSEB
                      are the MSE between the red, green and blue components of the input and enhanced image. The MSE represents the cumulative squared error between the enhanced and the input image. A formula for the MSE of a color image is determined as follows:
                        
                           (7)
                           
                              MSE
                              =
                              
                                 
                                    1
                                 
                                 
                                    MN
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       M
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       N
                                    
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             (
                                             R
                                             (
                                             i
                                             ,
                                             j
                                             )
                                             -
                                             
                                                
                                                   R
                                                
                                                
                                                   ′
                                                
                                             
                                             (
                                             i
                                             ,
                                             j
                                             )
                                             )
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       
                                          
                                             (
                                             G
                                             (
                                             i
                                             ,
                                             j
                                             )
                                             -
                                             
                                                
                                                   G
                                                
                                                
                                                   ′
                                                
                                             
                                             (
                                             i
                                             ,
                                             j
                                             )
                                             )
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       
                                          
                                             (
                                             B
                                             (
                                             i
                                             ,
                                             j
                                             )
                                             -
                                             
                                                
                                                   B
                                                
                                                
                                                   ′
                                                
                                             
                                             (
                                             i
                                             ,
                                             j
                                             )
                                             )
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                              ,
                           
                        
                     where R(i,
                     j), G(i,
                     j), B(i,
                     j), R′(i,
                     j) and B′(i,
                     j) are image pixels of the input and enhanced image with a size of M
                     ×
                     N. Both the PSNR and MSE are interrelated whereby the higher the PSNR and the lower the MSE, the better the image quality. The PSNR and the MSE results are presented in Tables 5–9. As seen in the tables, the proposed method had the highest PSNR and the lowest MSE for all eye images compared to the existing methods. For example, at a distance of 4m (Table 5), for the low contrast eye image with occlusion by the hair, the proposed method gave 52.1913 of PSNR and 0.1309 of MSE, followed by the Shukri et al. [27] method which gave 40.9721 of PSNR and 1.7328 of MSE, the Shin et al. [26] method which gave 7.0917 of PSNR and 3.8109 of MSE, and the Santos and Hoyle [25] method which gave 6.6080 of PSNR and 4.2599 of MSE. From this result, it was concluded that the proposed method provides greater image quality after the image enhancement. Results of the PSNR and MSE from Tables 6–9 are shown in Appendix A.

The AMBE was measured to estimate how close the predictions were to the final results: the method that produced less error would be identified as the method that could preserve more brightness and provide greater quality in the output image. The AMBE formula [7] of the color image is given as follows:
                        
                           (8)
                           
                              AMBE
                              =
                              I
                              -
                              
                                 
                                    I
                                 
                                 
                                    ′
                                 
                              
                              ,
                           
                        
                     
                     
                        
                           (9)
                           
                              I
                              =
                              
                                 
                                    1
                                 
                                 
                                    3
                                 
                              
                              (
                              mean
                              (
                              R
                              )
                              +
                              mean
                              (
                              G
                              )
                              +
                              mean
                              (
                              B
                              )
                              )
                              ,
                           
                        
                     
                     
                        
                           (10)
                           
                              
                                 
                                    I
                                 
                                 
                                    ′
                                 
                              
                              =
                              
                                 
                                    1
                                 
                                 
                                    3
                                 
                              
                              (
                              mean
                              (
                              
                                 
                                    R
                                 
                                 
                                    ′
                                 
                              
                              )
                              +
                              mean
                              (
                              
                                 
                                    G
                                 
                                 
                                    ′
                                 
                              
                              )
                              +
                              mean
                              (
                              
                                 
                                    B
                                 
                                 
                                    ′
                                 
                              
                              )
                              )
                              ,
                           
                        
                     where R, G, B, R′, G′, B′ are the red, green and blue components of the original, I and enhanced, I′ image. The results of the AMBE are presented in Tables 5–9, whereby the lower the AMBE, the greater the image quality. For example, at the distance of 4m (Table 5), for the medium contrast eye image with occlusion by off-focus, the Santos and Hoyle [25] method gave the highest AMBE (74.2563), followed by the Shin et al. [26] method (51.4321 AMBE), the Shukri et al. [27] method (48.5575 AMBE) and the proposed method (42.2241 AMBE). From this result, it was concluded that the proposed method can preserve more brightness and provide greater eye image quality. Results of the AMBE from Tables 6–9 are shown in Appendix A.

The results for the iris localization are presented in Tables 5–9, with two yellow circles drawn to represent the pupillary and limbic boundaries. Overall, for the eye images with medium and low levels of contrast, a high contrast ratio between the iris and the sclera simply allowed the circular Hough transform (CHT) method to find the edge of the limbic boundary. Even so, for the eye images with a very low contrast level (RMS values below 0.1), this boundary was difficult for the CHT to find because the intensity between the iris and the sclera was similar. In addition, the edge of spectacles or double eyelids could be mistaken as the edge of the limbic boundary by the CHT and thus, could lead to incorrect limbic localization. For example, at a distance of 4m (Table 5), the localization of the limbic boundary for the low contrast eye image with spectacles and off-focus was correct following the implementation of the contrast enhancement methods. Although the contrast enhancement methods were applied to the eye images, only some pupillary boundaries were successfully localized. This is because, when the contrast ratio between the iris and pupil is slightly increased, it is difficult for the CHT method to find the edge of the pupillary boundary. However, only the iso-CLAHE method improved the contrast ratio between the iris and pupil, leading to the correct determination of the pupillary boundary. Results of the iris localization from Tables 6–9 are shown in Appendix A.


                     Table 10
                      presents the results on the accuracy of iris localization according to the categories of eye image, method and distance. For the category of eye images with occlusion by spectacles, the Shukri et al. [27] and proposed methods performed with more than 90.0% accuracy of localization for the pupillary and limbic boundaries compared to the other methods. In fact, the proposed method achieved the highest accuracy of localization compared to the Shukri et al. [27] method, with 95.4% accuracy of localization for the pupillary boundary at a distance of 4m, 93.2% accuracy of localization for the pupillary boundary at distances of 5 and 6m, and 92.0% accuracy of localization for the pupillary boundary at distances of 7 and 8m. For the limbic boundary and overall at distances of 4–8m, the proposed method reached 96.6% and 92.0% accuracies of localization, respectively. For the eye images with occlusion by hair, most of the methods performed with less than 88.0% accuracy of the pupillary boundary localization. However, the proposed method performed with the highest accuracy of pupillary boundary localization for most distances (87.5% accuracy at distances of 4 and 5m, 85.0% accuracy at a distance of 6m, and 82.5% accuracy at distances of 7 and 8m) except, at a distance of 5m for which the Shukri et al. [27] method performed with the same accuracy (87.5%) of pupillary boundary localization as the proposed method. For the limbic boundary localization, the proposed method performed with the highest accuracy at distances of 4 and 5m (95.0%), however, at distances of 6–8m, the Shukri et al. [27] and proposed methods performed with the same accuracy (87.5%) for each distance. Overall, the proposed method performed with the highest accuracy of the pupillary and limbic boundary localization, with 85.0% accuracy at distances of 4 and 5m, and 82.5% accuracy at distances of 6–8m, respectively. For the off-focus eye images, the proposed method achieved the highest accuracy of pupillary boundary localization, with 94.4% accuracy at distances of 4 and 5m, 93.0% accuracy at a distance of 6m, and 91.6% accuracy at distances of 7 and 8m. For the limbic boundary localization, the proposed method reached more than 94.4% accuracy compared to the other methods, with 97.2% accuracy at distances of 4–6m, and 94.4% accuracy at distances of 7 and 8m. Overall, the proposed method achieved 97.2% accuracy of localization at distances of 4 and 5m, and 88.9% accuracy of localization at distances of 6–8m.


                     Table 11
                      presents the results of the accuracy and decidability index of the iris recognition system according to the different distances and methods. The contrast enhancement methods achieved more than 90.1% accuracy in the iris recognition system for the different distances compared to the Raffei et al. [21] method which achieved less than 87.1%. However, the proposed iso-CLAHE method provided the highest accuracy in the iris recognition system, with 95.1% accuracy at distances of 4 and 5m, and 94.7% accuracy at distances of 6–8m. This is because the correct iris localization gives the correct iris extraction, which enables the identification of the correct person. The Shukri et al. [27] method obtained the second highest accuracy in the iris recognition system, with 94.2% accuracy at distances of 4 and 5m, and 93.5% accuracy at distances of 6–8m. For the overall distance, the Raffei et al. [21] method achieved the least accuracy (86.0%) compared to the proposed method (94.5%). The decidability index was measured to determine the separation distance between the intra-class and inter-class distribution. The higher the decidability index, the larger the separation distance between the intra-class and inter-class distribution. The decidability index for all the contrast enhancement methods achieved an index of more than 2.15 for all distances compared to the eye images from the Raffei et al. [21] method which were produced without the implementation of a contrast enhancement method. An index of 2.18 was achieved for distances of 4 and 5m, an index of 2.16 was achieved for distances of 6 and 7m, an index of 2.08 was achieved for a distance of 8m, and an index of 2.11 was achieved overall. However, the proposed method showed the largest separation between the intra-class and inter-class distributions, with an index of 2.32 for distances of 4 and 5m, and an index of 2.30 for distances of 6–8m. This was due to the high accuracy of the limbic and pupillary boundary localizations of the iris, which led to high accuracy in the recognition of a person and subsequently to the high decidability index.

@&#CONCLUSION@&#

This study presents an intelligent iris recognition system for eye images that were captured using visible wavelength illumination, at different distances and in motion. Iris segmentation is one of the most important stages in an iris recognition system. The use of eye images with a low lighting or low contrast ratio between the iris and pupil is one of the issues leading to the incorrect pupillary localization which affects the accuracy of iris segmentation and subsequently affects the accuracy of an iris recognition system. The method proposed in this study called iso-CLAHE, manipulates the image intensity in the sub-regions using a newly formulated cumulative distribution function of color from the iso-luminance plane. In addition, a clip parameter based on the entropy of the image is suggested in order to avoid heuristic parameter selections. A root mean square measurement was also carried out in order to determine the contrast level in the eye image and whether or not to execute the process of contrast enhancement. The proposed method showed the most promising contrast enhancement results compared to existing methods as it can deal with eye images that have a low lighting or low contrast ratio between the iris and pupil. The proposed method increases the performance of iris localization and ultimately, increases the accuracy and decidability index of the iris recognition system. Future study should focus on sharpening off-focus or motion-blurred eye images in order to obtain better defined iris features.

@&#ACKNOWLEDGMENTS@&#

The authors highly appreciate the contribution made by the University of Beira Interior which provided the UBIRIS.v2 dataset. This research was funded by the GATES Scholars Foundation of GATES IT Solution Sdn. Bhd. Company (Grant no. GITS-SAP-03-02-(03)) and the MyPhd Scholarship of the Ministry of Education Malaysia.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.knosys.2014.11.002.


                     
                        
                           Supplementary data 1
                           
                        
                     
                  

@&#REFERENCES@&#

