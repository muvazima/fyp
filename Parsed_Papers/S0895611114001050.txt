@&#MAIN-TITLE@&#Adaptive-weighted bilateral filtering and other pre-processing techniques for optical coherence tomography

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           An integrated image processing toolbox for retinal image analysis is proposed.


                        
                        
                           
                           The system considers in a unified framework both fundal images and optical coherence tomography.


                        
                        
                           
                           An adaptive-weighted bilateral filtering (AWBF) is proposed to reduce speckles.


                        
                        
                           
                           The dual-tree complex wavelet transform is employed to improved OCT layer segmentation.


                        
                        
                           
                           A novel technique for multimodal retinal image registration is proposed.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Image enhancement

Despeckling

Bilateral filter

DT-CWT

Segmentation

Registration

@&#ABSTRACT@&#


               
               
                  This paper presents novel pre-processing image enhancement algorithms for retinal optical coherence tomography (OCT). These images contain a large amount of speckle causing them to be grainy and of very low contrast. To make these images valuable for clinical interpretation, we propose a novel method to remove speckle, while preserving useful information contained in each retinal layer. The process starts with multi-scale despeckling based on a dual-tree complex wavelet transform (DT-CWT). We further enhance the OCT image through a smoothing process that uses a novel adaptive-weighted bilateral filter (AWBF). This offers the desirable property of preserving texture within the OCT image layers. The enhanced OCT image is then segmented to extract inner retinal layers that contain useful information for eye research. Our layer segmentation technique is also performed in the DT-CWT domain. Finally we describe an OCT/fundus image registration algorithm which is helpful when two modalities are used together for diagnosis and for information fusion.
               
            

@&#INTRODUCTION@&#

Optical coherence tomography (OCT) has recently become a powerful tool in medicine, particularly in the diagnosis and monitoring of retinal damage in a range of diseases [1,2]. It provides high-resolution cross-sectional images of biological tissues using noninvasive imaging technology. While recent improvements in OCT technology offer higher scan speed and better signal sensitivity, the spatial-frequency bandwidth of the interference signals is still limited and causes a granular appearance, called ‘speckle’ [3].

The assessment of retinal disease using OCT images has so far focused on the delineation of the retinal layers and emphasized the detection of cell loss and anatomical disruption of the retinal architecture. Indeed, in glaucoma for example, OCT images reveal the thinning occurring in the retinal nerve fibre layer (RNFL) and retinal ganglion cells (RGCs) [4]. However, this is characteristic of late disease. Recently, studies showing that many retinal conditions begin with a loss of neuronal connectivity and consequent damage in the RGC/IPL complex (the combined RGC and inner plexiform layers (IPL)) make identifying such changes by OCT an important goal [5,6]. Work in [7] showed that the light scattering properties of retinal layers affected by retinal neural atrophy can be detected by OCT. Therefore, we hypothesize that the texture of light reflections within the RGC/IPL complex can possibly be used for detecting neuronal changes such as those seen in early glaucoma. This paper presents a framework to prepare OCT data for feature extraction and other image analysis tasks that help diagnosis. For example, through texture analysis, ganglion cell degeneration in the retina could be automatically detected.

The proposed process is illustrated in Fig. 1
                     . The first step involves image enhancements in which speckle noise, that appears as a grainy texture (as shown in Fig. 2
                     ) and degrades the quality of the OCT image, is removed. However, since OCT speckle results mostly from multiple forward scattering it may also contain diagnostically useful information. The extraction of this information, which we shall refer to as texture is challenging. The simplest and oldest way of removing speckle is median filtering [8]. Directional filters and anisotropic diffusion filters have also been employed to improve despeckling results and to preserve edges [9,10]. In recent years, the wavelet transform has been widely used due to its effectiveness and simplicity [11]. As the speckle noise can be modelled as multiplicative noise, the logarithmic operation is used to transform speckle noise into the classical additive white noise, thereby allowing simple image noise removing algorithms to be employed. We implement our algorithms in the wavelet domain using a dual tree complex wavelet transform (DT-CWT), followed by the proposed texture-preserving smooth process which employs a novel adaptive-weighted bilateral filter (AWBF).

The next step is layer segmentation. This is aimed at delineating the retinal layers, such as the RGC/IPL complex, which will be used in the disease detection process and for measuring layer thickness [12]. This process is also performed in the DT-CWT domain. Finally an OCT/fundus photography image registration algorithm is proposed. This is necessary because precise and reproducible control of the OCT image position on the fundus is especially important for morphometry, such as measurement of NFL thickness in glaucoma diagnosis [13]. In addition, some defects are easier to capture in one image modality than the other, so working on both types of images increases the diagnostic confidence [14]. The OCT/fundus image registration is one of the most challenging problems as it tries to correlate the retinal features across the different imaging modalities, a process which involves feature detection, warping and similarity measurement. Moreover, our OCT images are affected by eye movement.

The main contributions of this work are:
                        
                           (i)
                           An integrated system for OCT image preprocessing that serves the purpose of glaucoma detection.

In the enhancement component of our system, we propose an adaptive-weighted bilateral filtering (AWBF).

In the layer segmentation, we perform multiscale-based segmentation by taking advantage of improved directional selectivity.

In the OCT/fundus image registration component, we propose a technique for blood vessel extraction for OCT photograph and a new score for multi-modal registration to solve scaling problems.

The remaining part of this paper is organized as follows. The proposed image enhancement process, layer segmentation and OCT/fundus image registration are explained in Sections
                     2–4, respectively. The proposed system is evaluated with discussion in Section
                     5. Finally the conclusions of this work are stated in Section
                     6.

The proposed OCT image enhancement method is depicted in Fig. 3
                     . The first step is intensity adjustment, where a linear intensity histogram stretching is employed. Subsequently, a two-step denoising process is employed. The speckle resulting from interference of the waveforms from multiple scatterers within the OCT focal volume is typically large (i.e. is spatially correlated), while some noise due to interference from multiply scattered photons is generally small, typically a single pixel wide [15]. Therefore, we exploit a two-step process in our OCT image enhancement to remove the speckle noise so that the OCT image retains the information rich structure, i.e. the texture. Firstly, the spatially correlated speckle is removed by a published wavelet-based denoising algorithm using a Cauchy distribution [16] after applying a logarithmic operation to the OCT image. Then, multiple neighbouring B scans are registered to the current B scan in order to improve structure of retinal layers. The speckle from scattered photons is subsequently removed using the proposed adaptive-weighted bilateral filtering (AWBF). The shape of spatial filter in our proposed method is varied according to local entropy. Generally, the entropy is used for measuring uncertainty of a group of data. For OCT images, higher entropy can imply a larger amount of speckle. Therefore, we apply a wider bell-shaped spatial filter to such areas. Boundaries of retinal layers are preserved through weighting using a similarity function.

OCT images are 3D stacks captured as a series of slices corresponding to a sequence of xz scans, called B-scans; however, our proposed image enhancement is applied on 2D slices. The main reason for this is that, during image acquisition, misalignment across slices can occur due to the natural and involuntary movement of the subject's eyes. This can be seen as rough surface in the xy planes and discontinuities in the yz planes in Fig. 4
                     .

The wavelet transform decomposes an image into multiscale and oriented subbands. Statistical modelling of the image is subsequently easier because of the energy compaction property of the wavelet transform. Following the arguments in [16], the despeckling approach is developed based on the observation that the real and imaginary parts of the complex wavelet transform can be accurately modelled using a Cauchy distribution.

The parameters of Cauchy distribution for each subband are estimated using interscale dependencies of wavelet coefficients, while the noise model is estimated from noise standard deviation obtained from median absolute deviation (MAD) of coefficients at the first level of the wavelet decomposition [17]. Finally, a Bayes MAP method is employed for estimating the denoised wavelet coefficients.

Here, a few neighbouring despeckled B-scans are registered to the current despeckled B-scan to enhance some features, e.g. edges and curves. Registration of non-rigid bodies using the phase-shift properties of the DT-CWT proposed in [18] is employed. The algorithm is developed from the ideas of phase-based multidimensional volume registration, which is robust to noise and temporal intensity variations. Motion estimation is performed iteratively, firstly by using coarser level complex coefficients to determine large motion components and then by employing finer level coefficients to refine the motion field. In this paper, only two adjacent B-scans, left and right, are registered to the current B-scan. More neighbouring B-scans can be used but this increases the computational time.

We propose a novel adaptive-weighted bilateral filter (AWBF) for smoothing the OCT image and also simultaneously preserving texture in retinal layers. This is a critical feature of our method since it preserves texture based information which may be informative with regard to retinal pathology.

Traditional image filtering processes the target image using a defined kernel that is applied to each pixel and its neighbours. For smoothing the image, the low-pass filter is generally employed to compute a weighted average of pixel values in the neighbourhood. This can reduce noise but the image is blurred across edges and textures. A bilateral filter has hence been introduced as a non-iterative tool for edge-preserving filtering [19].

The generally used filter is a shift-invariant Gaussian filter, where both the closeness function 
                           
                              G
                              
                                 
                                    σ
                                    S
                                 
                              
                           
                           (
                           •
                           )
                         and the similarity function 
                           
                              G
                              
                                 
                                    σ
                                    r
                                 
                              
                           
                           (
                           •
                           )
                         are Gaussian functions with standard deviation σ
                        
                           S
                         and σ
                        
                           r
                        , respectively. A new weighted average at pixel p with window space S can be written as in Eq. (1) 
                        [20], where N
                        
                           p
                         is for normalization.


                        
                           
                              (1)
                              
                                 BF
                                 [
                                 
                                    I
                                    
                                       
                                          p
                                       
                                    
                                 
                                 ]
                                 =
                                 
                                    1
                                    
                                       
                                          N
                                          
                                             
                                                p
                                             
                                          
                                       
                                    
                                 
                                 
                                    ∑
                                    
                                       
                                          
                                             q
                                          
                                       
                                       ∈
                                       S
                                    
                                 
                                 
                                    G
                                    
                                       
                                          σ
                                          S
                                       
                                    
                                 
                                 (
                                 ∥
                                 
                                    
                                       p
                                    
                                 
                                 −
                                 
                                    
                                       q
                                    
                                 
                                 ∥
                                 )
                                 
                                    G
                                    
                                       
                                          σ
                                          r
                                       
                                    
                                 
                                 (
                                 |
                                 
                                    I
                                    
                                       
                                          p
                                       
                                    
                                 
                                 −
                                 
                                    I
                                    
                                       
                                          
                                             q
                                          
                                       
                                    
                                 
                                 |
                                 )
                                 
                                    I
                                    
                                       
                                          
                                             q
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The term ‘adaptive’ in an adaptive bilateral filter has been used to improve the BF[I]
                           p
                         in various applications [21], including despeckling [22]. A general mathematical expression is shown in Eq. (2).


                        
                           
                              (2)
                              
                                 ABF
                                 [
                                 
                                    I
                                    
                                       
                                          p
                                       
                                    
                                 
                                 ]
                                 =
                                 
                                    1
                                    
                                       
                                          N
                                          
                                             
                                                p
                                             
                                          
                                       
                                    
                                 
                                 
                                    ∑
                                    
                                       
                                          
                                             q
                                          
                                       
                                       ∈
                                       S
                                    
                                 
                                 
                                    w
                                    
                                       
                                          p
                                       
                                    
                                 
                                 (
                                 
                                    
                                       q
                                    
                                 
                                 )
                                 
                                    G
                                    
                                       
                                          σ
                                          S
                                       
                                    
                                 
                                 (
                                 ∥
                                 
                                    
                                       p
                                    
                                 
                                 −
                                 
                                    
                                       q
                                    
                                 
                                 ∥
                                 )
                                 
                                    G
                                    
                                       
                                          σ
                                          r
                                       
                                    
                                 
                                 (
                                 |
                                 
                                    I
                                    
                                       
                                          p
                                       
                                    
                                 
                                 −
                                 
                                    I
                                    
                                       
                                          
                                             q
                                          
                                       
                                    
                                 
                                 |
                                 )
                                 
                                    I
                                    
                                       
                                          
                                             q
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              w
                              
                                 
                                    p
                                 
                              
                           
                           (
                           
                              
                                 q
                              
                           
                           )
                         is a local weight applied at pixel p, computed from each q in S, and 
                           
                              ∑
                              
                                 
                                    
                                       q
                                    
                                 
                                 ∈
                                 S
                              
                           
                           
                              w
                              
                                 
                                    p
                                 
                              
                           
                           (
                           
                              
                                 q
                              
                           
                           )
                           =
                           1
                        . Examples of smooth OCT images obtained using the bilateral filter with various parameters are shown in Fig. 5
                        (a)–(f). It is obvious that using fixed parameters can achieve only one aim which is either to preserve texture but leave in high amount of speckle or to remove speckles which also remove texture information.

In the OCT images, different retinal layers show different scattering properties and also, as mentioned before, speckle can be either regarded as noise or as an informative signal. Therefore, we introduce an AWBF for which the shape of the closeness Gaussian function is changed adaptively depending on local information at pixel p and its neighbourhoods in S. That means, we choose 
                           
                              w
                              
                                 
                                    p
                                 
                              
                           
                         to also be a Gaussian function. We calculate the local entropy H
                        
                           p
                         of the image to determine σ
                        
                           S
                         as follows.


                        
                           
                              (3)
                              
                                 H
                                 [
                                 
                                    I
                                    
                                       
                                          p
                                       
                                    
                                 
                                 ]
                                 =
                                 −
                                 
                                    ∑
                                    
                                       i
                                       =
                                       0
                                    
                                    
                                       
                                          B
                                          N
                                       
                                    
                                 
                                 p
                                 (
                                 i
                                 )
                                 
                                 
                                    log
                                    2
                                 
                                 
                                 p
                                 (
                                 i
                                 )
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    w
                                    H
                                 
                                 =
                                 
                                    
                                       H
                                       [
                                       I
                                       ]
                                       −
                                       ζ
                                    
                                    
                                       max
                                       (
                                       H
                                       [
                                       I
                                       ]
                                       )
                                       −
                                       ζ
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    σ
                                    S
                                 
                                 =
                                 (
                                 
                                    σ
                                    S
                                    max
                                 
                                 −
                                 
                                    σ
                                    S
                                    min
                                 
                                 )
                                 
                                    w
                                    H
                                 
                                 +
                                 
                                    σ
                                    S
                                    min
                                 
                              
                           
                        where p(i) is greyscale histogram counts when the intensity of pixels in S equal to i and the intensity range is 0–B
                        
                           N
                         (our OCT images are 16 bits/pixel, so B
                        
                           N
                        
                        =216
                        −1). 
                           
                              w
                              H
                           
                         is a weighting matrix and ζ represents a black value. Here, as the speckle affects image contrast, we compute ζ using the average of a few top rows (i.e. row n to m) of the OCT image, so 
                           ζ
                           =
                           (
                           
                              ∑
                              
                                 i
                                 =
                                 n
                              
                              m
                           
                           
                              I
                              i
                           
                           )
                           /
                           (
                           (
                           m
                           −
                           n
                           +
                           1
                           )
                           ·
                           W
                           )
                        , where i is a row number of image I with the width W and the rows from n to m have to be in the space in the eye (i.e. not include any retinal layers), e.g. n
                        =5 and m
                        =20 for the image with the height of 1024 pixels. 
                           
                              σ
                              S
                              min
                           
                         and 
                           
                              σ
                              S
                              max
                           
                         are a predefined minimum value and a predefined maximum value, respectively.

For the similarity function, σ
                        
                           r
                         is calculated from the intensity values as shown in Eq. (6), where α is a range control which is generally set to 0.1 for most applications [23].


                        
                           
                              (6)
                              
                                 
                                    σ
                                    r
                                 
                                 =
                                 α
                                 ·
                                 (
                                 max
                                 (
                                 I
                                 )
                                 −
                                 min
                                 (
                                 I
                                 )
                                 )
                              
                           
                        
                     

The improvement achieved by the AWBF over traditional bilateral filtering is shown in Fig. 6
                         – the middle and the left images employed the same window size of the bilateral filters. Fig. 5(h) and (i) shows the improvement when using proposed AWBF with one B-scan and three B-scans, respectively. The image in Fig. 5(i) obviously demonstrates better structure of the optic nerve head (ONH) with significantly less speckle, but still reveals texture in each retinal layers. The drawback of the locally adaptive filtering is high computational time. We therefore implement the reduced complexity version which also shows desirable results. By experiments with 512×1024 retinal images, we define 
                           
                              σ
                              S
                              min
                           
                         and 
                           
                              σ
                              S
                              max
                           
                         to 5 (approximately equal to the average thickness of the photoreceptor inner and outer segment junction (IS/OS)) and 20 (big enough to remove noise in the inner plexiform layers (IPL), of which texture information is extracted), respectively. The enhanced image is then computed as described in Eq. (7).


                        
                           
                              (7)
                              
                                 
                                    I
                                    b
                                 
                                 =
                                 (
                                 1
                                 −
                                 
                                    w
                                    H
                                 
                                 )
                                 ·
                                 BF
                                 [
                                 I
                                 ,
                                 
                                    σ
                                    S
                                    min
                                 
                                 ,
                                 
                                    σ
                                    r
                                 
                                 ]
                                 +
                                 
                                    w
                                    H
                                 
                                 ·
                                 BF
                                 [
                                 I
                                 ,
                                 
                                    σ
                                    S
                                    max
                                 
                                 ,
                                 
                                    σ
                                    r
                                 
                                 ]
                              
                           
                        
                     


                        Fig. 7
                         compares the results of enhancement of our proposed method using full computation based on Eq. (5) and the low-complexity version as in Eq. (7). The results are insignificantly different. Although the full version produces smoother images (higher speckle-SNR [24]), the other seems to reveal better structure of retinal layers (higher Anisotropic Quality Index (AQI) which includes structural distortion in measurement [25]) and achieves lower computational times (by approximately 5-fold – using MATLAB R2012a with 64-bit OS i7-3770S CPU, the computational time is down to 0.70s per 512×1024 B-scan).

Several approaches have been proposed to segment OCT layers using both 2D and 3D techniques [26–29]. The method in [26] segments each A-scan line based on the coherence structure information extracted from the OCT signal intensity after enhancing with diffusion filtering. The most effective techniques have been based on learning methods that determine constraints and cost functions [27–29]. We employ an unsupervised method in which training and modelling are not required, thereby removing the need for a training database of images. Our proposed method is based on local information and is therefore data independent, while the learning techniques would require retraining if the OCT images are acquired with different OCT devices.

Since high-resolution OCT images are affected by eye movements, 3D segmentation techniques are unsuitable. We therefore employ an adapted version of a multiscale watershed segmentation [30]. The gradient map used in the watershed segmentation process is generated using the DT-CWT, which provides near shift-invariance and good directional selectivity [31].

Our method, shown in Fig. 8
                     , begins by flattening the B-scans so that the layers are mostly positioned horizontally. We use only a few B-scans to find the global curve parameters and then apply them to all B-scans in the OCT image. For the macula area, the middle B-scan (xz plane, y
                     =256 of total 512 B-scans) is used, while a couple of B-scans located outside the optic disc are used in the ONH area. We simply select the B-scans with index y
                     =512/4 and y
                     =512×(3/4). More precise positions can be determined by firstly detecting the optic disc and the B-scans just outside the disc are selected.

The flattening process starts by detecting the photoreceptor inner and outer segment junction (IS/OS) and then applying polynomial curve fitting to this point series. To detect the IS/OS, an Otsu threshold [32] is employed since the IS/OS appears the brightest layer in the image. The areas having intensity values higher than the threshold are marked. If the resulting binary map includes only one isolated area, the threshold is increased by a desired value, e.g. 10% and the process repeated until more than one area is determined. That is, the IS/OS is separated from inner retinal layers. The biggest area is subsequently selected. This area is generally the IS/OS and retinal pigment epithelium layer (RPE). The IS/OS is then simply detected as it constitutes the upper edge of this area. The least square method is employed to find the coefficients of the polynomial 3rd degree that approximately fits the IS/OS points. Subsequently each A-scan is shifted according to this estimated curve to create the smooth flattened B-scans.

For segmentation, the gradient map is generated from four highpass subbands of the DT-CWT instead of using all orientations, i.e. ±15° and ±45°, at 3 decomposition levels, because the layer boundaries in the flattened B-scans are unlikely to lay in the vertical direction. The watershed transform is employed to initiate the segmentation. Following the method in [30], similar texture areas are grouped using the weighted mean cut cost function. With prior knowledge of the average local thicknesses of the RNFL and GCC (Ganglion Cell Complex, i.e. RGC+IPL layers combined) of normal eyes, only the similar texture areas within slightly thicker than the average local thickness, e.g. 120%, are merged. These two thicknesses are employed because there are numerous studies demonstrating their change in glaucoma.

The segmentation results may still contain vertical boundaries which possibly correspond to blood vessels’ shadows or such areas show significantly high vertical gradients. To merge two adjacent areas, the pixels in these vertical lines are removed if they are connected to only the top and the bottom pixels indicating line boundaries. Empirically, this process is good enough to extract layers for further texture analysis, particularly the RGL/IPL complex.

After extracting the layers of interest, these layers are transformed back to the original unflattened shapes so that the characteristics of their texture are not changed.

Traditional OCT/fundus image registration methods are manual and rely on the superimposition of a 50% transparent fundus photograph on the corresponding OCT fundus image. The operator then manually shifts, scales and rotates the fundus (photographic) image to optimize alignment with the OCT image [33]. Obviously, this is time consuming. The technique in [34] employs brute-force search and refines results through the iterative closest point (ICP) algorithm and the same approach is taken in [35]. This technique requires prior knowledge of the approximate scale, position and angle of OCT fundus image on the fundus photograph. We therefore propose an automatic OCT/fundus image registration, comprising stack registration, centre detection, blood vessel extraction, scaling, and blood vessel registration.

For this step, we employ pure translation. Although rigid transformation usually gives better results, the rotation will affect the shadows of the blood vessels which will be later used for extracting the blood vessels. We set the middle B scan (xz plane, y
                        =256 of total 512 B-scans) as reference and successive B scans are registered with respect to it. Subsequently, the registered B scans become reference for the next successive slices. The example of the registered OCT image is shown in Fig. 9
                        .

The points of interest, optic disc and fovea, are used in order to remove translational mismatches between two images, and also to drive the point matching registration.

The centre of the ONH image can be found from the lowest point within the optic disc.

Similarly the thinnest part of the macular is located at the fovea. These points can easily be detected from the lowest point of the inner limiting membrane (ILM) derived by stacking all the ILM lines in all xz planes of the OCT image. The ILM lines are obtained from the layer segmentation process as explained in Section
                           3. Examples of the detected centres are displayed as a cross mark in Fig. 10
                           .

For the optic disc, we follow the technique proposed in [36]. Firstly the colour photograph is converted into HSV colour space as the luminance (V channel) exhibits high contrast regions which are more appropriate for optic disc localization. We then threshold those pixels with the highest value from the equalized histogram. Morphological processing (dilation and erosion) is subsequently employed to connect neighbouring pixels that are disconnected. The largest area of contiguous high luminance pixels corresponds to the location of the optic disc. Finally a centroid calculation is used to locate the centre of the optic disc.

The fovea is almost always in the centre of the image (because the gaze is into the camera), but is not straightforward to detect. Our method relies on prior knowledge of general fundus photography, in which the macula area is always darker than surrounding areas and that is lies approximately on a horizontal meridian running through the optic disc. These observations allow us to restrict the region for the fovea search. Before using the same approach to detection of the optic disc, the image is smoothed to remove small local minima arising from small blood vessels using a median filter with a 5×5 window. The fovea can then be detected using the same method as for optic disc detection, but in this case the threshold is selected to identify those pixels with low rather than high values.

Although a number of methods for blood vessel detection have been reported these have only been applied to fundus photographs [37]. In [38], authors proposed two k-nearest-neighbour (k-NN) based methods that utilize information from fundus photographs to segment the retinal vessels in ONH-centred SD-OCT volumes. However these methods cannot be used in the present study since their images were acquired simultaneously with the OCT image whereas ours were acquired sequentially.

To resolve this issue, we modified the technique proposed in [36] by including prior knowledge so that the extracted blood vessels could form the basis for the OCT/fundus photograph registration process (described in Section
                        4.5). The authors in [36] employed multi-scale products of wavelet coefficients to detect edges in the image. The advantage of the wavelet-based approach is that features of high saliency have large value coefficients in multiple adjacent scales, while the noise does not propagate across scales. That means the multi-scale products enhance the difference between the desired features and the background. After scale multiplication, the vessels are subsequently extracted from the image using an iterative thresholding approach. The initial threshold, τ
                        ∈[0, 1), is selected automatically from the point where the normalized histogram is divided into two and the average of the mean values of these histogram groups are equal to the value of such point.

We improve this simple technique by adjusting the threshold iteratively until detected vessel areas fall within boundaries. This ensures that the number of extracted vessels is optimal for registration process. The iteration is initiated by setting τ
                        =
                        τ
                        +0.01 until 
                           ∑
                           
                              p
                              v
                           
                           /
                           ∑
                           
                              p
                              b
                           
                           ≥
                           
                              T
                              l
                           
                        , where 
                           
                              p
                              v
                           
                         and p
                        
                           b
                         are pixels in the vessel and background areas, respectively. T
                        
                           l
                         is the smallest ratio to ensure that the result incorporates enough vessels for registration. The second iteration is performed by setting τ
                        =
                        τ
                        −0.0025 until 
                           ∑
                           
                              p
                              v
                           
                           /
                           ∑
                           
                              p
                              b
                           
                           ≤
                           
                              T
                              h
                           
                        , where T
                        
                           h
                         is the largest ratio which prevents the result from including some noise, or until the τ that has been used. Finally, morphological processing is applied to refine the results. Although the process involved several iterative steps none are repeated more than 5 times thereby ensuring that our approach is computationally feasible, but improves the performance compared to that of [36].

To extract the blood vessels, a fundus image must be generated from the OCT images. Most papers construct the projection of OCT image from summation of the OCT image in the z-direction as shown in Fig. 11
                           . However, we discovered that employing only some layers of the OCT image makes the blood vessels more distinct, particularly at the macula where the blood vessels are not so obvious and the pigmented layer causes a rough surface when summed up to create the fundus image. We therefore use only the NFL, GCL and IPL layers to generate the vascular maps for alignment. Fig. 12
                            demonstrates that the fundus image generated using only these three layers leads to more apparent blood vessels than those in the fundus image generated using all OCT layers. To extract blood vessels, the T
                           
                              l
                            and T
                           
                              h
                            are set to 0.1 and 0.3, respectively, for both macula and ONH areas.

The fundus photograph is firstly resized to have the height of 600 pixels in order to speed up the process and because the algorithm is size dependent.

The larger blood vessels are first detected with T
                           
                              l
                            and T
                           
                              h
                            set to 0.1 and 0.2, respectively. These two thresholds are empirically set by using 50 blood vessel segmentation results available online. The macular area is then reprocessed with T
                           
                              h
                            set to 0.3 to increase the number of vessels identified in this area that are suitable for alignment. Fig. 13
                            shows the results of the proposed blood vessel extraction on photographic images.

The photographic and OCT images have different and unknown resolutions. To match their scaling we first estimate the scaling ratio S
                        
                           init
                         based on vessel size.


                        
                           
                              (8)
                              
                                 
                                    S
                                    init
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   r
                                                   OCT
                                                
                                             
                                             
                                                
                                                   r
                                                   Photo
                                                
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      Θ
                                                      ¯
                                                   
                                                   OCT
                                                
                                             
                                             
                                                
                                                   
                                                      Θ
                                                      ¯
                                                   
                                                   Photo
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where r represents the radius of the circle from the centre of image obtained from Section
                        4.2 and 
                           
                              Θ
                              ¯
                           
                         is the mean diameter of major vessels (
                           
                              ϕ
                              i
                           
                           >
                           
                              ϕ
                              ¯
                           
                        , where ϕ is the diameter of ith vessel and 
                           
                              ϕ
                              ¯
                           
                         is the mean of all vessels’ diameters). Firstly, we fix r
                        
                           Photo
                         at approximately half way from centre to the edge of the image, i.e. r
                        
                           Photo
                        
                        =min(H
                        
                           photo
                        /4, W
                        
                           photo
                        /4, C
                        
                           i
                        /2, (H
                        
                           photo
                        
                        −
                        C
                        
                           i
                        )/2, C
                        
                           j
                        /2, (W
                        
                           photo
                        
                        −
                        C
                        
                           j
                        )/2), where H
                        
                           photo
                         and W
                        
                           photo
                         are height and width of the fundus photograph. The fovea or the optic disc locates at (C
                        
                           i
                        , C
                        
                           j
                        ). Then, the 
                           
                              
                                 Θ
                                 ¯
                              
                              Photo
                           
                         is computed from the extracted vessels and the initial scaling is obtained from Eq. (9).


                        
                           
                              (9)
                              
                                 
                                    S
                                    init
                                 
                                 =
                                 
                                    arg min
                                    S
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            Θ
                                                            ¯
                                                         
                                                         OCT
                                                      
                                                      (
                                                      S
                                                      ·
                                                      
                                                         r
                                                         Photo
                                                      
                                                      )
                                                   
                                                   
                                                      
                                                         
                                                            Θ
                                                            ¯
                                                         
                                                         Photo
                                                      
                                                   
                                                
                                                −
                                                S
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

There are various methods used for solving this problem. To speed up the process, we limit the search range S to S
                        ∈{R
                        −0.5, R
                        −0.25, R, R
                        +0.25, R
                        +0.5} if R
                        ≤1, where R
                        =
                        H
                        
                           oct
                        /H
                        
                           photo
                         and H
                        
                           oct
                         is the height of the OCT fundus image, or to S
                        ∈{1/(R
                        +0.5), 1/(R
                        +0.25), 1/R, 1/(R
                        −0.25), 1/(R
                        −0.5)} if R
                        >1.

It should be noted that where the magnification of the OCT images is known (as often assume in other registration work, e.g. [34]) this scaling step can be simplified accordingly.

We employ the simple and efficient approach described in [36] to estimate rotation between the fundus photograph and the fundus OCT image. The centres of two maps, either optic disc or fovea, are shifted to align in order to remove translational mismatches. One image is then rotated through 360° with step of 1° to determine the angle with the minimum difference in pixel values between the images using a XOR operator. A small range of scalings is used for testing, namely S
                        ∈{S
                        
                           init
                        
                        −0.2, S
                        
                           init
                        
                        −0.1, S
                        
                           init
                        , S
                        
                           init
                        
                        +0.1, S
                        
                           init
                        
                        +0.2}. The minimum XOR value, M
                        
                           XOR
                        , cannot be applied to images at different scales, since increasing the size of one image always increases nonintersecting areas. We thus introduce the normalized M
                        
                           XOR
                         as shown in Eq. (10).


                        
                           
                              (10)
                              
                                 
                                    NM
                                    XOR
                                 
                                 =
                                 
                                    
                                       
                                          M
                                          XOR
                                       
                                    
                                    
                                       1
                                       +
                                       (
                                       1
                                       /
                                       S
                                       )
                                    
                                 
                              
                           
                        
                     

One significant problem with OCT is the discontinuities between B scans because of involuntary eye movement during the image acquisition.

To minimize this effect, the discontinuities are first detected using local maxima of the gradient summation between B scans. We set the minimum distance between adjacent maxima to 20 pixels (20 B-scans). The area between two maximal values must contain enough information for the refinement process. Subsequently, the OCT fundus image is divided into many parts according to these discontinuity points. The registered position of each segment is refined individually by searching for an improved match (lower XOR summation) within the defined window. Example of the refined search is shown in Fig. 14
                        .

@&#RESULTS AND DISCUSSION@&#

This section presents some results using our image processing toolset for retinal OCT images. The OCT images used in this paper are the 3D stacks with the size of 512×512×1024 pixels. A-scan is one depth penetration (1×1024 pixels), while a B-scan is a 2D cross-sectional retinal image made of 512 A-scans (512×1024 pixels). Each OCT volume contains 512 B-scans. The OCT images are displayed on a greyscale with 16 bits/pixel (216 grey levels). The fundus photographs are JPEG colour images with varying resolutions in the range of 1670×810 pixels to 3008×1960 pixels. They were acquired from different machines and on different days to the OCT images.

We evaluated our AWBF using greyscale image with artificial speckle and real retinal OCT images in Section
                     5.1. The results of layer segmentation and OCT/fundus image registration are shown in Sections
                     5.2 and 5.3, respectively.

Since ground truths for the OCT images are not available, we created a test image (ground truth) and applied speckle noise to it so that objective assessment can be employed to evaluate the performance of the proposed technique. The results of enhancing this speckled image are presented in Section
                        5.1.1. Then, the results of using real OCT images are discussed in Section
                        5.1.2.

We created a test image comprising round and straight-line shapes with various intensities. Subsequently, speckle is added to the image using a speckle noise model assuming to be Log-Normal distributed [39]. For comparison, we employed the approach in [16], bilateral filtering and our AWBF. Objective results are shown in Fig. 15
                            (middle and right, using mean square error (MSE) and mean structural similarity (MSSIM) [40], respectively). The median values of noise indicate how much noise was added. Since this was multiplicative noise it increased with the smaller median values thereby generating high mean square errors. AWBF resulted fewer errors compared with other methods as shown in Fig. 16
                           . Traditional bilateral filtering provided the greater errors since it could not remove high frequency noise.

Firstly we investigated how importance of each step of the proposed method. As a reference is not available for quality assessment, we use three blind quality metrics, namely speckle-SNR defined as a ratio of the mean to the standard deviation of speckled images [24], quality by support vector regression (QSVR) measuring preservation of edge structure [41] and Anisotropic Quality Index (AQI) measuring the variance of the expected entropy [25]. The higher value implies better quality. Comparing to the quality when using all steps (All), the lower value implies the more importance of such process. Table 1
                            reveals the performance drop when omitting each step. We also used these metrics to investigate the window size of S affecting the performance of the AWBF and traditional bilateral filter (BF). The results are shown in Table 2
                           .

The results of the enhancement process are shown in Fig. 17
                           . The first column shows the raw OCT images, the second images after adjustment and despeckling with DT-CWT [16]. Images in the third column result form processing with our proposed AWBF showing improved noise removal and clearer delineation of retinal and optic nerve structures. They also show better quality comparing to the SURE-LET denoising method developed by the Center for Biomedical Imaging of the Geneva Lausanne Universities and the EPFL [42] (for fair comparison, we applied intensity adjustment before the SURE-LET process). Objective results are shown in Table 3
                           .

We next compared the utility of the image processing methods in the discrimination of normal and glaucomatous eyes, incorporating texture, as well as measures of retinal ganglion cell and IPL thickness. We hypothesize that textural changes would arise within these layers secondary to degenerative events at the cellular level. We extract textural features from the Inner plexiform layer (IPL) of 24 retinal OCT images, 14 normal eyes and 10 glaucoma eyes. We randomly selected 7 normal eyes and 5 glaucoma eyes for training and used the rest for testing. Experiments with different random training and testing dataset were repeated 100 times As the visual field tests were not available, the areas of dysfunction in the eyes are unknown. We therefore exploited the global parameters by averaging the values of features extracted from 80 subvolumes from each OCT image. The size of each subvolume is 30×30×
                           T
                           
                              k
                            pixels, where T
                           
                              k
                            is the thickness of IPL of subvolume k
                           ∈{1, 2, …, 80}. The features used in this test are run-length measurement (RLM), DT-CWT (CWT), grey-level co-occurrence matrix (GLCM), local binary pattern (LBP) and granulometry (GRN) [43]. The results are shown in Table 4
                           . In most cases, the enhanced images improve the performance of glaucoma detection over the raw OCT images and despeckled images. It therefore implies that the proposed method removes some ambiguity and reveals more information.

We evaluated our layer segmentation by comparing results with ground truths which were generated by manual segmentation (done by authors). The resulting unsigned border positioning errors are given in Table 5
                        . For ONH, the areas of optic cup were not included in the calculation. The errors were calculated from the area inside the circles with the radius of 180 pixels and centred at the fovea or the ONH. Large errors often occur at the dark areas near the edge of the image (see Fig. 17, rows 1 and 4 for examples) and also near the big shadows corresponding to blood vessels (Fig. 17, row 2). As the glaucoma eyes show thinner RNFL compared to the normal eyes, the fixed number of decomposition levels of the DT-CWT may be too high to distinguish the different texture property as the highpass subbands of higher level may be dominant. The segmentation between INL and OPL seems to be the most difficult, particularly in the macula image, since the boundary is not clear and not smooth because of low light reflection as shown in Fig. 18
                        . Fig. 19
                         shows the thickness maps of GCC generated manually and automatically. The thickness maps are varied from 20 to 50 pixels represented as colours from dark blue to red and the area where the thickness is thinner than 20 pixels is also shown in dark blue. This figure clearly shows that for glaucoma eyes it is more difficult to extract retinal layers; however, the results are good enough to identify which parts of the eyes are affected by glaucoma.

Example results of the OCT/fundus image registration are shown in Fig. 20
                        . The OCT photograph with 50% opacity is overlaid on the corresponding fundus photograph to reveal the alignment of blood vessels of two images. This figure shows clear difficulties from different scaling between two image modalities and discontinuity because of subject movement. Our method however makes OCT image and fundus photograph more meaningful for diagnosis, e.g. the areas affected by disease can be better identified. It should be noted that the fundus photograph is a 2D image, while OCT image is a 3D (volumetric) image. This means the fundus photograph and OCT fundus image may not be related by simple translation causing some parts of blood vessels to not be perfectly aligned. A non-rigid registration can be further employed.

@&#CONCLUSIONS@&#

This paper presented a series of algorithms for retinal OCT images comprising OCT image enhancement, inner retinal layer segmentation and OCT/fundus photograph registration. The enhancement process consists of intensity adjustment, wavelet-based despeckling, wavelet-based image registration and adaptive-weighted bilateral filtering which is a texture-preserving smoothing operation. The proposed AWBF is employed with adaptive weights using local entropy. Results show an improvement of image quality and improvement in accuracy of glaucoma detection. The OCT image enhancement obviously aids in layer segmentation and OCT/fundus photograph registration. The layer segmentation step is also a wavelet-based method. It achieves desirable results for glaucoma diagnosis. The OCT/fundus photograph registration process involves blood vessel extraction and registration, as well as B-scan discontinuity detection. The registration results clearly improve image understanding and are beneficial for image fusion.

@&#REFERENCES@&#

