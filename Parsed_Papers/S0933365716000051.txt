@&#MAIN-TITLE@&#Comparison of automatic summarisation methods for clinical free text notes

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Eight automatic text summarisation methods are described and tested.


                        
                        
                           
                           Word space models of distributional semantics are used in five of the presented methods.


                        
                        
                           
                           A composition based summarisation method outperforms the other considered methods.


                        
                        
                           
                           The utilised automatic evaluation approach is shown to correlate with manual evaluation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Automatic text summarisation

Summarisation evaluation

Distributional semantics

Word space models

Clinical text processing

Electronic health records

@&#ABSTRACT@&#


               
               
                  Objective
                  A major source of information available in electronic health record (EHR) systems are the clinical free text notes documenting patient care. Managing this information is time-consuming for clinicians. Automatic text summarisation could assist clinicians in obtaining an overview of the free text information in ongoing care episodes, as well as in writing final discharge summaries. We present a study of automated text summarisation of clinical notes. It looks to identify which methods are best suited for this task and whether it is possible to automatically evaluate the quality differences of summaries produced by different methods in an efficient and reliable way.
               
               
                  Methods and materials
                  The study is based on material consisting of 66,884 care episodes from EHRs of heart patients admitted to a university hospital in Finland between 2005 and 2009. We present novel extractive text summarisation methods for summarising the free text content of care episodes. Most of these methods rely on word space models constructed using distributional semantic modelling. The summarisation effectiveness is evaluated using an experimental automatic evaluation approach incorporating well-known ROUGE measures. We also developed a manual evaluation scheme to perform a meta-evaluation on the ROUGE measures to see if they reflect the opinions of health care professionals.
               
               
                  Results
                  The agreement between the human evaluators is good (ICC=0.74, p
                     <0.001), demonstrating the stability of the proposed manual evaluation method. Furthermore, the correlation between the manual and automated evaluations are high (> 0.90 Spearman's rho). Three of the presented summarisation methods (‘Composite’, ‘Case-Based’ and ‘Translate’) significantly outperform the other methods for all ROUGE measures (p
                     <0.05, Wilcoxon signed-rank test and Bonferroni correction).
               
               
                  Conclusion
                  The results indicate the feasibility of the automated summarisation of care episodes. Moreover, the high correlation between manual and automated evaluations suggests that the less labour-intensive automated evaluations can be used as a proxy for human evaluations when developing summarisation methods. This is of significant practical value for summarisation method development, because manual evaluation cannot be afforded for every variation of the summarisation methods. Instead, one can resort to automatic evaluation during the method development process.
               
            

@&#INTRODUCTION@&#

@&#BACKGROUND@&#

Information overload in the health sector is becoming an increasing problem for clinicians [1,2]. They have to read masses of text (such as clinical notes, guidelines and scientific literature) to satisfy their information needs. Lack of time and resources to do this properly causes problems such as errors, frustration, inefficiency and communication failures [3].

The contents of electronic health record (EHR) systems are largely composed of clinical notes (or clinical narratives) in the form of unstructured and unclassified text. The clinical notes written during a single care episode, i.e. a stay in a hospital, can be quite voluminous, especially for patients suffering from more complex and long-term health problems. Knowing the medical history of a patient is vital for a clinician, but scanning through clinical notes consumes precious time that could be better spent treating the patient.

Automatic summarisation of the free text content in care episodes could assist clinicians in at least two ways. First, it could provide an (indicative) overview of the documentation of a care episode. Together with structured data (such as laboratory test results, images, diagnostic codes and personal information) it could help clinicians to familiarise themselves with the content of the care episode and the patient's problems, which is particularly useful if the information is needed urgently. Second, it may help in writing a discharge summary of a care episode. Discharge summaries are crucial in communication between different health care providers and they are needed to ensure continuity of care. However, there are a number of challenges with them, ranging from being produced late to having insufficient information. For example, Kripalani et al. [4] showed that discharge summaries exchanged between hospitals and primary care physicians are often lacking some of the expected information, such as that related to treatment progression, counselling and follow-up proposals. Computer-assisted discharge summaries and standardised templates are measures for improving transfer time and the quality of discharge information between hospitals and primary care physicians [4]. The utilisation of automatic text summarisation could improve the timeliness and quality of discharge summaries even further.

Central to this work is the focus on resource-lean
                        
                           1
                        
                        
                           1
                           We are striving towards using as little manual labour as possible.
                         and language-independent methods. Such methods are important for languages such as Finnish, for which no major manually constructed lexical resources suited for the comprehensive semantic analysis of clinical text are available.

@&#RELATED WORK@&#

This study focuses on the extraction-based summarisation approach, in which the summary is generated by selecting a subset of sentences from the relevant text. This approach is viable because a sizeable portion of a clinical text summary is created by copying or deriving information from clinical notes [2,5–7]. See [8,9] for example, for more information on extraction-based text summarisation.

A central issue in extraction-based summarisation is how to determine what the most relevant content to be included in a summary is. Common techniques of extraction-based summarisation include topic-based sentence extraction [10,11], where the relevance of a sentence is computed with respect to one or more topics of interest; and centrality-based sentence extraction [12,13], where the sentences that are the most (strongly) associated with others are selected on the assumption that they constitute the best coverage of the documents. In order to avoid including redundant information, it is common to apply the maximal marginal relevance criterion [10] or similar techniques that take sentence overlap into account. Purely statistical (data-driven) approaches to text summarisation are often referred to as ‘knowledge poor’, whereas those using knowledge resources are considered ‘knowledge rich’. The latter could, for example, include the use of an ontology that models medical and clinical concepts as well as their relationships.

In their recent review, Mishra et al. [14] indicated that there is a growing interest in knowledge-rich approaches in the biomedical domain, coinciding with the increased availability of comprehensive lexical resources, such as the WordNet ontology [15] and the UMLS compendium [16] (including SNOMED-CT [17], ICD [18] and MeSH [19]). There are several language tools that rely on these resources, such as MetaMap [20], cTAKES [21] and SemRep [22]. Other commonly used resource types include, for example, annotated corpora designed for machine learning (ML) algorithms (see e.g. [6,23]). However, one disadvantage to approaches that rely on manually constructed resources is that they are often not applicable across domains or languages [24,25]. WordNet and UMLS (SNOMED-CT, in particular), for example, are only available in a few languages. The cost of adapting existing resources to new languages, domains or tasks, or constructing new resources, is often high.

The use of distributional semantic methods represents a resource-light approach to capturing terminology in clinical texts [26–31]. These methods rely on the distributional hypothesis 
                        [32] for constructing distributional semantic models from word co-occurrence statistics in an unsupervised manner, typically using a very large corpus of unannotated text. The aim is to model similarities, or relatedness, between linguistic items (e.g. words) in a way that reflects their relative semantic meaning. Distributional semantic models representing word-level semantic similarity are often referred to as word space models (WSMs). In a WSM, a word context vector is created for each unique word in the underlying corpus. Further, each context vector represents a point in the ‘word space’ and their internal distances reflect their semantic similarities. Similarities between context vectors are then calculated to quantify the semantic similarity as a numeric value (for example, using the cosine similarity function). Popular techniques and frameworks for constructing WSMs include latent semantic analysis (LSA) [33], random indexing (RI) [34] and Word2vec (W2V) [35]. The domain-specificity of the corpus used for constructing the model has been shown to be important for the usefulness of semantic similarities to the intended task [27]. Distributional semantic models in various forms have been extensively used in text summarisation, e.g. [9,13,36].

To the best of our knowledge, the task of automatically generating textual summaries from clinical notes has been pursued by relatively few researchers, which is also evident in recent reviews and related works, for example [14,24]. We have identified several pieces of work focusing on the task of automatically generating textual summaries from unstructured clinical notes. Liu [37] used the MEAD summarisation toolkit. Van Vleck et al. [2] performed structured interviews to identify and classify phrases that clinicians considered relevant to explaining a patient's history. Meng et al. [6] used an annotated training corpus together with tailored semantic patterns to determine what information should be repeated in a new clinical note or summary. Velupillai and Kvist [23] focused on recognising diagnostic statements in clinical text, learning from an annotated training corpus, and classifying these based on the level of certainty they have in them. Extracted diagnostic statements are then used to produce a text summary. Others have worked on more conceptual models for understanding and supporting the generation of information summaries in the clinical domain [38,39].

The evaluation of computer-generated summaries is typically performed by comparing the generated summary with a gold standard (or reference summary), which represents the ideal manually constructed summary or summaries. The ROUGE
                        
                           2
                        
                        
                           2
                           ROUGE is short for recall oriented understudy for gisting evaluation.
                         
                        evaluation package 
                        [40] has become a de facto evaluation metric in text summarisation. The required gold standard summaries are costly to create given the manual work required. This is particularly the case in specialised domains where domain experts are required. Lissauer et al. [3] evaluated computer-generated discharge summaries from neonate reports by manually comparing them to dictated summaries, as well as analysing them to see whether they contained the required information according to guidelines. Liu [37] performed automatic evaluation of computer-generated summaries of clinical nursing notes by using the original discharge reports as gold standard summaries. Moen et al. [41] applied both manual and automatic evaluation to the assessment of the reliability of automatic evaluation; the manual evaluation was performed by domain experts and the automatic evaluation was performed by using ROUGE to calculate the similarity between the computer-generated summaries and the original discharge summaries (produced by clinicians).

The main contributions of this study can be summarised as follows:
                           
                              •
                              Proposal and implementation of four novel automatic summarisation methods designed for summarising the free text in care episodes;

Proposal and implementation of a methodology for conducting the manual evaluation of automatically generated care episode summaries;

Empirical analysis of automatic evaluation measures through comparison with manual evaluations;

Performance assessment of the four novel automatic summarisation methods along with four baseline methods.

The data used in this study is Finnish clinical text, but since the applied methods are language-independent, the contributions should also be relevant to other languages. The overall set-up is illustrated in Fig. 1
                        .

@&#MATERIAL AND METHODS@&#

The data set used in this study consists of EHRs from patients with any type of heart-related problem that were admitted to a single university hospital in Finland between 2005 and 2009. Of these, the clinical notes written by physicians on the various wards that the patients visited were used. However, notes written by nurses were not included. Fig. 2
                         shows an example of a clinical note.

Ethical approval for the research was obtained from the ethics committee of the hospital district (17.2.2009 §67) and permission to conduct the research was obtained from the medical director of the hospital district (2/2009). The total set consisted of 66,884 care episodes, which amounts to 398,040 notes and 64 million words
                           3
                        
                        
                           3
                           A word is defined as tokens containing at least one letter.
                         in total. This full set, minus 308 care episodes reserved for optimisation and evaluation (see below), were used for constructing the WSMs (see Section 2.2).

The notes are mostly unstructured, consisting of clinical free text in Finnish. Various subheadings do occur in the clinical notes, but these are not standardised, structured, or uniquely recognised in our corpus. Thus, we treat these in the same way as the rest of the text. Some of the sentences are, according to the EHR system, considered to be metadata — such as names of the authors, dates, wards and so on. We treat the free text sentences and the meta-text sentences as two separate text types, so these are not mixed in the sense that they cannot belong to the same ‘sentence topic’ clusters, which are described in Section 2.3.

Each care episode has been manually labelled with ICD-10 codes by clinicians as a part of the original care process. These are normally applied at the end of the patient's hospital stay, or after they are discharged from hospital. Care episodes commonly have one primary ICD-10 code attached to them, and a number of optional secondary codes. In this study the primary ICD-10 code is used in constructing the RI-ICD WSM, as described in Section 2.2.

In the presented experiments, we restrict our evaluation to the care episodes which have the primary ICD code I25 — chronic ischemic heart disease, including subcodes (I25.0, I25.1, etc.). As a further restriction, to justify the use of text summarisation, we consider only care episodes consisting of seven or more clinical notes written by physician. In order to guarantee that the methods are tested on independent test data that is not used for developing the summarisation models, the 308 care episodes are split into two subsets:
                           
                              •
                              A summarisation optimisation set, consisting of 152 care episodes, used for optimising parameters related to the summarisation methods.

A summarisation evaluation set, consisting of 156 care episodes, used for evaluation in the conducted experiments. This is further split into two subsets of 20 and 136 care episodes, the former subset being evaluated in Experiment 1 and the latter in Experiment 2.

This splitting is performed according to the year in which the care episodes were carried out.

We use a method based on the RI technique, utilising the ICD-10 codes attached to care episodes (RI-ICD) to construct a WSM for the purpose of calculating (semantic) similarity between care episodes. We also use the RI technique in constructing a ‘cross-text’ translation model (RI-Translate), and the W2V method is used to construct a WSM for the purpose of calculating sentence-to-sentence similarities, as well as sentence-to-document
                           4
                        
                        
                           4
                           Documents are in this case the clinical notes.
                         similarities. The cosine similarity metric is used to calculate vector similarities.


                           RI 
                           [34] is a technique for constructing a (pre-)compressed WSM with a fixed dimensionality, done in an incremental fashion. This is achieved by initiating index vectors for each unique word in the corpus. An index vector is a vector of a fixed dimensionality, containing mainly zeros along with a small number of randomly assigned non-zeros, typically 1 or −1. During training, context vectors for words are constructed by adding index vectors to them. In this way, the dimensionality of the context vectors remains constant. In this work we use a version of RI where context features are based on the ICD-10 code classifications of care episodes. We have called this RI-ICD,
                              5
                           
                           
                              5
                              A vector dimensionality of 800 was used, and the number of non-zeros for the index vectors was set to four.
                            previously introduced in [42].

Another RI-based method used here is one intended for cross-lingual translation purposes, described in [43]. We refer to it as RI-Translate. The method constructs a bilingual WSM that connects words in the source language (SL) to their translated counterparts in the target language (TL). In practice, we operate with two models, one for SL and one for TL, where both belong to the same semantic space in that they are constructed with the same set of index vectors. For training, pre-aligned translation pairs (in this case, aligned sentences) connecting the SL to the TL are used as training instances. The training takes place as follows: for each translation pair (SL–TL), a unique index vector is generated and added to the corresponding context vectors for words in the SL and TL models. This will result in high cosine similarity between words in the SL model and the TL model that have often occurred in the same translation pairs.

When querying the system, the context vector(s) corresponding to the query in the SL model is used as the query. This query vector is then matched against the units in the TL model, using cosine similarity, in order to find the most likely translation(s). This method is used for summarisation purposes in the Translate method, as described below.

Word2vec [35] is a framework for constructing WSMs using a neural network. In this work we utilise the W2V 
                           CBOW architecture.
                              6
                           
                           
                              6
                              For W2V a window size of 5+5 and a dimensionality of 800 was used.
                            
                           Table 1
                            shows an example of how the W2V-based model captures semantic similarity relations. We use this model in the various summarisation methods for computing sentence-to-sentence similarities and sentence-to-document similarities.

Sentence context vectors are composed by normalising and summing (pointwise summation) the constituent word context vectors weighted by their sentence term frequency multiplied by their global inverted sentence frequency (TF*ISF). A similar approach is used for constructing context vectors representing clinical notes, but here weighting is based on term frequency multiplied with global inverted document frequency (TF*IDF) [44], where each clinical note is considered as a document.

We evaluated eight different summarisation methods. Oracle is an (unrealistic) reference method that has access to the true/original discharge summary when selecting sentences to extract, providing an upper boundary to how well an extractive summarisation method can work for our data. LastSentences and Random are simple reference methods that a successful summarisation method should be able to outperform. Centrality is a standard baseline approach that is commonly used in the field, and the remaining four methods, called RepeatedSentences, Case-Based, Translate and Composite, are proposed methods developed specifically with the clinical domain in mind.

For each care episode, the length of the summary generated by each summarisation method is set to have a fixed size equal to the word count of the accompanying discharge summary (i.e. the ‘gold standard summary’ for the care episode). Sentences are iteratively extracted from the clinical notes until the total word count becomes equal to or just exceeds the word count of the discharge summary. Therefore, generated summaries can have a word count equal to the discharge summary, or exceed this limit by a subset of the words in the last extracted sentence. This way of dynamically selecting the summarisation length is mainly done to enable the calculation of the automatic evaluation scores (F-score), described in Section 2.4.2, which assumes equal length of the target summary (the summary being evaluated) and the gold standard summary.

In the summarisation methods RepeatedSentences, Case-Based, Translate and Composite, a type of topic clustering is used to perform redundancy reduction. We found that each sentence is typically informative, self-sustaining in information content, and independent of other sentences within a single note. All sentences are first clustered into unlabelled sentence topics in an unsupervised way using the W2V model. A cosine similarity threshold λ, optimised on the summarisation optimisation set, is used for determining whether two sentences can be considered similar or not — whether or not they belong to the same topic based on their cosine similarity. The underlying approach is somewhat comparable to how similar paragraphs are detected and merged in McKeown et al. [45] with the aim of reducing sentence redundancy. Since we know when each sentence was written, and if we assume that we are able to cluster sentences that discuss the same topic across clinical notes (e.g. the state of a patient's pain), we can also assume that the latest written sentence belonging to a topic is the most representative of the latest information concerning that topic. Therefore, we allow the latest written sentence belonging to each topic cluster to be the representative sentence. In an attempt to most effectively model sentence topic clusters, the clustering approach is done as follows: first, we assume that all sentences in the first clinical note of a care episode belong to different topics (see Note1 in Fig. 3
                         for an illustration). Then we iterate through the care episode, from the first to the last written note, and assign each sentence to either existing topics (cos sim
                        ≥
                        λ) or new topics (cos sim < λ) based on their cosine similarities in relation to λ. In the utilised similarity comparison, the latest added sentence of a topic always represents that topic. A sentence can only belong to one topic, so if a sentence is similar to two or more topics, i.e. cos sim
                        >
                        λ, the sentence is assigned to the most similar topic.
                           7
                        
                        
                           7
                           In the unlikely event that the cosine similarities between a sentence and two or more topics are the same, the sentence is assigned to a random topic among these.
                        
                     

The original discharge summary is a text written by a clinician, typically a physician, to summarise a care episode. These summaries are thus written at the end of each care episode, and often contain extracts from the accompanying clinical notes. They also typically contain a certain amount of as-yet undocumented information which focuses on follow-up treatment, and are meant for the receiving ward (if any) or the primary care sector.

In this work, the original discharge summary serves as the gold standard summary for its accompanying care episode in the automatic evaluation approach that is used (see Section 2.4.2). In addition, some of the summarisation methods use these in their underlying training (Translate) or in the summarisation phase (Case-Based). Naturally, for a care episode that is to be summarised, the accompanying original discharge summary will not be available to the summarisation system in a realistic scenario.

This is a control method that has access to the original discharge summary during the summarisation process. It optimises the ROUGE-N2 
                           F-scores (see Section 2.4.2) for the generated summary according to the gold summary, using a greedy search strategy. That is, the method extracts sentences one by one from the clinical notes until it reaches the length threshold, always picking sentences that result in the highest possible ROUGE-N2 score. This method is cheating, since it has access to the original discharge summary in the summarisation process. Still, it represents the upper limit for what is achievable in terms of ROUGE-N2 scores for an extraction-based summary.

The latest written clinical note at any point should supposedly represent the current state of the patient. By selecting the latest information found in the last or latest written information, one can intuitively assume that this information is important in a (discharge) summary. In this method, the summary is simply constructed from the N last written sentences during the care episode, where N is the number of sentences needed to reach the length threshold. Intuitively, this represents a strong baseline.

This baseline method constructs summaries by randomly selecting sentences from the care episode until the length threshold is reached. It provides a lower boundary to the performance, which any meaningful summarisation approach should aim to significantly outperform.

Meng et al. [6] argue that information being repeated across clinical notes is an indicator of its relevance with respect to inclusion in subsequent notes in the sequence. The use of time features is also explored by Lim et al. [46] in the task of multi-document summarisation of news article documents. The underlying hypothesis for the RepeatedSentences method is that information that is repeated in multiple clinical notes throughout a care episode, with the emphasis on when it was written, is the most important information to include in a summary. Features from the initial sentence topic clustering step are used for scoring. A topic is assigned a score based on the sum of the order of when, in the care episode, its underlying sentences were written. For example, if a topic contains sentences from clinical note numbers 3, 5 and 6 (numbered relative to the dates they were written), the topic score becomes 14. The N highest scoring sentence topics (i.e. their representative sentences) are included in the final summary. The RepeatedSentences summarisation method is illustrated in Fig. 3.


                           Case-Based, or ‘case-based summarisation’, is here an analogy to case-based reasoning (CBR) [47] which performs a type of textual case-based reasoning (TCBR) [48]. CBR involves retrieving existing or older ‘cases’ with similar content as the target problem, and then reusing the solution of the retrieved case (or cases) to solve the target problem. In a similar manner, this principle is applied here in text summarisation. The underlying hypothesis is that patients with (the most) similar care episodes (according to the documented text in their clinical notes) have similar content in their discharge summaries. The sentences from these discharge summaries are then treated as the central ‘topics’ for what to include in the summary. This is in line with evidence-based practice (EBP) in that relevant care episodes are identified and the information found there is relied upon as ‘evidence’ for what should be included in the summary.

Given a target care episode that is to be summarised, we first retrieve the top five most similar care episodes using information retrieval on care episode level (i.e. ‘care episode retrieval’). For this the RI-ICD method is used (explained in [42], Section 4.1). Then we reuse these by iterating through each sentence in their discharge summaries. The representative/last sentences from each sentence topic in the target care episode (as described earlier) is then scored by their cosine similarity to each of these using the W2V model. Out of these, the N highest scoring sentences are included in the generated summary. Fig. 4
                            illustrates this using a modification of the ‘CBR cycle’ from [47].
                              8
                           
                           
                              8
                              
                                 Fig. 4 also contains the steps revise and retain, but these are outside the scope of this work.
                           
                        

Here we use the RI-Translate method, as explained in Section 2.2, for the purpose of text summarisation. Instead of translation between languages, it is used for ‘cross-text-type translation’ — translating from the text in clinical notes (care episodes without discharge summaries) to the text found in the discharge summaries, while limiting the translation candidates (i.e. sentences) to also come from the sentence topics in the clinical notes. The aim is thus to construct a type of translation system that can map sentences in clinical notes to the most probable sentences to be found in an accompanying discharge summary, based on translation statistics learnt from a large clinical corpus.

First, a translation, or cross-text-type WSM is constructed using the RI-Translate method. Here the source language (SL) consists of the text in the clinical notes, while the discharge summaries constitute the text target language (TL). Training instances are rather coarse, as each care episode represents a single training instance. More precisely, for each care episode, the context vectors of the words in the underlying clinical notes (SL) and those in its accompanying discharge summary (TL) have a unique index vector added to them.

When summarising a care episode, each sentence (in the corresponding clinical notes, pre-clustered into sentence topics) has two sentence vectors constructed, one using word context vectors from the SL model, and the other using the TL model. Then, each sentence vector built with the SL model is iteratively used to query the system. Sentences represented by the TL model are then ranked by their overall max cosine similarity scores to these queries, and the top N sentences are included in the final summary.

In this composite method, the sentence-scoring features from the methods RepeatedSentences, Case-Based and Translate are combined. We found that the best automatic evaluation scores (F-scores) from the summarisation optimisation set were achieved when the scores by Case-Based and Translate were kept as their initial cosine scores, while for RepeatedSentences, the sentence scores were first normalised by dividing on the max scoring sentence. This normalisation converts the scores to be within the same range as the cosine-based scores, ranging from 0 to 1. These three feature scores are then simply totalled to create the final feature score for each sentence. Finally the top N sentences are selected for the final summary.

The centrality (or centroid) principle is the most commonly relied on summarisation technique for many generic text types and domains. It is based on ranking sentences by how representative they are of the central information of the text that is to be summarised. In existing work, a range of methods have been used to compute sentence centrality in extraction-based summarisation. The PageRank method [49] has been extensively used for this purpose as a graph-based approach. We decided to base our implementation on the method presented in [13], which relies on a graph representation together with a WSM (RI). To construct the WSM, we used W2V instead of RI because preliminary testing indicated that this model performed better. Here, weighted PageRank for text is used, referred to as ‘TextRank’ [50]. Edges between nodes, i.e. between sentences, are weighted according to the pre-calculated sentence similarity using W2V. Each sentence also has an initial score similar to the cosine similarity between the sentence and the corresponding clinical note, represented as sentence vectors and document vectors, respectively. In addition, to adapt this approach to multiple documents, i.e. multiple clinical notes, we have extended this method with a sentence centrality ranking that works on multiple notes, in a similar way to how it is done in [51]. This is done by multiplying edge weights by one of two preset constants. Constant ı is multiplied with intra-note edge weights, i.e. edges going between sentences within the same clinical note; and inter-note edges are multiplied with the constant ϵ.
                              9
                           
                           
                              9
                              In the experiment we used a PageRank α value of 0.90, ı was 0.3, while ϵ was 1.0.
                           
                        

Simple post-processing is applied to each summary for the purpose of rearranging the sentence order. Sentences are sorted according to the date they were written (i.e. using the date of the clinical note they belong to). Internal ranking between sentences from the same date is carried out according to internal sentence order. If two sentences from two different notes have the same date stamp, ranking is performed according to their chronological note IDs. Meta-sentences (described in Section 2.1) are placed first and rearranged internally.

The following two experiments were conducted:
                           
                              •
                              
                                 Experiment 1: The first experiment focuses on determining the reliability of the automatic evaluation. This is done by comparing how the manual and automatic evaluations (four ROUGE measures) correlate in terms of the relative rankings of the eight summarisation methods. Here, 20 care episodes (a subset of the 156 care episodes in the summarisation evaluation set) are evaluated both manually and automatically. Spearman's rank correlation coefficient (Spearman's rho) [52] is calculated between the average manual evaluation scores and the average scores for each of the automatic evaluation metrics for each summarisation method.


                                 Experiment 2: In the second experiment, the summarisation methods are tested on a larger evaluation set of 136 care episodes (the 156 care episodes in the summarisation evaluation set minus the 20 used in Experiment 1). The evaluation is performed in an automated manner using four ROUGE measures. The aim is primarily to determine which summarisation method produces the best summaries. In order to test whether the different scores achieved by the different summarisation methods were statistically significant, we performed the Wilcoxon signed-rank test [53] based on the scores from each ROUGE measure, for each summarisation method pair.

In both experiments, we use the same eight summarisation methods described to construct summaries for each care episode. The utilised WSMs are first constructed using the full corpus described in Section 2.1, minus the optimisation and evaluation sets mentioned.

A preliminary version of our evaluation set-up has been described in [41].
                           10
                        
                        
                           10
                           The F-scores from the automatic evaluation are on average noticeably lower in this study than those reported in [41]. This is primarily because here we excluded a specific type of note from all care episodes, a type of summary for the patients, which is often written at the same time as the final discharge summary, and their contents tend to be very similar; sometimes identical. In addition, some of the methods used in this experiment are new or different.
                         Our comparison of manual and automatic evaluation is similar to the analysis conducted by Chin-Yew Lin in [40] on English newswire data when introducing the ROUGE measures.

One goal is to see if our automatic evaluation set-up is reliable, given the uncertainties related to using the original discharge summaries as gold standard summaries. This is done by independently analysing whether or not there is a correlation between how human evaluators rank the performance of the summarisation methods and how automatic evaluation metrics rank these same summarisation methods. Furthermore, we aim to reliably establish which of the tested summarisation methods (and underlying features) perform best.

The manual evaluation is conducted by three domain experts in the clinical field: two physicians and one nurse, all professionals in specialised care and each with over five years’ experience of working with patients suffering from heart-related health problems.

A pre-study focusing on the same type of manual evaluation was conducted in [41]. In this pre-study, a 30-item evaluation scheme (or tool) for manual evaluation was developed based on the hospital districts’ guidelines for writing discharge summaries. It used a 4-point scale ranging from −1 to 2, where, −1=not relevant, 0=not included, 1=partially included and 2=fully included. However, using this scheme turned out to be difficult and extremely time-consuming. One reason for this is that quite a few of the items were somewhat overlapping and very fine-grained, like ‘conclusions’, ‘assessment of the future’ and ‘status of the disease at the end of the treatment period’. Other items were rarely documented by clinicians (physicians) in the clinical notes written during an ongoing care episode, such as ‘status of the disease at the end of the treatment period’, ‘ability to work’ and ‘continued care plan’. In addition, a couple of the items were redundant as they concern what we refer to as structured information in the EHR system, such as ‘care place’ and ‘care period’, and there is little value in trying to extract this from the text. Therefore, this manual evaluation scheme was further developed to a more simplified version with only ten criteria items.
                              11
                           
                           
                              11
                              A pilot test was conducted in the process of developing the manual evaluation scheme.
                            Eight of the ten criteria were rated dichotomically ‘yes’ or ‘no’. These criteria items concern the contents of the discharge summary, where ‘yes’ means that the summary includes content related to the criteria. Moving from a 4-point scale to a 2-point scale was done to simplify the evaluation further. The two remaining criteria concern the readability of the summary and were rated on a scale of 0.0–1.0, where 0.0 was poor and 1.0 excellent. The scheme used in the manual evaluation is shown in Table 2
                           . Information about what type of note each sentence belongs to, and when it was written, was presented as metadata for the manual evaluators.

Each evaluator evaluated the same 20 care episodes, with eight summaries per care episode. The inter-rater agreement between the three evaluators was calculated with the intraclass correlation coefficient (ICC) [54] with a two-way mixed model using IBM SPSS Statistics version 22. Based on the existing literature, we found no fixed limit regarding the interpretation of ICC values; one suggestion is that values below 0.4 are poor, values from 0.4 to 0.59 are fair, values from 0.6 to 0.74 are good, and values from 0.75 to 1.0 are excellent 
                           [55]. The inter-rater agreement between the evaluators in this study was good (ICC=0.744, 95% CI 0.722–0.766, p
                           <0.001).

Given the quite concrete evaluation criteria in Table 2, one could intuitively assume that the best summarisation approach would be to focus on extracting those exact ten criteria items. As a result, we experimented with one summarisation method that aimed to do just that. However, this performed poorly in both manual and automatic evaluation. The main reason for this is that we do not have any good way of mapping the criteria descriptions to the content in the clinical notes. For example, there is no straightforward way of mapping ‘long-term diagnosis’ to a sentence not explicitly containing these exact or similar words. A sentence mentioning long-term diagnosis could be: ‘the patient has been suffering from high blood pressure for the last four years.’

Automated evaluation of summaries generated from a care episode is performed by using the accompanying original discharge summary as a gold standard. This exploratory approach circumvents the need for manually constructing such a gold standard.

The ROUGE evaluation toolkit 
                           [40] contains multiple n-gram-based evaluation metrics that are commonly used for automatic summarisation scoring, such as in the document understanding conferences (DUC) and the text analysis conferences (TAC) [56]. ROUGE basically works by calculating the n-gram overlap between a target summary (the summary that is to be evaluated), and one or more gold standard summaries. The outputs from these metrics are precision, recall and F-score, reflecting the overlap between the target and gold standard summaries. The average F-scores are what we report here. As there are several metrics to choose from, we use the following
                              12
                           
                           
                              12
                              We found the listed ROUGE metrics to be the most commonly used metrics in the literature.
                           :
                              
                                 •
                                 
                                    
                                       ROUGE-N1
                                     unigram co-occurrence statistics.


                                    
                                       ROUGE-N2
                                     bigram co-occurrence statistics.


                                    
                                       ROUGE-L
                                     longest common sub-sequence co-occurrence statistics.


                                    
                                       ROUGE-SU4
                                     skip-bigram and unigram co-occurrence statistics.

@&#RESULTS@&#

To visualise how the evaluations correlate, we have plotted the scores from the manual and automatic evaluations in a graph, shown in Fig. 5
                        .

The correlations between manual and automatic evaluations were calculated using Spearman's rho. The results are shown in Table 3
                        . Based on the statistical analysis and p-values in Table 3, the four ROUGE measures have a high correlation with the manual evaluations.

The results from the automatic evaluation of 136 care episodes are shown in Table 4
                        . The r columns show the internal ranking of each summarisation method for each evaluation measure. A more illustrative representation is shown in Fig. 6
                        .

We calculated significance levels using the Wilcoxon signed-rank test, with p
                        <0.05 (with Bonferroni correction for multiple hypothesis testing). Based on the p-values the methods could be divided into three groups. First, Oracle significantly outperformed all the other methods against all of the ROUGE measures (highest p-value: 2.12·10−22 ROUGE-N1 Oracle vs. Translate). Second, Composite, Case-Based and Translate significantly outperformed the methods in the third group — RepeatedSentences, LastSentences, Centrality and Random — against all ROUGE measures (highest p-value 3.74·10−4 ROUGE-N2 Translate vs. LastSentences). In this third group, no method significantly differed from the Random method in terms of at least one ROUGE measure. The p-values for all comparisons are included in the supplementary materials.

Based on the analysis we can divide the methods (not counting the Oracle method) into two groups: Composite, Case-Based and Translate are successful at producing summaries that outperform the simple baseline methods in all comparisons, whereas the Centrality and RepeatedSentences methods fall in the same group with the simple baseline approaches.

Without the Bonferroni correction, the significantly differing groups would be as follows:
                           
                              1.
                              
                                 Oracle
                              


                                 Composite
                              


                                 Case-Based, Translate
                              


                                 RepeatedSentences, LastSentences
                              


                                 Centrality, Random
                              

@&#DISCUSSION@&#

In this work we consider a variety of resource-lean and language-independent summarisation methods for clinical text. These methods circumvent the need for tailored language resources and tools. The proposed summarisation methods utilise WSMs constructed from word co-occurrence statistics in a large corpus of clinical text (see Section 2.1). This enables us to capture various semantic similarity relations in the clinical text in an automatic, data-driven way. The aim is not to construct perfect summaries that can fully replace individual clinical notes or completely automate the process of producing discharge summaries, for example. Rather, this work is a step towards exploring ways of automatically constructing indicative clinical text summaries by relying on purely statistical features for determining a sentence's significance.

We introduce a scheme that domain experts can use to manually compare the relative quality of different automatically produced summaries (and the underlying summarisation methods). The proposed scheme consists of a 10-item questionnaire measuring the expert's opinion of the readability of the summary, and whether it has relevant content. The scheme has been developed based on experiences from our preliminary study on evaluating clinical summarisation methods [41], resulting in a more streamlined tool that is easier to use consistently.

However, such manual evaluation requires human input and is thus impractical to use during summarisation method development, where rapid feedback is required when testing different method variations. Therefore, we also use the ROUGE toolkit for performing automated evaluation. We also seek to establish whether the automated ROUGE-based evaluation can be used in place of human evaluation in the context of clinical summarisation. This meta-evaluation is performed through rank correlation coefficient analysis between the manual and automated evaluation. Finally, we aim to establish which summarisation method performs best in the task of clinical summarisation.

The results from Experiment 1 show that there is a correlation between how the manual and automatic evaluations rank the different summarisation methods. This indicates that using an automated ROUGE-based evaluation set-up is feasible. Further, it shows that the automatic evaluation scores, with the applied evaluation set-up, are reliable for determining what summarisation method performs best. The observation that the manual evaluators preferred the Composite method to the Oracle method indicates that the greedy search strategy, based on the original discharge summary, does not necessarily produce the best possible extraction-based (discharge) summary.

The results from Experiment 2 show that the methods Composite, Case-Based and Translate all work better than the basic baseline methods (p
                     <0.05) (not taking into consideration the Oracle method), whereas the Centrality baseline fails to outperform even the Random baseline with this data. Composite, which consists of combined features from RepeatedSentences, Case-Based and Translate consistently has the highest ROUGE performances. However, the difference between these and the next best methods is not statistically significant against all ROUGE measures following the Bonferroni correction.

When producing the summaries, the Composite method combines the following basic principles:
                        
                           •
                           The importance of a sentence depends on how many times the same or similar information has been mentioned throughout a care episode (RepeatedSentences).

By looking at discharge summaries of other similar care episodes, one can assess the importance of a sentence based on whether or not the same or similar information has been written in these summaries (Case-Based).

If, using a WSM-based translation system, a sentence (its vector representation) can be ‘translated’ into a vector representation that is similar to how this same sentence would look in the translated word space, it should be considered for inclusion in the final summary (Translate).

Clustering sentences into topics that span across clinical notes in a care episode allows for the removal of redundancy.


                     Centrality is evaluated as being one of the lowest-scoring summarisation methods. Given its broad usage in text summarisation for other domains, this deserves a closer analysis. We asked the evaluators to comment on the structure and content of the summaries that this method produced using open-ended questions. The three questions were:
                        
                           1.
                           What important information is missing from the summary?

What information in the summary is unnecessary?

How logical is the structure of the summary?

The following sums up what they wrote based on the analysis of five summaries:
                        
                           •
                           
                              Disorganised structure of text, confusing, illogical order or structure.
                           


                              The end is missing.
                           


                              Cannot get an overall view of patients’ care episode.
                           


                              Important information is missing.
                           


                              Information is diffuse and fragmented.
                           


                              Sentences are not connected.
                           


                              Too many details about unimportant stuff.
                           

This seems to indicate that the most ‘central’ information, independent of when it was written, is not a good indicator of the information that clinicians want to have in the discharge summary. This method did not include sentence topic clustering, which was used in several of the other methods. This further supports the importance of such topic clustering despite the relatively poor performance of RepeatedSentences. In future work, other variations and implementations of centrality-based methods should be evaluated, e.g. through the use of the MEAD system [57], similarly to how it is done in [37].


                     LastSentences performs relatively poorly in comparison to many of the other summarisation methods. This is an interesting observation in that it suggests that reading only the latest written information or note(s) is suboptimal when the task is to write a discharge summary. It also suggests that there are reasons to believe that it is beneficial for clinicians to use text summarisation systems in their work, e.g. to assist in highlighting relevant information documented earlier in a care episode.

Even with our rather coarse-grained manual evaluation, when applied to a limited number of care episodes, a high correlation is seen with the automatic evaluation. Hence, this automatic evaluation approach can be used to rank the different summarisation methods in order of effectiveness. And since such manual evaluation is not affordable every time a summarisation method has been modified, or when a new method is developed, it should be possible to resort to this automatic evaluation during the method development process.

This study raises questions about the usability, reliability and usefulness of such (imperfect) automatic summarisation systems, particularly when used at the point of care. This is difficult to assess based on the utilised evaluation approach and scores achieved here. The question is: what does it actually mean to have a system that is able to generate textual summaries containing parts of or all (i.e. perfect evaluation score) the content one would expect to find in a manually-created discharge summary? One answer is that it would likely provide a good starting point for a clinician who is about to write the actual discharge summary. It is also likely that the same automatically generated summary would provide an indicative overview of the information having been documented during the corresponding care episode, from a clinician's perspective. However, patient safety issues must be considered before this kind of system is taken into practice. On the one hand, it is important that the most relevant information needed for safe care provision is assured in automatically generated summaries. On the other hand, as long as clinicians treat the generated summaries as an indicative summary, this could be a helpful feature in EHR systems, particularly in situations where time is of the essence. Future research including more user-centred evaluation is required to answer this question in more detail.

A weakness of this study is the validity of the evaluation. The utilised manual evaluation scheme is quite coarse-grained in that it contains ten criteria items, and the rating is done on the level of ‘yes’ or ‘no’. However, in the previously mentioned pre-study [41], a 30-item evaluation scheme was tested, using a four-point scale, but was found to be too detailed and time-consuming to use.

The automatic evaluation is performed using the original discharge summary as a gold standard summary, despite the fact that these discharge summaries are not themselves produced in a purely extractive way. This is reflected in the fact that the ROUGE scores are arguably quite low compared to scores reported in various other studies on text summarisation (see e.g. [40]). The scores achieved by Oracle indicate the maximum ROUGE-N2 scores achievable with an extractive-based summarisation system for our data. However, it is encouraging to see that there is a correlation in terms of relative goodness between manual and automatic evaluation, both here and in the pre-study [41], which is promising for future work in this direction.

An alternative evaluation approach would be to manually develop gold standard summaries in a purely extractive way for a set of care episodes, replacing the original discharge summaries as gold standard summaries in the automatic evaluation. This approach was not pursued here, as it is more resource-intensive, but it would possibly give us more reliable results. Another approach would be to use the summarisation system in a (simulated) clinical setting with clinicians as users. Such an evaluation approach is referred to as extrinsic evaluation, and could potentially shed light on the impact on documentation speed and quality, as well as on health care quality and patient outcomes. This type of evaluation could also potentially provide directions for future work on improving the summarisation system.

Currently it is difficult to assess the usefulness and potential impact that this type of summarisation system could have in a real clinical setting. On the one hand, it could be a convenient tool for clinicians in terms of providing an indicative textual overview of ongoing care episodes, for example. On the other hand, the possible imperfection of the information presented in the generated summaries must be considered in relation to potential patient safety issues. Future work should focus more on extrinsic evaluation by evaluating how the use of automatic text summarisation systems in a clinical setting will impact on documentation speed and quality, as well as health care quality and patient outcomes. Here we believe that a more user-guided summarisation system is needed, enabling real-time incremental summary generation, similar to the methods proposed in [58]. This would mean that the computer-generated summary, or the sentences that it suggests for inclusion in the final summary, are calculated based on analysing what content the user has already written in (or imported into) the summary.

@&#CONCLUSION@&#

This work on the automated summarisation of free text in care episodes introduces and evaluates both a framework for evaluating summarisation methods, as well as novel methods for performing the summarisation. Most of the presented summarisation methods rely on statistical information derived from a large corpus of clinical text, this includes various WSMs. The best performing summarisation methods, according to the applied evaluation, are Composite, Case-Based and Translate. The ROUGE-based evaluation measures are shown to correlate highly with the manual evaluation in terms of relative ranking. Based on these results, we believe that the explored sentence features, especially those in the Composite method, provide useful directions on how to approach this summarisation task in a resource-lean fashion. Further studies are needed to assess the applicability of such methods in real-world clinical settings.

The authors declare that they have no conflicts of interest.

@&#ACKNOWLEDGEMENTS@&#

This study was partly supported by the Research Council of Norway through the EviCare project (project no. 193022), Turku University Hospital (EVO 2014), and the Academy of Finland (project no. 140323). The study is part of the research projects of the IKITIK consortium (http://www.ikitik.fi). We would like to thank the manual evaluators for their contributions. We would also like to thank Filip Ginter for assisting us in the work on Word2vec. Finally, we would like to thank the reviewers for their insightful comments. This paper has been proofread by Lingsoft Language Services Oy, and this was financed by the Department of Computer and Information Science, Norwegian University of Science and Technology.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.artmed.2016.01.003.

The following are the supplementary data to this article: 
                        
                           
                        
                     
                  

@&#REFERENCES@&#

