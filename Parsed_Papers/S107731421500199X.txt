@&#MAIN-TITLE@&#Interdigital palm region for biometric identification


@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           First experimental study on the ridge pattern distribution in the ‘interdigital palm region’ for biometrics.


                        
                        
                           
                           Novel classification methodology of palms according to five classes.


                        
                        
                           
                           Study of complementarity of the interdigital and traditional palm regions.


                        
                        
                           
                           Evaluation of personal recognition using interdigital region reaching EER = 0.01% (on database on 416 subjects).


                        
                        
                           
                           Interdigital palm region image database from 416 subjects in this paper is made publicly available.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Biometrics

Forensics

Palmprint

Indexing

Taxonomy

Person recognition

@&#ABSTRACT@&#


               
               
                  The interdigital palm region represents about 30% of the palm area and is inherently acquired during palmprint imaging, nevertheless it has not yet attracted any noticeable attention in biometrics research. This paper investigates the ridge pattern characteristics of the interdigital palm region for its usage in biometric identification. An anatomical study of the interdigital area is initially carried out, leading to the establishment of five categories according to the distribution of the singularities and three regions of interest for biometrics. With the identified regions, our study analyzes the matching performance of the interdigital palm biometrics and its combination with the conventional palmprint matching approaches and presents comparative experimental results using four competing feature extraction methods. This study has been carried out with two publicly available databases. The first one consists of 2,080 images of 416 subjects acquired with a touchless low-cost imaging device focused on acquiring the interdigital palm area. The second database is the publicly available BiosecurID hand database which consists of 3,200 images from 400 users. The experimental results presented in this paper suggest that features from the interdigital palm region can be used to achieve competitive performances as well as offer significant improvements for conventional palmprint recognition.
               
            

@&#INTRODUCTION@&#

THE interdigital region is the area of the palm between the base line of the fingers and the transverse creases (distal and proximal ones) [1,2]. At the fetal stage, the skin in that area is stressed in different directions to allow the formation of the fingerwebs and the beginning of the fingers. It yields an area rich in random, unique and stable structures for each individual.

These structures, creases, wrinkles, minutiae and singularities can be found all around the hand palm. Creases and wrinkles or palm lines have been extensively studied by the biometric research community for personal identification. They can be conveniently acquired with low cost devices based on CCD/CMOS sensors that are able to provide low-resolution images around 150 dpi. As these structures are concentrated around the palm center, it is common to extract an invariant area around the palm center that contains the most prominent creases on the palm: the radial longitudinal transverse crease, the distal transverse crease and the proximal transverse crease as well as several secondary lines [3,4]. The availability of several free public databases may be one of the reasons why most research and commercial schemes are based on these palm creases. As seen in Fig. 1
                     , the use of central area is common as the region of interest, however it misses the interdigital region which is reasonable due to the relative lack of creases and wrinkles in this area.

Singular points, orientations and minutiae on the hand palm provide higher discriminative ability [3,5–7] and require a resolution over 300 dpi. Usual devices to acquire high resolution palm images are AFIS/APIS devices (e.g. Safran Morpho and Pappilon). Disadvantage of these acquisition devices are the missing palm regions of the acquired image due to the optical properties of the system and the anatomic characteristics of the palm. As can be seen at Fig. 1, the upper interdigital area is missed in these images because of the curvature of the metacarpal bones. Nevertheless, these devices are mainly used by forensic experts because of the accuracy of these structures for personal identification while scarcely being used by the biometric community due to the difficulties in obtaining automatically a reliable and stable singularities pattern because of the wrinkles and creases.

Problems with the automatic acquisition of a stable ridge-line pattern in the hand are moderate in the interdigital area because of the previously mentioned lack of creases and wrinkles. Unfortunately part of this area is lost in the high resolution images, see Fig. 1. Nevertheless we think a study of the features of this area should be added to actual palm based biometric devices because it could mean a step forward in performance of these biometric devices.

In this paper we propose the use of the interdigital area for biometric person recognition. As it contains a complex structure of singular points and ridge orientations, an exhaustive analysis of the interdigital area has been carried out.

The key contributions from this paper can be summarized as follows:

                        
                           (i)
                           This paper investigates and develops an anatomic taxonomy on the interdigital palm region ridge patterns for the biometrics applications. Our investigation suggests that the distribution of the singularities in the interdigital region can be used to classify palms from different subjects into a few classes (Section 3.1). We infer a model of the singularities distribution (Section 3.2) which leads to selection of effective regions of interest, based on the spatial distribution of the singularities in the interdigital area, that can maximize its performance for personal identification (Section 3.3).

After adapting the most relevant features to the interdigital area (Section 4), this work presents a study of the discriminative ability of the interdigital area (Section 5.1) and its complementarity with the commonly employed central palm for the personal authentication (Section 5.2). Our experimental evaluation is carried out using four competing feature extraction methods and two publicly available databases that comprises more than 800 users. The experimental results demonstrate the comparative discriminative ability of the interdigital region and its effectiveness in improving (up to 50% of improvement in EER) the performance of the conventional palmprint recognition. This work does not propose the interdigital area as an alternative to replace traditional palmprint approaches but as a complement to improve already existing ones.

This study required a database acquired with device that can acquire the interdigital area with a resolution that allows ridge singularities to be detected. This device was developed by us and used to acquire a database of palm images without skin distortion from 416 subjects. Furthermore, the database developed and employed in this study is made publicly available for further research in forensics and civilian biometric applications at www.gpds.ulpgc.es.

The rest of the paper is organized as follows: Section 2 describes the self-built acquisition device and the databases used; Section 3 deals with the taxonomy of the interdigital region based on its ridge pattern; feature extraction methods and experiments are reported in Sections 4 and 5, respectively. Finally Section 6 draws some conclusions where the interdigital trait results are compared with the state of the art of high and low resolution palm biometrics.

The device to acquire the interdigital region to study the ridge-pattern is made with a webcam (Logitech C600) and a compact fluorescent lamp (CFL) inside a case. The light incident from one side increases the contrast between creases and ridges for greater visibility. The case contains an opening hole of 8 ×4 cm to capture the whole interdigital region and three guiding pegs ensure the correct positioning of the hand during recognition/identification.

When the user places the hand over the guiding pegs, his/her interdigital region lies above the opening of the case and its image can be acquired without skin deformation, see Fig. 2.
                        
                     

The webcam used contains 
                           
                              N
                              ×
                              M
                              =
                              1600
                              ×
                              1200
                           
                         pixels. Its sensor size and focal length are 
                           
                              L
                              ×
                              P
                              =
                              0.4536
                              ×
                              0.3416
                              
                              
                                 cm
                                 
                              
                           
                        and 
                           
                              f
                              =
                              0.44
                              
                              
                                 cm
                                 
                              
                              ,
                           
                        respectively. Therefore, its horizontal and vertical angle of view are 
                           
                              
                                 α
                                 h
                              
                              =
                              2
                              ·
                              atan
                              
                                 (
                                 
                                    L
                                    /
                                    2
                                    f
                                 
                                 )
                              
                              =
                              54
                              .
                              
                                 53
                                 ∘
                              
                           
                         and 
                           
                              
                                 α
                                 v
                              
                              =
                              2
                              ·
                              atan
                              
                                 (
                                 
                                    P
                                    /
                                    2
                                    f
                                 
                                 )
                              
                              =
                              42
                              .
                              
                                 43
                                 ∘
                              
                           
                        , respectively. The webcam was located 
                           
                              h
                              =
                              8
                              
                              
                                 cm
                                 
                              
                           
                        below the vertical of the open hole, so it covers an area of 
                           
                              2
                              ·
                              h
                              ·
                              tan
                              
                                 (
                                 
                                    
                                       α
                                       h
                                    
                                    /
                                    2
                                 
                                 )
                              
                              ×
                              2
                              ·
                              h
                              ·
                              tan
                              
                                 (
                                 
                                    
                                       α
                                       v
                                    
                                    /
                                    2
                                 
                                 )
                              
                              =
                              8.2
                              ×
                              6.2
                           
                         cm which is slightly greater than the opening. In this case the resolution obtained is 468 dpi in the horizontal direction and 464 dpi in the vertical direction. The proposed low cost device captures the whole interdigital area (the region from the transverse creases to the base of the fingers) and allows the singularities and ridge orientations to be studied.

The database acquired with the described device is made up of 2580 images from 416 people. The age of the subjects ranges from 15 to 94 with 234 males and 182 females. The first 100 users were acquired in two sessions separated one month while the remaining 316 users provided images in only one session. Each of the sessions contains 5 images. The guiding pegs help to reduce the within class rotation and translation variability during the acquisition while ensuring the visibility of the region under the ring, middle and index fingers for all users. The quality of acquired images mainly varies due to external factors such as nature of work (manual laborers and office workers) practiced by subjects. Each user was manually classified into three quality levels according to the visibility of the ridge pattern, using good (21%) meaning clearly visible ridges and creases, medium (54%), meaning partial visibility and poor (25%) meaning complicates ridge or crease distinctions.

BiosecurID [8] is a public available database which is used to support the performance analysis done in this paper. The hand section of the BiosecurID multimodal database is made up of 400 people with 16 images per user acquired in 4 different sessions. The images were collected using a scanner at 2550 × 3510 pixels with 48 bits color depth and include real distortions such as rings and close fingers. The first two sessions of the database were only used with the aim of limiting comparisons with the GPDSInterd database.

Regarding image quality, the scanner light of the BiosecurID database is uniform and orthogonal to the hand surface which is optimum for hand-shape or palm print biometrics but not for ridge pattern analysis at singularity level. Additionally the skin is deformed by the contact of the hand on the scanner surface. These considerations pose a more challenging scenario.

The interdigital area is dense in ridge traits and clear due to relative lack of lines and creases. A detailed study of this region reveals similar structures at the singularity level across different people. Understanding of these structures is an important step for the design of biometric schemes based on this trait, and can also advance other challenges on biometric technology such as the generation of synthetic data [9], forensic applications or indexing [10] among others.

The pattern on the interdigital region of the palmprint presents similarities with the fingerprint pattern [7]: (i) they are also created during gestation and remain constant during our life; (ii) the most common singularities can be categorized in the same manner as deltas or cores; (iii) the number and location of such singularities is different for each person and; (iv) their location determines the aspect and orientation of the ridge pattern [1]. The position of these singularities can be used to model the ridge pattern and such models are the key to improving the performance on low quality images [11].
                        
                     

The singularities are defined as regions where the ridge pattern display significant changes of orientation (cores and deltas are the most extended). A precise analysis of the number and position of the singularities in the interdigital region was carried out by manually locating the cores and deltas on the interdigital region of the 416 people from the GPDSInterd Database. As expected, the number and spatial distribution of singularities depend on the person.

A detailed analysis of the singularities distribution reveals that the interdigital palm regions can be classified into five classes (see Fig. 3) as follows:

                           
                              •
                              
                                 Class 1 (3 deltas and 1 core): Characterized by the presence of one delta near the base of each finger and a core between the ring and middle fingers. It is quite remarkable that no image in the database shows a structure with three deltas and one core between the middle and index fingers instead of the ring.


                                 Class 2 (2 deltas): This class also displays one delta in the base of the index and middle fingers but none in the ring finger. There are no cores on this class probably because of the absence of the delta in the base of the index finger.


                                 Class 3 (4 deltas and 2 cores): This class can be considered the most complex structure, where a pattern contains four deltas and two cores. There are four deltas around the base of each finger. The pattern shows one core between the ring and middle fingers (as in class 1) and a second core located between the ring and middle deltas.


                                 Class 4 (3 deltas): This class presents a pattern with three deltas, one for each finger. The deltas are located once again around the base of each finger and there are no cores in this pattern.


                                 Class 5 (Other distributions): Although the above taxonomy covers most cases, this class is included for the possibility of other singularity distributions that do not appear in our database due to its limited size. In fact, there is one subject in the database who does not belong to any of the proposed classes in this paper (0.26% in our database).


                        Table 1
                         shows the percentages of occurrence for each of the five classes. The table does not include 27 discarded interdigital regions because their poor pattern quality do not allow an accurate classification.

The number of singularities in the interdigital region ranges from two to six and the spatial distribution tends to cluster in three regions (see Fig. 4). The delta singularities are distributed near the base of each finger while the cores reflect a more dispersed location but are also distributed below the fingerwebs between delta areas which correspond to the natural ridge-line pattern of skin ending.

In a similar way as [12], the spatial distribution of the three observed clusters is estimated by a mixture of Gaussian distributions [13]. Let (x, y) be the locations of all the singularities on our database. Its spatial distribution is estimated by a Gaussian Mixture Model:

                           
                              (1)
                              
                                 
                                    p
                                    
                                       (
                                       
                                          x
                                          ,
                                          y
                                          
                                             |
                                             Θ
                                          
                                       
                                       )
                                    
                                    =
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       M
                                    
                                    
                                       α
                                       j
                                    
                                    N
                                    
                                       (
                                       
                                          x
                                          ,
                                          y
                                          |
                                          
                                             μ
                                             j
                                          
                                          ,
                                          
                                             c
                                             j
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        being p(x, y|Θ) the approximated probability distribution and Θ the parameters mean μj
                         and variance cj
                         of the Gaussian 
                           
                              j
                              =
                              1
                              ,
                              …
                              M
                           
                         belonging to the mixture that models all singularity positioning. αj
                         is the mixing probability for each Gaussian. Our experiments suggest that M=3 produces realistic distributions gathered in three disjoint regions which corresponds with the observed spatial distributions that suggest the interdigital region can be divided into three bounded regions corresponding to the skin area below each of the finger bases (see Fig. 5).

A first interdigital biometric device design would select a unique region of interest (ROI) that covers the entire interdigital area. Concisely, in the case of the GPDSInterd database as a rectangle of 1200 × 480 pixels which includes the pattern from the first up to the third metacarpal bone, as seen in Fig. 6.
                     

Nevertheless, it is well-known that an accurate selection of the ROI can significantly improve performance. From this perspective, the ROI can be redesigned because the ridge pattern is strongly influenced by the singularities. It is reasonable to propose three smart ROIs below the base of each finger considering that the patterns on such regions are quasi-independent. We cannot assume that they are fully independent because it is well known that the ridge pattern is influenced in first place by the nearest singularities but also by closer ones.

Concisely, the three proposed Smart ROIs consist of three rectangular areas around each finger's base. The rectangle is chosen because most feature extraction methods in palmprint recognition state of the art use this shape most often [3,4]. Such rectangles are extended to cover most of the ridges orientation around the singularities and include a 10% maximum overlap of their area. Fig. 6 draws the three Smart ROIs over an interdigital image. Given that hand positioning is controlled by guiding pegs, the three Smart ROIs are determined by fixed rectangles of size 581 × 421 pixels for the index area, 451 × 541 pixels for the middle area and 363×393 pixels for the ring area, see Fig. 6. Note that these regions are the same for all the GPDSInterd database.

Location of the ROI is generalized to other databases by the following geometrical hand relations: the center of each Smart ROI is located by extending each of the finger axis lengths by 15% (we used the finger axis extraction proposed in [14]). The three Smart ROIs are defined as three rectangles orthogonal to the finger axis and sizes equals to: 351 × 391 pixels for the index area, 325×391 pixels for the middle area and 325 × 391 pixels for the ring area (see Fig. 7
                        ) if the image resolution is 150dpi. Note that the overlap between all three Smart ROIs is not fixed (for the generalized extraction) and it depends on the shape of the hand (length of the fingers, position of the valleys, etc...).

This section evaluates four state-of-the-art feature extraction methods which exploit both low resolution and high resolution features in order to study its individual performance and their combination. The methods under study are: Band Limited Phase Only Correlation (BLPOC) [15,16], Orthogonal Line Ordinal Features (OLOF) [17], Scale Invariant Feature Transform (SIFT) [18] and Dense Scale Invariant Feature Transform (DSIFT) [19].

Features based on the correlation of the phase have been evaluate to measure the similarity between two interdigital images. These have already been proposed for low resolution palmprint [15,16]. Consider two images, f(x, y) and g(x, y), with size M × N pixels and F(u, v) and G(u, v) as their 2D Discrete Fourier Transform. The Phase-Only-Correlation (POC) considers the phase of the 2D DFT of the images as:

                           
                              (2)
                              
                                 
                                    Z
                                    
                                       (
                                       
                                          u
                                          ,
                                          v
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          G
                                          
                                             (
                                             
                                                u
                                                ,
                                                v
                                             
                                             )
                                          
                                          
                                             F
                                             *
                                          
                                          
                                             (
                                             
                                                u
                                                ,
                                                v
                                             
                                             )
                                          
                                       
                                       
                                          |
                                          
                                             G
                                             
                                                (
                                                
                                                   u
                                                   ,
                                                   v
                                                
                                                )
                                             
                                             
                                                F
                                                *
                                             
                                             
                                                (
                                                
                                                   u
                                                   ,
                                                   v
                                                
                                                )
                                             
                                          
                                          |
                                       
                                    
                                    =
                                    
                                       e
                                       
                                          j
                                          
                                             {
                                             
                                                
                                                   θ
                                                   G
                                                
                                                
                                                   (
                                                   
                                                      u
                                                      ,
                                                      v
                                                   
                                                   )
                                                
                                                −
                                                
                                                   θ
                                                   F
                                                
                                                
                                                   (
                                                   
                                                      u
                                                      .
                                                      v
                                                   
                                                   )
                                                
                                             
                                             }
                                          
                                       
                                    
                                 
                              
                           
                        where Z(u, v) is the cross-phase spectrum, 
                           
                              u
                              =
                              −
                              M
                              /
                              2
                              ,
                              …
                              ,
                              
                              M
                              /
                              2
                           
                        , 
                           
                              v
                              =
                              −
                              N
                              /
                              2
                              ,
                              …
                              ,
                              
                              N
                              /
                              2
                           
                         and θG
                        (u, v) and θF
                        (u, v) are phase components. The POC function p(m, n) is defined as the inverse DFT of Z(u, v) as:

                           
                              (3)
                              
                                 
                                    p
                                    
                                       (
                                       
                                          m
                                          ,
                                          n
                                       
                                       )
                                    
                                    =
                                    
                                       1
                                       
                                          M
                                          N
                                       
                                    
                                    
                                       ∑
                                       
                                          u
                                          =
                                          −
                                          
                                             M
                                             2
                                          
                                       
                                       
                                          M
                                          2
                                       
                                    
                                    
                                       ∑
                                       
                                          v
                                          =
                                          −
                                          
                                             N
                                             2
                                          
                                       
                                       
                                          N
                                          2
                                       
                                    
                                    Z
                                    
                                       (
                                       
                                          u
                                          ,
                                          v
                                       
                                       )
                                    
                                    
                                       e
                                       
                                          j
                                          2
                                          π
                                          
                                             (
                                             
                                                
                                                   
                                                      m
                                                      u
                                                   
                                                   M
                                                
                                                +
                                                
                                                   
                                                      n
                                                      v
                                                   
                                                   N
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                     

BLPOC [16] is an improvement of POC similarity measure that focuses on the low frequency components filtering out noisy higher frequencies. Concisely, BLPOC limits the ranges of the inverse DFT of the phase correlation function in the frequency domain. Assume that the ranges of the inherent frequency band of image texture are given by 
                           
                              u
                              =
                              −
                              
                                 U
                                 ′
                              
                              ,
                              …
                              ,
                              U
                              
                                 
                                 ′
                              
                           
                         and 
                           
                              v
                              =
                              −
                              
                                 V
                                 ′
                              
                              ,
                              …
                              ,
                              V
                           
                        , where 0 ≤ U′ ≤ M/2, 0 ≤ V′ ≤ N/2. Thus, the effective size of the spectrum is given by 
                           
                              
                                 M
                                 ′
                              
                              =
                              2
                              
                                 U
                                 ′
                              
                              +
                              1
                           
                         and 
                           
                              
                                 N
                                 ′
                              
                              =
                              2
                              
                                 V
                                 ′
                              
                              +
                              1
                           
                        . The BLPOC function is defined as:

                           
                              (4)
                              
                                 
                                    
                                       p
                                       
                                          U
                                          
                                             
                                             ′
                                          
                                          V
                                          
                                             
                                             ′
                                          
                                       
                                    
                                    
                                       (
                                       
                                          m
                                          ,
                                          n
                                       
                                       )
                                    
                                    =
                                    
                                       1
                                       
                                          M
                                          
                                             
                                             ′
                                          
                                          N
                                          
                                             
                                             ′
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          u
                                          =
                                          −
                                          U
                                          
                                             
                                             ′
                                          
                                       
                                       
                                          U
                                          
                                             
                                             ′
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          v
                                          =
                                          −
                                          V
                                          
                                             
                                             ′
                                          
                                       
                                       
                                          V
                                          
                                             
                                             ′
                                          
                                       
                                    
                                    Z
                                    
                                       (
                                       
                                          u
                                          ,
                                          v
                                       
                                       )
                                    
                                    
                                       e
                                       
                                          j
                                          2
                                          π
                                          
                                             (
                                             
                                                
                                                   
                                                      m
                                                      u
                                                   
                                                   
                                                      M
                                                      
                                                         
                                                         ′
                                                      
                                                   
                                                
                                                +
                                                
                                                   
                                                      n
                                                      v
                                                   
                                                   
                                                      N
                                                      
                                                         
                                                         ′
                                                      
                                                   
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                     

In the case of genuine matching, the BLPOC provides a sharper correlation peak than the original POC. The BLPOC is also more robust against the noise and is better at distinguishing between genuine matching from impostor matching.

The Orthogonal Line Ordinal Features (OLOF) method was originally introduced in [17] for palmprint texture feature extraction. This method proposes using two Gaussian filters, specifically as 
                           
                              O
                              F
                              (
                              θ
                              )
                              =
                              g
                              (
                              
                                 x
                                 ,
                                 y
                                 ,
                                 θ
                              
                              )
                              −
                              g
                              (
                              
                                 x
                                 ,
                                 y
                                 ,
                                 θ
                                 +
                                 π
                                 /
                                 2
                              
                              )
                           
                        . Each interdigital palm image is filtered using three angles OF(0), OF(π/6), and OF(π/3). In order to improve the robustness against brightness variations, the filters OF(θ), are turned to have a zero average with the aim of improving the robustness against brightness variations. The pattern features are obtained by filtering the images on the three ordinal filters and binarizing the results with a zero threshold. The matching distance between the query image features matrix and the gallery image feature matrix is computed using the mean of the three normalized Hamming distances.

Local descriptors have emerged recently as a way to improve feature extraction methods in the presence of distortions such as scale, rotation, translation and occlusion. The local descriptors applied in this paper are from the Scale Invariant Feature Transform (SIFT) algorithm. The SIFT was originally proposed in [18] and it was studied for contactless palmprint biometrics in [20]. The feature extraction method employed is divided into five steps:

                           
                              (i)
                              Scale-space extrema detection: The Difference of Gaussian function is calculated in order to identify potential regions that show characteristics invariant to scale and rotation. The interdigital palm image is transformed to the filtered image 
                                    
                                       L
                                       (
                                       
                                          x
                                          ,
                                          y
                                          ,
                                          σ
                                       
                                       )
                                       =
                                       G
                                       (
                                       
                                          x
                                          ,
                                          y
                                          ,
                                          σ
                                       
                                       )
                                       *
                                       I
                                       (
                                       
                                          x
                                          ,
                                          y
                                       
                                       )
                                    
                                 , where * corresponds to the convolution operator, I(x, y) is the input image and G(x, y, σ) is a Gaussian function with bandwidth σ. The Difference-of-Gaussian function is defined as:
                                    
                                       (5)
                                       
                                          
                                             
                                                
                                                   
                                                      D
                                                      
                                                         (
                                                         
                                                            x
                                                            ,
                                                            y
                                                            ,
                                                            σ
                                                         
                                                         )
                                                      
                                                   
                                                
                                                
                                                   =
                                                
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            G
                                                            
                                                               (
                                                               
                                                                  x
                                                                  ,
                                                                  y
                                                                  ,
                                                                  k
                                                                  σ
                                                               
                                                               )
                                                            
                                                            −
                                                            G
                                                            
                                                               (
                                                               
                                                                  x
                                                                  ,
                                                                  y
                                                                  ,
                                                                  σ
                                                               
                                                               )
                                                            
                                                         
                                                         )
                                                      
                                                      *
                                                      I
                                                      
                                                         (
                                                         
                                                            x
                                                            ,
                                                            y
                                                         
                                                         )
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                
                                                   =
                                                
                                                
                                                   
                                                      
                                                      L
                                                      
                                                         (
                                                         
                                                            x
                                                            ,
                                                            y
                                                            ,
                                                            k
                                                            σ
                                                         
                                                         )
                                                      
                                                      −
                                                      L
                                                      
                                                         (
                                                         
                                                            x
                                                            ,
                                                            y
                                                            ,
                                                            σ
                                                         
                                                         )
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

Keypoint localization: The local maxima and minima of D(x, y, σ) is evaluated by comparing it to its neighbors. A candidate point is selected only if it is larger or smaller than all of these neighbors. The candidate points with lowest contrast are rejected after performing a detailed fit to the nearby data for location, scale, and ratio of principal curvatures in the pattern. Interpolation is done using the quadratic Taylor expansion of the Difference-of-Gaussian scale-space function D(x, y, σ) with the candidate keypointat the origin as:
                                    
                                       (6)
                                       
                                          
                                             D
                                             
                                                (
                                                x
                                                )
                                             
                                             =
                                             D
                                             +
                                             
                                                
                                                   ∂
                                                   
                                                      D
                                                      T
                                                   
                                                
                                                
                                                   ∂
                                                   x
                                                
                                             
                                             +
                                             
                                                1
                                                2
                                             
                                             
                                                x
                                                T
                                             
                                             
                                                
                                                   
                                                      ∂
                                                      2
                                                   
                                                   
                                                      D
                                                      T
                                                   
                                                
                                                
                                                   ∂
                                                   
                                                      x
                                                      2
                                                   
                                                
                                             
                                             x
                                          
                                       
                                    
                                 where D and its derivatives are evaluated at the candidate keypoint and 
                                    
                                       x
                                       =
                                       (
                                       x
                                       ,
                                       y
                                       ,
                                       σ
                                    
                                 ) is the offset from this point.

Orientation assignment: Each keypoint is represented by 16 orientations obtained from the local image gradient directions. For a filtered image sample L(x, y) at scale σ, the gradient magnitude, and orientation, are processed using pixel differences:

Keypoint descriptor: Local gradients are measured at the selected scale around each keypoint. The keypoint will be defined by a descriptors vector 
                                    
                                       
                                          {
                                          
                                             d
                                             
                                                i
                                                
                                             
                                          
                                          }
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       M
                                    
                                  with M descriptors.

Matching: The score matching between the gallery descriptors 
                                    
                                       
                                          {
                                          
                                             d
                                             i
                                             g
                                          
                                          }
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       M
                                    
                                  and the query descriptors 
                                    
                                       
                                          {
                                          
                                             d
                                             i
                                             q
                                          
                                          }
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       N
                                    
                                  is defined by the Euclidean distances between them. Once the distances for all keypoints inside the descriptors are evaluated, we then consider if there is a match between keypoints when the distance between the descriptors is less than 1 [18].

The SIFT flow algorithm (Dense-SIFT) was proposed for the alignment of images inside a large image corpus containing a variety of representations [19]. In this paper we adapted the method for person recognition with interdigital palm imaging inspired by the HOG algorithm proposed in [21].

The differences between the Dense-SIFT and the original SIFT algorithm lies in the location where the features are extracted, which is also referred to as the keypoint. Unlike the keypoint localization based on Difference-of-Gaussian proposed for SIFT, the Dense-SIFT algorithm estimates the descriptors uniformly for every region in the image. Therefore the first three steps of the SIFT algorithm are substituted by a uniform distribution of the keypoints along the entire image. For every region in an image (the center of each region is 6 pixels equal spaced) the algorithm divides its neighborhood into a 4×4 cell array, quantizes the orientation into 8 bins in each patch, and generates a 4×4×8=128-dimensional vector as the SIFT descriptor. Therefore the distribution of the descriptors is uniform on the entire interdigital palm image and there are no regions with a greater number of interest points as in SIFT. The classification employed in this paper is the same as in the SIFT algorithm and the score represents the number of matches between descriptors from the query and gallery images.

@&#EXPERIMENTS AND RESULTS@&#

Experimentation is conducted on two databases: the GPDSInterd and the BiosecurID databases. They are designed to answer the following questions: What is the interdigital biometric performance? Do Smart ROIs improve the performance of the Unique ROI? Does the skin deformation due to sensor contact reduce the performance? and Does the combination of the interdigital and palm biometrics improve the palm performance?

The first and second questions will be researched by experimenting with the GPDSinterd database which is a database acquired by our self-developed device (see Section 2.1). This database is ideal when exploring the limits of the feature extraction methods applied to this palm region. The third and fourth questions will be studied with the BiosecurID database. This database comprises whole hand images and offers the opportunity to analyze the complementarity between traditional palmprint approaches based on the center of the palm and the interdigital region. In both cases, the experimental results will be given in the unisesssion and multisession scenarios as follows:

                        
                           -
                           
                              Unisession: in this experiment, the genuine scores are obtained by comparing the images from the first session using a leave-one-out methodology. Each image is used as a query sample and the rest as a gallery set. The scores are combined with the max rule (one score for each gallery sample) and this process is repeated for all images from the first session.


                              Multisession: in this case we compare images obtained from different sessions. The gallery set uses the images from the first session while the images of the second session are used as query samples. The scores are also combined with the max rule (one score for each gallery sample) and the process is repeated for all images from the second session.

Therefore, we obtain 5 × 416 = 2,080 genuine scores and 5 × 415 × 416 = 863,200 impostor scores for the GPDSInterd database and 4 × 400 = 1,600 genuine scores and 4 × 399 × 400 = 638,400 impostor scores for the BiosecurID database. The impostor scores are obtained using the images from the first session of all the users in the database.

An analysis of the interdigital performance and the influence of the Smart ROIs follows. Table 2
                        
                         shows the results obtained with the four features in the unisession and multisession scenarios using the Unique and Smart ROIs. To compute the classification score of the three smart ROIs, the three regions are matched separately and combined at score level by averaging the three classification scores.

The results show the effectiveness of the smart ROIs selection which clearly make a significant improvement on the Unique ROI performance in all the studied cases. The results suggest that such improvement is related to a more efficient matching instead of an increment in the amount of information since the three Smart ROIs together cover an area similar to the one in the Unique ROI.

With regards to features, the Dense-SIFT outperforms the other approaches such as SIFT and OLOF while the BLPOC is the least efficient. Regarding the multisession experiment, there is a loss of performance being the Smart ROIs more robust than the Unique ROI. Due to its superior performance, the rest of the experiments are developed using the Smart ROI extraction.

Image quality has a great impact on the performance of different approaches. Degradation in quality does not affect pattern features in the same way as in the interdigital palm region. In order to better understand performance of the interdigital palm features studied in this paper, we manually classified all of the images from the GPDSInterd database into three quality levels, according to the terminology proposed in NIST SD27 [22]: good (21% of the images), medium (54% of the images) and poor (25% of the images). Each user is manually classified into one of the categories according to the visibility of the pattern (see Fig. 8) including both principal creases and ridge pattern. Good quality means clearly visible ridges and creases, while medium quality means partial visibility. Finally, poor quality reflects poor visibility which complicates ridge or crease distinctions.


                        Table 3
                         shows the EER obtained using images from the three proposed quality levels. As expected the performance of the images clearly reflects the different quality levels. The results show competitive performance for all feature extraction approaches in the case of good quality images but the worsening of the results is evident when image quality decreases. This degradation is evident in the poor images where the performance goes from 0% for good quality to more than 3% for the poor quality images. The multisession experiments show a worsening but results are consistent with previous experiments and BPOC seems to be the less promising feature approach. The results underline the importance of quality in the performance of such high resolution patterns.

Finally, the combination of different algorithms is evaluated to explore the limits of the interdigital region for human recognition. Table 4
                         presents the performance of the multi-algorithms approach combining the different feature extraction methods at score level using the smart ROIs. As the range of each feature score varies, they are normalized by the min/max technique [23] which transforms the ranges of the classification score to [0–1]. The combination of the scores is based on a weighted sum given by 
                           
                              
                                 s
                                 
                                    f
                                    u
                                    s
                                    i
                                    o
                                    n
                                 
                              
                              =
                              
                                 w
                                 1
                              
                              
                                 s
                                 
                                    S
                                    I
                                    F
                                    T
                                 
                              
                              +
                              
                                 w
                                 2
                              
                              
                                 s
                                 
                                    D
                                    S
                                    I
                                    F
                                    T
                                 
                              
                              +
                              
                                 w
                                 3
                              
                              
                                 s
                                 
                                    O
                                    L
                                    O
                                    F
                                 
                              
                           
                        , where {sSIFT, sDSIFT, sOLOF
                        } are the a posteriori normalized scores and {w1,  w
                        2, w
                        3} the weighting factors obtained through the performances given at Table 2 as 
                           
                              
                                 w
                                 X
                              
                              =
                              1
                              −
                              E
                              E
                              
                                 R
                                 X
                              
                              /
                              
                                 (
                                 
                                    E
                                    E
                                    
                                       R
                                       
                                          S
                                          I
                                          F
                                          T
                                       
                                    
                                    +
                                    E
                                    E
                                    
                                       R
                                       
                                          D
                                          S
                                          I
                                          F
                                          T
                                       
                                    
                                    +
                                    E
                                    E
                                    
                                       R
                                       
                                          O
                                          L
                                          O
                                          F
                                       
                                    
                                 
                                 )
                              
                           
                         in the case of combing three features.

The combination outperforms the individual performance in general, which suggests complementarities between the different features. An analysis of the location of the SIFT descriptors reveals that the keypoints from the SIFT algorithm are mainly located in regions near the principal lines. The SIFT method captures the variability around these regions and despises the rest of the information on the pattern. The combination of local information from descriptors such as SIFT or Dense-SIFT and methods focused on the whole texture as OLOF allows the best competitive performances to be achieved. Noteworthy is the competitive performance obtained in the multisession experiment with a consistent EER lesser than 0.01%.

This section answers the remaining questions posed earlier on the influence of skin deformation by sensor contact and the complementarity between the interdigital and palm regions. Palmprint recognition approaches have been historically based on the texture of the center of the palm [3,4,14,17,20]. The interdigital area can be combined with the palm at sensor level, increasing the palm area up to include the interdigital area, at feature level, at score and decision level. In this section we explore the score level combination.


                        Table 5
                         shows the performance of the central palm area (Fig. 9
                        a), the interdigital Smart ROIs (Fig. 9b) and the combination of both schemes at score level by averaging their scores. The extraction of the Smart ROIs was detailed in previous Section 3.3. The center and the size of the central palm ROI are located minimizing the quadratic error of the circumference obtained by the Cartesian coordinates of the points 
                           
                              (
                              
                                 x
                                 i
                              
                              ,
                              
                                 y
                                 i
                              
                              )
                              ,
                              
                              i
                              =
                              1
                              ,
                              3
                              ,
                              5
                           
                        , as was proposed in [14]. The ROI is selected as the rectangle with size equal to 0.75 × radius of the circumference and center equal to the center of the circumference. The errors locating the ROI have a large impact on the performance of any palmprint recognition algorithm. Therefore both ROIs (central palm ROI and interdigital ROIs) are extracted using the same landmarks (fingertips and finger valleys obtained according to the same algorithm proposed in [14,24]) and therefore, location errors have similar impact on the performances.

The results show how the combination outperforms the central palm and interdigital regions in most cases (with improvements up to 54%). For global features such as BLPOC and OLOF, the inclusion of the interdigital region does not produce significant improvements. However this improvement is clearer for the algorithms based on local features as SIFT and Dense-SIFT.

With respect to the palm and interdigital regions, the results suggest that the interdigital area outperforms the central area for local descriptors but the global features are more competitive for central palm based schemes. In all cases, the best performance is achieved combining both schemes.

Finally, Table 6 shows the robustness of the feature extraction algorithms in presence of ROI location errors. The errors have been simulated by moving the center of each Smart ROI in the query samples a fixed distance with random direction. The results show the fast degradation of the performance and how this fall affects different feature extraction methods in a variety of ways. The local features are more robust while global features struggle to deal with these errors (even when they are included). However, it is important to note that the simulated errors are large (at 150 dpi 0.5 mm≈30 pixels; 1 mm≈60 pixels; 2 mm≈120 pixels) and these errors are not typical when competitive ROI extraction algorithms are used and there are no failure acquisitions [25].
                     

The features from the interdigital region have shown their usefulness to improve performance of palmprint recognition systems. The experimental results on the BiosecurID database suggest that each region of the palm shows different structures or patterns and the way to improve the overall performance is a combination of them. Moreover, although the datasets include different users which can possibly explain the differences in the achieved results, the higher quality and negligible skin deformation of the GPDSInterd dataset is the key reason for its superior performance.

This paper has presented a detailed study on interdigital palm region images for their usage in biometric applications. Our experimental evaluation to establish the discriminability of such region employed four state-of-the-art feature extraction methods for personal identification. The study on the palm of 416 people reveals similarities between palms associated to the spatial distribution of the singularities on the region which allow their classification into five classes and efficient ROIs selection. This classification can be applied in synthesis, indexing or forensic applications. The classification into a few classes allows forensic experts to carry out an initial screening of the palmprints lifted in real crime scenes, see Fig. 9. Obviously, this classification is not always possible and depends on the size and quality of the lifted latent print (note that the images showed in Fig. 10
                      were obtained by Forensic Experts from real crime scenes). The automatic classification of palms is an interesting research line. The latest dictionary based learning methods proposed for fingerprint applications [26,27] could be analyzed for further application in the context of automatic singularity detection in palm images.

The experimental section includes the evaluation of four feature extraction approaches for person recognition using interdigital imaging from two databases and two different ROI extraction strategies. Experimental results suggest that features from the interdigital region are reliable for human recognition. The proposed smart ROIs selection based on the taxonomy of palms clearly outperform the performance of a unique ROI. The self-developed acquisition device has shown competitive performances which suggests discriminate ability of this region of the palm. The combination of central and interdigital areas of the palm at score level allows overall performance to be improved (in a similar way as soft biometrics increase the performance of other biometric traits [28]).These results encourage exploring new schemes which include the different regions of the palm and combinations among them, see Fig. 9c.

@&#ACKNOWLEDGMENTS@&#

This work was supported by the Spanish Ministry of Science and Innovation, Spanish Government, under Project TEC-2012-38630-C04-02 and Juan de la Cierva Contract from Spanish MECD (ref. JCI-2012-12357).

@&#REFERENCES@&#

