@&#MAIN-TITLE@&#A framework for joint estimation of age, gender and ethnicity on a large database

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A framework for joint estimation of age, gender and ethnicity in a single step;


                        
                        
                           
                           A novel finding on feature dimensionality in estimating age, gender and ethnicity;


                        
                        
                           
                           A rank theory based analysis of dimensionality problem in using CCA based methods;


                        
                        
                           
                           A ranking of CCA and PLS based methods under our joint estimation framework;


                        
                        
                           
                           Investigation of LS formulations of the CCA based methods for our problem.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Joint estimation of age, gender and ethnicity

A general framework

Partial least squares (PLS)

Canonical correlation analysis (CCA)

Regularized CCA

Kernel PLS (KPLS)

Kernel CCA (KCCA)

Regularized KCCA

Least squares CCA

@&#ABSTRACT@&#


               
               
                  Human age, gender and ethnicity are valuable demographic characteristics. They are also important soft biometric traits useful for human identification or verification. We present a framework that can estimate the three traits jointly. The joint estimation framework could deal with the mutual influence of age, gender, and ethnicity implicitly. Under this joint estimation framework, we explore different methods for simultaneous estimation of age, gender, and ethnicity. The canonical correlation analysis (CCA) based methods, and partial least squares (PLS) models are explored under our joint estimation framework. Both the linear and nonlinear methods are investigated to measure the performance. We also validate some extensions of these methods, such as the least squares formulations of the CCA methods. We found some consistent ranking of these methods under our joint estimation framework. More importantly, we found that the CCA based methods can derive an extremely low dimensionality in estimating age, gender and ethnicity. An analysis of this property is given based on the rank theory. The experiments are conducted on a very large database containing more than 55,000 face images.
               
            

@&#INTRODUCTION@&#

Recently human age estimation in face images has become an active research in computer vision and pattern recognition [1,2], because of many potential applications in the real world. Age estimation is useful for creating an age-specific human–computer interaction (AS-HCI) system [3], electronic customer relationship management (ECRM) [4], and business intelligence [5].

In addition to age estimation, face images can also be used to extract gender and ethnicity information. The three major characteristics, i.e., age, gender and ethnicity, are valuable demographic information of an individual or statistics about a population.

Automated estimation of the demographics is of great value in practice, such as business intelligence, local community planning, and new school locating [6]. Age, gender and ethnicity are also useful soft biometric traits that can be used for human identification or verification.

However, current computational techniques are still not robust enough for practical uses. Thus it is demanding to develop a robust and effective system to recognize age, gender and ethnicity for a given individual or a population.

In the literature, there are different methods for the estimation of each single trait, e.g., age estimation [7–12,1], gender classification [13–17], or ethnicity estimation [18–21]. However, there are very few approaches to estimate all three traits together.

In Ref. [22], a classification of ethnicity, gender, and age groups was executed, with each trait classified independently. This kind of approach can be illustrated by the framework shown in Fig. 1
                        . An implicit assumption is that the three traits can be classified independently, and there is no relation among the three traits.

In our previous studies [23,24], we found that age estimation can be influenced by the gender and ethnicity differences significantly. In Ref. [24], we show that the age estimation errors can be increased significantly on the Yamaha age and gender database, when the males and females are mixed together for age estimation. This result explains why the previous approaches, e.g., Refs. [9–12], executed the age estimation for males and females, separately, on the Yamaha database. In Ref. [23], we studied the influence of gender and ethnicity on age estimation systematically, using the MORPH database [25]. We found that the age estimation errors can be increased when the estimation is performed across gender, ethnicity, or across both.

To deal with the influence of gender and ethnicity on age estimation, we proposed a framework [23] which has a three-step procedure on the extracted features from face images: (1) dimensionality reduction, (2) gender and ethnicity group classification, and (3) age estimation performed on each classified group. This framework can be illustrated by Fig. 2
                        . Although this framework mainly focuses on improving age estimation, it also gets the gender and ethnicity characteristics in the second step.

In this paper, we present a new framework which can estimate the age, gender, and ethnicity jointly in a single step. This single-step framework is much simpler than our three-step procedure [23], and can deal with the influence of gender and ethnicity on age estimation implicitly. Because the labels of gender and ethnicity for each aging pattern are integrated into the single-step age estimation process, we call it implicitly dealing with the influence of gender and ethnicity on age estimation. In contrast, an explicit approach is the three-step procedure [23] where the gender and ethnicity groups are recognized first, before performing age estimation. The new framework of single-step, joint estimation has not been investigated before by other researchers, to the best of our knowledge.

We explore several different methods under our new framework for a single-step, joint estimation of age, gender, and ethnicity. The basic idea of our approaches is to explore the multi-label regression formulation for the joint estimation problem.

Based on the multi-label regression formulation, our joint estimation framework is very simple, as shown in Fig. 3
                        .

To solve the multi-label regression problem, we explore two broad categories of methods, i.e., the partial least squares (PLS) models [26,27] and canonical correlation analysis (CCA) based methods [28,29]. We found that these methods can do dimensionality reduction and joint estimation of age, gender, and ethnicity all together within a single step. This is an interesting observation in our problem, and may inspire new explorations of these methods for other pattern recognition problems.

Specifically, we found that the canonical correlation analysis (CCA) based methods, including linear CCA, regularized CCA, and kernel CCA, can find only three basis vectors to project the original features of several thousand dimensions. Thus, only three dimensions of features are needed (after the transformation) to estimate all three traits. This is a novel finding in estimating age, gender and ethnicity [6]. Further, we use the rank theory to analyze the feature dimensionality problem in using the CCA based methods for our specific task. Hopefully, our analysis may inspire more investigations for other recognition problems to derive similar results with a minimum number of feature dimensions.

The PLS based methods, i.e., linear PLS and kernel PLS, can reduce the dimensionality to 20 or 30 from thousands (in an original feature space), and can achieve a good performance to estimate all three traits [30]. However, the dimensionality based on PLS methods cannot reach a small number as the CCA based methods.

Under our joint estimation framework, we compare the CCA based methods with the PLS based, and derive a ranking of these methods in solving the joint estimation problem.

Further, we explore some extensions of the CCA based methods, such as the recent development of least square formulations [31]. The least square CCA methods have shown excellent performance on traditional machine learning databases, however, it is unknown whether these methods can perform well in our problem.

Given face images, a high dimensionality could be resulted in, e.g., thousands of features are extracted using the biologically-inspired features (BIFs) proposed in Ref. [12] for age estimation. Here, we show that the BIF can be used to represent the face images for all of the three characteristics, i.e., the age, gender, and ethnicity.

We also investigate whether the performance has any changes when other learning methods are used for age estimation. For this purpose, the CCA or PLS based methods are used to generate new features that have low dimensionality and discriminative capability, but not predict the ages. The aging functions are learned by other methods for age estimation.

Our major contributions in this paper include:
                           
                              1.
                              A framework is presented for joint estimation of age, gender and ethnicity in a single step;

A novel finding on feature dimensionality in estimating age, gender and ethnicity;

A rank theory based analysis of the dimensionality problem in using the CCA based methods;

A ranking of the CCA and PLS based methods in joint estimation of age, gender, and ethnicity;

An investigation of the least squares formulations of the CCA based methods for the joint estimation problem.

In the remaining of the paper, we describe the extracted features briefly, and then introduce the CCA based methods and do dimensionality analysis using the rank theory in Section 3. The least squares formulations of CCA are also introduced and compared to the standard methods. The PLS and kernel PLS methods are described in Section 4. The experimental evaluations are presented in Section 5, and finally we give some discussions and draw conclusions.

Recently, the biologically-inspired features (BIFs) [32] have shown good performance in age estimation [12,24], as well as object category recognition [33,34] and face recognition [35]. A specially-designed BIF with two layers [12,24,23], i.e., the simple layer S1 and complex layer C1, shows much lower age estimation errors than previous approaches. The S1 layer contains a set of Gabor filters with parameters designed based on the visual cortex models [33], with the form,
                        
                           (1)
                           
                              
                                 G
                                 
                                    x
                                    y
                                 
                                 =
                                 exp
                                 
                                    
                                       −
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                         ′
                                                      
                                                      2
                                                   
                                                
                                                +
                                                
                                                   γ
                                                   2
                                                
                                                
                                                   
                                                      
                                                         y
                                                         ′
                                                      
                                                      2
                                                   
                                                
                                             
                                          
                                          
                                             2
                                             
                                                σ
                                                2
                                             
                                          
                                       
                                    
                                 
                                 ×
                                 cos
                                 
                                    
                                       
                                          
                                             2
                                             π
                                          
                                          λ
                                       
                                       
                                          x
                                          ′
                                       
                                    
                                 
                                 ,
                              
                           
                        
                     where x′=
                     xcosθ
                     +
                     ysinθ and y′=−
                     xsinθ
                     +
                     ycosθ are the rotations of the Gabor filters with angle θ which varies between 0 and π. The aspect ratio is fixed as γ
                     =0.3, the effective width σ, the wavelength λ as well as the filter sizes s were adjusted accordingly as in Ref. [33]. The filter banks can start from 5×5 or 7×7. Details about the related parameters can be found in Table 1 in Ref. [12].

The orientation θ varies from 0 to π uniformly with different intervals, resulting in different numbers of total orientations, such as 4, 6, 8, 10, and 12.

The C1 layer contains some non-linear operations including the “MAX” pooling and an “STD” operation [12], in order to have some invariance to translation, rotation, and scaling, as well as a characterization of the aging details.

In our study, we use the BIF method for feature extraction. Different from Refs. [12,24,23], here we use the BIF to characterize faces for all three traits, i.e., age, gender, and ethnicity. Given the BIF features, our focus is to investigate the proposed single-step framework for joint estimation of age, gender, and ethnicity. Both the CCA and PLS based methods are explored under the joint estimation framework. Further, we study the minimum number of features needed for the joint estimation problem, and which methods can derive a minimum number of features.

Canonical correlation analysis (CCA) is introduced by Hotelling [28] to describe the linear relation between two multidimensional variables as the problem of finding basis vectors for each set such that the projections of the two variables on their respective basis vectors are maximally correlated [28,29].

The CCA methods have been applied to solve some computer vision problems, e.g., image annotation [36], action classification [37], and face recognition [38,39]. But the CCA methods have not been exploited before by other researchers for age estimation or recognizing all three traits (age, gender and ethnicity), to the best of our knowledge. More importantly, there are few studies on the dimensionality issue in CCA based methods in solving computer vision or pattern recognition problems.

We will briefly describe the CCA method and its extensions in this section, and do dimensionality analysis of the CCA based methods for our problem.

Let p-dimensional x and q-dimensional y denote the two sets of real-valued zero-mean random variables (i.e., x
                        ∈
                        R
                        
                           p
                         and y
                        ∈
                        R
                        
                           q
                        ). Let p
                        ×
                        N matrix X be the data matrix of the first set, and q
                        ×
                        N matrix Y be the data matrix of the second set. N is the number of training samples. The canonical correlation analysis (CCA) computes two projection vectors, w
                        
                           x
                        
                        ∈
                        R
                        
                           p
                         and w
                        
                           y
                        
                        ∈
                        R
                        
                           q
                        , such that the correlation coefficient
                           
                              (2)
                              
                                 
                                    ρ
                                    =
                                    
                                       
                                          
                                             w
                                             x
                                             T
                                          
                                          
                                             
                                                XY
                                                T
                                             
                                          
                                          
                                             w
                                             y
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            w
                                                            x
                                                         
                                                         T
                                                      
                                                   
                                                   
                                                      
                                                         XX
                                                         T
                                                      
                                                   
                                                   
                                                      w
                                                      x
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            w
                                                            y
                                                         
                                                         T
                                                      
                                                   
                                                   
                                                      
                                                         YY
                                                         T
                                                      
                                                   
                                                   
                                                      w
                                                      y
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        is maximized [28,29]. Since ρ is invariant to the scaling of Wx
                         and Wy
                        , CCA can be formulated equivalently as
                           
                              (3)
                              
                                 
                                    
                                       max
                                       
                                          
                                             w
                                             x
                                          
                                          ,
                                          
                                             w
                                             y
                                          
                                       
                                    
                                    
                                       
                                          
                                             w
                                             x
                                          
                                          T
                                       
                                    
                                    
                                       
                                          XY
                                          T
                                       
                                    
                                    
                                       w
                                       y
                                    
                                    ,
                                 
                              
                           
                        subject to 
                           
                              
                                 
                                    
                                       w
                                       x
                                    
                                    T
                                 
                              
                              
                                 
                                    XX
                                    T
                                 
                              
                              
                                 w
                                 x
                              
                              =
                              1
                           
                        , and 
                           
                              
                                 
                                    
                                       w
                                       y
                                    
                                    T
                                 
                              
                              
                                 
                                    YY
                                    T
                                 
                              
                              
                                 w
                                 y
                              
                              =
                              1
                           
                        .

It can be shown [29] that Wx
                         can be obtained by solving the following generalized eigenvalue problem,
                           
                              (4)
                              
                                 
                                    
                                       
                                          XY
                                          T
                                       
                                    
                                    
                                       
                                          
                                             
                                                YY
                                                T
                                             
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       
                                          YX
                                          T
                                       
                                    
                                    
                                       w
                                       x
                                    
                                    =
                                    λ
                                    
                                       
                                          XX
                                          T
                                       
                                    
                                    
                                       w
                                       x
                                    
                                    ,
                                 
                              
                           
                        where λ is the eigenvalue that corresponds to the eigenvector Wx
                        . It has also been shown [29] that multiple projection vectors under certain orthonormality constraints consist of the top l eigenvectors of the generalized eigenvalue problem in Eq. (4). Thus the feature dimension of data X can be reduced to a lower value.

In our study, X represents the data matrix, and Y represents the label space. After the dimension of data X is reduced, we use a least squares fitting to build the relation between the dimension reduced feature and label Y. Then the prediction of Y for the test data is based on the least square fitting result. This simple least square fitting method can work well, and is also applied to other CCA extensions.

In regularized CCA or rCCA, a regularization term is added to each data set to stabilize the solution [31]. The corresponding generalized eigenvalue problem is given by
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                          
                                          
                                             
                                                XY
                                                T
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         1
                                                         −
                                                         
                                                            γ
                                                            y
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         YY
                                                         T
                                                      
                                                   
                                                   +
                                                   
                                                      γ
                                                      y
                                                   
                                                   I
                                                
                                             
                                             
                                                −
                                                1
                                             
                                          
                                          
                                             
                                                YX
                                                T
                                             
                                          
                                          
                                             w
                                             x
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                          
                                          =
                                          λ
                                          
                                             
                                                
                                                   
                                                      1
                                                      −
                                                      
                                                         γ
                                                         x
                                                      
                                                   
                                                
                                                
                                                   
                                                      XX
                                                      T
                                                   
                                                
                                                +
                                                
                                                   γ
                                                   x
                                                
                                                I
                                             
                                          
                                          
                                             w
                                             x
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        where 0≤
                        γx
                        
                        ≤1 and 0≤
                        γy
                        
                        ≤1. It has been pointed out [27] that (1) when γx
                        
                        =0, γ
                        y
                        =0 (5) is to solve the standard CCA; (2) when γx
                        
                        =1, γ
                        y
                        =1 (5) is to solve the PLS eigenvalue problem and (3) by continuously changing of γx
                        , γ
                        y a regularized CCA is solved. In our study, we set γx
                        
                        =
                        γ
                        y
                        =0.09, and found that the rCCA is significantly better than the standard linear CCA. The γx
                         and γ
                        y cannot take too large values, e.g., close to 1, which may result in larger estimation errors and lower accuracies.

Kernel CCA, or KCCA, is to apply the kernel trick to the CCA [29].

In KCCA, the directions wx
                         and wy
                         can be rewritten as the projection of the data onto the direction α and β, as
                           
                              (6)
                              
                                 
                                    
                                       w
                                       x
                                    
                                    =
                                    X
                                    α
                                    ,
                                    
                                    
                                       w
                                       y
                                    
                                    =
                                    Y
                                    β
                                    .
                                 
                              
                           
                        
                     

Let Kx
                        , Ky
                         be the kernel matrices corresponding to the two representation. Then the canonical correlation can be written as
                           
                              (7)
                              
                                 
                                    ρ
                                    =
                                    
                                       
                                          
                                             α
                                             T
                                          
                                          
                                             K
                                             x
                                          
                                          
                                             K
                                             y
                                          
                                          β
                                       
                                       
                                          
                                             
                                                α
                                                T
                                             
                                             
                                                
                                                   
                                                      K
                                                      x
                                                   
                                                   2
                                                
                                             
                                             α
                                             ⋅
                                             
                                                β
                                                T
                                             
                                             
                                                
                                                   
                                                      K
                                                      y
                                                   
                                                   2
                                                
                                             
                                             β
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        which is to be maximized.

We used the Gaussian kernel for data X and linear kernel for label set Y, based on the property of our problem.

To force non-trivial learning on the correlation by controlling the problem of overfitting, a regularization can be applied to the KCCA. Then the generalized eigenvalue problem becomes
                           
                              (8)
                              
                                 
                                    
                                       
                                          
                                             
                                                K
                                                x
                                             
                                             +
                                             κI
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       K
                                       y
                                    
                                    
                                       
                                          
                                             
                                                K
                                                y
                                             
                                             +
                                             κI
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       K
                                       x
                                    
                                    α
                                    =
                                    λα
                                    ,
                                 
                              
                           
                        where κ
                        >0 is a regularization parameter. In our experiments, we set κ
                        =0.05. It is called the regularized kernel CCA, or simply rKCCA. If κ is set to zero, it becomes the standard KCCA without regularization. In our study, this regularization is helpful, although the accuracy improvement is not big in some cases. Experimentally, we found the same thing as CCA or rCCA, that is, only three feature dimensions are needed for rKCCA or KCCA in estimating age, gender and ethnicity (see Figs. 5, 6, and 7
                        
                        
                        
                        ).

In addition to solving the generalized eigenvalue problems, there are other formulations for the CCA optimization problem. To have a better and deeper understanding of the CCA based methods for our joint estimation of age, gender and ethnicity, we also explore other formulations of CCA in our work.

Very recently, the least squares formulations have been proposed to deal with the CCA optimization [31]. It has been shown to perform better than the generalized eigenvalue problem in a couple of machine learning problems. Here, we investigate the performance of these new formulations for our joint estimation problem.

The classical CCA technique can be extended to the least squares formulations [31], using the regularization method. Similar to ridge regression [40], the 2-norm regularized least squares CCA formulation, called LS2-CCA, can be obtained. It minimizes the following objective function by using the target matrix T,
                           
                              (9)
                              
                                 
                                    
                                       L
                                       2
                                    
                                    
                                       W
                                       λ
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          q
                                       
                                       
                                    
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                          
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                         i
                                                         T
                                                      
                                                      
                                                         w
                                                         j
                                                      
                                                      −
                                                      
                                                         T
                                                         
                                                            i
                                                            ,
                                                            j
                                                         
                                                      
                                                   
                                                
                                                2
                                             
                                          
                                          +
                                          λ
                                          |
                                          |
                                          
                                             w
                                             j
                                          
                                          |
                                          |
                                          
                                             
                                             2
                                             2
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where T is defined by
                           
                              (10)
                              
                                 
                                    T
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   YY
                                                   T
                                                
                                             
                                          
                                          
                                             −
                                             
                                                
                                                   1
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    Y
                                    ,
                                 
                              
                           
                        
                        W
                        =[w
                        1, ⋯, w
                        
                           q
                        ], q is the number of output labels, N is the number of training examples, and λ
                        >0 is the regularization parameter.

In addition to the 2-norm formulation, the 1-norm can also be used. The 1-norm regularized least-squares CCA formulation is called LS1-CCA, which minimizes the following objective function:
                           
                              (11)
                              
                                 
                                    
                                       L
                                       1
                                    
                                    
                                       W
                                       λ
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          q
                                       
                                       
                                    
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                          
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                         i
                                                         T
                                                      
                                                      
                                                         w
                                                         j
                                                      
                                                      −
                                                      
                                                         T
                                                         
                                                            i
                                                            ,
                                                            j
                                                         
                                                      
                                                   
                                                
                                                2
                                             
                                          
                                          +
                                          λ
                                          |
                                          |
                                          
                                             w
                                             j
                                          
                                          |
                                          |
                                          
                                             
                                             1
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The LS1-CCA can be solved efficiently by some software packages that are publicly available [31].

It has been shown that the least squares formulations of the CCA can be equivalent to the classical CCA under some conditions [31]. We evaluate if the performance of the LS-CCA methods is better than other CCA techniques in our problem, so that various CCA formulations can be explored under our joint estimation framework.

The LS-CCA can also hold the low dimensionality property as the classical generalized eigenvalue formulations. In our experiments, we found that three dimensions are used for the LS-CCA methods in our joint estimation problem (see Experiments).

Now we analyze the dimensionality after mappings by the CCA-based methods under our joint estimation framework.

Assuming XX
                        
                           T
                         is nonsingular, Eq. (4) can be written as
                           
                              (12)
                              
                                 
                                    
                                       
                                          
                                             
                                                XX
                                                T
                                             
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       
                                          XY
                                          T
                                       
                                    
                                    
                                       
                                          
                                             
                                                YY
                                                T
                                             
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       
                                          YX
                                          T
                                       
                                    
                                    
                                       w
                                       x
                                    
                                    =
                                    λ
                                    
                                       w
                                       x
                                    
                                    ,
                                 
                              
                           
                        or
                           
                              (13)
                              
                                 
                                    M
                                    
                                       w
                                       x
                                    
                                    =
                                    λ
                                    
                                       w
                                       x
                                    
                                    ,
                                 
                              
                           
                        with M
                        =(XX
                        
                           T
                        )−1
                        XY
                        
                           T
                        (YY
                        
                           T
                        )−1
                        YX
                        
                           T
                        . Please note that XX
                        
                           T
                         can be singular, for example, when the number of features is larger than the number of samples. In this case, a dimensionality reduction may be performed to make it nonsingular.

Now, we analyze the rank of matrix M based on the linear algebra theory. Suppose the covariance matrices of XX
                        
                           T
                         and YY
                        
                           T
                         are positive definite. Then we have
                           
                              (14)
                              
                                 
                                    
                                       
                                          
                                          rank
                                          
                                             M
                                          
                                          
                                          =
                                          rank
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            XX
                                                            T
                                                         
                                                      
                                                   
                                                   
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   
                                                      XY
                                                      T
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            YY
                                                            T
                                                         
                                                      
                                                   
                                                   
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   
                                                      YX
                                                      T
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                          
                                          
                                          ≤
                                          
                                          min
                                          
                                             
                                                rank
                                                
                                                   X
                                                
                                                ,
                                                rank
                                                
                                                   Y
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                          
                                          
                                          
                                          ≤
                                          min
                                          
                                             
                                                r
                                                
                                                   X
                                                
                                                ,
                                                c
                                                
                                                   X
                                                
                                                ,
                                                r
                                                
                                                   Y
                                                
                                                ,
                                                c
                                                
                                                   Y
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                          
                                          
                                          
                                          =
                                          min
                                          
                                             p
                                             q
                                             N
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        where r(·) and c(·) denote the number of rows and columns of a matrix, respectively. We have r(X)=
                        p, c(X)=
                        N, r(Y)=
                        q and c(Y)=
                        N.

Based on the rank analysis in Eq. (14), we know that the rank of the matrix M is at most equal to the minimum value among the three numbers, p, q, and N. Thus, the number of non-zero eigenvalues of matrix M is less than or equal to min{p,q,N}. As a result, the eigenvalue problem in Eq. (12) has at most min{p,q,N} eigenvectors, corresponding to non-zero eigenvalues.

Our rank analysis is similar to Theorem 3.1 in Ref. [31], which proved that the eigenvalue problem of CCA has q non-zero eigenvalues based on the assumption that Y has rank q.

In our case of estimating age, gender and ethnicity, we have p
                        =4376, q
                        =3, and N
                        =10,530 based on our experimental setup in this study. It is obvious that min{p,q,N}=3. So the maximum number of non-zero eigenvalues of Eq. (12) is three. Based on the assumption that XX
                        
                           T
                         is nonsingular, the generalized eigenvalue problem in Eq. (4) is equivalent to the problem in Eq. (12). Thus, we can say that the generalized eigenvalue problem in Eq. (4) has at most three non-zero eigenvalues in our study, under the assumption that XX
                        
                           T
                         is nonsingular.

Thus, we only need at most three eigenvectors corresponding to the three non-zero eigenvalues to project the data of X, even if it is a large scale problem with the number of training examples N
                        =10,530.

Our experiments (see Section 5) show that it is true to use only three feature dimensions (or three projections of the data X) for estimation.

On the other hand, we found that we do need to use three feature dimensions to estimate the age, gender and ethnicity altogether in our experiments (see Section 5). This indirectly indicates that the three traits are different, which means one trait cannot include another. So the rank of matrix Y is three, rather than less than three.

Another interesting thing that we observed in our experiments (see Section 5) is that when more than three features are used for estimation, the estimation error or accuracy will almost keep the same (straight lines in Figs. 5, 6, and 7). This kind of phenomenon has not been observed often in many computer vision problems. Further, the performance of the PLS methods has a very different behavior with respect to the feature dimension (see Figs. 5, 6, and 7). Our work may inspire further exploration of the CCA based methods and comparison with the PLS for other computer vision and pattern recognition problems.

For rCCA, let us denote Rxx
                        
                        =(1−
                        γx
                        )XX
                        
                           T
                        
                        +
                        γxI, Ryy
                        
                        =(1−
                        γy
                        )YY
                        
                           T
                        
                        +
                        γyI, with 0≤
                        γx
                         and γy
                        
                        ≤1. Obviously, Rxx
                         and Ryy
                         are positive definite. So, Eq. (5) can be written as
                           
                              (15)
                              
                                 
                                    
                                       
                                          
                                             R
                                             xx
                                          
                                          
                                             −
                                             1
                                          
                                       
                                    
                                    X
                                    
                                       Y
                                       T
                                    
                                    
                                       
                                          
                                             R
                                             yy
                                          
                                          
                                             −
                                             1
                                          
                                       
                                    
                                    Y
                                    
                                       X
                                       T
                                    
                                    
                                       w
                                       x
                                    
                                    =
                                    λ
                                    
                                       w
                                       x
                                    
                                    ,
                                 
                              
                           
                        and because
                           
                              (16)
                              
                                 
                                    
                                       
                                          
                                          
                                          
                                          
                                          rank
                                          
                                             
                                                
                                                   
                                                      
                                                         R
                                                         xx
                                                      
                                                      
                                                         −
                                                         1
                                                      
                                                   
                                                
                                                X
                                                
                                                   Y
                                                   T
                                                
                                                
                                                   
                                                      
                                                         R
                                                         yy
                                                      
                                                      
                                                         −
                                                         1
                                                      
                                                   
                                                
                                                Y
                                                
                                                   X
                                                   T
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                          =
                                          rank
                                          
                                             
                                                X
                                                
                                                   Y
                                                   T
                                                
                                                
                                                   
                                                      
                                                         R
                                                         yy
                                                      
                                                      
                                                         −
                                                         1
                                                      
                                                   
                                                
                                                Y
                                                
                                                   X
                                                   T
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                          ≤
                                          min
                                          
                                             
                                                rank
                                                
                                                   
                                                      
                                                         XY
                                                         T
                                                      
                                                   
                                                
                                                ,
                                                rank
                                                
                                                   
                                                      
                                                         R
                                                         yy
                                                         
                                                            −
                                                            1
                                                         
                                                      
                                                      Y
                                                      
                                                         X
                                                         T
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                          ≤
                                          min
                                          
                                             
                                                rank
                                                
                                                   
                                                      
                                                         XY
                                                         T
                                                      
                                                   
                                                
                                                ,
                                                rank
                                                
                                                   
                                                      Y
                                                      
                                                         X
                                                         T
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                          =
                                          rank
                                          
                                             
                                                
                                                   XY
                                                   T
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                          ≤
                                          q
                                          ,
                                       
                                    
                                 
                              
                           
                        therefore, the system in Eq. (15) has at most q non-zero eigenvalues.

For rKCCA, we can do similar rank analysis. Denote G
                        =(K
                        
                           x
                        
                        +
                        κI)−1
                        K
                        
                           y
                        (K
                        
                           y
                        
                        +
                        κI)−1
                        K
                        
                           x
                        , then rank(G)=
                        rank(K
                        
                           y
                        (K
                        
                           y
                        
                        +
                        κI)−1
                        K
                        
                           x
                        )≤
                        min{rank(K
                        
                           x
                        ), rank(K
                        
                           y
                        )}. Since we used linear kernel for Y, rank(K
                        
                           y
                        )=
                        rank(Y
                        
                           T
                        
                        Y)=
                        rank(Y). Thus, we can show that the eigenvalue problem in Eq. (8) has at most q non-zero eigenvalues.

In the next section, we briefly introduce other methods that can be applied to our joint estimation framework, too, and compare them with the variety of CCA based methods experimentally.

The partial least squares (PLS) algorithm [26,27] can also model the relation between two data sets. It uses latent variables to learn a new space to make the data correlate to each other. The linear and kernel PLS methods have been adapted to estimate age, gender and ethnicity in our recent study [30]. We can compare the PLS based methods with the CCA, to measure the performance difference quantitatively.

Given two data matrices, X and Y, as defined in Section 3.1, and let N be the number of training samples, the PLS method computes two weight vectors, w and c, such that it is the solution to the following optimization problem:
                           
                              (17)
                              
                                 
                                    cov
                                    
                                       t
                                       u
                                    
                                    =
                                    ma
                                    
                                       x
                                       
                                          |
                                          w
                                          |
                                          =
                                          |
                                          c
                                          |
                                          =
                                          1
                                       
                                    
                                    cov
                                    
                                       
                                          
                                             X
                                             T
                                          
                                          w
                                          ,
                                          
                                             Y
                                             T
                                          
                                          c
                                       
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              cov
                              
                                 t
                                 u
                              
                              =
                              
                                 
                                    
                                       
                                          t
                                          T
                                       
                                       u
                                    
                                    N
                                 
                              
                           
                         denotes the sample covariance between the score vectors t and u, and t
                        =
                        X
                        
                           T
                        
                        w and u
                        =
                        Y
                        
                           T
                        
                        c. The classical optimization problem can be solved by the nonlinear iterative partial least squares (NIPALS) algorithm [41]. The NIPALS algorithm starts with random initialization of the Y space score vector u and repeats a sequence of iterations until convergence [41].

After the extraction of the score vectors t and u at each iteration, matrices X and Y are deflated by subtracting their rank-one approximations based on t and u. Different forms of deflation can define several variants of the PLS [27].

Regressions can be performed for both X and Y on t
                        1 and u
                        1, respectively, after extraction of score vectors t
                        1 and u
                        1, such that we have
                           
                              (18)
                              
                                 
                                    X
                                    =
                                    
                                       p
                                       1
                                    
                                    
                                       t
                                       1
                                       T
                                    
                                    +
                                    
                                       X
                                       1
                                    
                                    ,
                                    
                                    Y
                                    =
                                    
                                       q
                                       1
                                    
                                    
                                       u
                                       1
                                       T
                                    
                                    +
                                    
                                       Y
                                       1
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              
                                 p
                                 1
                              
                              =
                              
                                 
                                    
                                       X
                                       
                                          t
                                          1
                                       
                                    
                                    
                                       
                                          t
                                          1
                                          T
                                       
                                       
                                          t
                                          1
                                       
                                    
                                 
                              
                           
                         and 
                           
                              
                                 q
                                 1
                              
                              =
                              
                                 
                                    
                                       Y
                                       
                                          u
                                          1
                                       
                                    
                                    
                                       
                                          u
                                          1
                                          T
                                       
                                       
                                          u
                                          1
                                       
                                    
                                 
                              
                           
                         are the loading vectors, while X
                        1 and Y
                        1 are the residuals. If the L
                        2 norm of the residual Y
                        1 is small enough, the iteration process will terminate, otherwise, the above process is repeated until k steps. Then we get
                           
                              (19)
                              
                                 
                                    X
                                    =
                                    
                                       p
                                       1
                                    
                                    
                                       t
                                       1
                                       T
                                    
                                    +
                                    ⋯
                                    +
                                    
                                       p
                                       k
                                    
                                    
                                       t
                                       k
                                       T
                                    
                                    +
                                    
                                       X
                                       k
                                    
                                    ,
                                    
                                    Y
                                    =
                                    
                                       q
                                       1
                                    
                                    
                                       u
                                       1
                                       T
                                    
                                    +
                                    ⋯
                                    +
                                    
                                       q
                                       k
                                    
                                    
                                       u
                                       k
                                       T
                                    
                                    +
                                    
                                       Y
                                       k
                                    
                                 
                              
                           
                        or
                           
                              (20)
                              
                                 
                                    
                                       
                                          
                                          X
                                          
                                          =
                                          
                                             
                                                PT
                                                T
                                             
                                          
                                          +
                                          
                                             R
                                             X
                                          
                                       
                                    
                                    
                                       
                                          
                                          Y
                                          
                                          =
                                          
                                             
                                                QU
                                                T
                                             
                                          
                                          +
                                          
                                             R
                                             Y
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        where P
                        =[p
                        1, p
                        2, ⋯, p
                        
                           k
                        ] and Q
                        =[q
                        1, q
                        2, ⋯, q
                        
                           k
                        ] are the loading matrices, T
                        =[t
                        1, t
                        2, ⋯, t
                        
                           k
                        ] and U
                        =[u
                        1, u
                        2, ⋯, u
                        
                           k
                        ] are the score matrices, and R
                        
                           X
                         and R
                        
                           Y
                         are the residuals.

As a result, we can have the regression relation:
                           
                              (21)
                              
                                 
                                    Y
                                    =
                                    
                                       B
                                       T
                                    
                                    X
                                    +
                                    
                                       R
                                       Y
                                    
                                    ,
                                 
                              
                           
                        where B can be estimated [27] by
                           
                              (22)
                              
                                 
                                    B
                                    =
                                    XU
                                    
                                       
                                          
                                             
                                                T
                                                T
                                             
                                             
                                                X
                                                T
                                             
                                             XU
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       T
                                       T
                                    
                                    
                                       Y
                                       T
                                    
                                    .
                                 
                              
                           
                        
                     

More details of the PLS method can be referred to Ref. [41].

When a strong nonlinear relation exists between two sets of data X and Y, the kernel trick can be used to derive the kernel PLS [42,43].

Since the development of kernel machines, especially the kernel support vector machine (K-SVM) [44], the kernel trick has been used for many non-linear mappings. Similar to other kernel machines, the kernel PLS maps the data into a high dimensional feature space to realize the non-linearity.

Let x
                        
                           i
                         (p-dimensional vector) and y
                        
                           i
                         (q-dimensional vector) denote the feature vector and the corresponding label vector for the i-th sample, for i
                        =1, 2, ⋯, N. A nonlinear mapping ϕ maps the feature vector x
                        
                           i
                         to a high-dimensional space Ω, and Φ
                        =[ϕ(x
                        1) ϕ(x
                        2) ⋯ ϕ(x
                        
                           N
                        )] denotes the matrix of the mapped X-space data using the kernel trick. The inner product of two vectors x
                        
                           i
                         and x
                        
                           j
                         is then replaced by the kernel function k(x
                        
                           i
                        , x
                        
                           j
                        ). The KPLS can then be derived, resulting in the change of Eqs. (21) and (22) to
                           
                              (23)
                              
                                 
                                    Y
                                    =
                                    
                                       B
                                       T
                                    
                                    Φ
                                    +
                                    
                                       R
                                       Y
                                    
                                    ,
                                 
                              
                           
                        and
                           
                              (24)
                              
                                 
                                    B
                                    =
                                    Φ
                                    U
                                    
                                       
                                          
                                             
                                                T
                                                T
                                             
                                             KU
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       T
                                       T
                                    
                                    
                                       Y
                                       T
                                    
                                    ,
                                 
                              
                           
                        where K
                        =Φ
                           T
                        Φ is the Gram matrix. The (i, j)-th element of K is computed by the kernel function k(x
                        
                           i
                        , x
                        
                           j
                        ). Then, the kernel PLS regression estimate of the output for a given input sample x can be written in the form of
                           
                              (25)
                              
                                 
                                    y
                                    =
                                    YT
                                    
                                       
                                          
                                             
                                                U
                                                T
                                             
                                             KT
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       U
                                       T
                                    
                                    k
                                    ,
                                 
                              
                           
                        where k is a vector whose i-th element is given by k
                        
                           i
                        
                        =
                        k(x, x
                        
                           i
                        ).

@&#EXPERIMENTS@&#

We empirically investigate our new framework for joint estimation of age, gender and ethnicity, and the related issues, such as which methods can perform better under the framework, and how many features are needed to estimate age, gender and ethnicity. We evaluate the estimation accuracies and conduct experiments on a very large database called MORPH [25].

We introduce the MORPH database with some details and our experimental setup. Then we present the results based on the CCA and its extensions, and compare with several other methods. The running time of the CCA and PLS based methods is also presented.

Probably MORPH [25] is the only large database that contains age, gender, and ethnicity. Since MORPH-I is too small, we used MORPH-II for our study, which contains about 55,000 face images. The distributions of the face images are shown in Table 1
                        . Following previous studies on MORPH II [23,30], we divide the whole MORPH database 
                           W
                         into three sets, 
                           
                              S
                              1
                           
                        , 
                           
                              S
                              2
                           
                        , and 
                           
                              S
                              3
                           
                        . The distribution of the data in each set is shown in Table 2
                        . In our experiments, either 
                           
                              S
                              1
                           
                         or 
                           
                              S
                              2
                           
                         is used for training, while all the remaining are used for testing, denoted by 
                           
                              W
                              \
                              
                                 S
                                 1
                              
                           
                         and 
                           
                              W
                              \
                              
                                 S
                                 2
                              
                           
                        , respectively. The number of test examples is much larger than training examples. Note that only two ethnic groups (White and Black) are used for the study, since the number of examples is too small in other groups. Therefore, the race classification accuracies are only measured for the two ethnic groups. The MORPH database has unbalanced gender and ethnicity distributions, which was also noticed in a recent study [23].

The face images in set 
                           W
                         were preprocessed. The faces were detected and aligned, and also cropped and resized to 60×60, as suggested in our previous study [30]. Only the gray level images were used. Our biologically inspired features (BIFs) [12] can represent the aging patterns from each face image. The BIF is also capable of enduring small rotations, translations, and scaling in face image alignment. Different from Ref. [12], here we explore the BIF to characterize the facial patterns for age, gender, and ethnicity altogether.

The experimental results for joint estimation are shown in Table 3
                         and Figs. 5, 6, and 7. We used the linear CCA, regularized CCA, kernel CCA, and the rKCCA to estimate age, gender and ethnicity on the MORPH database. We also compare with the linear and nonlinear PLS models, as well as with some other approaches to age estimation on MORPH.

We found that all of the CCA based methods only need three features to estimate all three traits simultaneously, as shown in Table 3 and Figs. 5, 6, and 7. Interestingly, when more than three features are used, the accuracies or errors will stay almost the same, i.e., straight lines in the figures, without changing accuracies. If a less number of features is used, the performance drops steeply. This sudden change and then keeping steady is different from gradual changes in many feature selection or dimensionality reduction approaches [6]. The behavior of CCA based methods is quite interesting, and it is not often to observe this kind of phenomenon in other computer vision and pattern recognition problems. From Figs. 5, 6, and 7, one can also see that when more eigenvectors are used for feature projection, corresponding to zero eigenvalues, there is no improvement for the task. Those feature dimensions should be discarded. As a result, only three feature dimensions are needed.

To emphasize the importance of the regularization technique in both linear CCA and kernel CCA, and make the difference clearer between the methods of regularized and without, we show the results for both cases, i.e., rCCA and rKCCA. Note that the KCCA results were not shown in the short conference version [6], while the results in Ref. [6] should be the rKCCA results although they were labeled as KCCA in Ref. [6].

From Figs. 5, 6, and 7, one can also observe that the linear and kernel PLS models have gradually changed accuracies or errors with respect to the feature dimensions, and need to use 30 and 35 “latent variables,” respectively, in order to get the highest accuracies or lowest errors. The accuracies of the PLS model may drop when more feature dimensions are used. The number of feature dimensions is determined by the latent variables in PLS and KPLS methods. The number of dimensions, 3, for the CCA based methods, is much smaller than the 30 or 35, based on the linear and kernel PLS methods, and is much smaller than the dimension of 200 selected by the OLPP method in Ref. [23], also is much smaller than the original dimension of 4376 based on the BIF representation [12].

The CCA and PLS models can have a vector as the output. So it is convenient to put age, gender, and ethnicity labels altogether into the output vector. Then the CCA and PLS models can estimate age, gender, and ethnicity using the single learning step. As shown in columns 5 and 6 in Table 3, the accuracies of gender and ethnicity estimation are very high for both CCA and PLS models, and comparable to the complex three-step process (dimensionality reduction, race and gender group classification, and age estimation within each group) proposed in our previous work [23]. Please note that for ethnicity estimation we only reported accuracies for the Black and White, since other race groups were not used in training because the number of samples is too small. The linear SVM method used in Ref. [12] (without dimensionality reduction) only deals with age estimation, without considering gender or ethnicity. The linear or kernel SVM cannot do age, gender and ethnicity simultaneously. Each age is considered as one class for the SVM methods.

Let us look at the age estimation results shown in the last column in Table 3, which is the average of column 7 when two different sets, 
                           
                              S
                              1
                           
                         and 
                           
                              S
                              2
                           
                        , were used for training alternately. The rKCCA obtains an MAE of 3.98years, which is for the first time to have an MAE below 4years on the MORPH database [6]. This low MAE is even smaller than the kernel CCA which has an MAE of 4.06years, and the kernel PLS with an MAE of 4.04years, and is smaller than any linear models, such as 5.37years for CCA, 4.42years for rCCA, and 4.56years for PLS. It is also smaller than the 4.45 using a complex 3-step procedure in Ref. [23]. Compared with the MAE of 4.45years in Ref. [23], the error reduction rate for rKCCA is 10.6%. Considering the age estimation is performed on a very large database, this error reduction rate is statistically significant. The linear SVM has an MAE of 5.09years when working on the original BIF features without dimensionality reduction. The kernel SVM gives an MAE of 4.91 which is even higher than the rCCA model. Comparing the rKCCA with the linear and kernel SVMs, the error reduction rates are 21.8% and 18.9%, respectively. These two error reductions are very significant.

We also explored the least squares formulations for our joint estimation problem. The results are shown in the last two rows in Table 3. We found that both the LS1-CCA and LS2-CCA have similar performance on age estimation, comparable to the rCCA, which are better than the standard CCA method. However, the least squares formulations cannot perform better than the KCCA, rKCCA, or KPLS methods in our problem.

For gender and ethnicity estimation, the CCA based methods can perform very well with higher accuracies. The standard CCA performs slightly worse that other formulations. Again, the least squares formulations have comparable results with the rCCA and PLS, however, the KCCA, KPLS, and rKCCA perform better than the linear methods for gender and ethnicity estimation.

We also show the cumulative curves of the age estimation results in Fig. 4, for all CCA based methods. CS curves are another way to characterize the age estimation performance.

The cumulative score [8] is defined as CS(j)=
                        N
                        
                           e
                           ≤
                           j
                        /N
                        ×100%, where N
                        
                           e
                           ≤
                           j
                         is the number of test images on which the age estimation makes an absolute error no larger than j years. We can observe that the rKCCA and KCCA perform the best, rCCA, LS1-CCA, LS2-CCA next, and the linear CCA is the lowest.

All these results tell us that the classical CCA formulations can still perform very well in our problem. The recently developed least squares formulations of the CCA cannot show super performance in our joint estimation problem, although these formulations have shown good performance in other machine learning problems [31].

In addition to the age estimation errors and gender and ethnicity classification accuracies, we would like to examine the running time for the CCA and PLS based methods. The real running time for both training and testing was recorded and is shown in Table 4
                        . Note that the time for feature extraction by the BIF is not included. One can see that the linear methods (CCA, rCCA, and PLS) are much faster than the kernel extensions (KCCA, rKCCA, and KPLS) in both training and testing stages. The reason why the KCCA and KPLS are so slow is that the kernel matrix is too big, i.e., 10,530×10,530 in training, while 44,602×10,530 in testing. It is determined by the number of training and testing examples, respectively.

We note that the LS1-CCA takes a longer time than the kernel PLS or kernel CCA methods in training. That is because the optimization with the L1 norm is very slow when a large number of training examples are presented. The L2 norm optimization is much faster than the L1 norm in our problem. In testing, the CCA methods with least squares formulations are much faster than the kernel methods.

So, considering both the accuracies and running time, we recommend to use the rCCA method for practical applications. It is very fast, and its accuracy is higher than the CCA and PLS. The KCCA, rKCCA, and KPLS have smaller errors, but the kernel computation is very heavy.

Although our main focus in this paper is to develop a new framework for joint estimation of age, gender, and ethnicity using a single-step procedure, we are also interested in another problem, that is, using some other methods for the aging function learning for age estimation, while the PLS and CCA based methods for feature projections (i.e., generating new features with low dimensionality and being discriminative). It is also based on the experimental results shown in Table 3 where the gender and ethnicity classifications have very high accuracies while the age estimation still has room to improve. This investigation can tell us if any further improvement can be achieved when other learning functions are used for age estimation.

We chose to use the support vector machines (SVMs) and support vector regression (SVR) [44] for aging function learning. In our earlier studies [9], we have investigated the performance of the SVM and SVR systematically for aging function learning on the FG-NET and Yamaha aging databases. Here, we study age estimation performance when the features are represented by the PLS and CCA based methods on the large MORPH database. Note that the SVR is usually for a single-label regression, it cannot deal with all three labels, i.e., age, gender, and ethnicity simultaneously.

We used the PLS and rCCA as the representative approaches for linear projections to generate new features, while the rKCCA as the representative for non-linear projections for feature representation. The experimental results are shown in Table 5
                        .

From the table, we can observe that the rKCCA method can derive a good representation of the aging patterns in face images. The MAEs can be reduced to 3.92 when the SVMs are used for aging function learning, either linear or non-linear (with the RBF kernel). Note that only three features are used based on the rKCCA method. When the SVR method is used for aging function learning, the MAEs are slightly higher than the SVMs, without improvement over the case where the rKCCA is used for both dimensionality reduction and age estimation (see Table 3).

For the rCCA features, the SVMs and SVR can give age estimation results slightly higher than the rCCA method only (see Table 3), but the improvements are very small. This indicates that the rCCA method itself can work well for both dimensionality reduction and age estimation (and gender and ethnicity recognition).

For the PLS features, the linear SVMs and SVR can give similar performance as using the PLS method only, however, the non-linear SVMs and SVR can have slightly larger errors for age estimation than the PLS method only. This shows that it is not proper to execute non-linear SVMs or SVR on the features extracted by the PLS method. This kind of phenomenon may be studied further in the future.

The age estimation results under our joint estimation framework can also be compared to other recently developed methods. Through the comparisons, one can see the relative performance of our approaches. The comparison results are shown in Table 6
                        . Our BIF features were provided to [45] and various learning methods were run on MORPH II with the same features, thus a direct comparison of different learning methods can be obtained from Ref. [45]. As shown in Table 6, the ordinal hyperplane ranking method can have an MAE of 6.28years, the AGES method [3] gets an MAE of 6.61, and the earlier AAS method [7] can have a large MAE of 10.10years. The recently developed IIS-LLD (IIS-Learning from Label Distribution) and CPNN (conditional probability neural network) methods [45] can improve the MAE to 5.67, and 4.87, respectively. The local BIF [46] can reduce the MAE to 4.20years. However, all these results still have larger MAEs than our approaches: 3.98 and 3.92 based on the rKCCA and rKCCA+SVM methods, respectively. To the best of our knowledge, our approaches are for the first time to get a MAE below 4years for age estimation on MORPH II [6].

@&#DISCUSSION@&#

Recently, the PLS and CCA based methods have shown great performance in solving computer vision problems. It is essential to evaluate and compare the PLS and CCA based methods in a variety of vision problems, so that we can have a deeper understanding about their behaviors for both general and specific vision applications.

Sharma and Jacobs [39] have done a nice job very recently by comparing the linear CCA and linear PLS methods (they neither discuss the kernel based methods, nor analyze minimum numbers of dimensions) for face recognition with respect to pose variation, image resolution change, and photo-sketch matching. Their results show that the linear PLS outperforms the linear CCA in most cases. Although their face recognition is very different from our problem of estimating age, gender, and ethnicity jointly, their observation is quite consistent with ours, that is, the linear PLS outperforms the linear CCA.

However, we also found that the regularized CCA (or rCCA) performs better than the PLS, and the standard (linear) CCA as well, in our problem. Since there is no discussion on regularization or any equations related to rCCA in the presentation in Ref. [39], our guess is that only the standard CCA was evaluated in Ref. [39]. Based on our results, we conjecture that the regularized CCA may outperform the PLS for the face recognition problems discussed in Ref. [39]. If that is true, one has to evaluate and compare different extensions of the PLS and CCA methods for computer vision problems, rather than just the standard formulations.

So, our study about the CCA and PLS based methods in this paper is not only important and useful for estimating demographics, but also helpful to inspire more careful and detailed explorations of these methods for other computer vision problems. As a result, better performance and less number of feature dimensions might be expected in more applications.

We also explored the least squares formulations of the CCA [31] in our joint estimation problem. We found that the least squares formulations can perform better than the linear CCA, however, they cannot perform better than the KCCA, rKCCA and KPLS methods, or the rCCA method in our problem. Thus, the various extensions of the CCA methods need to be investigated carefully for specific computer vision and pattern recognition problems in order to measure the performance exactly.

We have presented a new framework for joint estimation of age, gender and ethnicity. Under the proposed framework, we explored different methods to accomplish the one-step procedure for our joint estimation problem. We have also presented a novel finding that only three dimensions of features are needed to estimate age, gender and ethnicity altogether. The experimental validations have been performed on a very large database with more than 55,000 face images. We have analyzed the feature dimensionality problem using the rank theory for the CCA based methods. A systematic comparison of the behaviors between the CCA and PLS based methods has been given, including accuracies or errors with respect to dimensions, as well as the running time. We also explored the least squares formulations of the CCA under our joint estimation framework. Experimentally, we have shown that the rCCA has a comparable running time, but lower errors than the CCA and PLS. It is also much faster than the kernel methods in both training and testing. Our dimensionality analysis of the CCA based methods provides a new insight about the CCA, and may inspire further exploration of the behaviors of the CCA and PLS based methods in a broader range of visual computing problems.

@&#ACKNOWLEDGMENT@&#

The authors would like to thank K. Ricanek for providing the MORPH database for this study. The authors are grateful to the anonymous reviewers and guest editors for their detailed comments and suggestions to improve the paper.

@&#REFERENCES@&#

