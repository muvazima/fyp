@&#MAIN-TITLE@&#Local part model for action recognition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a new local part model for action recognition.


                        
                        
                           
                           A feature sampling strategy with high feature density is used.


                        
                        
                           
                           We explore and prove the benefits of using accurate optical flow algorithm for action recognition.


                        
                        
                           
                           High performance and fast action recognition are achieved.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Bag-of-features (BoF)

Action recognition

Random sampling

Local part model

Multi-channel SVM

@&#ABSTRACT@&#


               
               
                  This paper introduces an action recognition system based on a multiscale local part model. This model includes both a coarse primitive level root patch covering local global information and higher resolution overlapping part patches incorporating local structure and temporal relations. Descriptors are then computed over the local part models by applying fast random sampling at very high density. We also improve the recognition performance using a discontinuity-preserving optical flow algorithm. The evaluation shows that the feature dimensions can be reduced by 7/8 through PCA while preserving high accuracy. Our system achieves state-of-the-art results on large challenging realistic datasets, namely, 61.0% on HMDB51, 92.0% on UCF50, 86.6% on UCF101 and 65.3% on Hollywood2.
               
            

@&#INTRODUCTION@&#

The recognition of human actions in videos remains a very active field of research which has a significant impact on a wide range of applications such as intelligent video surveillance, video retrieval, human-computer interaction and smart home systems. Over the last decade, the advances in the area of computer vision and pattern recognition have fuelled a large amount of research with great progress in human action recognition. Much of the early progress [1–3] has been reported on atomic actions with several categories based on staged videos captured under controlled settings, such as KTH [3] and Weizmann [1]. More recently, there are emerging interests for sophisticated algorithms in recognizing actions from realistic video. Such interests involve two prospects: 1) In comparison to image classification evaluating millions of images with over one thousand categories, action recognition is still at its initial stage. It is important to develop reliable, automatic methods which scale to large numbers of action categories captured in realistic settings. 2) With over 100h of video uploaded to YouTube every minute,
                        1
                     
                     
                        1
                        
                           http://www.youtube.com/yt/press/statistics.html
                        
                      and millions of surveillance cameras all over the world, the need for efficient recognition of visual events in videos is crucial for real world applications.

In this paper, we address the problem of fast human action recognition from uncontrolled, realistic video. We propose a solution that achieves both accurate recognition performance and high computational efficiency.

Recent studies [4,5] have shown that low-level local spatio-temporal features and bag-of-features(BoF) can achieve remarkable performance for action recognition on realistic videos. Such approaches have several advantages, such as simplicity, compact video representation, relatively independent representation of events, and better tolerance to illumination, occlusion, deformation and multiple motions etc. However, there are still a number of challenges that need to be addressed for action recognition applied in large scale real-world videos.

First, the bag-of-features model only contains statistics of unordered features, and any information related to temporal ordering and spatial structure is lost. In consequence, such approaches have difficulty to discriminate between actions characterized by their structure and event-orderings, such as “stand up” and “sit down”. A more discriminative method should include global structure information and ordering of local events.

Most interest point detectors used for action classification have been extended from the 2D spatial domain. They were originally designed for feature matching, not for selecting the most discriminate patches for classification. Interest point detectors [6] or selected features [7] by unsupervised learning have been shown to be very useful for simple KTH dataset [3] with single, staged human actions and uncorrelated backgrounds. We argue that it is more suitable to include the background information for real-life challenging datasets [8–11] because some of their background features are highly correlated with the foreground actions (e
                     .
                     g. diving with water background and skiing with snow background), and thus provide discriminative information for the foreground categories.

It should also be noted that most existing action recognition methods use relatively expensive feature extractor, which could constitute a limiting factor considering the huge amount of data to be processed. In particular, the use of dense trajectories, which is the secret sauce in most state-of-the-art methods, imposes a costly preprocessing step that prevents these methods to be used in real-time scenarios. Moreover, sparse interest point representations may miss important aspects of the scene and therefore do not generate enough relevant information for classification. In contrast, dense sampling methods can provide a very large number of feature patches and thus can potentially produce excellent recognition performance; better results are generally observed as the feature density increases [5,6]. However, the increase in the number of processed points adds to the computational complexity even if simplifying techniques, such as integral video and approximative box-filters, are used.

To overcome these challenges, we proposed a local part model (LPM) to better represent spatio-temporal activities. Our local part model includes both a coarse root ST patch covering local content statistics and finer overlapping part ST patches integrating local structure and temporal relations. To further improve the efficiency of the approach, we use random sampling for feature extraction. An important contribution of this paper resides in the high efficiency of the approach while still producing competitive performances. Our method indeed runs at 30 to 70 fps, depending on the feature used for recognition. This gain in efficiency is achieved by having recourse to two main strategies. First the use of random sampling and integral video for feature extraction. Second by avoiding costly dense trajectory computations and instead relies on global optical flow estimation. We demonstrate in this paper that the use of accurate flow fields is beneficial for action recognition in real-life applications.

The paper is organized as follows: The next section reviews the related works. Section 3 describes the details of our methods. Section 4 introduces different descriptors we used. In Section 5, we present the experimental setup and datasets we tested on. Section 6 summarizes our results and the comparison of our method with other approaches. In terms of recognition accuracy, this is a significant improvement over the state-of-the-art, as well as over our two previous conference publications [12,13]. This paper is built upon these two previous publications. It includes the following additions: 1) an analysis of the impact of dimensionality reduction for efficient action recognition; 2) an improvement of Local Part Model through the use of multiple channels resulting in better performance; 3) an evaluation of the use of more accurate optical flow estimation on performance; and 4) an experimental analysis on different components of the Local Part Model as well as additional experiments on datasets with large numbers of action categories captured in realistic settings. The code to perform random sampling with our Local Part Model is available on-line.
                        2
                     
                     
                        2
                        
                           http://www.site.uottawa.ca/laganier/projects/actionLPM/index.html
                        
                     
                  

@&#RELATED WORKS@&#

Laptev and Lindeberg [14] were the first to introduce space–time interest point by extending the Harris-Laplace detector to the 3D space. Schüldt et al. [3] built a space–time Harris corner detector with automatic scale selection to detect salient sparse spatio-temporal features. To produce denser space–time feature points, Dollár et al. [2] used a pair of 1D Gabor-filter to convolve with a spatial Gaussian to select local maximal cuboids. Willems et al. [15] proposed the Hessian3D detector and extended the SURF descriptor to detect relatively denser and computationally efficient space–time points. Oshin et al. [16] introduced a Relative Motion Descriptor and used RANSAC to obtain saliency information during interest point detection. Recent works have proposed the use of densely sampled feature points [12,6] and dense trajectories [5,17–19] for action recognition.

Dense sampling has shown to produce good results for image classification [20,21]. For action recognition, Wang et al. demonstrated in [6] that dense sampling at regular space–time grids outperformed state-of-the-art interest point detectors. Similar results have also been observed in [12,17,5]. Compared to interest point detectors, dense sampling generally captures more information by sampling every pixel in each spatial scale. However, such approaches are often computationally intractable for large video datasets.

Uniform random sampling [22], on the other hand, can provide performances comparable to dense sampling. A recent study [23] showed that action recognition performance can be maintained with as little as 30% of the densely detected features. Given the effectiveness of the uniform sampling strategy, one can think of using biased random samplers in order to find more discriminant patches. Yang et al. [24] were able to identify more features on the object of interest by using a prior distribution over patches of different locations and scales. Liu et al. [25] selected the most discriminative subset from densely sampled features using the AdaBoost Algorithm. [26,23] were based on the idea that eye movement of the human viewers is the optimal predictor of visual saliency. They measured the eye movement of human observers watching videos, and used the data to produce an “empirical” saliency map. By using such saliency maps, they pruned 20–50% of the dense features and achieved better results. However, the requirement of prior eye movement data renders such methods impractical for real applications. In addition, because of computational constraints, these methods didn't explore high sampling density schemes to improve their performance.

To deal with the “out-of-ordering” problem of the bag-of-features representation, Hamid et al. [27] proposed an unsupervised method for detecting anomalous activities by using bags of event n-grams. In their method, human activities were represented as overlapping n-Grams of actions. While overlapping n-grams can preserve the temporal order information of events, it causes the dimensionality of the space to grow exponentially as n increases. Thurau and Hlavác [28] introduced n-grams of primitive level motion features for action recognition. Laptev et al. [4] extended image representation of spatial pyramid [29] to the spatio-temporal domain. The authors divided a video into a grid of coarse spatio-temporal cells. The whole video was then represented by the ordered concatenation of the per-cell BoF models. Such ordered concatenation adds global structural information. Gaidon et al. [30] focused on explicitly capturing the spatial and temporal structure of actions with structure model. Tang et al. [31] used a variable-length discriminative HMM model which infers latent sub-actions to explicitly model the presence of sub-events.

As for real-time action recognition algorithms, both Ke et al. [32] and Willems et al. [15] used approximative box-filter operations and integral video structure to speed-up feature extraction. Patron-Perez and Reid [33] employed a sliding temporal window within the video and used first-order dependencies to effectively approximate joint distribution over feature observations given a particular action. Yeffet and Wolf [34] efficiently classified the actions with Local Binary Patterns and an approximated linear SVM classifier. Yu et al. [35] extended the efficient 2D FAST corner detector to the 3D domain V-FAST detector, and applied semantic texton forests for fast visual codeword generation. Whiten et al. [36] exploited very efficient binary bag-of-features matching with the Hamming distance rather than the Euclidean distance through an extension of the popular 2D binary FREAK descriptor.

Nowak et al. [22] have shown that the most important factor impacting a system's performance is the number of sampled patches. While the performance of dense sampling is improved as the sampling step size decreases [5], such approach becomes rapidly computationally intractable due to the very large number of patches produced. To achieve both computational efficiency and high accuracy, our approach increases the sampling density by decreasing the sampling step size, and at the same time controls the number of sampled patches used by the classifier. We experimentally found that, with proper sampling density, state-of-the-art performance can be achieved by randomly discarding up to 80% of densely sampled patches.

In the design of our approach, we aim at maintaining both global structure information and ordering of local events for action recognition. Our method incorporates both spatio structural information as [37,29,4] and ordering of the events as [27,28], but avoids the increased dimensionality of the n-grams method [27]. Inspired by the multiscale, deformable part model (DPM) [38], we propose a 3D multiscale part model for video event analysis. However, instead of adopting deformable “parts”, we use overlapping “parts” with fixed size and location on the purpose of maintaining both local structure information and ordering of local events for action recognition.

Our local part model includes both a coarse primitive level root ST patch covering local event-content statistics and higher resolution overlapping part ST patches incorporating local structure and temporal relations. The underlying building blocks for our models are local spatio-temporal (ST) volume patches, which can be extracted through dense sampling or by a local spatio-temporal feature detectors, such as Harris3D 
                        [14], Cuboid 
                        [2], Hessian3D 
                        [15].

As shown in Fig. 1
                        , for an original video V
                        
                           p
                         of size W
                        ×
                        H
                        ×
                        T, we create a spatially down-sampled video V
                        
                           r
                         of size W/2×
                        H/2×
                        T. We then extract local 3D patches at regular positions and scales in space and time from V
                        
                           r
                        . These local ST patches define our root model. For every ”root”, a group of finer “parts” are extracted from the video V
                        
                           p
                         at a location (2x
                        0, 2y
                        0, t
                        0) defined by the root reference position (x
                        0, y
                        0, t
                        0).

Our model consists of a coarse root patch and a group of higher resolution part patches. The number of part patches could be any combination of 1 to 3 for each dimension. Our evaluation shows that 2×2×2 parts give both good performance and low feature dimension. Both the coarse root patch and the higher resolution part patches can be represented by any of the local descriptors, such as HOG/HOF 
                        [4], HOG3D 
                        [39], MBH 
                        [5], ESURF 
                        [15] 
                        etc. However, our local part model also incorporates the local structure relations and temporal ordering information by including local overlapping “events”. It thus provides improved discriminative power for action recognition.

In our previous paper [12,13], each of these patches is represented by a local descriptor, and all histograms are concatenated into one vector that is 9 (1 root+8 parts) times the original feature dimension of the used descriptor. For better performance, the vector from each ST patches is normalized, followed with a re-normalization over the concatenated vector from 8 parts. Because the vector size of part model is 8 times as large as that of the root model and both of them are normalized unit vector, such an approach results in large codeword quantization errors. In addition, it also occludes the individual discriminative power of independent root model and part models. In this paper, we substantially improve the approach by treating the root and 8 parts as two separate channels. For each channel, a standard bag-of-features approach is applied. The resulting histograms of visual word occurrences from root and parts are concatenated into one histogram for SVM classification. We will discuss this in details in our experimental section.

The volume patches on which local part model can be built need to be extracted at different spatial and temporal scales. One strategy to improve computational efficiency is to use spatio-temporal pyramids as those in [4,5,40]. However, for each spatio-temporal scale, the video needs to be rescaled and stored. If we build a spatio-temporal pyramid with 
                           
                              σ
                              xy
                           
                           =
                           
                              σ
                              t
                           
                           =
                           
                              
                                 2
                              
                              2
                           
                         over a total of 8 spatial scales and 2 temporal scales, the total memory needed would be increased by a factor of:
                           
                              (1)
                              
                                 
                                    
                                       1
                                       +
                                       
                                          
                                             2
                                          
                                          2
                                       
                                    
                                 
                                 ×
                                 
                                    
                                       1
                                       −
                                       
                                          
                                             
                                                
                                                   2
                                                
                                                2
                                             
                                          
                                          8
                                       
                                    
                                    
                                       1
                                       −
                                       
                                          
                                             2
                                          
                                          2
                                       
                                    
                                 
                                 ≈
                                 5.46
                                 .
                              
                           
                        
                     

Therefore, we employ the same strategies as those in [32,39] by using integral video, a memory-efficient alternative. Integral video is a spatio-temporal extension of the integral image proposed by Viola and Jones [41] for efficient computation of Haar features. An integral video at spatial location (x,
                        y) and time t is defined as the sum of all pixel values at locations less than or equal to (x,
                        y,
                        t). The integral video v
                        
                           i
                         can be described as:
                           
                              (2)
                              
                                 
                                    v
                                    i
                                 
                                 
                                    x
                                    y
                                    t
                                 
                                 =
                                 
                                    
                                       ∑
                                       
                                          
                                             t
                                             ′
                                          
                                          <
                                          =
                                          t
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             
                                                y
                                                ′
                                             
                                             <
                                             =
                                             y
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                
                                                   x
                                                   ′
                                                
                                                <
                                                x
                                             
                                          
                                          v
                                          
                                             
                                                x
                                                ′
                                             
                                             
                                                y
                                                ′
                                             
                                             
                                                t
                                                ′
                                             
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        where v(x',
                        y',
                        t') is the pixel value at the location (x',
                        y',
                        t') of the input image sequence. Given an input video of size (W,
                        H,
                        T), the computed integral video size is (W
                        +1,
                        H
                        +1,
                        T
                        +1). Therefore the total memory requirement for an integral video is only marginally more than the original video.

Since we use random sampling, no feature detection is required. The feature computation time is mainly spent on the feature descriptors. With integral video, each descriptor of a ST patch can be computed very efficiently through 7 additions/subtractions multiplied by the total number of root and parts, independent of the volume's size and location.

In our method, for each clip, we compute two integral videos, one for the root at half resolution, and another one for the parts at full resolution. Note that we compute the image gradient (for HOG) and dense optical flow (for HOF and MBH) at full resolution for the part models, and then reuse them for the root model.

We found that, in addition to preserving local temporal context, local part model applies well to the context of dense sampling. Under the LPM, the representation is built from a coarse global root model associated with several overlapping part models. For every coarse root patch at half the video resolution, a group of finer part patches are acquired from the full resolution video at the locations defined by the root patch. The dense sampling grid is determined by the root patch applied on half the spatial resolution of the processed video. At this resolution, very high sampling density can be achieved with far less samples. It is important to note that working at this lower resolution does not result in performance drop because the fine-grained information is still included in the part components.

On the sampling grid, a feature point is determined by 5 parameters (x,
                           y,
                           t,
                           σ,
                           τ), where δ and τ are the spatial and temporal scale, and (x,
                           y,
                           t) is its space–time location in the video. For a 3D patch s
                           =(x,
                           y,
                           t,
                           δ,
                           τ), a feature can be computed on a local ST patch of width w
                           
                              i
                           , height h
                           
                              i
                            and length l
                           
                              i
                            given by
                              
                                 (3)
                                 
                                    
                                       w
                                       i
                                    
                                    =
                                    
                                       h
                                       i
                                    
                                    =
                                    
                                       δ
                                       0
                                    
                                    ×
                                    
                                       δ
                                       i
                                    
                                    
                                    and
                                    
                                    
                                       l
                                       i
                                    
                                    =
                                    
                                       τ
                                       0
                                    
                                    ×
                                    
                                       τ
                                       i
                                    
                                 
                              
                           where δ
                           0 and τ
                           0 are the initial spatial and temporal scale, respectively, and δ
                           
                              i
                            and τ
                           
                              i
                            are the spatial and temporal step factor for consecutive scales, respectively. In our experiments, we set the scale steps as 
                              
                                 δ
                                 i
                              
                              =
                              
                                 τ
                                 i
                              
                              =
                              
                                 
                                    
                                       2
                                    
                                 
                                 i
                              
                           , given i
                           =0,1,2,...,
                           K as the ith step of the scales. With MBH/HOF descriptor, we set the minimal spatial size to 20×20 pixels and minimal temporal size to 14 frames. With a total of 8 spatial scales and 2 temporal scales, the video is sampled 16 times.

Each sampled local ST patch is subdivided into a grid (η
                           
                              x
                           
                           ×
                           η
                           
                              y
                           
                           ×
                           η
                           
                              t
                           ) of ST cells. For each cell, we compute histograms of oriented gradient (HOG), or histograms of other descriptors, such as HOF, MBH. The histograms from all ST cells are concatenated into one descriptor vector.

A key factor governing sampling density is the overlapping rate of two successive ST patches. In our approach, we consider a very high sampling density with 80% overlap for both spatial and temporal sampling. Table 1
                            shows the comparison of the average number of generated features per frame for different methods. The features produced with Harris3D, cuboid and dense sampling in [6] are extracted from videos with a spatial resolution of 360×288 pixels. At same resolution, we generate 19.7 times more features than the “Dense” sampling method does in [6]. Note that one feature in our method includes 9 ST patches (1 root+8 parts) whereas others have one ST patch per feature.

For an image of size N
                           ×
                           N, the number of possible sampled patches is N
                           4 
                           [42]. Nowak et al. have shown in [22] that the performance is always improved as the number of randomly sampled patches is increased with as many as 10,000 points per image. For video recognition, such an approach would be computationally prohibitive. One solution is to do sampling at lower spatial resolution. As discussed above, the dense sampling grid of our Local Part Model is constructed from the root model, which is applied at half the resolution of the processed video. A subsequent approach is to perform random sampling which selects a small subset of the potential feature samples.

The video size in the datasets varies a lot for different clips, especially for total number of frames. For example, the clip with minimal frames on HMDB51 dataset has a size of 560×240×19, while another clip has a size of 360×240×1063. Table 2
                            show the number of generated features per video from dense sampling with three different sampling parameters, which are defined with different minimal patch sizes and stride steps. If we use dense sampling method with the parameters of “Sampling 3” and generate 9768 for the clips of 19 frames, the total number of features generated from a video of 160 frames would be 196,298. Such large number of features for a 160-frame video is both computationally expensive and unnecessary. For the clips with very long duration, most actions consist of repeated motions and similar structures. Schindler and Val Gool [43] show that snippets of 5–7 frames are enough to achieve a comparable performance on staged, controlled datasets. For realistic videos, it may need more frames to classify them due to the scale change, rotation, different viewpoints, multiple players and various backgrounds. However, it is a reasonable practice to sample features more densely from a clip with less frames. In practical applications using dense sampling, it is impossible to change the parameters based on the video duration.

Random sampling, on the other hand, can control the total number of features without any parameter changes. As shown in Table 2, we can use the parameters of “Sampling 2” and densely sample 1998 features from the clip with 19 frames. At the same time, randomly choose 10,000 features on the 160-frame video. Thus, random sampling can generate denser features for clips with less frames given the fixed parameters. It also improves efficiency by processing less features for videos with long duration. In our experiments, we randomly select 10,000 features (90K patches) for each clip, and report the classification results on it. They are chosen uniformly from the dense grid, so samples at finer scales predominate. We set the maximal video length to be 160 frames. If the video is larger than 160 frames, we simply divide it into several segments, and select features at same rate for each segment.

Principal Component Analysis (PCA) is a standard tool for dimensionality reduction and has been used to solve various computer vision problems, such as feature descriptor [44], object classification [45], face recognition [46], image representation [47], and human detection [48,49].

In our LPM, we randomly sample 10K cuboids made of 10K “root” patches and 80K “part” patches from every clip with up to 160 frames. The root and parts are treated as two separate channels, and each patch is represented by a local descriptor. However, the concatenated feature from a group of 8 parts has 8 times the dimension of the used descriptor. LPM controls the complexity of a very high sampling density by representing a group of sampled part patches with a single high dimension vector. Because the root patch and its 8 part patches are sampled from overlapping volumes, they tend to produce a certain amount of redundant information. LPM is therefore a good candidate for feature reduction.

Let d
                        =
                        D
                        0 be the feature dimension of the descriptor we use. One LPM feature has two channels, with 1 root at dimension d
                        
                           r
                        
                        =
                        d
                        =
                        D
                        0 and 8 part of dimension d
                        
                           p
                        
                        =8×
                        d
                        =8×
                        D
                        0. We empirically found that, without noticeable performance penalty, the root dimension can be reduced by half and the part dimension can be reduced to 1/8 at the same time. After applying PCA, the root channel dimension is 
                           
                              d
                              
                                 r
                                 '
                              
                           
                           =
                           
                              1
                              2
                           
                           
                              D
                              0
                           
                        , and part channel dimension is d
                        
                           p'
                        =
                        D
                        0, which is equal to the dimension of an original descriptor. In comparison with other methods [44,47,18] that reduces the feature dimension by half, our approach uses a much higher reduction rate (1/8) for the part model.

There are some advantages of using PCA. On the efficiency side, reducing the dimensionality of the feature has an obvious benefit. In a bag-of-features approach, the description vectors are compared using the L
                        2 distance. Considering that we need to match 10K features with 4K words for each clip, the reduction in dimensionality to 1/8 for part model results in significant benefits in terms of computational efficiency. On the performance side, there may be positive impact and practical meaning when applying robust feature encoding strategies, such as Fisher vector(FV) encoding [47] or VLAD encoding [47], a simplified version of the Fisher vector. Fisher vector extends the BOF by encoding high-order statistics between the descriptors and a Gaussian Mixture Model (GMM). Due to the fact that decorrelated data can be fitted more accurately by a GMM with diagonal covariance matrices, it is advantageous to apply PCA dimensionality reduction on Fisher vector. In addition, for a D dimension descriptor, the FV signature with K words has an increased dimension of K(2D
                        +1) (VLAD has a dimension of KD). Therefore, reducing the feature dimension makes the LPM approach more suitable to Fisher vector and VLAD solutions.

For each given sample point (x,
                     y,
                     t,
                     σ,
                     τ), a feature descriptor is computed for a 3D video patch centered at (x,
                     y,
                     t). The descriptors are critical for the performance of the video recognition. In the literature, there are four popular local descriptors which are often used to encode local motion pattern and structure information. These descriptors include gradient-based HOG [48] and HOG3D [39] and flow-based HOF [4] and MBH [50]. We will evaluate the performance of LPM on these descriptors.

The gradient-based HOG and HOG3D descriptors are very efficient to compute. Both HOF and MBH need to compute dense optical flow, which is computationally expensive. For efficiency purpose, if no specified otherwise, we compute gradient-based descriptors with full resolution, and compute flow-based descriptors with half resolution. For both HOG and HOG3D, the gradient computation is performed on greyscale images.

We evaluate our method on four public action benchmarks, Hollywood2 [9], UCF50 [10], HMDB51 [8] and UCF101 [11] datasets. We randomly sample 3D patches from the dense grid, and use them to represent a video with a standard bag-of-features approach. To generate codewords, we randomly select 120,000 training features, and use k-means to cluster them into 2000 and 4000 visual words.

The sampled 3D patches are represented by descriptors, and the descriptors are matched to their nearest visual words with Euclidean distance. The resulting histograms of visual word occurrences are fed into a non-linear SVM implemented by LIBSVM [51] with histogram intersection kernel [52]. For multi-class SVM, we use one-versus-all approach, which is observed [53] to have better results than one-against-one multi-class SVM.

To combine multiple channels of different descriptors, most methods use RBF-χ
                     2 kernel [54,55]:
                        
                           (4)
                           
                              K
                              
                                 
                                    x
                                    i
                                 
                                 
                                    x
                                    j
                                 
                              
                              =
                              exp
                              
                                 
                                    −
                                    
                                       
                                          ∑
                                          c
                                       
                                       
                                          1
                                          
                                             A
                                             c
                                          
                                       
                                       D
                                       
                                          
                                             x
                                             i
                                             c
                                          
                                          
                                             x
                                             j
                                             c
                                          
                                       
                                    
                                 
                              
                              ,
                           
                        
                     where D(x
                     
                        i
                     
                     
                        c
                     ,
                     x
                     
                        j
                     
                     
                        c
                     )) is the χ
                     2 distances between the samples for the c-th channel, and A
                     
                        c
                      is the mean value of the χ
                     2 distance between the training samples for the c-th channel. While this approach produced “comparable results” [55], we argue that the mean value of the χ
                     2 distance is not representative to the discriminative power of each individual channel. For instance, in our experiments, the MBH outperforms HOG on HMDB51 dataset by a large margin (56.4% vs 28.3%). Therefore, it is intuitive to add more weight to descriptors with higher classification power. We propose a histogram intersection kernel for multi-channel classification:
                        
                           (5)
                           
                              
                                 K
                                 
                                    I
                                    
                                    H
                                 
                              
                              
                                 
                                    x
                                    i
                                 
                                 
                                    x
                                    j
                                 
                              
                              =
                              
                                 
                                    ∑
                                    c
                                 
                                 
                                    
                                       w
                                       c
                                    
                                    
                                       ma
                                       
                                          x
                                          
                                             k
                                             ∈
                                             c
                                          
                                       
                                       
                                          
                                             w
                                             k
                                          
                                       
                                    
                                 
                                 min
                                 
                                    
                                       x
                                       i
                                       c
                                    
                                    
                                       x
                                       j
                                       c
                                    
                                 
                                 ,
                              
                           
                        
                     where w
                     
                        c
                      is classification accuracy for the c-th channel, which can be learnt from the training data. max
                     
                        k
                        ∈
                        c
                     (w
                     
                        k
                     ) is the maximal value from w
                     
                        k
                      of all channels.

One advantage of this approach is its computational efficiency. Our histogram intersect kernel is similar to the approach in Spatial Pyramid Matching [29]. It is simply a weighted sum of histogram intersections. Given that w
                     
                        i
                     
                     min(a,
                     b)=min(w
                     
                        i
                     
                     a,
                     w
                     
                        i
                     
                     b) for positive numbers, we can concatenate the weighted histograms of all channels, and use a single efficient intersection kernel SVM [56].

Recent studies [18,19] show that Fisher vector [47] can improve performance over standard bag-of-features methods on action recognition. To provide a direct and fair comparison with existing methods, we also evaluate our methods with Fisher vector encoding. Following Wang et al. [18] we use improved fisher vector [57] by applying the signed square-rooting followed by L
                     2 normalization. We set the number of visual words to K=256 and randomly sample 150,000 features from the training set to estimate the GMM and learn PCA projection matrix. The resulting Fisher vectors are fed into a linear SVM with C
                     =32.5. To combine different descriptors for Fisher vector, we simply concatenate the computed FVs from the respective channels.

The Hollywood2 dataset [9] collects videos from Hollywood Movies. Although it only has 12 classes and 1707 high quality clips, it is a very challenging dataset due to large intra-class variation, multiple persons, camera motion, unconstrained and cluttered background etc. Following [9], we compute the average precision (AP) for each class, and the performance is evaluated by the mean AP (mAP) over all classes.

The UCF50 dataset [10] contains 50 classes and 6680 realistic videos taken from YouTube. The videos are subdivided into 25 groups, where each group consists of a minimum of 4 action clips. The video clips in the same group may have similar background or be played by the same subjects. The dataset is very large and relatively challenging due to camera motion, cluttered background, large scale variations, etc. We report Leave-One-Group-Out (25-fold group-wise) Cross-Validation.

The UCF101 dataset [11] is by far the largest human action dataset with 101 classes and 13,320 realistic video clips taken from YouTube. As an extension of UCF50 dataset, it includes all clips of 25 groups from UCF50 and adds 51 categories. The dataset is very large and relatively challenging due to camera motion, cluttered background, large scale variations, etc. We follow the original experimental setup of the authors by reporting mean accuracy over three distinct training and testing splits. For split 1, split 2 and split 3, clips from groups 1–7, groups 8–14 and groups 15–21 are selected respectively as test samples, and the rest for training.

The HMDB51 dataset [8] is perhaps the most realistic and challenging action dataset. It has 51 action categories, with at least 101 clips for each category. The dataset includes a total of 6766 video clips extracted from Movies, the Prelinger archive, Internet, Youtube and Google videos. Three distinct training and testing splits have been selected from the dataset, with 70 training and 30 testing clips for each category. We use the original non-stabilized videos with the same three train-test splits as the authors [8], and report the mean accuracy over the three splits in all experiments.

We will focus our experiments on UCF101 and HMDB51. However, we will also compare our performance on UCF50 and Hollywood2 with state-of-the-art. Fig. 2
                         shows the sample of frames from the datasets.

There are few parameters for our method, which determine the feature dimensions. Our parameters are optimized for fast processing at half the spatial resolution of the tested datasets. In addition, we use the simplified HOG3D, HOG, HOF and MBH descriptors, mainly by controlling the total number of cells to reduce the dimensionality.

The root patches are randomly sampled from the dense sampling grid of the processed video at half the resolution. For each root patch, we sample 8 (2×2×2) overlapping part patches from the full resolution video. The histograms of 1 root patch and 8 part patches are treated as two separate channels. The root channel has the same dimension as that of the used descriptor, but the histograms of 8 part patches are concatenated to create a part channel with 8 times the dimension of the root channel.

We test our method with a feature dimension of 96 instead of the default 960 in [39]. The parameters are: number of histogram cells M
                           =2, N
                           =2 (default M
                           =4, N
                           =3); number of sub-blocks 1×1×3; and polyhedron type dodecahedron(12) with full orientation. The minimal patch size is 24×24×14 for full resolution and 16×16×10 for half resolution. With one HOG3D descriptor at dimension of 96 (2×2×2×12), our local part model feature has a dimension of 96 for the root channel and 768 for the part channel.

The minimal patch size is 20×20×14. Each patch is subdivided into a grid of 2×2×2 cells, with no sub-block division. With 8 bins quantization, one descriptor of HOF, MBHx or MBHy has a dimension of 64. This leads to a root channel of dimension 64 and a part channel of dimension 512.

The minimal patch size is 24×24×14. Each patch is subdivided into a grid of 2×2×2 cells, with no sub-block division. With 8 bins quantization, one descriptor of HOG has a dimension of 64. The local part model feature thus has a dimension of 64+512.

Normalization is very important to the performance. For all descriptors used, we apply the L2-norm over the feature vectors. For the part channel, we apply L2-norm over the histogram of each patch (with dimension of 64), and then renormalize the whole concatenated vector.

For bag-of-features normalization, we simply apply L1-norm to convert the feature into unit-length vector to eliminate the difference between short and long documents. We do not use the tf–idf method. Our experiments show no significant improvement when using tf–idf scheme.

@&#RESULTS@&#

This section evaluates our Local Part Model and random sampling strategies on four datasets, HMDB51, UCF101, UCF50 and Hollywood2. To compensate for the random sampling, we repeated every experiment 3 times, and reported average accuracy and standard deviation over 3 runs. For all descriptors, we simply concatenated histograms of word occurrences from root and part channels without weight. Same strategy was applied to combine MBHx and MBHy channels. However, to combine multiple descriptors, we used the weighted multichannel approach (c.f. Eq. (5)). Unless stated otherwise, the optical flow was computed with Farnebäck's algorithm [58] that exhibited a good compromise between speed and accuracy.

We first investigated the effectiveness of the proposed local part model by evaluating different deployments on MHDB51 and UCF101 datasets with four descriptors. We computed the part model at the half spatial resolution of the original videos, and the root model at 1/4 resolution. Table 3
                         summarizes results of different models. For all descriptors, we see that the use of both root and parts improves the recognition accuracy, which demonstrates the effectiveness of our local part model. In addition, the part model shows better performance than the individual root model in all experiments. This is expected as there are 8 times as many part patches as there are root ones, and the root patches are sampled at half the resolution of the video on which we compute the part patches.

Note that the part channel includes 8 overlapped patches, which add local relations. Also, the root channel provides additional global information. With both root and part channels, the performance is improved on root channel with up to 4%(HOF) on UCF101 and 3.8% (HOG) on HMDB51. In comparison with part channel, the biggest improvements are 2.4% on HMDB51 and 1.8% on UCF101. As a comparison, the popular spatio-temporal pyramids (STP) [4], an extension of spatial pyramids for images [29], have shown the improvements of 1.4% on UCF50 and 1.9% on HMDB51 [54]. However, the STP in [54] has 6 channels by including 6 grid structures, we only use two channels.

Motion analysis algorithms have been developed for decades, but the state-of-the-art optical flow methods do not always produce the expected results for many real-world video sequences [60]. In computing optical flow for MBH descriptor, we follow Wang et al. [54] and adopt Farnebäck's algorithm [58] as a good trade-off between speed and accuracy. In this section, we will explore the potential benefits of using other state-of-the-art optical flow algorithms.

Wang et al. [54] compared the efficient Farnebäck optical flow algorithm with a state-of-the-art method, large displacement optical flow (LDOF) from Brox and Malik [61]. They found that the overall performance of the two optical flow algorithms is similar. To estimate very fast moving small human body parts, LDOF combines descriptor matching and the continuation method to calculate arbitrarily large displacements. However, adopting descriptor matching in optical flow estimation may lead to difficulties in distinguishing small/slow motions, which has equal importance in human action recognition. Also, the use of descriptors based on spatial histograms may result in inaccuracies at motion discontinuities. Discontinuities in the flow field often appear in area with high image gradients, which is critical to motion boundary encoding of the MBH descriptor. When using LDOF on MBH, Wang et al. [54] observed a 3% improvement over YouTube dataset and a 0.8% drop in performance over Hollywood2 dataset. This could attest our aforementioned hypothesis because YouTube dataset contains sports video with fast moving body parts, whereas Hollywood2 dataset has many slow motions, such as “Kiss” and “AnswerPhone”.

To this end, we explored a more accurate discontinuity-preserving optical flow algorithm. We evaluated the duality-based TV_L1 (Dual_TV_L1) method from Zach et al. [59]. It is based on the classical homogeneous regularization method of Horn and Schunck [62]. The original work by Horn and Schunck uses quadratic L
                        2-regularity, which does not allow for discontinuities in the optical flow fields. Neither does Farnebäck's algorithm. The Dual_TV_L1 employs L
                        1-regularity to better preserve discontinuities.


                        Table 4
                         shows the performance comparison on MBH descriptor built on two different optical flow methods, Farnebäck and Dual_TV_L1. For both methods, we used the OpenCV implementation with default parameters for Dual_TV_L1 and optimized parameter for Farnebäck. On all three datasets, the MBH descriptor from Dual_TV_L1 achieves significantly better performance. One possible explanation for such improvement is that the Dual_TV_L1 produces more accurate flow field both over human motions and at motion discontinuities.

For HOF descriptor, one very interesting observation is that the performance drops by around 1–2% when using Dual_TV_L1 method. Brkić et al. [63] also observed slight performance penalty with Dual_TV_L1 when using HOF-like descriptor. One possible reason is that Dual_TV_L1 and LDOF may estimate flow field with more accurate camera motion, which offsets the benefit of getting more accurate human motions. Compared with MBH, the HOF has no mechanism to reduce the effect of camera motion. The results of dense trajectories [54] show higher performance penalty (around 4%) for HOF when using LDOF. One possible explanation is that the trajectory tracking in dense optical flow field may also be affected by camera motion. The MBH descriptor could also be impacted by trajectory tracking. Unlike dense trajectories, we do not track points over dense optical flow field, and the optical flow computation only affect the MBH and HOF descriptors alone. Therefore, the MBH descriptor in our system benefits more directly from the improvement of the dense optical flow estimation.

Note that the Dual_TV_L1 is computationally expensive. However, Zach et al. reported a real-time GPU implementation in [59].

In bag-of-features approaches, a standard step is vector quantization which matches extracted features to their nearest visual words, most often using Euclidean distance. Fast approximate nearest neighbor search (FLANN) method is a widely used technique to handle large databases in computer vision applications. However, as an efficient approximate search method, it often compromises performance by introducing approximation errors. On the other hand, brute-force matching produces exact results at the price of an increased computational cost given the high feature dimension space and the large database size. To improve the computational efficiency of brute-force matching, the feature space can be reduced by applying PCA dimensionality reduction.

To learn PCA projections, we computed the covariance matrix based on all 4000 and 2000 visual words extracted from k-means. Then, the eigenvectors associated with the most energetic eigenvalues from the covariance matrix were stored in memory and used as the projection matrix M for dimensionality reduction. We first applied PCA on all codewords to reduce the root vectors from 64 to 32 and the part vectors from 512 to 64. The vectors from sampled ST patches were also reduced, and then matched (brute-force) to the codewords with reduced dimensionality.


                        Figs. 3 and 4
                        
                         illustrate the performance comparison of different BOF matching methods on HMDB51 and UCF101 dataset when using 2K and 4K visual words. First, in all cases, there is a clear performance benefit on using 4000 visual words rather than 2000 visual words. Also, in comparison to 4K words, there is a higher performance penalty for 2K words when using FLANN or PCA instead of brute-force. One possible reason is that vector quantization with fewer words is more prone to quantization errors due to approximation, which may result in similar ST patches being assigned to different visual words in different clips. Such penalty is more significant on HMDB51 dataset, which is more challenging and therefore more sensitive to quantization errors.

For 4000 visual words, applying PCA to reduce dimensionality shows no drop in performance penalty except for 0.5−1.5% for HOF. As discussed in Section 3.4, this is a clear advantages considering we reduced the dimension of the part model to 1/8. We will evaluate the computational efficiency of these different BoF matching methods in Section 6.5.


                        Table 5
                         shows the comparison of our method with the state-of-the-art. We chose the parameters listed in Section 5.2. For all descriptors, we used 4000 codewords for both the root and the part channels. We combined four descriptors (as shown in Table 5) with the weighted histogram intersection kernel (c.f. Eq. (5)). For Fisher vector (FV) encoding, we used VLFeat library [64] with K
                        =256, and combined three descriptors by concatenating their respective FVs.

On HMDB51, our method achieves 59.6% on single MBH descriptor and 61.0% on combined descriptors, which outperforms the state-of-the-art result (57.2% [18]) by 3.8%. With MBH computed from Farnebäck optical flow method, our accuracy is 58.4%. Note that our results are obtained with three descriptors (HOG+HOF+MBH) and Fisher vector encoding, whereas Wang et al. [18] use four descriptors (trajectories+HOG+HOF+MBH) and extensive camera motion estimation.

On UCF50, our result (92.0%) slightly surpasses the state-of-the-art (91.2% [18]). On UCF101, an extension of the UCF50, we report 85.4% with single MBH descriptor and 86.6% with the combined three descriptors. It also slightly outperforms the state-of-the-art result [18] (85.9%) which is obtained with Fisher Vector encoding and extensive camera motion estimation. When using Farnebäck optical flow method, we achieve similar results (85.4%). Unlike [18], our method doesn't use camera motion compensation and human detection to improve the performance.

On Hollywood2, we further improve state-of-the-art results by 1% (65.3%). Note that our new results significantly outperform our previous results (Previous LPM [13]) on HMDB51 by 13.4% and on UCF50 by 8.7%. In this previous approach [13], we used one-verse-one multi-class SVM, single channel LPM method and Farnebäck optical flow estimation, which all lead to the lower performances.

Our method demonstrates very good performance on large scale challenging datasets with more realistic scenarios. One possible explanation for such good performances may reside in our random sampling conducted on a very high dense sampling grid. With 90K patches per video on HMDB51, we have around 900 patches per frame, which is much more than [54]. Compared with approaches that use interest point detectors, we have more patches sampled from the test videos, and with uniform random sampling our method also includes correlated background information. Such background information may improve discriminative power for recognition on real-life videos.

We also tested the computational complexity on all four descriptors. Brute-force, FLANN and PCA methods were evaluated and compared. Except for codewords matching, all other stages remain the same. The runtime was estimated on an Intel i7-3770K PC with prototype implemented in C++. We set only one core active @ 3.5GHz in Bios, and disable both Hyper-threading and Turbo-boost. For all experiments, 4K words were used. We used the default parameters as in Section 5.2, and randomly chose 10K features (10K root patches +80K part patches) from each clip with up to 160 frames. The optical flow for MBH and HOF was computed with Farnebäck algorithm. The runtime for HOG was estimated with the original datasets' video resolution, and for the other descriptors we used half the original resolution.


                        Table 6
                         summarizes the efficiency comparison at different stages for the HMDB51 and UCF101 datasets when using different descriptors. Applying feature reduction with PCA on Brute Force matching results in over 3× speed-ups for “BoF matching” and 2× speed-ups in total system process. The FLANN is found to speed up matching process by 18×. The detailed results are listed in Table 6.

In general, considering random sampling variance, there is no significant performance difference among the three matching methods. The PCA matching performs similar to the Brute-force one. This is very interesting results since we reduced the dimension of part channel to its 1/8. The largest penalty for FLANN is 0.8% on HMDB51 for MBH descriptor. Note that our previous work [13] showed larger performance drop for FLANN. The possible explanation is that using separate root and part channels makes it more robust to approximation errors.

Compared with existing methods, a major strength of our method resides in its very high computational efficiency. We achieved state-of-the-art classification results by analyzing the videos at half the resolution (except for HOG descriptor). Also, by using randomly sampled ST patches, we are able to use integral video to accelerate the processing. In [18], the use of curved trajectories imposes the use integral image only. It also employs very expensive camera motion compensation method and state-of-the-art human detection algorithm to improve the performance.

@&#CONCLUSIONS@&#

This paper introduces an action recognition system based on a multiscale local part model. High performance is obtained by including both a coarse global local root model and high resolution part model made of overlapping patches. Our system still exhibits excellent performance even after a significant dimensionality reduction to 96 (root 32+part 64) dimensions, i.e., of the same size as a single original HOF/HOG/MBHx/MBHy vector. We introduced the idea of using very high sampling density for accurate classification. A random sampling strategy is also proposed for efficient action recognition. Our system experiments showed state-of-the-art results on realistic large scale datasets.

The most notable conclusion is that, without losing efficiency, random sampling with very high density can generate larger numbers of patches, and therefore achieves good performance. Another very important fact for the performance resides in the local motion descriptors, which ideally should accurately encode both local structure and motion information. As one of such descriptors, MBH has been shown to outperform other descriptors by encoding motion boundary and suppressing camera motion. Yet, a more accurate optical flow estimation can significantly improve the MBH performance.

Our method is capable of recognizing realistic human action on large scale video. It could also be applied in the context of action localization, abnormal detection and video retrieval. In the future, we would like to apply salient feature sampling. We also want to explore the different combinations of parts or deeper part hierarchies (i.e., parts with parts).

@&#REFERENCES@&#

