@&#MAIN-TITLE@&#A file-deduplicated private cloud storage service with CDMI standard

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A private cloud storage service with file deduplication mechanism is proposed.


                        
                        
                           
                           We also integrate the Cloud Data Management Interface (CDMI) standard in the proposed system.


                        
                        
                           
                           Superiority of transmission and storage efficiency by comparing with the existing system “Gluster Swift”


                        
                        
                           
                           Much suitable for the service environment where most of the transmitted data are small files


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Cloud storage

Private cloud

Data deduplication

CDMI

DFS

@&#ABSTRACT@&#


               
               
                  The emergence of cloud environments makes users convenient to synchronize files across platform and devices. However, the data security and privacy are still critical issues in public cloud environments. In this paper, a private cloud storage service with the potential for security and performance concerns is proposed. A data deduplication scheme is designed in the proposed private cloud storage system to reduce cost and increase the storage efficiency. Moreover, the Cloud Data Management Interface (CDMI) standard is implemented in the proposed system to increase the interoperability. The proposed service provides an easy way to for user to establish the system and access data across devices conveniently. The experiment results also show the superiority of the proposed interoperable private cloud storage service in terms of data transmission and storage efficiency. By comparing with the existing system Gluster Swift, the proposed system is demonstrated much suitable for the service environment where most of the transmitted data are small files.
               
            

@&#INTRODUCTION@&#

Cloud storage is a service model where data is maintained, managed and backed up for users over network. Different from traditional local device storage, in general, cloud storage is an on-demand self-service which can be easily accessed via standard Internet APIs and communications protocols [1]. Since cloud storage service provides users with abundant storage space and gives users convenience in synchronizing files across platform and devices, it is now adding its appeal for many citizens. The different forms of cloud storage design are public cloud storage and private cloud storage. Public cloud storage, such as Dropbox [2] and Google Drive [3], are provided by service providers and the storage infrastructure is hosted by the cloud vendor at the vendor's premises. The charge of the storage is depended on how many resources the user used [4] and it can scale storage space up or down elastically in accordance with the request. The data of different customers are likely to be stored at the service provider site and mixed together on the cloud storage systems. The public cloud storage makes immediate acquiring of data more convenient for the user. However, the data security and privacy are still critical issues in public cloud environments, especially for the exclusive clients who care about the privacy and ownership of files. Chen and Zhao point out that data security and privacy issues are the top concerns of consumers, especially for large enterprise [5]. It is difficult to set powerful security boundaries and protect data privacy in cloud storage. After surveying with 402 participants, Ion et al. [6] also indicate that most of the cloud storage users are worried about data privacy issue. Hence, the private cloud storage appeared on the scene.

Different from public cloud storage, private cloud storage [7] is built for the exclusive use of one person or organization with low bandwidth environment. It is suitable for the users who want to have customization service or who don't trust the public cloud storage vendors. The users of private cloud storage must establish the infrastructure personally and they just need to download the software from private cloud service provider and install it on their own hardware. Consequentially, private cloud storage is equipped with the attributes of public cloud storage and protects the security of private files meanwhile. However, there are still some aspects of private cloud storage service that need concern. Firstly, since private cloud storage users have the responsibility to establish the infrastructure, the space of storage system has to be used in an efficient way. According to the research of Symantec [8], there are 37% duplicated data in Taiwan enterprises and 42% duplicated data in global enterprises. The duplicate copies of data would result in redundant consumptions of storage space and network bandwidth. Moreover, most of the files transmitted in enterprise are images or documents with small size. Therefore, how to efficiently reduce the redundant cost of storage spaces, especially dealing with small files, has already been a complex and challenging issue for enterprises [9]. Secondly, elasticity is also an essential factor for a private cloud storage system. An elastic system means that the potential system must have the ability to dynamically add or reduce resource according to the requirements. The private cloud storage owner would easily manage storage and get resource benefit if the underlying infrastructure provided primitives for elasticity. Thirdly, the interoperability of a storage system with international standard interface should be treated as another important characteristic. The unique interface released by a single vendor would remain under the change control of the vendor. It may be essentially locking customers into that service. Compared to the storage system with unique interface, the storage system with a standard interface which accommodates requirement from multiple vendors and can be extended for proprietary functions would be more convenient and less restrictive for users.

To solve the issues mentioned above, this research constructs a data deduplication private cloud storage system with Cloud Data Management Interface (CDMI) [10] standard based on the fundamental distributed file system (DFS). A data deduplication scheme which can detect and remove the redundant data in the storage is designed in the proposed system to reduce cost and increase the storage efficiency. The deduplication algorithm is also designed to be suitable for the service environment where most of the transmitted data are small files. Moreover, the international standard interface CDMI is implemented in the proposed system to increase the interoperability. Finally, Gluster [11] is chosen as the fundamental DFS in our proposed private cloud storage system. Integrating with distributed file system as back end storage makes the proposed system have the elasticity, good performance at backend storage, and be managed conveniently [12]. By integrating these characteristics, the proposed private cloud storage service can provide users an easy way to establish the system and access data across devices conveniently. The experiment results also demonstrate the superiority of the proposed system in terms of data transmission and storage efficiency.

The remainder of this paper is as follows. Section 2 provides background and related works of the proposed system; Section 3 elaborates on the proposed system's architecture, components and the details of working; Section 4 describes the experimental results; and, finally, we give concluding remarks and future works in Section 5.

@&#BACKGROUND@&#

Data deduplication is a technique to optimize the storage space. This technique keeps only one copy of all identical part in the storage system without saving the redundant part. Since the amount of data is growing with astonishing speed, the enterprise would not only save cost on physical devices by data deduplication but also manage data efficiently. From the side of where data is processed, deduplication can be divided into target deduplication and source deduplication [13]. Target deduplication happens after front-end user, which is client, transmits files to back-end storage server and redundant data is eliminated at the back-end storage side, which is server. The storage device has the responsibility for processing the redundant data without influencing client's operating. It doesn't consume client's resource and they don't know the deduplication is occurring. On the other hand, source deduplication does data deduplication before it is transferred. There are some calculations on client side, like calculating hash value. Source deduplication consumes client's resource but it has the advantage of saving the network traffic bandwidth [14].

Deduplication can be further classified into two approaches by the unit of comparison. One is chunk level deduplication and the other is file level deduplication. Data Domain Deduplication file System (DDDFS) [15] is one of the file system which performs chunk level deduplication. It divides the file into variable sized chunks and uses Secure Hash Algorithm (SHA-1) [16] to find the hash value of each chunk. Each chunk is checked with a set of chunk indices maintained in chunk store for duplicate detection. Hence, it eliminates duplicate chunks of data even if the corresponding files are not identical. With performing file-level deduplication, Xu et al. [9] designed a file deduplication framework on Hadoop system, where Secure Hash Algorithm 2 (SHA-2) [16] was utilized to conduct the data mapping through the whole file. Therefore, it eliminates duplicate copies of the same file instead of chunks. Lokeshwari et al. [17] further proposed an optimized cloud storage with high throughput deduplication approach, where chunk level and file level deduplication methods were both implemented. They discussed the approach of deduplication from two dimensions, i.e. efficiency and throughput, in private cloud storage environment. Chunk level deduplication has the advantage of better efficiency, which means the degree of how many space is saved. However, chunk level deduplication has lower throughput than file level deduplication. The throughput means the overhead generated by the process of deduplication. Besides, in the study of practical deduplication, Meyer [18] found that file level deduplication should be a highly efficient way to lower storage consumption in the situation with sparseness.

In public cloud environment, the stored data are usually encrypted, since security and privacy are the main concerns from users' perspective for data outsourcing. However, traditional deduplication approaches are incompatible with the encrypted data in the public cloud. To secure the data in public cloud while realizing deduplication, Douceur et al. presented a convergent encryption approach [19], where identical data copies will generate the same convergent key and the same cipher text. Therefore, it allows the deduplication procedure performed on the cipher texts. To further achieve efficient and reliable secure deduplication, Li et al. [20] proposed a new construction called Dekey for convergent key management on both user and public cloud storage sides. They utilized the secret sharing techniques and apply deduplication to the convergent keys, which significantly limited the storage overhead in realistic environments. Later. Li et al. also presented a secure authorized deduplication approach based on [21]. In their approach, the private cloud is regarded as a proxy to allow users to securely perform duplicate check with differential privileges. The data operation is managed in private cloud while the users only outsource their data storage by utilizing public cloud.

The studies of secure deduplication are mainly presented on account of the data outsourcing in public cloud environment. If the users' data was stored in the private cloud system and managed by themselves, the security and privacy will not be the main concerns from users' perspective. Instead the designed storage system should give the priority to the convenience and reliability. The goal of the designed proposed system can be concluded as providing a convenient and efficient private cloud storage system. Therefore, with the comparison and analysis of different deduplication strategies mentioned before, source deduplication with file level deduplication strategy will be adopted in our proposed system.

The Cloud Data Management Interface (CDMI) [10] defines the functional interface that can be used by applications to create, retrieve, update and delete data elements from the Cloud. A CDMI client has the ability to find the capabilities of the cloud storage offering and this interface allows a CDMI client manages containers and the data. Further, this interface defines the rule about how to set metadata on containers and the data which is contained in them. The administrator also can use the interface to manage containers, accounts, security access and monitoring/billing information [22]. CDMI is used in a cloud by defining Representational State Transfer (REST) HTTP operations. The basic flow of CDMI is illustrated in Fig. 1
                        . CDMI client communicates with CDMI server upon RESTful protocol and the methods include PUT, GET, DELETE, and so on. After CDMI server receives the request, it sends response according the status of work.

REST is an efficient software architectural style which was introduced and defined in 2000 by Fielding and Taylor [23]. There are some reasons mentioned by Luo and Li [24] on using REST approach for cloud storage:
                           
                              (1).
                              Simplicity rules;

Common infrastructures in various languages on various platforms;

Low learning curve leads to developers' adoption;

Developers' adoption creates eco-system around API;

Eco-system eases adoption by vendors and customers;

Scale-out implementation feasibility.

Nowadays, as an international standard interface, CDMI is now adding its appeal for many organizations. More and more commercial cloud storage vendors and academic communities have deployed CDMI compatible systems. It has huge impact in cloud storage field because of promoting the interoperability. The unique interface released by a single vendor would remain under the change control of the vendor [25]. It may essentially be locking customers into that service. Compared to the storage system with unique interface, the storage system with CDMI which accommodates requirement from multiple vendors and can be extended for proprietary functions would be more convenient and less restrictive for users [26].

Gluster is an open source, distributed file system capable of scaling to several petabytes and handling thousands of clients [11]. It is designed to provide a good performance across a wide variety of workloads and equipped with some characteristics, such as, elasticity, linear scale-out and convenient user space [27]. For an enterprise, it is important that a storage system has the ability to add or remove resources to a storage pool when users need and not interrupt the running system. Gluster was designed to fit this goal. It allows users add or delete resource flexibly without disrupting the system. Actually, Gluster can scale out linearly in performance and capacity. It means that an enterprise can aggregate lots of the weaker CPU, disks and other resources to create a more powerful storage pool. The characteristic of Gluster is its unique architecture to avoid the overhead and risk while managing many nodes at the same time.

Gluster Swift is one of the storage systems which use Gluster as a backend storage layer. It integrates the OpenStack Swift API [28] implementation from OpenStack with the Gluster and enables files or directories created on any file system to be accessed as objects via the Swift API. Gluster Swift is a highly available, distributed, eventually consistent object storage system. It allows users to store and retrieve files and content through a simple Web Service interface as objects. Organizations can use Swift to store lots of data efficiently, safely, and cheaply. Nevertheless, Gluster Swift is only suitable for Swift API which is less interoperable than CDMI.

The proposed system is designed for providing an easy way for users to establish their private cloud storage and access data across devices conveniently. GlusterFS which plays the role of powerful backend storage ensures the elasticity of the proposed system. In addition, with integrating the Cloud data management interface (CDMI) standard, users could efficiently interact with the proposed system and access their files. File deduplication is one of the characteristics in our proposed system for eliminating duplicate copies of repeating data in storage space and saving bandwidth. We design a deduplication mechanism to users for choosing whether they agree to share file or not. While a file is going to upload, the system will ask if the user wants to share this file. If the answer is “Yes”, the file will be marked as a sharable file for deduplication. Otherwise, deduplication mechanism would not applicable to this file. The detailed architecture and workflows of the proposed system will be illustrated in Sections 3.1 and 3.2, respectively.

As Fig. 2
                         illustrates, the architecture of the proposed private cloud storage system contains five components, which are Client, Front-end node, Database, Adaptor node and Storage node. Client communicates with Controller in Front-end node and do information exchange. Apache server in Front-end node redirects the requests which are sent from CDMI request sender to enhance load balance. Adaptor node receives CDMI request and stores files via Gluster client. Storage nodes consist of GlusterFS server, which can create different types of volume for different purposes. The details of the proposed components are elaborated in the following sub-sections.

The Client component consists of Operate interface, Hash generator and CDMI request sender. Operate interface provides a clean and user friendly interface to users that makes them easy to login the system and operate four functionalities, upload file, download file, create folder, and delete file. The workflows of functionalities will be illustrated in Section 3.2 in detail. After user's operation, Operate interface exchanges information with Controller to make the system work. For the users who may not be willing to share their files with others, we also design an alternative mechanism to users for choosing whether they agree to share file or not. While uploading a file, the system will ask if the user wants to share this file. The user's answer will be sent to Controller accompanied with hash value and saved into in Database. If the answer is “Yes”, the file will be marked as a sharable file for deduplication. If the user's answer is “No”, the deduplication mechanism would not be applicable to this file, which means that even if there is already an identical file in the storage system, the workflows of this file will still belong to the scenario where file is not duplicated.

Hash generator is designed in the proposed system for the purpose of file deduplication. It receives a file as input, and output a unique hash value by using hash algorithm SHA-2 (Secure Hash Algorithm 2). SHA-2 is a set of cryptographic hash functions designed by the National Security Agency (NSA) and published in 2001 by the NIST as a US Federal Information Processing Standard [16]. The calculated hash value of this file is saved in the File metadata in Database accompany with the file information. We use the following two conditions to determine whether the files are the same: (a) check if the calculated file's hash value is the same as any hash value of shareable files in File metadata; (b) if cached hash value hit in the File metadata, then check whether the file size is the same; To avoid the SHA hash collision, the duplicated files with the same content will be detected only if the two conditions are met. Once any condition is not met in the comparison phase, the comparison will be immediately stopped to save computing and memory resources.

The CDMI request sender sends CDMI content type request to the Apache Server according to user's operation, which includes file content upload, file content download and folder creation. Furthermore, the file metadata transmission and file content transmission is separated in the proposed system to bring it into play of the cloud characteristic-parallelized [29]. However, in CDMI, file content is encoded to base64 and put into JSON string for transmitting. When the file size is too large, the CDMI server can't handle this string. Thus, we make large file transmission as some small files transmission: Clients will get file's size and compute how many small files have to be transmitted. We add index in the end of these file's name. When users want to download file, the proposed system can follow the file name, file size, and the index to download file.

The Front-end Node works out with Controller and Apache server. The Controller provides an interface to system administrator. The button on the interface includes add user, and update user's information. As a cloud storage system, the system's performance would descend while many users operate the system and access their data simultaneously. To solve this bottleneck, we use apache server as the load balancer. There are three different load balancer scheduler algorithms that can be used in the proposed system: Request counting, Weighted traffic counting and Pending request counting [30,31]. According to incoming requests and load factor, Request counting algorithm distributes these requests to backend workers and each worker gets a proportional number of requests by load factor. Weighted traffic counting is similar with Request counting but has a little difference, where the weighted traffic counting deals with the numbers of byte rather than numbers of requests by load factor. In Pending request counting algorithm, the scheduler keeps track of how many requests each worker is assigned at present. When a new request is coming, then it will be assigned to the worker that has least request in the waiting queue. As a result, each request will be processed fastest ideally. In the proposed system, system manager can choose adaptable load balancer scheduler algorithm to achieve better performance depending on the specific situation.

MYSQL is adopted as the database component in the proposed system and queried by Controller to access User profile and File metadata. User profile includes information of user's account, password and user space. File metadata includes the information of each file, which are file size, ownership, share status, hash value, file path and real path. Share status represents whether this file is sharable for duplication or not. Hash value records the file's hash value, which is the main fingerprint along with file size for the detection of duplication file. File path records the virtual file's path for the corresponding user. Real path records the real path of the file stored in the backend storage. Controller must insert all of the file's metadata while each file is uploaded. The information is used to establish a virtual middle layer file system [32] for the purpose of file deduplication.

Adaptor node consists of CDMI server and GlusterFS client. The SNIA Reference Implementation is adopted as the CDMI Server to receive CDMI request. The way how we store the files to GlusterFS server is that: while the file is going to store in the GlusterFS server, the connection between GlusterFS client and GlusterFS server is first established via mount. After CDMI server receives the request from Apache server, the target is then stored to the folder which is mounted to GlusterFS volume. The different types of GlusterFS volume will be illustrated in the following sub-section.

GlusterFS is implemented as the Storage node. There are three types of GlusterFS volume that can be chosen adaptively for different purposes [33]. The first one is distributed volume, where files are distributed to various bricks of the volumes. It can suffer significant data loss during a disk or server failure because directory contents are spread randomly across the bricks in the volume. The second one is replicated volume, where files are copied and put on various bricks of the volumes. It provides high availability of data when nodes failures occur. The last one is striped volume, where files are split into different chunks and put across bricks in the volumes. By taking advantage of the different volume characteristics, GlusterFS plays a powerful backend storage role in the proposed system. In the proposed system, system manager can also choose adaptable GlusterFS volume to achieve better performance depends on the specific situation.

The proposed private cloud storage service provides three main functionalities, which are upload file, download file, and delete file. The workflows of the functionalities are presented in this sub-section.

For the implement of upload functionality, Fig. 3
                         shows the upload workflow while file is not duplicated. Firstly, Hash generator in Client calculates a hash value of the file that will be uploaded and sends it to Controller. Controller will compare the hash value with all file metadata stored in Database and find the file is not duplicated. Then, Controller will reply a message to notify Client that file doesn't exist. After that, the file's metadata will be sent to Controller to insert it into database and CDMI sender in Client will send upload file request. After receiving the request, Load balancer will redirect it to CDMI Server by the apache load balancer scheduler algorithms. Finally, Adaptor node will write the file to Storage node and sent the uploading finished response to Client. Different with upload workflow for non-duplicated file, Fig. 4
                         shows the upload workflow while file is duplicated. While Controller replies a message to notify Client that file has existed, the Client will just send this file's metadata to Controller to insert it into database. Then, Controller will just create a metadata file that without any other content besides file name to Storage node. The main purpose of the metadata is to help Controller project right content for user's request.


                        Fig. 5
                         shows the workflow of download functionality. According to user's choice, Client will send a file_path to Controller. Then Controller will search database to find the real_path of this file and reply it to Client. After that, CDMI sender in Client will send CDMI download file request and Load balancer will redirect the request to CDMI server by the apache load balancer scheduler algorithms. Finally, Adaptor node will get the requested file from GlusterFS and transmit it back to Client.

The delete workflow is classified into two types according to the relationship among file_path, real_path and whether the file is sharable. While file_path is equal to real_path and the file is sharable, the file is moved to the other place rather than delete directly. Fig. 6
                         shows the delete workflow with moving file. The Operate interface sends a delete message which contains file_path to Controller first. Then Controller will query database by file_path and find a suitable path for moving files. Assume the file_path of the file which is going to be deleted is called path A, and the real_path that path A corresponds to is called path B. In order to find a suitable path, another file's real_path which is equal to path B is found first and the suitable path will be the corresponding file_path of that file. After removing, the Database will be updated and Client will receive the delete finished response. Fig. 7
                         shows the other situation, while file_path is not equal to real_path, or file path is equal to real path and the file is not sharable, the file can be deleted directly. After receiving a delete message which contains file_path, Controller will delete file according to the file_path directly and Client will receive the delete finished response.

In this section, three experiments are designed to estimate the proposed private cloud storage system. Firstly, performances of uploading and downloading workflow with difference files are measured to evaluate the proposed system. The time ratio of each step taken in each workflow is also analyzed to figure out how much overhead would result from the deduplication mechanism. Secondly, performance comparison between Gluster-Swift's and the proposed system in similar environment is performed. According to the upload and download response time, the superiority of the proposed system can be demonstrate in term of data transmission efficiency. Finally, we exhibit the space benefit and time benefit of data deduplication mechanism used in the proposed system to demonstrate the storage efficiency. The transmitted files' sizes are scheduled from 1kB to 64MB in the experiments.

The components to be established in the proposed system including Client component, Front-end Node, Database, Adaptor Node, and Storage Node. Due to the limitation of hardware equipment, three physical machines are used in the experimental environment. On one of the physical machines, we establish one virtual machine for Front-end Node and Database, two visual machines for Adaptor Nodes. Three visual machines for Storage Nodes are built on the second physical machine. The Client is established in the last physical machine. Table 1
                      shows the hardware specification of different nodes.

In this experiment, we measure the response times of uploading and downloading workflow with difference files which are created by FIO [34] with the sizes range from 1kB to 16MB. The operation of each file is repeated 10 times to calculate an average values as the final result. In addition, the time cost of sub-steps during each workflow are also measured to evaluate the efficiency. The uploading workflow can be divided into hash value check, file encoding and file transmission three sub-steps, and the downloading workflow consists of file location check, file decoding and file transmission sub-steps. The time ratios of each sub-step are analyzed to figure out how much overhead would result from data deduplication mechanism.

In terms of uploading performance, Fig. 8
                         shows the time cost of each sub-step during uploading workflow with difference files. The total uploading response time is the sum of time cost in each sub-step. It is obvious that the total response time of uploading and time cost of file transmission would significantly increase with the file size increase. The response time is less than 1000ms while uploading a file with size less than 1M, which is a satisfactory performance in general. The variant of time cost for file encoding is not obvious but it still can be found with a stable incensement while the file size becomes bigger. Whereas, the time cost of hash value check is steadily maintained nearly 200ms, regardless the size of uploading file. Since the hash value check sub-step has high correlation with the data duplication, it also indicates that the proposed data duplication mechanism is stable while dealing with difference files. To further figure out how much overhead would result from deduplication mechanism, Fig. 9
                         shows the time ratio of each sub-step taken in file uploading. It can be found that the ratio for hash value check time declines while the file size becomes big. When file size is 1kB, hash value check time takes 40.5% of the upload file operation, but when file size is 16MB, hash value check time takes only 6%. Which means the influence of additional action for deduplication would decrease for transmitting bigger files.

With respect to the downloading performance, Fig. 10
                         shows the time cost of each sub-step during downloading workflow with difference files. The total downloading response time is the sum of time cost in file location check, file decoding and file transmission sub-steps. In general, the downloading response time is less than uploading response time while dealing with a same size file. Except that, the characteristic of Fig. 10 is similar with Fig. 8. That is because file decoding during downloading workflow is just an inverse operation of file encoding during uploading workflow, and the file location check during downloading is similar with the hash value check s during uploading which both are high correlated with the duplication mechanism. It also shows in Fig. 8 and Fig. 10, the time cost of file encoding or decoding would be larger than the time cost of data duplication operation while transmit file size is bigger than 4M. Fig. 11
                         illustrates the time ratio of each sub-step during file downloading. It also can be found that time ratio of file location check which represented the influence of additional action for deduplication declines with file size increasing. When file size is 1kB, file location check time takes 39% of the upload file operation, but when file size is 16MB, hash value check time takes only 4%.

In this part of experiment, we measure and compare the upload response time and download response time of the proposed system and Gluster-Swift' in a similar environment. The file sizes for testing are ranged from 1kB to 64MB. The same storage nodes and the same type of volume established for Gluster Swift are the same as the proposed system.


                        Fig. 12 and Fig. 13
                        
                         illustrate the performance of proposed system and Gluster Swift in terms of the upload response time and the download response time, respectively. While uploading a 1kB file, it spends 650ms with proposed system and 2316ms with Gluster Swift. It can be found that the proposed system's uploading performances are better than Gluster Swift's for transmitting small files. In terms of download response time, while file size is 1kB to 2MB, the performance of the proposed system and Gluster Swift is not too obvious. However, while the file size is enlarged Gluster Swift shows better performance both on upload and download response time. Afterwards, with the increase of transmitted file size the growth of response time for Gluster Swift is steady. However, it increases rapidly for the propose system, thus the gap between proposed system and Gluster Swift enhances dramatically. The reason for explaining this phenomenon is that CDMI standard is aggregated in the proposed system, where file content is encoded into base64 for transmitting JSON string. When a big file is uploaded, it has to be separated into small sections and uploaded in batches. The operation of separating and uploading in batches is time consuming which results in the proposed system's performance being not as good as Gluster Swift while dealing with files whose sizes are bigger than 4MB. The experiment also indicates that the proposed system is much suitable for the service environment where most of the transmitted data are small files, such as service for enterprises.

In the part, we exhibit the space benefit and time benefit of data deduplication mechanism used in the proposed system to demonstrate the storage efficiency. Fig .14
                         shows the upload response time of different situations, which are upload non-duplicated file in proposed system, upload file in Gluster Swift and upload duplicated file in proposed system. Since Gluster Swift doesn't support deduplication mechanism, the upload response time is the same no matter whether the file is duplicated or not. In Fig. 14, it can be found that the proposed system is more efficient while duplicated file is transmitted. By adopting source deduplication in the proposed system, the upload workflow for duplicated file just includes hash value computing and information exchange between Client and Controller. As a result, the upload response time is shortened significantly and the whole upload bandwidth is the benefit of our implementation. Even the response time rises with file size becoming bigger due to hash value computing, we still can find the remarkable effect on saving time for transmitting duplicated files in the proposed system. Furthermore, the large duplicate copies of data in enterprises would result in redundant consumptions of storage space and network bandwidth. Hence, the storage efficiency will increase significantly by utilizing the benefit of space saving in the proposed private cloud storage service.

@&#CONCLUSION@&#

A data deduplication private cloud storage service with Cloud Data Management Interface (CDMI) standard based on the fundamental Gluster file system is constructed in this paper. Three experiments are designed to estimate the efficiency and characteristics of the proposed system. The experiments show that the proposed system is efficient for data transmission, even if there are overhead times caused by data deduplication mechanism. Compared with Gluster Swift, the proposed system's performances are better while dealing with small files. Furthermore, the remarkable effect on saving time and space for transmitting duplicated files in the proposed system is also demonstrated. The proposed private cloud storage service provides an easy way to for user to establish the system and access data across devices conveniently. It also demonstrates that the proposed system is much suitable for the service environment where most of the transmitted data are small files.

The proposed system doesn't perform very well while dealing with bigger files since the file level deduplication is adopted. In the future, we want to design a hybrid data deduplication scheme, where the big file is separated into small files as blocks to achieve a deduplication level that is between file and true block. The hybrid data deduplication scheme would save more storage space and require less of time and resources in computation. In addition, the proposed system doesn't provide sharing file mechanism. Usually users want to share their files to someone rather than just save data at cloud storage. Therefore, if there is a file sharing mechanism to make user get a download link easily, the availability of the proposed service will be enhanced.

@&#ACKNOWLEDGEMENTS@&#

This paper was supported by the National Science Council of Taiwan under Grant NSC103-2221-E-009-133-MY2. The authors would like to acknowledge the NSC for funding this project.

@&#REFERENCES@&#

