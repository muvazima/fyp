@&#MAIN-TITLE@&#Linguistically-augmented perplexity-based data selection for language models

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Word-level linguistic information for perplexity-based data selection.


                        
                        
                           
                           Evaluation and analysis for four languages: English, Spanish, Czech and Chinese.


                        
                        
                           
                           Combination of models lead to lower perplexity than the state-of-the-art baseline.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Data selection

Language modelling

Computational linguistics

@&#ABSTRACT@&#


               
               
                  This paper explores the use of linguistic information for the selection of data to train language models. We depart from the state-of-the-art method in perplexity-based data selection and extend it in order to use word-level linguistic units (i.e. lemmas, named entity categories and part-of-speech tags) instead of surface forms. We then present two methods that combine the different types of linguistic knowledge as well as the surface forms (1, naïve selection of the top ranked sentences selected by each method; 2, linear interpolation of the datasets selected by the different methods). The paper presents detailed results and analysis for four languages with different levels of morphologic complexity (English, Spanish, Czech and Chinese). The interpolation-based combination outperforms the purely statistical baseline in all the scenarios, resulting in language models with lower perplexity. In relative terms the improvements are similar regardless of the language, with perplexity reductions achieved in the range 7.72–13.02%. In absolute terms the reduction is higher for languages with high type-token ratio (Chinese, 202.16) or rich morphology (Czech, 81.53) and lower for the remaining languages, Spanish (55.2) and English (34.43 on the English side of the same parallel dataset as for Czech and 61.90 on the same parallel dataset as for Spanish).
               
            

@&#INTRODUCTION@&#

Recent years have witnessed a shift in the field of computational linguistics, which has moved from symbolic to statistical approaches. Language models (LMs) are a fundamental piece in statistical applications that produce natural language (e.g. machine translation, speech recognition). The LM is the component of any such system that takes care of fluency. LMs, as any other statistical model, should be trained on data of the same domain, genre and style as the data it will be applied to in order to perform optimally. This poses a problem, as in the majority of scenarios the amount of so-called domain-specific data is limited.

A popular strand of research in recent years to tackle this problem is that of training data selection. Given a limited domain-specific corpus and a larger pool of general-domain text, the task consists of finding suitable data for the specific domain in the general-domain sources. The underlying assumption is that a general-domain corpus, if broad enough, contains a subset of data which is similar to the target specific domain. That data, therefore, would be useful for training models for that specific domain.

Current data selection approaches (e.g. Moore and Lewis, 2010) support the selection of data from general-domain corpora that result in LMs which perform better (e.g. in terms of perplexity) for a given specific domain than LMs built on the whole general-domain corpora.

While these approaches rely solely on the use of surface forms, some research on language modelling looked at using linguistic information, specially for dealing with highly inflected languages. The rationale being that the fact that these languages have a larger set of different surface forms leads to sparsity problems, if the methods applied rely solely on surface forms. For example, research comparing English and Russian (Whittaker and Woodland, 1998) reported that a 65,000 vocabulary has 1.2% out-of-vocabulary (OOV) rate on the British National Corpus, while a vocabulary of the same size for Russian leads to 7.5% OOV rate on a Russian corpus of similar size. A vocabulary size of 375,000 is needed in Russian to achieve 1.2% OOV rate.

Work on language modelling for inflected languages has looked at using different types of information such as classes (Whittaker and Woodland, 1998; Brychcin and Konopík, 2011), part-of-speech (PoS) tags (Heeman, 1999), stems and endings (Maučec et al., 2004). LMs built on different types of information (e.g. word and class-based) can then be interpolated to reduce perplexity (Maltese et al., 2001).

Given the positive impact brought to language modelling by using linguistic information, we anticipate that this type of information could be useful as well for data selection. In our research, we explore the use of linguistic information for the selection of data from general-domain corpora to train domain-specific LMs. This paper builds upon our previous work (Toral, 2013), in which we showed that the use of linguistic phenomena, in particular named entities (NEs) and lemmas, in data selection and a naïve combination method lead to lower perplexity of the selected LMs for English and Spanish. Here we delve deeper into this topic, by testing and analysing in detail the results of our approach on a broader set of languages, including a highly inflected language (Czech) and a language with a high type-token ratio (Chinese). Another novelty of this paper is the use of a more advanced method to combine the different models (linear interpolation).

The rest of the paper is organised as follows. Section 2 presents an overview of the state-of-the-art in data selection, concentrating on perplexity-based approaches, as that is the focus of this work. Section 3 details our methodology for linguistically-augmented data selection, describing the individual models and the combination approaches. Section 4 describes the experimental setting and presents the results obtained. Section 5 delves deeper into the results, analysing the contribution of the different models from a number of perspectives. Finally, Section 6 provides the conclusions and proposals for future work.

@&#BACKGROUND@&#

Early research on perplexity-based selection for LMs (Gao et al., 2002; Lin et al., 1997) ranked text segments (e.g. sentences) of general-domain corpora by calculating their perplexity with respect to a domain-specific LM. The segments whose perplexity is lower than a given threshold are selected.

A related method (Klakow, 2000) builds a unigram LM from the entire general-domain corpus and scores each text segment from that corpus by the change in the log-likelihood of the domain-specific data according to the unigram LM, if that segment was removed from the corpus used to estimate the LM. The segments whose removal decrease the log-likelihood higher than an a threshold are selected.

The so-called Moore–Lewis approach is a more recent method that considers not only the perplexity with respect to the domain-specific LM but also the perplexity with respect to the general-domain data (Moore and Lewis, 2010). To this end, a LM is built on a random subset (equal in size to the domain-specific corpus) of the general-domain corpus. Each sentence s in the general-domain corpus is scored according to the following equation:
                        
                           (1)
                           
                              score
                              (
                              s
                              )
                              =
                              
                                 H
                                 I
                              
                              (
                              s
                              )
                              −
                              
                                 H
                                 O
                              
                              (
                              s
                              )
                           
                        
                     where H
                     
                        I
                      is the cross-entropy
                        2
                     
                     
                        2
                        Note that the use of perplexity or cross-entropy is equivalent as they are monotonically related.
                      with respect to the in-domain LM and H
                     
                        O
                      is the cross-entropy with respect to the general-domain LM. Sentences are ranked in ascending order, thus the method aims to select sentences similar to the in-domain data (low H
                     
                        I
                     ) and different from the general-domain data (high H
                     
                        O
                     ). The authors carried out an experiment for English, with Europarl (Koehn, 2005) as the domain-specific corpus and LDC Gigaword
                        3
                     
                     
                        3
                        
                           http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2007T07.
                      as the general-domain one. Compared to previous approaches, the use of an additional LM from the general-domain corpus allows the selection of data for a LM which is better (lower perplexity on a domain-specific test set) and smaller.

While perplexity-based data selection is the focus of this paper, it is worth mentioning that other approaches have also been applied to this research topic. One such example is the use of information retrieval techniques. (Eck et al., 2004) use the cosine distance similarity measure with TF-IDF term weights to select sentences to adapt LMs while (Falavigna and Gretter, 2012) proposes a more efficient version of TF-IDF for data selection purposes. Other techniques include the use of centroid similarity, n-gram ratios and resampled LMs (Maskey and Sethy, 2009). Finally, the combination of different approaches has also been explored, such as perplexity, cosine TF-IDF and edit distance (Wang et al., 2013). That paper evaluated these three data selection approaches extrinsically on machine translation. While the perplexity-based method outperformed the other two methods, a combination of the three yielded the best results.

@&#METHODOLOGY@&#

We use surface forms and different types of linguistic information at the word level in perplexity-based data selection. Our hypothesis is that ranking by perplexity on n-grams that represent linguistic patterns (rather than n-grams that represent surface forms, as done in previous approaches, cf. Section 2) captures additional information, leading to better generalisation and the ability to combat data sparseness, and thus may select valuable data that is not selected solely according to surface forms.

Specifically, we explore the use of three types of linguistic information at word level: lemmas, NE categories and PoS tags. All these three types of information group different surface forms into classes, and thus they reduce data sparsity and vocabulary size. They differ with respect to which surface forms are grouped together and the degree of vocabulary reduction that can be attained.

NE categories group together proper nouns that belong to the same semantic class (e.g. person, location, organisation). The distributional properties of NEs (Toral and Way, 2011) (a huge amount of different instances and a very low number of occurrences per instance) lead to sparsity if surface forms that hold NEs were to be used for selection.

Lemmas group together word forms that share the same root. We hypothesise that the use of lemmas is especially useful for highly inflected languages, as in these languages the ratio of surface forms to lemmas is particularly high, and thus by grouping together different surface forms that share the same lemma, we are effectively reducing the sparsity.

Finally, PoS tags group together words that share the same grammatical function (e.g. adjectives, nouns, verbs). While PoS tags have weak predictive power as a result of the lack of lexical information, and thus they are expected to perform poorly on their own, they have been reported to be useful when used in combination with lexical models (Cussens et al., 2000).

By taking into account these types of information, we experiment with the following models:
                        
                           •
                           Forms (hereafter f) uses surface forms. This model replicates the Moore–Lewis approach and provides the baseline in this study.

Forms and NEs (hereafter fn) uses surface forms, with the exception of any word (or word sequence) detected as a NE, which is substituted by its category (e.g. person, location, organisation).

Lemmas (hereafter l) uses lemmas.

Lemmas and NEs (hereafter ln) uses lemmas, with the exception of any word (or word sequence) detected as a NE, which is substituted by its category.

Tags (hereafter t) uses PoS tags.

Tags and NEs (hereafter tn) uses PoS tags, with the exception of any word (or word sequence) detected as a NE, which is substituted by its category.

A sample sentence, according to each of these models, is shown in Fig. 1
                     . In this example the PoS tagset comes from Penn Treebank
                        4
                     
                     
                        4
                        
                           https://www.comp.leeds.ac.uk/ccalas/tagsets/upenn.html.
                      while the NE tagset comes from Freeling
                        5
                     
                     
                        5
                        
                           http://nlp.lsi.upc.edu/freeling/.
                      and is based on the EAGLES annotation guidelines.
                        6
                     
                     
                        6
                        
                           http://www.ilc.cnr.it/EAGLES96/annotate/annotate.html.
                     
                  

We use these models to perform data selection both individually and combined. The following subsections detail both procedures, respectively.

Our approach to data selection with linguistically-motivated models follows the same procedure as the cross-entropy method previously used on surface forms (Moore and Lewis, 2010). We build two LMs, one for the domain-specific corpus and one for a random subset of the general-domain corpus of the same size as the domain-specific corpus. Each sentence in the general-domain corpus is then scored according to cross-entropy with the two LMs (cf. Eq. (1)). Finally we evaluate the data selected. Given an in-domain test set, we extract a subset of the highest ranked sentences (according to a threshold), we build a LM on that subset and compute the perplexity of the test set according to that LM.

The difference of our approach with respect to cross-entropy on surface forms is that in our case the domain-specific and general-domain corpora are pre-processed according to the linguistic model that we use (see Fig. 1). The LMs are built on these pre-processed versions of the corpora. We also use the pre-processed version of the general-domain corpus for the scoring phase. Once the sentences have been scored they are replaced with the corresponding sentences in the original corpus, keeping the ranking order. This allows the evaluation phase to be performed on subsets of the original corpus, even if they have been ranked according to a linguistically-motivated model.

We also investigate the combination of the different individual models. We propose two combination methods, which we will refer to as naïve and advanced.

The naïve combination (noted as c in the results) proceeds as follows. Given the sentences selected by all the individual models considered for a given threshold, we iterate through them following the ranking order (i.e. we traverse the first ranked sentence by each of the models, then we proceed to the set of second best ranked sentences, and so forth). As we iterate through the sentences we keep a sentence if it has not been seen before, i.e. we keep all the distinct sentences. We stop the procedure when we have obtained a set of sentences whose size is that indicated by the threshold, i.e. the size of the set of sentences in the combination is the same as the size of the set of sentences produced by any of the models.

The advanced combination (noted as ci in the results) proceeds similarly, the difference being that the selected sentences are not kept in one unique set. Instead, we consider as many sets as there are individual models. As we iterate through the sentences these are kept in sets that correspond to their provenance model. As with the naïve approach, the procedure stops when the number of distinct sentences kept, which is the union of the sentences across the sets in the advanced combination, is the same as the number of sentences produced by any of the models. Finally, we build LMs for the sentences contained in each of these sets and perform linear interpolation on these LMs with a development set.

For a given threshold and set of individual models, both combination models contain the same sentences, the difference being that, while in the naïve combination these sentences are concatenated, in the advanced combination (multiple instances) of these sentences are placed in different LMs (according to the linguistic preprocessing model), and these LMs are given weights according to the interpolation.

@&#EXPERIMENTS@&#

We carry out experiments for the following four languages: English, Spanish, Czech and Chinese. Although the experiments are run on each language independently from the others, we have used parallel corpora for some of these language pairs (English–Spanish and English–Czech). By running experiments for two different languages using parallel corpus data, we can extract more meaningful conclusions from the comparison of the results. All the corpora used in this study are deduplicated at sentence level.

For both English–Spanish and English–Czech we use corpora from the WMT translation task series.
                           7
                        
                        
                           7
                           
                              http://www.statmt.org/wmt14/translation-task.html.
                         For both language pairs the domain-specific corpus is News Commentary version 8 (hereafter NC), while the general-domain is United Nations
                           8
                        
                        
                           8
                           
                              http://www.uncorpora.org/.
                         (Eisele and Chen, 2010) (hereafter UN) for English–Spanish and CzEng 1.0
                           9
                        
                        
                           9
                           
                              http://ufal.mff.cuni.cz/czeng/.
                         (Bojar et al., 2012) for English–Czech. For both language pairs we use newstest2012 (test set for WMT 2012) as the development set and newstest2013 (test set for WMT 2013) as the test set.

For Chinese the domain-specific corpus is the Chinese side of the News Magazine Corpus (LDC2005T10),
                           10
                        
                        
                           10
                           
                              http://catalog.ldc.upenn.edu/LDC2005T10.
                         while the general-domain data is collected from the UM-Corpus (Tian et al., 2014),
                           11
                        
                        
                           11
                           
                              http://nlp2ct.cis.umac.mo/temp/umcorpusform_demo.php.
                         CWMT News and the Sci-Tech corpus. Two random sets of 2000 sentences each are taken out of the domain-specific data to be used as development and test sets.


                        Table 1
                         details the general-domain (referred to as out) and domain-specific (referred to as in) corpora used for each language, including the number of sentences and words, the vocabulary size and the type-token ratio. Type-token ratios are similar for English and Spanish both for domain-specific (.01441 and.01714) and general-domain corpora (.00193 and.00173). As expected due to its highly inflected nature, ratios are higher for Czech when compared to the equivalent data in English, both for domain-specific (.04372 vs..01775) and general-domain corpora (.01114 vs..00849). Finally, the ratios for Chinese are rather high at.02161 and.04079 for domain-specific and general-domain corpora, respectively. This has to do with the considerably larger set of characters of this logogram-based language when compared to the other languages of this study, whose writing systems are based on alphabets.

In order to perform data selection using the linguistically-augmented models (cf. Section 3), these corpora have been processed with the following NLP tools:
                           
                              •
                              For the English–Spanish data, we have used Freeling 3.0 (Padró and Stanilovsky, 2012) to perform lemmatisation, PoS tagging and NE recognition. These corpora are tokenised and truecased using the corresponding scripts from the Moses toolkit (Koehn et al., 2007).

The English–Czech parallel data has been processed by the TectoMT framework (popel and Žabokrtský, 2010) using the following pipeline. The Czech language side was tokenised by the Czech TectoMT tokeniser and PoS-tagged and lemmatised (technical suffixes of the lemmas produced by the tagger were omitted) by the Featurama tagger.
                                    12
                                 
                                 
                                    12
                                    
                                       http://featurama.sourceforge.net/.
                                  The Czech NEs were labelled by the TectoMT component based on the NE recognizer of Strakova (Straková et al., 2013). Truecasing was done by changing the case of the first character of each word to correspond with the case of the first character of its lemma.

The English language side was tokenised by the English TectoMT tokeniser, PoS-tagged by the Morce tagger (Spoustová et al., 2007) and lemmatised using the rule-based lemmatizer by Popel (2009). The English NEs were labelled with the Stanford NE recogniser.
                                    13
                                 
                                 
                                    13
                                    
                                       http://nlp.stanford.edu/ner/.
                                  Truecasing was done the same way as on the Czech side.

The Chinese corpora have been processed (word segmentation, PoS tagging and NE recognition) with the Stanford CoreNLP toolkit.
                                    14
                                 
                                 
                                    14
                                    
                                       http://nlp.stanford.edu/software/.
                                  From this toolkit we have used a CRF-based word segmenter (Tseng, 2005; Chang et al., 2008), a maximum-entropy PoS tagger (Toutanova et al., 2003) and a CRF-based NE recogniser (Finkel et al., 2005) with built-in Chinese models.

Due to the different nature of the languages considered and to the processing tools used, not all the linguistic models that have been introduced (cf. Section 3.1) have been used for all of the four languages considered. Table 2
                         shows the individual models that have been used for each of the languages. The model that uses PoS tags only (t) is not used for Spanish–English nor for Czech–English as the corpora processing pipeline contains NE tags already integrated with the PoS-tagged output. Models that use lemmas (l and ln) are not used for Chinese as this linguistic concept does not apply to this language.

All the LMs used in the experiments are built with IRSTLM 5.80.01 (Federico et al., 2008), they consider n-grams up to order 4 and they are smoothed using a simplified version of the modified Kneser-Ney method (Chen and Goodman, 1996). IRSTLM is also used to compute perplexities. Linear interpolation of LMs is carried out with SRILM (Stolcke, 2002) via the Moses toolkit.
                           15
                        
                        
                           15
                           
                              https://github.com/moses-smt/mosesdecoder/blob/RELEASE-2.1/scripts/ems/support/interpolate-lm.perl.
                        
                     

@&#RESULTS@&#

We have performed data selection with cross-entropy for each of the languages considered (cf. Section 4.1) with the baseline and the individual linguistic models (cf. Section 3.1). We have also combined the data selected by the different individual models (cf. Section 3.2).

We report our results in terms of the perplexities obtained on a test set by LMs built on different subsets of the data selected by each of the models. These subsets correspond to different thresholds, i.e. percentages of sentences selected from the general-domain corpus. These are the first 1/32 ranked sentences, 1/16, 1/8, 1/4, 1/2 and 1.


                        Figs. 2–6
                        
                        
                        
                        
                         show test data perplexities obtained by LMs built on the data selected by each model from the general-domain corpus, for English (English–Spanish dataset), Spanish, English (English–Czech dataset), Czech and Chinese, respectively. In each figure, the x-axis indicates the percentage of the data selected (as 1/x) while the y-axis indicates the perplexity value.

The trends observed regarding the different models are common across all the figures. All the individual linguistic models, except for the ones that use PoS tags, perform similarly to the baseline. The models that use tags perform slightly worse, as expected, due to their lack of lexical information.

Both combination models outperform the baseline and the individual linguistic models, the only exception to this being the naïve combination performing worse than individual models f and fn for Chinese. The advanced combination (linear interpolation) outperforms the naïve combination in all the scenarios.


                        Table 3
                         gives a detailed account of the results obtained by each individual model for the different languages considered. We show the scores by each model for the threshold for which the baseline obtains its best result (1/8 for English–Spanish, 1/2 for English–Czech and 1/16 for Chinese).
                           16
                        
                        
                           16
                           For some languages, the best score achieved by some models is obtained for a different threshold than that for which the baseline obtains its best result. In these cases, if the best score by a model is obtained for a lower threshold than that of the best score by the baseline (e.g. tn, c and ci for English in English–Czech) we show this score. Conversely, if the best score by a model is obtained for a higher threshold than the best score by the baseline (e.g. t, tn, c and ci for Chinese) we still show the score for the threshold of the best baseline score as otherwise the comparison would not be fair, as the size of the data selected by the model is bigger than the data selected by the baseline.
                         For the baseline we show the absolute perplexity, while for the linguistic models we show relative values compared to the baseline (as percentages).

As previously seen in Figs. 2–6, the baselines and the individual models that use lexical information (fn, l and ln) obtain very similar scores, while models that use tags (t and tn) lag behind. Different individual models get the best result for different languages (l for English in English–Spanish, ln for Spanish, fn for Czech and English in English–Czech and f for Chinese), although the differences being so small they may be considered non significant.


                        Table 4
                         presents the results for the combination models and compares them to the baseline. The naïve combination outperforms the baseline in all the scenarios (4.05% and 4.85% lower perplexities in English–Spanish and 3.78% and 0.49% in English–Czech) except for Chinese (5.38% higher perplexity). The advanced combination outperforms the baselines in all the scenarios, the relative improvements being in the range 7.72–13.02% depending on the language. In absolute terms the reduction is higher for languages with high type-token ratio (Chinese, 202.16) and rich morphology (Czech, 81.53) and lower for Spanish (55.2) and English (34.43 on the same dataset as Czech and 61.90 on the same dataset as Spanish).

We have shown that the trends in the results are similar for all the languages covered. It may be worth to comment briefly on the results obtained for the two English data sets (in English–Spanish and English–Czech). The first thing to note is that the general-domain English corpora in these two language pairs are rather different. The English corpus in the first language pair consists solely on UN resolutions and thus it is a rather homogeneous dataset. Conversely, the English corpus in the second language pair (CzEng) consists of subsets coming from very different sources including fiction, subtitles, technical documentation, parallel websites, etc. Despite these differences, we observe similar trends in our results for the two English data sets:
                           
                              •
                              Individual linguistic models, except tn, perform similarly to the baseline on both datasets (cf. Table 3).

The linguistic model tn performs notably worse than the baseline in both datasets, although the percentages are rather different depending on the dataset (3.9% and 16.18%).

The combination methods outperform the individual methods, with similar scores for both datasets (−3.78% and −4.05% for naïve, −8.22% and −11.98% for advanced).

@&#ANALYSIS@&#

This section delves deeper into the results by analysing the linguistic models from a number of perspectives. For each language and model, we report on the vocabulary size, overlap with other models and weight given in interpolation. Finally, we analyse the subsampling in our combination models. The following subsections cover these analyses in detail.

First, we look at the vocabulary size of the models for the different languages of the study. This way we can assess the level of vocabulary size reduction brought by each linguistic model to each language. Table 5
                         shows the number of different 1-grams in the LMs built on the domain-specific corpus (cf. Table 1) for each of the models and the languages.

As previously hypothesised (cf. Section 3), all the linguistically-motivated models result in substantial reductions of the vocabulary size, and therefore of sparsity when using these models for data selection. Replacing words that express NEs by their NE category (model fn) results in a reduction of 9.64–24.79%. Using lemmas (model l) results in a high reduction for Czech (59.78%) due to the highly inflected nature of this language, while it yields only a modest reduction for English (17.94%). Similarly but to a lesser extent, the reduction achieved for Spanish (34.09%) is higher than for English (8.39%). Combining lemmas and NE categories (model ln) conjoins the reductions brought by the previous two models. Finally, using PoS tags results in a very drastic reduction, more pronounced for English and Chinese (both higher than 99.9%) than for the other two languages (99.55% for Spanish and 98.93% for Czech) due to the comparatively smaller size of the English and Chinese tagsets (52 and 43 tags, respectively) compared to Spanish (397) and Czech (1497).

The degree of novel information gathered by using a linguistic model may be an indication of its potential usefulness. In other words, if a linguistic model brings only very little information that has not been selected by the baseline, its impact would be limited as it would be considered to be redundant. Table 6
                         shows, for each model and language, the percentage of data (unique 4-grams) that is selected only by that model, i.e. those 4-grams do not occur in the data gathered by any other of the models. We measure the overlaps on 4-grams as our LMs consider n-grams up to order 4 (cf. Section 4.1). More formally, given a set of 4-grams m
                        
                           i
                         produced by a model and another set m
                        
                           all−i
                         containing the union of the 4-grams produced by all the models except the model that produced the set m
                        
                           i
                        , we report the cardinality of the relative complement of m
                        
                           all−i
                         in m
                        
                           i
                         divided by the union of all the 4-grams, as shown in Eq. (2).


                        
                           
                              (2)
                              
                                 unique
                                 (
                                 
                                    m
                                    i
                                 
                                 ,
                                 
                                    m
                                    
                                       all
                                       −
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       |
                                       
                                          m
                                          i
                                       
                                          
                                       (
                                       
                                          m
                                          
                                             all
                                             −
                                             i
                                          
                                       
                                       )
                                       |
                                    
                                    
                                       |
                                       
                                          m
                                          i
                                       
                                       ∪
                                       
                                          m
                                          
                                             all
                                             −
                                             i
                                          
                                       
                                       |
                                    
                                 
                              
                           
                        
                     

The models with highest percentage of unique information are by far the ones that use PoS tags, ranging from 9.49% (model t for Chinese) to 22.52% (model tn for English in English–Spanish). This is related to the fact that these models are the only ones that do not contain lexical information.

All the models contain more than 1% of unique 4-grams, with the exception of some models in English–Czech. In this respect, it should be noted that the threshold used for this language pair (1/2) is considerably higher than the one used in the other two scenarios (1/8 for English–Spanish and 1/16 for Chinese), cf. Section 4.2. In other words, each model in English–Czech contains half of the total data present in the general-domain corpus (compared to 1/8 and 1/16 of the data for English–Spanish and Chinese, respectively). It is thus expected that the percentage of unique 4-grams in data selected by models in these languages be lower than in the other languages.

A complementary view to the percentage of unique information brought by each model is given by the overlap between pairs of models. These pairwise comparisons provide a more detailed account, when compared to the overall overlap presented above. The overlaps between pairs of models allow us to assess how different these models are from each other. Tables 7–9
                        
                        
                         show the percentage overlap between pairs of individual models for English–Spanish, English–Czech and Chinese, respectively. More formally, given two sets of 4-grams m
                        
                           i
                         and m
                        
                           j
                         produced by two models, we apply the Jaccard similarity coefficient, as shown in Eq. (3).


                        
                           
                              (3)
                              
                                 J
                                 (
                                 
                                    m
                                    i
                                 
                                 ,
                                 
                                    m
                                    j
                                 
                                 )
                                 =
                                 
                                    
                                       |
                                       
                                          m
                                          i
                                       
                                       ∩
                                       
                                          m
                                          j
                                       
                                       |
                                    
                                    
                                       |
                                       
                                          m
                                          i
                                       
                                       ∪
                                       
                                          m
                                          j
                                       
                                       |
                                    
                                 
                              
                           
                        
                     

Two types of overlaps can be clearly identified in Tables 7–9. Overlaps between models that use lexical information (f, fn, l and ln) and overlaps between models that do not use lexical information (t and tn) are above 65% in English–Spanish, 78% in English–Czech and 84% in Chinese. Conversely, all the overlaps between a model that uses lexical information (f, fn, l and ln) and a model that does not (t and tn) are below 37% in English–Spanish, 71% in English–Czech and 13% in Chinese. This can be related to the high percentage of unique data in models that use PoS tags (cf. Table 6).

In order to assess the importance of each model, we consider the interpolation of LMs built on data selected by the different individual models performed in the advanced combination (cf. Section 3.2). The assumption here is that the higher the weight given by the interpolation procedure to the LM built on data selected by using an individual model, the more important that model is. Table 10
                         shows the weights assigned in the interpolation to LMs built on different models, for each language.

The weights given to the models range from.1104 (l for English, in English–Czech) to.3829 (f for English in English–Czech). We can conclude then that all the models contribute, to a certain degree, useful information to the interpolation. Models that use PoS tags (t and tn), despite performing generally worse than the other models in isolation (cf. Section 4.2), are given comparatively high weights (from.2059 for Czech to.3530.
                           17
                        
                        
                           17
                           Summing up the weights given to models t (.1676) and tn (.1854).
                         for Chinese.) These high weights may be related to these models contributing a high percentage of data not present in the other models, as shown in the overlap analysis (cf. Table 6).

Another interesting fact is that the use of NEs is favoured by the interpolation. For all languages, most models that incorporate NEs (ln and tn, but not fn) get a higher weight than the same models without NEs (e.g..1842 for model ln compared to.1417 for model l for Spanish and.1854 for model tn compared to.1676 for model t for Chinese).

One indicator pointing to the usefulness of linguistic models regards the fact that if their weights are put together they are weighted more than surface forms, the latter accounting for weights ranging from.1862 (Spanish) to.3829 (English in English–Czech). In other words, linguistic methods overall are weighted in the range.6171 (English in English–Czech) to.8138 (Spanish).

Since the combination of models trained on different subsets of data has proved useful (i.e. the advanced combination outperforms the baseline and all the other lingusitically-based models in all our experiments), we would like to check whether (i) this has to do with the linguistic types of information used to select the different subsets of data, or whether (ii) it is due to the fact that, in general, selecting different subsets of data and combining them leads to better results than those obtained by a single subset, as a consequence of the variance stabilisation phenomenon.

In order to carry out this analysis we compare the results of the combination models that use subsets selected according to different linguistic models and combination models built on random subsets of data. To build the latter, we take n random subsamples of the data and apply the two combination strategies used in this study (naïve and advanced).

The following experiment has been carried out for English (in English–Spanish). We take the whole dataset and generate 5 random subsamples, each with size equal to 1/8 of the data (subset for which the baseline obtained its best result, cf. Fig. 2). We take 5 random subsamples for a fair comparison to our combination, which uses 5 models (f, fn, l, ln and tn). Once we have the 5 random subsamples, we run the two combination procedures, analogously as how we have done it with our models. The results are shown in Table 11
                        .

The naïve combination of random subsamples leads to a perplexity value of 881.24 (compared to 495.76 with our models, and higher than any perplexity value in our experiments for this dataset, cf. Fig. 2). The advanced combination leads to a perplexity of 739.56 (compared to 454.78 with our models, and higher than any perplexity value in our experiments for this dataset with 1/8 of the data). The weights assigned to the different randomly-selected models are rather homogeneous (.210836,.193565,.207448,.188611 and.19954), compared to the less homogeneous weights assigned to our models, cf. Table 10.

From these results (considerably higher perplexities and rather homogeneous weights) we rule out the hypothesis that the good results obtained by our combination approach are due to variance stabilisation as a result of mixing different subsets. We can conclude, instead, that the good performance of our combination is to be attributed to the linguistic types of information used to select the different subsets.

@&#CONCLUSIONS@&#

This paper has explored the use of different types of linguistic information at word level (lemmas, NEs and PoS tags) for the task of training data selection for LMs following the perplexity-based approach. By using these types of information, we have introduced five linguistically-motivated models. We have also presented two methods to combine the individual linguistic models as well as the baseline (surface forms): a simple selection of top ranked sentences selected by each method and a linear interpolation of LMs built on the data selected by the different methods.

The experiments are carried out on four languages with different levels of morphologic complexity (English, Spanish, Czech and Chinese). Our combination model based on linear interpolation outperforms the purely statistical baseline in all the scenarios, resulting in language models with lower perplexity. In relative terms the improvements are similar regardless of the language, with perplexity reductions achieved in the range 7.72–13.02%. In absolute terms the reduction is higher for languages with high type-token ratio (Chinese, 202.16) or rich morphology (Czech, 81.53) and lower for the remaining languages, Spanish (55.2) and English (34.43 on the same dataset as Czech and 61.90 on the same dataset as Spanish).

We have also analysed the models in detail by looking at the vocabulary reduction they achieve, their overlap and their importance. We have shown evidence that all the models are valuable as (i) they bring a significant reduction in vocabulary size when compared to the baseline, (ii) they provide a significant percentage of novel data which is not selected by the baseline and (iii) they are given significant weights in the interpolation.

As for future work, we consider three main possible lines of research.

The first line regards the exploration of other types of information, instead of word-level linguistic knowledge. In this respect, recent work in language modelling follows an unsupervised approach by using semantic spaces for clustering words (Brychcin and Konopík, 2014). The benefit is obvious, especially for under-resourced languages, as this approach avoids the use of tools such as PoS taggers, lemmatisers and NE recognisers. It would be interesting to compare the performance of linguistic and unsupervised models in data selection.

The second line regards the use of different types of information concurrently. While we have used one type of linguistic information in each model, another possibility is to combine different types of linguistic knowledge in a single LM, following for example the factored LM approach (Bilmes and Kirchhoff, 2003).

Finally, the third line has to do with the application of our approach to parallel data. This could be done by simply following the extension of the Moore–Lewis method to perform data selection from parallel data (Axelrod et al., 2011) or in combination with other methods which are deemed to be more suitable for parallel data, e.g. (Mansour et al., 2011; Banerjee et al., 2013).

@&#ACKNOWLEDGMENTS@&#

The research leading to these results has received funding from the European Union Seventh Framework Programme 
                  FP7/2007-2013 under grant agreement PIAP-GA-2012-324414 (Abu-MaTran), from the Czech Science Foundation (Grant No. P103/12/G084), from the Science and Technology Development Fund of Macau and from the Research Committee of the University of Macau under reference no. MYRG070 (Y1-L2)-FST12-CS and MYRG076 (Y1-L2)-FST13-WF.

@&#REFERENCES@&#

