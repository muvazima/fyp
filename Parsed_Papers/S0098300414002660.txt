@&#MAIN-TITLE@&#A fast and efficient algorithm to map prerequisites of landslides in sensitive clays based on detailed soil and topographical information

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We describe a novel method to identify slope and soil criteria of landslides in sensitive clays which is computationally fast and efficient.


                        
                        
                           
                           Provides more reliable results close to stable areas.


                        
                        
                           
                           The model can deposit with laterally varying geotechnical properties.


                        
                        
                           
                           Takes the thickness of the deposits into account.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Landslide susceptibility

Sensitive clay

Visibility algorithm

GIS

@&#ABSTRACT@&#


               
               
                  We present an algorithm developed for GIS-applications in order to produce maps of landside susceptibility in postglacial and glacial sediments in Sweden. The algorithm operates on detailed topographic and Quaternary deposit data. We compare our algorithm to two similar computational schemes based on a global visibility operator and a shadow-casting algorithm. We find that our algorithm produces more reliable results in the vicinity of stable material than the global visibility algorithm. We also conclude that our algorithm is more computationally efficient than the other two methods, which is important when we may want to assess the effects of uncertainty in the data by evaluating many different models. Our method also provides the possibility to take other data into account. We show how different soil types with different geotechnical properties may be modelled. Our algorithm may also take depth information, i.e. the thicknesses of the deposits into account. We thus propose that our method may be used to provide more refined maps than the overview maps in areas where more detailed geotechnical/geological data have been acquired. The efficiency of our algorithm suggests that it may replace any global visibility operators used in other applications or processing schemes of gridded map data.
               
            

@&#INTRODUCTION@&#

Landslides in sensitive clays are the major mass wasting processes in Sweden in terms of loss of human lives and of economic costs (Nadim et al., 2008). Significant landslide risk exists in many areas of Sweden, and Southwest Sweden is one of the areas in Scandinavia with the highest risk. The presence of high sensitive clays or quick clays implies that a major landslide hazard can exist even with moderate slopes (Osterman, 1963; Torrance, 1983, 2014; Viberg, 1984; Berggren et al., 1991). Similar conditions occur in e.g. Canada, Norway, Finland and Alaska (Brenner et al., 1981; Lebuis et al.,1983; Hilmo, 1989; Rankka et al., 2004). The term ‘quick’ refers to a clay for which the internal structure collapses when disturbed. Technically, clays are classified as quick if the sensitivity (defined as the ratio of the undrained and the remoulded shear strength) is at least 50, and its remoulded shear strength is below 0.4 kPa (Osterman, 1963; Viberg, 1982; Karlsson and Hansbo, 1989). Quick clays are predominantly found in sediments deposited in shallow seawater during the last deglaciation that have been uplifted by isostatic rebound. On land, fresh water leaching changes the ion concentration in the pore water reducing the quick clay's remoulded strength (Osterman, 1963; Torrance, 1983, 2014). The highly sensitive clays are rarely – if at all – visible at the surface, often they have been covered by more recent (e.g., fluvial, deltaic) sediments.

In Sweden, the most used method to obtain clay sensitivity is using undisturbed and remoulded shear strength measurements from laboratory tests (i.e., cone fall tests) performed on undisturbed soil samples. Because of the cost of undisturbed samplings, geotechnical and geophysical methods have been developed to investigate the presence of sensitive clays in soil deposits as an alternative to laboratory analysis. Geotechnical soundings (e.g., CPT – cone penetration tests) have been widely used in Scandinavia for decades (Lundström et al., 2009 and references therein) to identify areas with sensitive clays, since sounding resistance is correlated to clay sensitivity. Geophysical techniques based on electrical resistivity have recently shown potential for detecting clay sediments leached by fresh water and for understanding the environment they are situated in (Solberg et al., 2008; Malehmir et al., 2013). The accuracy and time efficiency of these geophysical techniques vary significantly, and their operational costs are generally high. Therefore methods to assess landslide hazard in Sweden, Canada, and Norway usually use only geological and topographic information (Lundström and Andersson, 2008) to identify areas for detailed geotechnical investigation. The methodology used in Sweden to map landslide hazard in sensitive clays (Berggren et al., 1991) involves computing cross-sectional angles rather than slope surface angles (see 
                     Fig. 1a) and, therefore, specially designed tools.

Below we describe an algorithm that, based on surface topography and Quaternary soil information, is used in Sweden to identify areas prone to landslides (Swedish Geotechnical Institute, 2001; Fallsvik, 2007). Including some recent refinements, we describe a novel method which:
                        
                           1)
                           is computationally fast and efficient,

provides more reliable results close to stable areas,

can model deposits with laterally varying geotechnical properties, and

takes the thickness of the deposits into account.

The algorithm performance is demonstrated using simulations and it is compared with implementations of other computational schemes used today for producing overview maps of landslide susceptibility.

Because of the difficulties in directly mapping sensitive clays, topographic and geological criteria are extensively used in Sweden as a first step in the analysis of landslide hazard (e.g. Fallsvik, 2007). The use of digital elevation data and detailed information on marine deposits allows identification of areas prone to landslides. The topographic criterion is based on analysis of historical data from Sweden, Norway and Canada, which shows that landslides in sensitive clays have not occurred in slopes with a height-length ratio below 1:10 or 10% (Viberg, 1982; Lundström and Andersson, 2008). This ratio does not represent the surface slope angle, but a cross-sectional angle (Fig. 1). Identifying areas above a given cross sectional angle is straight forward in one dimension (Fig. 1a), but may be slightly more complicated in two dimensions (b), because the path a slide may take is not along a straight line, or there may be barriers of stable soil blocking certain paths etc. In this context a barrier simply represents material that is classified as not susceptible to landslides (either bedrock or Quarternary material with a low clay content), and (as far as is known) not underlain by sensitive clay.

@&#METHODOLOGY@&#

A computer algorithm, commonly called the “visibility approach”, was first developed mimicking the established, originally manual approach, i.e. stepping through a map position by position checking if any other positions are above a certain cross-sectional angle from the source position. Computing visibility on a geographical grid is a rather slow procedure, as the number of computations is proportional to N
                        2 where N is the total number of nodes in the investigated area. For a typical area under investigation, 25×25km2 sampled at 5m, N=25 million nodes. The visibility approach thus requires on the order of 1015 computations. Even with modern computers, run time can therefore be an issue, especially if high level applications packages, such as ArcGIS (www.esri.com), with their in-built overheads, are used. Computational geometry is a well-studied subject (e.g. Goodman and O’Rourke, 1997) and the number of methods and optimizations are numerous, depending on – among other things – the model representations (e.g. discrete objects, lines, or pixels). For slope stability assessment standard GIS tools, e.g. looking only at topography, may be severely suboptimal, as the combined effects of both soil type and slope should be considered. For computational efficiency, our algorithm was lifted out of the ArcGIS environment and coded in the C-programming language.

Referring to Fig. 1, our algorithm is based on computing slope angles between neighbouring geographical nodes or grid points and aggregating into an effective steepness for a continuous, but not necessarily linear, slope. Nodes with sensitive soil are identified. All neighbouring nodes are checked to see if the local slope is greater than the stability criterion of 10%. If so, a pseudo elevation corresponding to the slope stability limit is associated to this node. The procedure is then repeated to include further nodes farther down the slope. This procedure is used so that very local topographical variations on long slopes with sensitive clays do not preclude the identification of the whole slope as a risk area. In other words the algorithm applies the visibility approach locally at each surrounding grid node (
                        Fig. 2). A flow scheme of the algorithm is shown in 
                        Fig. 3. The number of computations is thus on the order of NPQ where N is the number of nodes in the model (regardless of how densely the model is sampled), P is the number of surrounding nodes checked at each node for the sliding criteria, and Q is the number of iterations. As shown in Fig. 2, P=16 and Q rarely exceeds a few hundred, depending on the elevation span in the investigated area. The number of computations in the example above with N=2.5×107 is thus reduced to the order of N×104, or 1010, i.e. a reduction of five orders of magnitude compared to a global visibility approach.

As seen is Fig. 2, the interval between local slope directions that are investigated is 22.5° (360°/16). In an effort to reduce computing speed even further, a test with only examining the 8 nearest cells in the “inner ring” in the stencil in Fig. 2 was investigated (8 search directions, 45° between directions). It turned out, however, that this did not provide accurate enough results, probably due to the limited number of horizontal directions investigated. In some of the test cases we examined the resulting maps were slightly different than the ones produced from the original visibility approach.

Besides the visibility approach discussed above, other algorithms developed for the same purpose of identifying areas prone to landslides, include a “shadow-casting” scheme (Lindberg et al., 2011). Here, the elevation model is first inverted. Valleys become hills, and vice versa. By simulating a distant light source at a given angle above the horizon the hills cause shadows which can indicate possibly unstable areas. Varying the direction of the light source over a large number of directions can simulate the visibility approach fairly efficiently. How many directions are needed to approximate the visibility approach depends on the morphology of the landscape, but as the surface topography of undisturbed post-glacial marine sediments is generally rather smooth, only a fairly low number of angles is likely to be sufficient (e.g. on the order of 50). In the application of Lindberg et al. (2011), the possibility of barriers blocking the path of the shadows is taken into account by applying the shadow-casting iteratively. The authors claim the effectiveness of their algorithm is on the order of N
                        2, i.e. the same as a visibility approach (again, N represents the number of model nodes). Without knowing the details of their algorithm, we suggest that such an approach could be significantly more effective than that. If we use the same type of description as above for our own algorithm, its efficiency may be described as NPQ. P, or the number of nodes investigated in all directions, is generally larger than in our algorithm, though Q (the number of iterations) should be lower. For a fairly quadratic area P is most likely on the order of RN
                        1/2, where R is the number of directions used. With this assumption, and a fairly low Q, it is possible to notice that for a large area (a large N), their algorithm is likely more efficient than the visibility approach but less effective than our algorithm (N
                        2>N
                        1.5
                        Q>NPQ). In the following model tests we will compare the results and efficiency of both a visibility algorithm and a shadow-casting algorithm to ours.

To illustrate important features of the different algorithms, model simulations for a simple example model are shown. One purpose of these tests was to reveal if our algorithm appears sensitive to the direction of the sloping surfaces. We tested for this by comparing results from identical but differently oriented models. The obtained results are also compared to the results produced with a scheme built on a global visibility analysis, i.e. the “direct” implementation of the proposed methodology.

The example elevation and soil type models are shown in 
                        Fig. 4. The elevation model consists of four hills, one in each quadrant (Fig.4a). The model is symmetric, in the sense that each hill is rotated in steps of 90° relative to the lower left one with the aim of revealing any asymmetry artefacts caused by the iterative scheme. The hills each have four sides with different slopes (just below 10%, 12.5%, 15%, and 18% respectively). Whereas two of the slopes are parallel to the grid axes orientations, the two others face 26.6° and 5.7° to the nearest axis respectively, thus not in optimal orientation for the 22.5° steps of the visibility operator in Fig. 2. The area is 1×1km2, and the grid spacing is 1m in both directions. The soil type model is similarly discretized, with green signifying a soil type that may slide if the slope conditions exceed a stability criterion, and purple signifying stable areas that will not slide regardless of slope conditions (Fig. 4b).

@&#RESULTS@&#

The resulting map is shown in 
                     Fig. 5 All areas that satisfy both soil and slope criteria are marked red. As the cross sectional angle is used, not only node-to-node slopes of more than 10% are red, but also an area above the slope may be prone to a landslide. The stable areas obviously provide some “protection” against landslides, as the area at risk above a barrier is smaller than if the barrier would not have been there. In this example, all barriers have some influence on the area affected, but this is not necessarily the case for all possible geometries. Examining the four symmetrical hills there appears to be no difference between the extents of the red areas. This is confirmed by rotating the resulting risk map in steps of 90° and subtracting the results. Thus our algorithm is insensitive to the orientation of the slopes and provides consistent results, independent of the directions the hills' faces.

For comparison, the same model was analysed with a global visibility analysis. As the results of this analysis were also identical for the four differently oriented hills in the model, only the results for the lower left hill are shown (
                     Fig. 6a). When the stable areas (purple) were replaced by unstable (green) material in the model, the results of the global visibility analysis and the local iterative operator were identical (not shown). This is further support that our algorithm is not sensitive to slope direction. However, the presence of a barrier has different effects in the two algorithms. It turns out that our iterative scheme identifies larger risk areas partly behind the stable soil than the global visibility analysis. Fig. 6b compares the results of the two algorithms: Red marks the area identified by both, and orange marks areas identified by the local visibility operator only (the opposite is never true, i.e. there are no areas identified by the global visibility operator that the local iterative scheme does not find). To help explain this, arrows representing primary and secondary mass movements have been drawn in Fig. 6b. Whereas the global visibility algorithm only identifies areas unstable given the initial topography (black arrows), our iterative scheme reassesses the situation after every iteration. The areas behind the stable areas may thus become unstable given that some material has been removed at the toe of the slope (white arrows). Moreover, we think that the areas delimited by the global visibility operator in the presence of barriers of stable material (e.g., bedrock) sometimes have an unrealistic shape, while the local operator gives more reasonable results.

Computing times for the algorithms are shown in 
                     Table 1. The computing times are more than 195 times longer for the global visibility operator than our scheme, which is in reasonable agreement with the estimated 637 for this example (see above). Similarly, the shadow-casting algorithm needs 113 times longer to reach a solution. The explanation for the slight discrepancies between the estimated and measured relative computing times are probably mostly that the model example contains non-sliding nodes that “shadow” other nodes and limit the number of nodes included in the calculations. This is of course model-dependent and the simple expression derived above does not take such effects into account. Naturally, this has the largest effect for the algorithm that makes the most “unnecessary” computations, therefore the discrepancy is largest for the global visibility scheme. In Table 1 the computing time with a slightly modified visibility scheme is also shown for comparison. In this implementation of the visibility algorithm the search area is limited based on a pre-analysis of the topography, significantly reducing the computing times. The speed-up is model dependent, and will not be as significant for all models. Although this has not been investigated, it is possible that a similar implementation could also speed up the shadow-casting algorithm.

That secondary areas (i.e. areas considered safe when only the initial conditions are considered) are identified as at risk when a local visibility operator is iterated opens up further possibilities. If after geotechnical or geophysical investigations the presence of any significant amounts of quick clay in an area can be ruled out, rather than completely clearing the area from risk regardless of the slope, the angle of stability may be raised. This is easily achieved with an iterative scheme, as is demonstrated in 
                     Fig. 7. In a modification to the previously shown example model, an area (blue) in the upper left part of the model is introduced to represent soil that might become unstable, but at a different cross-sectional angle than the green area (Fig. 7a). Here the blue area is assumed to be stable for slopes up to 15%, and the green area can, as before, only sustain 10% slopes. The west side of the hill has a slope of just below 15%, and as is shown in Fig. 7b no landslides are initiated in the blue area. Secondary mass movements (thin arrows in Fig. 7b) in the southern end of the blue area indicate that risk conditions may be reached if a landslide occurs in the green area. Consequently, the edge of the blue area on the western slope is also considered prone to landslides. The northern side slopes almost 20%, thus landslides may still occur also in the blue area on this slope, but above the slope a smaller area than before is classified as prone to landslides. The number of different classes of soils with different slope angles allowed in the algorithm is in principle unlimited.

Another example of information that can be incorporated if available is the thickness of the soil(s). This information will be taken into account where it exists, but does not need to be known everywhere in order not to cause artefacts in the map. To demonstrate this feature, we have modified our modelling example to illustrate a case when the core of a hill consists of more stable material and the sensitive soils thin towards the top of the hill (
                     Fig. 8). The hill has the same shape as before (c.f. 
                     Fig. 4a), but now the thickness of the sensitive sediments is known (Fig. 8a). The sensitive soils are 6m thick at the base of the hill, and on top of the hill they are only about 1m thick to the south and to the west. In the northeastern corner they are about 4m thick. The blocks of stable material in the soil type model are now represented by the thickness of the sliding soils being zero at these locations (c.f. 
                     Fig. 8a and b). When this new information is taken into account, the area prone to landslides stops when stable material is reached. In the results shown in Fig. 8b, orange represents the areas that have been “cleared” (identified as low risk areas) with the new information on soil layer thickness. In this example a soil thickness of 4m in the northeast is not enough to limit the area prone to landslides on top of the hill, but the area to the west and south is significantly smaller than before where the sediments are only about 1m thick.

@&#DISCUSSION@&#

The main purpose of designing this algorithm was to improve computation times compared to a global visibility analysis, which 15 years ago was the difference between having a useful algorithm producing results within a day or two compared to several weeks, up to months. As illustrated here by a modelling example, our local visibility operator applied iteratively produces robust results much faster than a global visibility algorithm. For large datasets, this speed up in computation time is still important on today's computers. Computational speed becomes even more important when complete information about e.g. soil properties is not available, and in some cases it may be important to assess the implications of uncertainties in our knowledge by investigating many different models with slightly different soil classifications. A second motivation was the computer memory. In order to compute a seamless map of an as large area as possible, as little memory overhead as possible is an advantage. The proposed algorithm needs less than 6bytes per grid node for the basic options, about twice for the most advanced. It is difficult for solutions programmed in higher-order computer languages, such as ARC-info, to be as effective in this regard, though they may be comparable in computer speed.

It turns out that our local visibility operator gives different results compared to the global operator in the proximity of soils considered as stable, thus it is able to identify areas ‘behind’ stable material as prone to landslides that a global visibility operator does not see. We argue this is a significant advantage of the local operator. Moreover, these additional areas identified by the local operator may be interpreted as areas that become unstable after the main failure has occurred. We consider that it is important that an algorithm designed to provide “worst case scenarios” should also identify these areas. Though the possibility cannot be completely ruled out, we have not yet encountered a constructed model nor a real dataset for which the opposite is true – i.e. that a global visibility analysis finds a risk area that the local scheme does not. A similar result may be obtained by applying the global visibility operator repeatedly until all “secondary” areas have been identified (in the test model shown here only two sweeps were needed) but the computational cost for this approach quickly becomes unrealistic for larger areas and a standard desktop computer.

As was demonstrated in the example models, the iterative local visibility operator allows several classes of unstable soil types, each with different maximum allowed slope angle, to be included in the calculations. This may be useful e.g. in cases where there is good knowledge about the mechanical properties of a certain soil type in one area (node), but less reliable information for other nodes, where some kind of “worst case” strength would be chosen. If information on the thickness of the sedimentary layers is available, this can also be used as it may affect the area at risk. In this case the algorithm will replace the interface between the unstable and stable layer below as the sliding surface. Lindberg et al. (2011) points out that this possibility is not explored in their shadow-casting algorithm. How thin a layer need to be in order to have an effect on the risk areas computed will depend on the geometrical factors, i.e. the slope lengths and angles etc.

We suggest that this algorithm can prove a useful tool not only prior to conducing geotechnical investigations, but also to use interactively as new information becomes available. For example, if new geotechnical information indicating low soil sensitivity becomes available for an area a higher slope angle can be allowed and the algorithm can be run again to evaluate how the situation has changed. Similarly, if the unstable layer thickness is thought to be an issue for how large an area above a slope may be at risk, hypothesis tests with the algorithm can be carried out with various thicknesses to determine what information appears necessary to quantify risk, and thus which geotechnical or geophysical methods could be used to supply this information.

@&#CONCLUSIONS@&#

We have presented the computer algorithm that has been developed and used for more than 10 years in Sweden to identify areas of possible landslide hazard based on detailed elevation and soil type information. The computer algorithm is built on a local “visibility” scheme that is used iteratively. As this iterative scheme reassesses the situation between iterations, areas susceptible to landslides in the vicinity of barriers that are sometimes missed by a global visibility operator are found. We also show that our method is computationally much faster than a global visibility operator. For large datasets and uncertainty analyses, even with modern computers algorithm speed is important. Based on the tests shown we suggest that our algorithm may be used to help determine where further investigations should be made, and interactively to update maps showing areas prone to landslides as new information (e.g. on soil thickness) becomes available. Thus we do not suggest that our algorithm is a replacement for conducting thorough geophysical and geotechnical investigations, but can be used as a complement to, and support for, those investigations. For example, the algorithm may be used for hypothesis testing and the value of complementary information can be evaluated prior to an expensive data acquisition.

@&#ACKNOWLEDGEMENTS@&#

We would like to thank the two anonymous reviewers for the careful reading of the manuscript. Their comments have significantly improved the manuscript. We are grateful to Prof. Roland Roberts for thoroughly reviewing the language.

@&#REFERENCES@&#

