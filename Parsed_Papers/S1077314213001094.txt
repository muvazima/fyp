@&#MAIN-TITLE@&#Head direction estimation from low resolution images with scene adaptation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A head pose estimation method that adapts to individual scenes is proposed.


                        
                        
                           
                           The method automatically collects training dataset for head pose estimators.


                        
                        
                           
                           The method handle appearance differences within the same scene.


                        
                        
                           
                           The method demonstrate high performance without manually collected training data.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Head direction estimation

Low resolution image

Appearance-based approach

Scene adaptation

Graph-based image segmentation

Unsupervised learning

@&#ABSTRACT@&#


               
               
                  This paper presents an appearance-based method for estimating head direction that automatically adapts to individual scenes. Appearance-based estimation methods usually require a ground-truth dataset taken from a scene that is similar to test video sequences. However, it is almost impossible to acquire many manually labeled head images for each scene. We introduce an approach that automatically aggregates labeled head images by inferring head direction labels from walking direction. Furthermore, in order to deal with large variations that occur in head appearance even within the same scene, we introduce an approach that segments a scene into multiple regions according to the similarity of head appearances. Experimental results demonstrate that our proposed method achieved higher accuracy in head direction estimation than conventional approaches that use a scene-independent generic dataset.
               
            

@&#INTRODUCTION@&#

Estimating the human visual focus of attention has recently become a popular research trend, as such research has numerous applications in our daily life. For example, it can be used to estimate the attention of people walking along the street [1,2]. Having attention information enables us to easily infer interaction between people and to consequently analyze human interaction or identify on-going activities without requiring any human assistance [3,4]. Head direction is known to be an important factor in inferring the focus of attention of humans. Therefore, techniques for estimating head direction are considered important and have attracted great interest recently.

Various image-based approaches have been proposed for estimating eye gaze. However, most of them
                        1
                        We refer readers to [5] for a recent survey on approaches to head pose estimation.
                     
                     
                        1
                      require high resolution images [6,7] or special equipment such as depth cameras [8,9] or actively controlled pan-tilt-zoom cameras [10]. Hence, one of the major remaining technical challenges is how to estimate human gaze or head direction with low resolution images. In some application scenarios such as visual surveillance, the head regions in input images are often quite small. Small images contain limited information; accurately estimating head direction in such cases remains a challenging task.

The use of appearance-based approaches is thought to be promising for estimating head directions from low resolution images. Compared with model-based methods such as active appearance models [11,12], which rely on geometric facial models and require localization of facial elements, appearance-based methods directly use pixel values of an image as an input to extract image features and are known to be effective even with low resolution images.

Appearance-based head direction estimation approaches rely heavily on a dataset used for training estimators. This is due to the fact that head appearances can change significantly from scene to scene. Even in the same scene, there could be substantial differences in head appearance due to extreme differences in illumination or camera viewing angle. Therefore, a training dataset is best taken from the same location as the target data. However, collecting ground-truth training samples is a labor-intensive and time-consuming task, and it is prohibitively expensive to collect ground-truth data manually every time a head direction estimation method is applied to different scenes.

We propose an appearance-based head direction estimation method in order to overcome these problems. Our method is based on two key ideas: automatically acquiring a training image dataset with ground-truth head directions and segmenting a scene into multiple regions with similar head appearances. We first construct a training image dataset by tracking pedestrians in a scene of interest to capture head images where their walking directions are regarded as a ground truth head orientation. To address the problem of appearance differences within a scene, the scene is segmented into multiple regions based on the similarity of head appearances, and a head direction estimator is then trained for each region. This approach enables us to test each head image with the estimator trained with data taken from the same region. Higher accuracy can thus be expected because the data used to train the estimator have a similar appearance to the test data.

This paper is organized as follows. Section 2 describes related works on head direction estimation from low resolution images. Section 3 explains the details of our proposed framework. Section 4 describes the method to automatically acquire a scene-specific dataset. Section 5 introduces an adaptive scene segmentation method that solves the problem of appearance differences within a scene. We describe detailed experiments and a thorough analysis of the proposed method in Section 6, and we conclude the paper in Section 7.

@&#RELATED WORK@&#

Many attempts have been made recently to estimate head directions from low resolution images. Robertson and Reid [13] used skin color as a descriptor and a binary tree algorithm to construct the head direction classifier. Body direction is also used to filter out poses that are not physically plausible. Benfold and Reid [14] proposed a descriptor that learns a model of skin color automatically and used randomized ferns for head direction estimation. Orozco et al. [15] proposed an image descriptor using similarity distance maps with class-mean appearance templates, and a multi-class Support Vector Machine (SVM) for head direction classification. Benfold and Reid [1] utilized pedestrian tracking to accurately locate head position and perform head pose estimation. Their approach tracks pedestrians using a Kalman filter based tracking method. The head directions of the pedestrians are then estimated using a randomized ferns classifier with the histogram of oriented gradients (HOGs) features and color triplets comparisons (CTCs) as fern decisions. Schulz et al. [16] proposed an approach that integrates pedestrian head localization and head pose estimation to achieve high head pose estimation accuracy. Schulz and Stiefelhagen [17] trained eight head pose detectors, one for each pose class, to detect pedestrian heads. Their approach also integrates head pose predictions over time using particle filtering to achieve improved robustness and efficiency.

These studies used head images with resolution as low as 20×20pixels for training and testing the classifiers. While they are shown to work well for low resolution head images, they suffer from one important problem: a large number of training images with ground-truth labels, i.e., correct head orientations, are required. Orozco et al. [15] used 800 manually cropped head images, 100 for each direction class from the i-LIDS [18] dataset. Gourier et al. [19] turned downsampled images from the Pointing’04 dataset into low resolution 23×30 dimension images. Robertson et al. [20,13] used ground-truth samples produced by a human user drawing the line-of-sight of pedestrians in the images. Schulz et al. [16] used 7675 positive head pose samples and a set of negative non-head samples to construct the head pose classifier.

The work that is most relevant to our proposed method is that of Benfold and Reid [21]. They proposed an approach to solve the problem of a lack of training data. They constructed an unsupervised head direction estimator using a conditional random field model based on the same premise that people turn their head to where they are walking. However, their approach requires tracking information of the test data, which are sometimes unavailable such as in still images. Another unsupervised approach was recently proposed by Chen and Odobez [22]. Their approach jointly estimates the body pose together with the head pose, and this makes the method more robust by filtering out physically impossible head poses. However, both of these two methods do not consider appearance differences within the same scene.

Appearance-based head direction estimation involves determining a head direction p from a feature vector 
                        h
                      of an input head image. We define p as the head direction in an image plane. In our work, head direction estimation is defined as a regression task, where head directions are defined as continuous angles as illustrated in Fig. 1
                     .

With a set of training samples D
                     ={(
                        h
                     
                     
                        k
                     ,
                     p
                     
                        k
                     )}, the mapping p
                     =
                     f(
                        h
                     ) between the head direction and the feature vector can be learned through various regression algorithms. The mapping function then can be used to estimate a head direction p
                     ∗ from a new feature vector 
                        h
                     
                     ∗ in test scenes.

As discussed above, an important problem yet largely ignored in previous studies is how to obtain appropriate training samples D. Since we implicitly assume the mapping function f(
                        h
                     ) is identical in both training and test datasets, estimation accuracy highly depends on how similar the head images are in both datasets. Due to various factors such as lighting conditions or camera positions, head appearances in different scenes and even within the same scene can be significantly different. An example of such differences in appearance of people within the same scene is shown in Fig. 2
                     . Even though pedestrians are walking in the same direction, their head appearances are different when captured from different locations. In other words, if lighting conditions or camera positions are significantly different between the locations where training and test images are taken, mappings between the direction and the appearance would also be different. Nevertheless, it is not always possible to collect training samples for every test case.

The framework of our proposed method is summarized in Fig. 3
                     . Our method acquires training data from an input video sequence by using walking directions as a cue to infer head directions. The scene is then segmented into multiple regions with similar head appearances, and a head direction estimator is constructed for each region.

In order to obtain walking trajectories of pedestrians in the video, we employed the head tracking method by Benfold and Reid [1]. The method is based on a Kalman filter [23] with two types of measurements: the head locations given by a HOG-based head detector [24] and the velocity of head motion computed from multiple corner features [25,26]. In each frame, a head image I, a head location 
                        u
                     
                     = (x,
                     y) in the image plane, and a measurement error 
                        c
                     
                     =(c
                     (x),
                     c
                     (y)) are collected for analysis, where the terms c
                     (x) and c
                     (y) are the respective variances of the measurement on x and y axes of the Kalman filter. The pedestrian tracking algorithm is applied to the entire input sequence, and a trajectory, i.e., a set of head images {I
                     1,…,
                     I
                     
                        N
                     }, head locations {
                        u
                     
                     1,…,
                     
                        u
                     
                     
                        N
                     } and error measurements {
                        c
                     
                     1,…,
                     
                        c
                     
                     
                        N
                     }, is acquired for each pedestrian. Here, N denotes the length of the trajectory and it varies depending on the trajectory.

This section describes our technique to aggregate a scene-specific dataset. Given tracked trajectories of pedestrians, we estimate their walking direction, which can be assumed to indicate their head directions in the images. Erroneous samples that will cause errors in the trained estimators are rejected, and then we collect the remaining training samples to construct the head direction dataset. The proposed method is described in more detail in the following sections.

To account for the fact that pedestrians do not always walk straight, our method first divides each possibly curved trajectory into straight line segments. More specifically, each trajectory S is divided into segments {S
                        1,…,
                        S
                        
                           M
                        } by polyline simplification using the Douglas–Peucker algorithm [27]. This algorithm constructs a minimal set of lines so that the orthogonal distances from each point to its nearest line is less than a given threshold d
                        max. Since pedestrians get to appear smaller as they move away from the camera, the threshold d
                        max should be defined in a location-dependent way. Therefore, based on the fact that the physical size of the curve is proportional to the observed head size, we define the threshold d
                        max as 
                           
                              
                                 
                                    d
                                 
                                 
                                    max
                                 
                              
                              =
                              
                                 
                                    τ
                                 
                                 
                                    p
                                 
                              
                              ·
                              
                                 
                                    
                                       
                                          s
                                       
                                       
                                          ¯
                                       
                                    
                                 
                                 
                                    t
                                 
                              
                           
                        , where τ
                        
                           p
                         is a scale-invariant constant and 
                           
                              
                                 
                                    
                                       
                                          s
                                       
                                       
                                          ¯
                                       
                                    
                                 
                                 
                                    t
                                 
                              
                              =
                              
                                 
                                    ∑
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    N
                                 
                              
                              
                                 
                                    
                                       
                                          s
                                       
                                       
                                          x
                                       
                                    
                                    (
                                    
                                       
                                          u
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    ·
                                    
                                       
                                          s
                                       
                                       
                                          y
                                       
                                    
                                    (
                                    
                                       
                                          u
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                              
                              /
                              N
                           
                         is the average head length calculated assuming a square shape observed over a trajectory. s
                        
                           x
                        (
                           u
                        ) and s
                        
                           y
                        (
                           u
                        ) are the expected width and height of the head at the position 
                           u
                        . They are calculated assuming that the average human height is 1.7m, and heads are modeled as cylinders that are 22.0cm tall and 20.0cm in diameter, in the same manner as [1]. An example of polyline simplification is shown in Fig. 4
                        . In the figure, the curved line shows the raw tracking result, and the straight lines show line segments obtained using the polyline simplification algorithm.

Walking directions obtained from polyline simplification of a trajectory do not always correspond to head orientations since people can move their heads freely even while they are walking. This brings errors in the training labels. Head direction estimation algorithms are not always robust to such outliers, and thus it is preferable to reject them prior to the learning stage. To address this problem, we introduce a strategy to reject unreliable segments from the tracking results.

There are three kinds of segments that cause erroneous training samples: (1) segments with large tracking errors, (2) segments with short length or slow movement, and (3) segments with large image variance. The details of each kind are as follows. Let us assume that a segment contains T head locations {
                           u
                        
                        1,…,
                        
                           u
                        
                        
                           T
                        }.

Although the pedestrian trackers can resume their tracking and are robust to a few mis-detections, a large number of mis-detections can produce erroneous trajectories and poor head image localizations. These situations will result in a large number of erroneous points and large line fitting errors, which should be rejected.

To calculate the number of erroneous points, a point 
                              u
                           
                           
                              t
                            is identified to be erroneous if the error measurement of the tracker is significantly large:
                              
                                 (1)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  c
                                                               
                                                               
                                                                  t
                                                               
                                                               
                                                                  (
                                                                  x
                                                                  )
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  s
                                                               
                                                               
                                                                  x
                                                               
                                                            
                                                            (
                                                            
                                                               
                                                                  u
                                                               
                                                               
                                                                  t
                                                               
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                2
                                             
                                          
                                          +
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  c
                                                               
                                                               
                                                                  t
                                                               
                                                               
                                                                  (
                                                                  y
                                                                  )
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  s
                                                               
                                                               
                                                                  y
                                                               
                                                            
                                                            (
                                                            
                                                               
                                                                  u
                                                               
                                                               
                                                                  t
                                                               
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                    >
                                    α
                                    .
                                 
                              
                           where 
                              
                                 
                                    
                                       c
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                c
                                             
                                             
                                                t
                                             
                                             
                                                (
                                                x
                                                )
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                t
                                             
                                             
                                                (
                                                y
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                            is the measurement error of the tracker. Since these measurement errors should be evaluated according to their physical size, head width s
                           
                              x
                           (
                              u
                           
                           
                              t
                           ) and height s
                           
                              y
                           (
                              u
                           
                           
                              t
                           ) at the location of the tracker 
                              u
                           
                           
                              t
                            are introduced to scale the measurements.

Using this measure, we reject segments if the ratio of erroneous segment points to the total number of points in the segment is larger than a predefined threshold τ
                           
                              e
                           . Note that α indicates the acceptable error level, while τ
                           
                              e
                            controls the number of acceptable erroneous points in the segment. These parameters are not independent to each other, thus we first chose α to reject trajectory points where the head detector failed. When the head detector of the tracker module failed twice in a row, α is set to the error level of the tracker at that moment. After α is selected, τ
                           
                              e
                            is then chosen accordingly.

To reject segments with large line fitting errors, we calculate the summation of the orthogonal distances from each point to the estimated line over a segment and divide the summation by the length of the segment. We then reject a segment if
                              
                                 (2)
                                 
                                    
                                       
                                          1
                                       
                                       
                                          |
                                          
                                             
                                                u
                                             
                                             
                                                T
                                             
                                          
                                          -
                                          
                                             
                                                u
                                             
                                             
                                                1
                                             
                                          
                                          |
                                       
                                    
                                    ·
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             t
                                             =
                                             1
                                          
                                          
                                             T
                                          
                                       
                                    
                                    
                                       
                                          |
                                          a
                                          ·
                                          
                                             
                                                x
                                             
                                             
                                                t
                                             
                                          
                                          +
                                          b
                                          ·
                                          
                                             
                                                y
                                             
                                             
                                                t
                                             
                                          
                                          +
                                          c
                                          |
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      a
                                                   
                                                   
                                                      2
                                                   
                                                
                                                +
                                                
                                                   
                                                      b
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ⩾
                                    
                                       
                                          τ
                                       
                                       
                                          l
                                       
                                    
                                    ,
                                 
                              
                           where τ
                           
                              l
                            is a threshold indicating the maximum acceptable level of line fitting errors. 
                              u
                           
                           
                              t
                           
                           =(x
                           
                              t
                           ,
                           y
                           
                              t
                           ) is a point in the segment. The left-hand side of Eq. (2) is a scale-independent line fitting error of the estimated line ax
                           +
                           by
                           +
                           c
                           =0.

Pedestrians making slow or no movements are often seen in a scene, e.g., people talking to each other in the same location. Using the walking direction to estimate head directions in such situations would give erroneous results. Therefore, segments that are short in length or that have slow movements need to be rejected. Rejecting segments with short length also removes cases where false positive objects are detected as heads, which usually stay within a small area. Therefore, we reject a segment if
                              
                                 (3)
                                 
                                    
                                       
                                          |
                                          
                                             
                                                u
                                             
                                             
                                                T
                                             
                                          
                                          -
                                          
                                             
                                                u
                                             
                                             
                                                1
                                             
                                          
                                          |
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      s
                                                   
                                                   
                                                      ¯
                                                   
                                                
                                             
                                             
                                                s
                                             
                                          
                                       
                                    
                                    ⩽
                                    
                                       
                                          τ
                                       
                                       
                                          n
                                       
                                    
                                    
                                    or
                                    
                                    
                                       
                                          |
                                          
                                             
                                                u
                                             
                                             
                                                T
                                             
                                          
                                          -
                                          
                                             
                                                u
                                             
                                             
                                                1
                                             
                                          
                                          |
                                       
                                       
                                          T
                                          ·
                                          
                                             
                                                
                                                   
                                                      s
                                                   
                                                   
                                                      ¯
                                                   
                                                
                                             
                                             
                                                s
                                             
                                          
                                       
                                    
                                    ⩽
                                    
                                       
                                          τ
                                       
                                       
                                          v
                                       
                                    
                                 
                              
                           where τ
                           
                              n
                            and τ
                           
                              v
                            are predefined thresholds for detection of segments with short length and slow movements, respectively, and 
                              
                                 
                                    
                                       
                                          
                                             s
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       s
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       T
                                    
                                 
                                 
                                    
                                       
                                          
                                             s
                                          
                                          
                                             x
                                          
                                       
                                       (
                                       
                                          
                                             u
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       ·
                                       
                                          
                                             s
                                          
                                          
                                             y
                                          
                                       
                                       (
                                       
                                          
                                             u
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                 
                                 /
                                 T
                              
                            is the average of the head-length factors over the segment.

Pedestrians in the video are sometimes observed turning their head while they walk, which also leads to erroneous direction estimation results. Because large variations in head appearance are expected in such cases, segments with large image variations should be rejected. We calculate the variance of resized head image vectors 
                              
                                 {
                                 
                                    
                                       I
                                    
                                    
                                       ^
                                    
                                 
                                 }
                              
                            whose dimensions are denoted by C. A segment is considered to have large variance if
                              
                                 (4)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   t
                                                   =
                                                   1
                                                
                                                
                                                   T
                                                
                                             
                                          
                                          |
                                          
                                             
                                                
                                                   
                                                      I
                                                   
                                                   
                                                      ^
                                                   
                                                
                                             
                                             
                                                t
                                             
                                          
                                          -
                                          
                                             
                                                I
                                             
                                             
                                                ¯
                                             
                                          
                                          
                                             
                                                |
                                             
                                             
                                                2
                                             
                                          
                                       
                                       
                                          T
                                          ·
                                          C
                                       
                                    
                                    ⩾
                                    
                                       
                                          τ
                                       
                                       
                                          var
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 
                                    
                                       
                                          
                                             I
                                          
                                          
                                             ^
                                          
                                       
                                    
                                    
                                       t
                                    
                                 
                              
                            denotes the tth resized image, 
                              
                                 
                                    
                                       I
                                    
                                    
                                       ¯
                                    
                                 
                              
                            is a mean image calculated from all resized images 
                              
                                 
                                    
                                       I
                                    
                                    
                                       ^
                                    
                                 
                              
                            in the segment, and τ
                           
                              var
                            is a predefined constant. While high τ
                           
                              var
                            makes the algorithm accept more samples, low τ
                           
                              var
                            makes the algorithm more selective about the stability of head images.

Most outlier segments are rejected in the outlier segment rejection process, and the remaining segments contain correct data. Since only one orientation is assigned to each segment, most of the images in each segment are redundant. Using all the images for training would result in an excessively large dataset, which increases the computational time for many machine learning tools. Therefore, one representative image per segment is selected and used as training data.

In this work, we select the image that is most similar to the mean image of the segment. For each segment, the Mahalanobis distance from the mean image is calculated for every resized image 
                           
                              
                                 
                                    
                                       
                                          I
                                       
                                       
                                          ^
                                       
                                    
                                 
                                 
                                    t
                                 
                              
                           
                         in the segment and the image with the lowest distance is selected. This enables us to select the representative image while avoiding effects that can be seen in the mean image, e.g., blur or distortion.

As mentioned before, appearance differences of training samples in the scene can reduce the accuracy of the estimator. This section addresses our approach of segmenting a scene into multiple regions in each of which the heads with the same direction have a similar appearance. Because there is no definitive way to define regions with similar head appearances, an unsupervised clustering approach is taken to segment a scene into such regions. In this work, spectral clustering is used to segment a scene. Given a set of points and a similarity matrix defining the similarity of each pair of points, spectral clustering techniques cluster the set into disjoint subsets with high intra-cluster similarity and low inter-cluster similarity. Normalized cut [28], which is one of the most common spectral clustering algorithms, is applied in our approach.

Our approach first divides the scene into K rectangular unit regions V
                     ={v
                     1,…,
                     v
                     
                        K
                     } which are used as the set of nodes. Fig. 5
                      shows an example of 10×10 unit regions (K
                     =100). Then normalized cut is applied to cluster the regions V into R clusters, A
                     ={A
                     1,…,
                     A
                     
                        R
                     }, where A
                     
                        i
                     
                     ≠∅, A
                     
                        i
                     
                     ⊂
                     V, A
                     
                        i
                     
                     ∩
                     A
                     
                        j
                     
                     =∅ (1⩽∀i,j
                     ⩽
                     R,i
                     ≠
                     j) and 
                        
                           
                              
                                 ⋃
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 R
                              
                           
                           
                              
                                 A
                              
                              
                                 i
                              
                           
                           =
                           V
                        
                     . In the following sections, we discuss how to calculate the similarity weight function w(v
                     
                        i
                     ,
                     v
                     
                        j
                     ) for each pair of unit regions and how to choose the appropriate number of regions.

With the dataset D obtained using the method described in Section 4, we define D
                        
                           v
                         as training samples captured at the unit region v. Our proposed similarity weight w(v
                        
                           i
                        ,
                        v
                        
                           j
                        ) between two unit regions v
                        
                           i
                         and v
                        
                           j
                         is defined with the distance weight w
                        
                           d
                         and the sample weight w
                        
                           s
                         as w(v
                        
                           i
                        ,
                        v
                        
                           j
                        )=
                        w
                        
                           d
                        (v
                        
                           i
                        ,
                        v
                        
                           j
                        )·
                        w
                        
                           s
                        (v
                        
                           i
                        ,
                        v
                        
                           j
                        ).

The distance weight, w
                        
                           d
                        (v
                        
                           i
                        ,
                        v
                        
                           j
                        ), measures how close two unit regions v
                        
                           i
                         and v
                        
                           j
                         are. This takes into account the fact that training samples acquired from nearby locations tend to be more similar than those acquired from distant locations. The distance weight also makes segmented regions spatially smooth. We define distance weight w
                        
                           d
                         as follows:
                           
                              (5)
                              
                                 
                                    
                                       w
                                    
                                    
                                       d
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       v
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       e
                                    
                                    
                                       -
                                       
                                          
                                             ‖
                                             
                                                
                                                   X
                                                
                                                
                                                   i
                                                
                                             
                                             -
                                             
                                                
                                                   X
                                                
                                                
                                                   j
                                                
                                             
                                             
                                                
                                                   ‖
                                                
                                                
                                                   2
                                                
                                                
                                                   2
                                                
                                             
                                          
                                          
                                             
                                                
                                                   σ
                                                
                                                
                                                   d
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where 
                           X
                        
                        
                           i
                         and 
                           X
                        
                        
                           j
                         are the positions of the unit regions v
                        
                           i
                         and v
                        
                           j
                        , respectively and σ
                        
                           d
                         is a predefined constant.

Sample weight w
                        
                           s
                        (v
                        
                           i
                        ,
                        v
                        
                           j
                        ) measures the similarity between training samples acquired from the two unit regions v
                        
                           i
                         and v
                        
                           j
                        . Two unit regions v
                        
                           i
                         and v
                        
                           j
                         should be merged into the same region if the training samples 
                           
                              
                                 
                                    D
                                 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                           
                         are similar to 
                           
                              
                                 
                                    D
                                 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                           
                        . Sample weight is defined as
                           
                              (6)
                              
                                 
                                    
                                       w
                                    
                                    
                                       s
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       v
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       e
                                    
                                    
                                       -
                                       
                                          
                                             
                                                
                                                   d
                                                
                                                
                                                   u
                                                
                                             
                                             (
                                             
                                                
                                                   v
                                                
                                                
                                                   i
                                                
                                             
                                             ,
                                             
                                                
                                                   v
                                                
                                                
                                                   j
                                                
                                             
                                             )
                                          
                                          
                                             
                                                
                                                   σ
                                                
                                                
                                                   s
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where σ
                        
                           s
                         is a constant and d
                        u(v
                        
                           i
                        ,
                        v
                        
                           j
                        ) is a function that measures the difference between samples in two unit regions. The comparison is done between training samples corresponding to similar head directions, i.e.,
                           
                              (7)
                              
                                 
                                    
                                       d
                                    
                                    
                                       u
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       v
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                (
                                                
                                                   
                                                      h
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ,
                                                
                                                   
                                                      p
                                                   
                                                   
                                                      i
                                                   
                                                
                                                )
                                                ∈
                                                
                                                   
                                                      D
                                                   
                                                   
                                                      
                                                         
                                                            v
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                   
                                                
                                                ,
                                                (
                                                
                                                   
                                                      h
                                                   
                                                   
                                                      j
                                                   
                                                
                                                ,
                                                
                                                   
                                                      p
                                                   
                                                   
                                                      j
                                                   
                                                
                                                )
                                                ∈
                                                
                                                   
                                                      D
                                                   
                                                   
                                                      
                                                         
                                                            v
                                                         
                                                         
                                                            j
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       d
                                       (
                                       
                                          
                                             h
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             h
                                          
                                          
                                             j
                                          
                                       
                                       )
                                       ·
                                       ϕ
                                       (
                                       
                                          
                                             p
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             p
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                (
                                                
                                                   
                                                      h
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ,
                                                
                                                   
                                                      p
                                                   
                                                   
                                                      i
                                                   
                                                
                                                )
                                                ∈
                                                
                                                   
                                                      D
                                                   
                                                   
                                                      
                                                         
                                                            v
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                   
                                                
                                                ,
                                                (
                                                
                                                   
                                                      h
                                                   
                                                   
                                                      j
                                                   
                                                
                                                ,
                                                
                                                   
                                                      p
                                                   
                                                   
                                                      j
                                                   
                                                
                                                )
                                                ∈
                                                
                                                   
                                                      D
                                                   
                                                   
                                                      
                                                         
                                                            v
                                                         
                                                         
                                                            j
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       ϕ
                                       (
                                       
                                          
                                             p
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             p
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                 
                                 ,
                              
                           
                        where (
                           h
                        
                        
                           i
                        ,
                        p
                        
                           i
                        ) and (
                           h
                        
                        
                           j
                        ,
                        p
                        
                           j
                        ) are the feature vectors and head direction labels for a training sample in the dataset 
                           
                              
                                 
                                    D
                                 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                           
                         and 
                           
                              
                                 
                                    D
                                 
                                 
                                    
                                       
                                          v
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                           
                        , respectively. Here, ϕ(p
                        
                           i
                        ,
                        p
                        
                           j
                        ) is defined using a threshold
                           2
                           In our experiments, head samples with differences less than 45.0° were defined as being similar, i.e., we set θ
                              =45.0 (°).
                        
                        
                           2
                         
                        θ:
                           
                              (8)
                              
                                 ϕ
                                 (
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       p
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   if
                                                   
                                                   |
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   -
                                                   
                                                      
                                                         p
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   |
                                                   <
                                                   θ
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   otherwise.
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The value d(
                           h
                        
                        
                           i
                        ,
                        
                           h
                        
                        
                           j
                        ) measures differences between a pair of samples, and is defined as the weighted distance between the feature vectors:
                           
                              (9)
                              
                                 d
                                 (
                                 
                                    
                                       h
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             (
                                             
                                                
                                                   h
                                                
                                                
                                                   i
                                                
                                             
                                             -
                                             
                                                
                                                   h
                                                
                                                
                                                   j
                                                
                                             
                                             )
                                          
                                          
                                             T
                                          
                                       
                                       M
                                       (
                                       
                                          
                                             h
                                          
                                          
                                             i
                                          
                                       
                                       -
                                       
                                          
                                             h
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                 
                                 ,
                              
                           
                        where M
                        =diag[M
                        
                           i
                        ] is the diagonal matrix indicating the importance of each feature in the feature vectors. M
                        
                           i
                         should be large if the ith feature has a strong impact on distinguishing head directions. Although the importance matrix M can be obtained by using several approaches, in this work, M was obtained from the variable importance vector calculated from the random trees estimator [29], which was trained using the whole dataset D.

In addition to the weight function w, it is also important to select the appropriate number of regions. It is preferable for a scene to be segmented into as many regions as possible to take advantage of having samples with similar appearances inside the same region. However, if a region is too small, the number of training samples will be insufficient, and the trained estimators will have significant generalization errors.

We perform cross validation on the scene segmented with different numbers of regions and select the one that minimizes the cross-validation error. The cross validation errors is defined as the weighted sum of the validations errors in each region: for a segmentation 
                           A
                        
                        ={A
                        1,
                        A
                        2,…,
                        A
                        
                           R
                        },
                           
                              (10)
                              
                                 
                                    
                                       E
                                    
                                    
                                       g
                                    
                                 
                                 (
                                 R
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       |
                                       D
                                       |
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          r
                                          =
                                          1
                                       
                                       
                                          R
                                       
                                    
                                 
                                 
                                    
                                       E
                                    
                                    
                                       c
                                    
                                 
                                 (
                                 
                                    
                                       D
                                    
                                    
                                       r
                                    
                                 
                                 )
                                 ·
                                 |
                                 
                                    
                                       D
                                    
                                    
                                       r
                                    
                                 
                                 |
                                 ,
                              
                           
                        where D
                        
                           r
                         is the set of training samples captured within region A
                        
                           r
                        , and E
                        c(D
                        
                           r
                        ) is the 5-fold cross validation error using the training data D
                        
                           r
                        . For each sequence, cross-validation errors for scene segmentation with R(1⩽
                        R
                        ⩽
                        R
                        max) are calculated, and the number R
                        ∗ that minimizes the cross-validation error is then selected, i.e., R
                        ∗
                        =arg min
                           R
                        
                        E
                        g(R). We consider head directions estimated by using our proposed method in Section 4 as ground-truth data, and therefore we do not use manually-labeled ground truth data for computing cross-validation errors.

As a result of the above processes, we obtain a set of regions 
                           
                              
                                 
                                    A
                                 
                                 
                                    ′
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             A
                                          
                                          
                                             1
                                          
                                          
                                             ′
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             A
                                          
                                          
                                             
                                                
                                                   R
                                                
                                                
                                                   ∗
                                                
                                             
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                              
                           
                         in each of which the appearance of the training samples is similar. Estimators 
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    f
                                 
                                 
                                    
                                       
                                          R
                                       
                                       
                                          ∗
                                       
                                    
                                 
                              
                           
                         are then created for each region, and each of them is trained with the samples in its corresponding region; i.e., estimator f
                        
                           j
                         for region 
                           
                              
                                 
                                    A
                                 
                                 
                                    j
                                 
                                 
                                    ′
                                 
                              
                           
                         is trained with the dataset 
                           
                              
                                 
                                    D
                                 
                                 
                                    train
                                    ,
                                    j
                                 
                              
                              =
                              ⋃
                              
                                 
                                    
                                       
                                          
                                             D
                                          
                                          
                                             v
                                          
                                       
                                       ;
                                       v
                                       ∈
                                       
                                          
                                             A
                                          
                                          
                                             j
                                          
                                          
                                             ′
                                          
                                       
                                    
                                 
                              
                           
                        . The estimator in each region is applied for test samples captured in its corresponding region; i.e., the test samples in region 
                           
                              
                                 
                                    A
                                 
                                 
                                    j
                                 
                                 
                                    ′
                                 
                              
                           
                         are tested with the estimator f
                        
                           j
                        . Note that test samples are separated from training samples and are not included in the dataset D.

@&#EXPERIMENTAL RESULTS@&#

We conducted experiments using five video sequences that were recorded using different cameras in different scenes. The details of each sequence and the numbers of samples obtained as a result of the training data acquisition approach are summarized in Table 1
                     . Example frames in the videos are shown in Fig. 6
                     . Examples of the obtained head images are also shown with the estimated walking direction shown next to the image.

The parameters were set as follows for every scene; τ
                     
                        p
                     
                     =0.8, α
                     =0.5, τ
                     
                        l
                     
                     =5, τ
                     
                        e
                     
                     =0.4, σ
                     
                        d
                     
                     =1000, σ
                     
                        s
                     
                     =0.1, τ
                     
                        n
                     
                     =3.0, τ
                     
                        v
                     
                     =0.02, τ
                     
                        var
                     
                     =0.0035. The effect of applying the rejection methods is analyzed in Section 6.3, and the robustness against the parameter setting is analyzed in Section 6.4.

To evaluate the performance of our proposed method, we compared our method with regressors trained with a generic dataset that consists of head images collected from other scenes. We constructed a generic dataset using 1477 training samples taken from the Gaze Direction Dataset [30], which was used in [1]. Fig. 7
                         shows examples of head images included in the generic dataset.

All of the head images were resized to 40×40pixels (C
                        =1600), and all of the scenes were divided into 16×9 unit regions (K
                        =144). An image descriptor similar to the one in [21] was used here. The descriptor is the concatenation of two features. The first feature measures color difference between two pixels at two different locations. The second feature measures difference between two different bins from Histograms of Gradients (HOGs) features extracted from the head images divided into 4× 4 cell grids and normalized spatially across 2×2 blocks of cells. In our work, 400 pairs of points were chosen randomly for each feature.

The experiment was conducted using two regressors: Support Vector Regression (SVR) and regression with random trees [29]. Both of them were implemented by using OpenCV library [31]. SVR is one of the most common machine learning tools used in head direction and gaze estimation [15,32]. The combination of random trees and the above-discussed descriptor is similar to the estimator in [21] and was used for a fair comparison between our results with those reported in [21]. Our method finished training within 10min, and testing took less than 1ms per test sample on an Intel Core 2 Duo 3.00GHz CPU.

A comparison of the mean absolute angle errors (MAAEs) between our method and the baseline using the generic dataset is summarized in Fig. 8
                        . Here, we also compared the results without using the scene segmentation method. The Generic results were calculated based on regressors trained using the generic dataset, the Undivided results were calculated based on regressors trained using samples acquired without scene segmentation, and Proposed results were calculated based on our proposed method. Benfold result shows the angle error stated in [21].

The graphs show that the errors in regression tasks using the dataset obtained with our method are significantly smaller than those of the generic dataset. Scene segmentation further reduces errors for scenes with large variations in lighting conditions, such as sequence 3, or large differences in camera viewing angles such as sequence 4. Our result is comparable to that of Benfold and Reid [21].

To test the effectiveness of region segmentation in our method, we measured the relation between cross-validation errors and actual estimation errors. We applied our method with different numbers of regions and recorded their respective cross validation errors. In our experiments, we set the maximum number of regions to calculate cross-validation errors to 10. The comparison of estimation errors and cross-validation errors for SVR with different number of regions are shown in Fig. 9
                        . Both the cross-validation errors and the estimation errors increase when the number of regions increases to more than 5 and have been omitted from the graph for clearer representation. The number of regions that minimizes cross validation errors was chosen as the optimum number of regions. It can be seen that minimizing the cross-validation errors on training samples also minimize the estimation errors on test samples. The results of scene segmentation are shown in Fig. 10
                        . It can be seen that in sequences 3 and 4, areas with a large camera angle or illumination differences were segmented automatically. No significant change in performance was seen in the other sequences where head appearances remain relatively uniform in the scene.

To measure the effectiveness of our outlier rejection rules described in Section 4.2, we tested our proposed method with omitting each rule. An example result from sequence 4 is shown in Fig. 11
                        . Similar trends were observed in the other datasets, although we did not include those results here. It can be seen that our proposed method achieves the best estimation accuracy while maintaining the smallest dataset size.

These results indicate that short segment length and slow movement criteria significantly reduce estimation errors. This is intuitively reasonable because these rules reject trajectories where pedestrians are talking to each other, which are often observed in scenes. Rejection of short length segments also further reduces the errors by rejecting trajectories generated by false positive objects. In addition, it can be seen that rejection of samples with high variance significantly reduces the number of captured samples. This improves the training speed for large datasets.

In this section, the effects of different parameter values on the results are analyzed. We conducted experiments by applying the proposed method and changing each parameter value by 20%. We did not perform the analysis on the Town Centre dataset because the tracking results provided by the authors were used, and tracker error variance values were not available.

We show two example results of parameter tests in Fig. 12
                        . In the figure, the center columns with dotted lines show the default value mentioned in Section 6. The left and right columns show the default values that were changed by 20%. In each experiment, only one parameter was changed, and the other parameters were kept at their default values. Generally speaking, if the parameters are set too strictly, estimation errors increase due to the lack of sample variations. If the parameters are set to be more tolerant, the number of samples increases while the estimation errors are not significantly reduced.

Although rejecting segments with large variance reduces estimation errors as stated in Section 6.3, it is apparent that estimation errors significantly increase with a stricter threshold. This indicates that although image variance helps in rejecting images with incorrect head directions, variations in head appearance are also important for training regressors. Increasing the threshold value by 20%, however, did not significantly affect the estimation result. This is because image segments where people turn their head usually have large variance, and thus, there is a large margin for the variance threshold to reject such segments. Increasing the polyline fitting threshold will cause curved lines to be estimated as straight lines. This significantly increased estimation errors, especially in sequences 2 and 3 which contain a small number of samples. Reducing the threshold increased the number of samples but did not significantly reduce estimation errors. This is because if the line estimated by polyline simplification is sufficiently straight, reducing the threshold will further divide the line but will not yield any benefits.

@&#CONCLUSION@&#

We proposed a method of appearance-based head direction estimation that can automatically adapt to test scenes. The key idea behind the proposed framework is to use walking directions as a cue to infer head directions. A pedestrian tracker is first applied to the input video sequence, and then head direction for each pedestrian is estimated based on his/her walking direction. Outlier segments are rejected, and then a scene-specific dataset of head images labeled by their walking directions is automatically acquired. Each scene is then segmented into multiple regions according to the appearance of acquired head images with the same direction. Finally, a head direction estimator for each region is created by using training samples acquired from that region. The results of our experiments verified that our method estimates head direction accurately without any need to manually collect a ground-truth dataset in real scenes. This is a significant advantage compared to existing methods when applied to practical scenarios.

Appearance-based head direction estimation from low-resolution images is itself a difficult task, and there is still room for improvement in both feature description and estimation techniques. We believe that investigating the learning algorithm itself is an important future task.

@&#ACKNOWLEDGMENTS@&#

This research was supported by JST, CREST.

@&#REFERENCES@&#

