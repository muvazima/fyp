@&#MAIN-TITLE@&#Quality assessment of data discrimination using self-organizing maps

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We introduce two SOM-based methods for data clusterization quality evaluation.


                        
                        
                           
                           Each of these methods can be taken as a basis for the feature selection algorithm.


                        
                        
                           
                           This algorithm is suitable when the classes are separated nonlinearly.


                        
                        
                           
                           The algorithm does not require splitting the data into the training and test sets.


                        
                        
                           
                           The algorithm is not time-consuming.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Artificial intelligence

Classification

Data mining

Feature selection

Self-organizing maps

Artificial neural networks

@&#ABSTRACT@&#


               
               
                  Motivation
                  One of the important aspects of the data classification problem lies in making the most appropriate selection of features. The set of variables should be small and, at the same time, should provide reliable discrimination of the classes. The method for the discriminating power evaluation that enables a comparison between different sets of variables will be useful in the search for the set of variables.
               
               
                  Results
                  A new approach to feature selection is presented. Two methods of evaluation of the data discriminating power of a feature set are suggested. Both of the methods implement self-organizing maps (SOMs) and the newly introduced exponents of the degree of data clusterization on the SOM. The first method is based on the comparison of intraclass and interclass distances on the map. Another method concerns the evaluation of the relative number of best matching unit’s (BMUs) nearest neighbors of the same class. Both methods make it possible to evaluate the discriminating power of a feature set in cases when this set provides nonlinear discrimination of the classes.
               
               
                  Availability
                  Current algorithms in program code can be downloaded for free at http://mekler.narod.ru/Science/Articles_support.html, as well as the supporting data files.
               
            

@&#INTRODUCTION@&#

Today, artificial intelligence methods are being implemented for diagnostic tasks increasingly extensively. While undertaking this implementation, one can be faced with the problem of making the most appropriate feature set selection. Variables that describe the objects to be classified must be placed in a set that will provide the most reliable classification. At the same time, the data dimensionality should be as small as possible [1]. If the variables in each class show a normal statistical distribution and the classes can be linearly discriminated, some solutions can be found easily. We can mention as an example the problem of the selection of genes for the molecular diagnostics of tumors and the reduction of their number in the set [1,2]. Different approaches to the feature selection problem are observed in [3]. Usually, feature selection methods are divided into filter and wrapper approaches [3,4]. In [3], an embedded method is also mentioned, and arguments for using two stages for feature selection are provided. The first stage should include filter techniques, and the second stage should include the wrapper approach. Filter techniques work quickly, but they have a disadvantage in that the feature dependencies are not accounted for. Wrapper methods are computationally intensive and have a risk of overfitting. In the most difficult cases, distributions are not normal or even multimodal, and discrimination between the classes is nonlinear. In these cases, artificial neural networks (ANNs) could be implemented to make a discrimination of the classes. Filter methods are not applicable here [3]. To obtain the appropriate set of features in the training vectors, we must introduce some criteria for comparing different sets and, according to these criteria, select the set that will allow the most reliable discrimination into classes [5]. Such a scoring function is necessary for the wrapper methods. The most straightforward method is to evaluate the classification quality for different combinations of features using, for this purpose, the test samples set. However, there can be cases in which the number of samples is too small to be split into training and test sets for this purpose and obtain significant statistics. Additionally, this method is time-consuming.

@&#METHODS@&#

In the current study, we suggest the use of a wrapper feature selection technique, but not in the traditional way. In general, we take an unsupervised classifier and find a feature set, which demonstrates the most pronounced clusterization of the predefined classes; but then, we work with this set using a supervised classifier, for example, the perceptron. In particular, we suggest Kohonen self-organizing maps (SOMs) for an implementation. We suggest criteria that can present the data clusterization quality as well. When the data sets are mapped onto the SOM, one can evaluate visually how accurate the clusterization of the vectors from the different classes is. Herein, good clusterization on the SOM could indirectly indicate that these vectors can be successfully classified by the supervised artificial neural networks. In [6], the efficacy of revealing the unknown structure of the data by the SOM is shown. The SOM makes it possible to address clusters, which have a nonlinear structure that is well studied and is shown applications to the tasks from the Fundamental Clustering Problem Suite (FCPS) [7].

Thus, we have the task of quantifying the quality of the predefined classes’ clusterization on the SOM. If such a value is introduced, one could combine the components of the feature vectors into a set that could easily be classified by supervised artificial intelligence methods such as perceptrons. The idea of using SOM for feature extraction was realized in [8]. However, that study applied only a wrapper method, where SOM was used as a classifier, and the misclassified cases percentage was used for a classification quality evaluation; additionally, a test set was required. Evaluating the quality of the data clusterization on the SOM does not require a test set, which makes this method efficient when addressing small data sets. In the current study, we introduce two different estimators of this type. Within the framework of the study, we consider the case of two classes. Let us define these classes as A and B.

One of the values that we suggest is based on the calculation of the ratio of the topologic distances on the SOM. Let us calculate the average distance between the best matching units (BMUs) that belong to class A:
                        
                           
                              
                                 
                                    D
                                 
                                 
                                    A
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          A
                                          ,
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                          ∈
                                          A
                                          ,
                                          i
                                          ≠
                                          j
                                       
                                    
                                    Dist
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                    )
                                 
                                 
                                    
                                       
                                          n
                                       
                                       
                                          A
                                       
                                    
                                 
                              
                              ,
                           
                        
                     where xi
                      and xj
                      are the i-th and the j-th vectors of class A mapped onto the corresponding BMUs on the SOM; Dist(xi
                     ,
                     xj
                     ) is a topological distance on the SOM between two BMUs, where the i-th and j-th vectors are mapped; nA
                      is the number of pairs of vectors in which both vectors belong to class A and both vectors map onto different BMUs (if both vectors in the pair are mapped onto the same BMU, then this pair is not included in the statistics). By analogy, the average distance between the best matching units (BMUs) that belong to class B can be defined as
                        
                           
                              
                                 
                                    D
                                 
                                 
                                    B
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          B
                                          ,
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                          ∈
                                          B
                                          ,
                                          i
                                          ≠
                                          j
                                       
                                    
                                    Dist
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                    )
                                 
                                 
                                    
                                       
                                          n
                                       
                                       
                                          B
                                       
                                    
                                 
                              
                              .
                           
                        
                     
                  

Next, let us define the average distance between the best matching units (BMUs) that belong to the different classes:
                        
                           
                              
                                 
                                    D
                                 
                                 
                                    AB
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          A
                                          ,
                                          
                                             
                                                x
                                             
                                             
                                                j
                                             
                                          
                                          ∈
                                          B
                                       
                                    
                                    Dist
                                    (
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                    )
                                 
                                 
                                    
                                       
                                          n
                                       
                                       
                                          AB
                                       
                                    
                                 
                              
                              ,
                           
                        
                     where nAB
                      is the number of all possible pairs of vectors, one of which belongs to class A and the other of which belongs to class B. Similar to in the case with vectors that belong to the same class, those pairs that mapped onto the same BMU are excluded from the statistics. Then, we can define αT
                      as follows:
                        
                           
                              
                                 
                                    α
                                 
                                 
                                    T
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          D
                                       
                                       
                                          AB
                                       
                                    
                                 
                                 
                                    
                                       
                                          D
                                       
                                       
                                          A
                                       
                                    
                                    +
                                    
                                       
                                          D
                                       
                                       
                                          B
                                       
                                    
                                 
                              
                              .
                           
                        
                     
                  

In this way, we compare the distances between the BMUs of different classes (numerator) and the BMUs of the same class (denominator).

The αT
                      means an index of the clusterization quality. Indeed, if the clusterization on the SOM is bad, then the BMUs that correspond to the vectors of both classes are mixed on the SOM. In this case, the distances between the BMUs of the same class are approximately equal to the distances between the BMUs of different classes, and αT
                      possess its lowest values. If the clusterization is good, then the distances between the BMUs of the same class turn out to be smaller, while the distances between the BMUs of the different classes increase, and αT
                      possesses values that are greater than in the case of bad clusterization. Thus, we should select such variables for training vectors in such a way that αT
                      will be as large as possible.

Several methods similar to this were suggested for evaluation of the data clustering on the SOM [9,10]. However, as opposed to those studies, we address known classes and evaluate their discrimination on the SOM.

Another method of the clusterization quality assessment relates to the analysis of the nearest neighborhood of the BMUs. In the case of good clusterization in the neighborhood of each BMU, which corresponds to a specific class, there will be the BMUs that correspond to the same class (excluding cases when the BMU is located on the border of the classes). If the clusterization is not good, then the BMUs that correspond to the two classes will be in the same neighborhood. We introduce the value of αN
                      based on these considerations.

Let BMUA
                      and BMUB
                      be BMUs onto which the vectors of classes A and B, respectively, are mapped. If two or more vectors of two different classes are mapped onto one BMU, then it will be marked as BMUAB
                     . Then, for each BMUA
                      and BMUB
                     , let us calculate the relative number of vectors mapped onto the SOM in the unit radius and of the same class as this BMU: rA
                     
                     =
                     NA
                     /(NA
                     
                     +
                     NB
                     ) for BMUA
                      and rB
                     
                     =
                     NB
                     /(NA
                     
                     +
                     NB
                     ) for BMUB
                     , where NA
                      and NB
                      are the numbers of the vectors of classes A and B, respectively, which are mapped onto this BMU and the BMUs in the unit radius. If there are no other BMUs in the unit radius, then the radius should be increased by one. For each BMUAB
                     , the calculations of the rA
                      and rB
                      should be performed in the same way, but with zero radius, which means the ratio of the vectors mapped onto this BMU only. Afterward, the value of αN
                      can be calculated
                        
                           
                              
                                 
                                    α
                                 
                                 
                                    N
                                 
                              
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          r
                                       
                                       
                                          A
                                       
                                    
                                    +
                                    
                                       ∑
                                    
                                    
                                       
                                          r
                                       
                                       
                                          B
                                       
                                    
                                 
                                 
                                    |
                                    A
                                    |
                                    +
                                    |
                                    B
                                    |
                                 
                              
                              ,
                           
                        
                     where |A| and |B| are the number of vectors of classes A and B, respectively. Better data clusterization on the SOM provides a greater value of αN
                     .

In [11], feature selection with the SOM is presented. However, this algorithm does not give any quantitative evaluations of the data clusterization quality. It only tests whether the classes overlap on the SOM or not.

In the present study, we have used the SOM toolbox as a part of the Matlab package [12]. SOM featured a rectangular grid and had a size of 30×30. Linear initialization of the SOM was used [13].

@&#RESULTS@&#

In Fig. 1
                        , there is an example of two data sets that have a two-dimensional distribution. The degree of distinction between them cannot be evaluated by linear methods. On the other hand, it is obvious that the data sets can be easily separated by an artificial neural network. This distinction is also pronounced on the SOM (Fig. 1). Next, let us construct the same distributions, making them fuzzier, as is shown in Fig. 2
                        . In these distributions, the variance is equal to 0.05, 0.35, and 0.6 with respect to the approximating curve. It is obvious that while the distributions become fuzzier, the clusterization quality decreases. Let us calculate αT
                         and αN
                         for all three cases that are shown in Figs. 1 and 2. One can see that while the clusterization quality decreases, these values change, and αT
                         and αN
                         decrease. In the first case, their values are equal to 0.63 and 0.95, respectively. In the second, the values are equal to 0.59 and 0.77, and in the third, they are equal to 0.55 and 0.67.

Next, we performed validation of our results by the artificial neural network (the ANN) with forward error propagation and three layers.

The first layer contained two neurons, and the third layer contained one neuron. The activation function was a hyperbolic tangent sigmoid transfer function: tansig(x)=2/(1+exp(−2x))−1. We divided each of the classes randomly into two subsets in the ratio 70:30, which correspond to the working and test sets, respectively. Then, we used the bootstrap technique [14] with B
                        =500 to find the number of neurons in the hidden layer that will provide the best training. Bootstrap values were calculated for the ANNs that had different numbers of neurons in the hidden layer (from 1 to 20). After the best number of neurons in the hidden layer was estimated, we trained the ANN by the working set and calculated the percentage of correctly classified samples from the test set. The results are presented in Table 1
                        .

In Table 1, one can see that αT
                         and αN
                         decrease along with a decrease in the correct classification ratio.

The suggested methods for the classification quality evaluation were applied to the real world data. We used two real world feature selection tasks. First – gene selection for human brain tumor diagnostics based on an evaluation of gene expression values in brain tumors. Second – a selection of human speech features for dysphonia measurement in Parkinson’s disease diagnostics.

Tumor diagnostics can be performed by implementing gene expression analysis. The levels of some gene expressions in tumors differ from normal tissue. This finding was shown for different types of tumors and especially for glioblastomas [15]. There are many studies in which the SOMs were successfully implemented for the gene expression clusterization, e.g., [16,17] Additionally, it was shown that the gene expression profile in glioblastomas can be discriminated from healthy tissue by implementing artificial intelligence methods [18,19]. The selection of genes for tumor diagnostics is a well-known problem in oncology. The most common approach to this problem is based on the assumption of normal distributions and the linear discrimination of features [20,21].

For a sample problem that uses our method, we used the task of discriminating two stages of brain tumors, i.e., anaplastic astrocytoma (AA) and glioblastoma (GB) (grades III and IV by WHO) by the expression of a small number of genes. We used the databases that were obtained from NCBI GEO (http://www.ncbi.nlm.nih.gov/geo/, datasets GDS1816, GDS1815, GDS1816, GDS1975, GDS1976 and GDS3069). The number of cases in the dataset that we used was 94AA and 94GB. We took 14 genes and attempted to select three of them that could provide us with the best discrimination of these grades of tumor. As long as this real-world task is only a demonstration of the method, these 14 genes were chosen mostly arbitrarily; we had used some of them in some of our previous studies [19]. Special study on the selection of the optimal gene set for brain tumor diagnostics is planned in the future.

All of the procedures were performed with the SOM parameters as described above. Moreover, the data preprocessing was performed using logarithmic normalization. In Fig. 3
                           (left pane), one can see the SOM, which was trained by the vectors of all 14 gene expression values. The values of αT
                            and αN
                            here are equal to 0.51 and 0.66, respectively.

Checkup classification was performed using repeated random subsampling. We divided each of the classes randomly into two subsets in the ratio of 70:30 for the training to test set sizes. We have repeated the procedure of the ANN training and testing for different random divisions for a total of 100 times. The percentage of correctly classified vectors was calculated for the ANNs with different numbers of neurons in the hidden layer (from 1 to 10). The results are given in Fig. 3(right pane).

Then, we started the iterative procedure of the selection of three out of fourteen genes, training the SOM by the vectors of these three variables and calculating the values of αT
                            and αN
                            for each set of genes. In total, there were 
                              
                                 
                                    
                                       C
                                    
                                    
                                       14
                                    
                                    
                                       3
                                    
                                 
                                 =
                                 14
                                 !
                                 /
                                 3
                                 !
                                 ×
                                 (
                                 14
                                 -
                                 3
                                 )
                                 !
                                 =
                                 364
                              
                            combinations of genes.

In Fig. 4
                           , one can see the SOMs that have the best data clusterization, according to the αT
                            and αN
                            values, and Fig. 5
                            demonstrates the SOMs that have the worst data clusterization.

The best clusterization on the SOM, according to αT
                           , was achieved when the training vectors contained values for the expressions of the following genes: ASPM, SAA2, and KRT19P2 (αT
                           
                           =0.5576). When αN
                            was used as a clusterization quality estimator, then we obtained another set (αN
                           
                           =0.64), which was ASPM, SAA2, and LRDD. Here, the values of αT
                            and αN
                            in the case of the worst classification were equal to 0.503 and 0.57, respectively (Fig. 5).

The results of the classification by the ANN are pictured in Fig. 3, right pane (using all 14 genes), in Fig. 6
                            (the best clusterization on the SOM) and in Fig. 7
                            (the worst clusterization). The ANN configuration was the same as in the case with 14 genes, except that the number of input neurons was equal to three.

In Figs. 3(right pane), 6 and 7, the average percentage of the correctly classified cases by the ANNs is shown. In Fig. 8
                           , the statistics on this percentage is represented for the number of neurons in the hidden layer, where the percentage of the correct classification is the highest. For the set that contains all (i.e., fourteen) genes, the better subsets of genes according to αT
                            and αN
                            and the worst subset according to αN
                            equals one neuron, and the worst subset according to αT
                            equals three neurons. Fig. 8 shows the results for the test sets.

We have compared these mean values using a t-test. The results are presented in Table 2
                           .

In this task we used the dataset and results presented in [22]. The task of this study was the selection of the speech vocalization characteristics that may be used to differentiate people with Parkinson disease (PD) from healthy people. It was shown [23,24] that PD patients have a vocal impairment. In [22] ten speech vocalization features were selected as the initial feature set. Classification of two classes (healthy and PD) was performed implementing the support vector machine (SVM) method [25]. Classification correctness was evaluated using the bootstrap method with B
                           =50. All possible subsets of this feature set (1023 in total) were tested and those which gave the best classification quality were selected. Data from 31 subjects (23 with PD) were used in the study. The voice of each subject was recorded several times. Thus, the dataset used for processing included 195 cases (147 – PD). The features used for classification are shown in Table 3
                           . A detailed description of these characteristics and the methods used to calculate them is given in [22].

In Table 4
                           , column one, some feature sets (the most representative from the point of view of the original authors of the study) and the correct classification ratio they gave are presented. Feature sets, only published in [22] and containing more than one feature, are shown.

In our study we could not exactly reproduce the same results of SVM classification, as in the original one, due to some ambiguity in the detailed algorithm realization description in [22]. However, our results of the SVM classification look like those in the original study, taking the scatter into account.

In our study we used bootstrap with B
                           =50 – the same as in [22]. Also, for each combination of the features, the best set of parameters (penalty value and kernel bandwidth) has been searched – in the same way as in [22], through an exhaustive search over a range of values. However, as far as the range borders and the grid step has not been mentioned in [22], we selected them by ourselves. All in all, we used a “grid-search” [29] totaling 100 pairs of values. For each features combination, we selected the best on the grid bootstrap value. The correct classification percentage in most feature sets is not as good as in [22], and we suppose that in the original study more sophisticated methods of the parameters search were used. Nevertheless, our main aim was the evaluation of the computing time, to compare it with that suggested in the present study methods.

After that, we calculated αT
                            and αN
                            for all possible combinations of the features. Due to the 2-dimensional SOMs we used, we excluded single-featured feature sets from our calculations, using 1013 sets in total. The results are shown in Table 5 
                            and Fig. 9
                           .

In Table 5, we can see that, in general, both criteria (αT
                            and αN
                           ) are in close agreement with the correct classification rate achieved in the original study, as shown in Table 4. The exceptions are the first set for the αT
                            criterion and the third set for the αN
                            criterion. However, it should be mentioned that the confidence intervals of the correct classification percentage overlap in the first four features combinations in rating. Only the fifth features combination percentage is significantly less than the others. The αT
                            and αN
                            values for this combination are less than for the others, too.

@&#DISCUSSION@&#

When the sets of genes that correspond to the best values of αT
                      or αN
                      are used, the classification quality of the test set is better in comparison with the worst values. However, as seen in Figs. 3 and 6, inclusion of all 14 genes in the set does not make the classification quality much better in the test data set. Data classification using only the three genes found by these methods provides marginally greater percentage of correctly classified cases than using all of the genes. The same effect was described by Xiong et al. [20]. An increase in the number of variables gives significantly better classification of the training data set (at least when our data are used), but it does not play such a significant role in the practical applications compared with the test data set classification quality. Moreover, one can see that the classification quality of the training set in most of the cases increases along with the number of neurons in the hidden layer of the ANN. Whenever we address the test set, there is an optimal number of neurons. The increase does not show any positive effect. This finding is an effect of ANN overtraining, which is described in [30].

We have checked the obtained results in the gene selection problem using the ANN in the same way as in a real-world applications task (Section 3.2), and we calculated the rate of correctly classified cases from the test set for all of the combinations of three genes. This approach took more than 100h of calculations. Then, we sorted the gene sets in the rating according to the correct classification percentage on the test set. Thus, we used a type of wrapper method as a referent method for selecting the best combination of three genes. The same genes set as we have obtained using αT
                         took first place in the rating, while the αN
                         was placed only 11th in the rating (of a total of 364 combinations). The selection of the best features set using αT
                         took approximately 43min and using αN
                         took 8min. Thus, if very many variables are under study, and therefore, the number of combinations of variables tested is very large, the evaluation of αN
                         could be more reasonable as long as it takes less time than the evaluation of αT
                        .

Another advantage of the proposed method is that there is no need to split the data set into the training and test subsets. This consideration is important if the number of cases in the dataset is limited.

The αT
                         calculation algorithm is based on the comparison of the interclass and intraclass distances. There are many algorithms that are based on this approach, e.g., the Davies–Bouldin Index [31], However, all of them are intended to be used in feature space and not on the SOM. Data clusterization on the SOM, however, has some specific considerations. For example, there is a tendency to map several vectors onto one BMU, even in situations in which the size of the SOM is much larger than the number of vectors. Those BMUs that contain several vectors have a large influence on the overall results and overestimate the clusterization quality, especially in a situation in which such BMUs are far from each other on the SOM and are mixed with other BMUs that contain vectors from the other class. As a result, we had to move from the centroids-based calculations (which are performed in [31]) to the vector-based calculations, such as taking a group average in [32]. This approach gave us an opportunity to exclude BMUs that contain several vectors from statistics, fixing this vulnerability.

Another weakness of the αT
                         appeared in our experiment with 14 genes (see Fig. 3). Here, one of the classes on the SOM is split into two clusters, and another class is mapped between them. This arrangement could occur during the SOM training process, and classes in this case can be easily discriminated. However, αT
                         shows a low value (which is a sign of bad clusterization quality) here, due to similarity of the average interclass and intraclass distances.

We have compared the suggested clusterization indexes with the Davies–Bouldin Index, which is realized in the Matlab SOM toolbox [12], applying it to two classes rather than clusters. The Davies–Bouldin Index showed less calculation time than any of the others, but the best gene triple according to this index was only 7th in the ANN test rating. Moreover, similar to αT
                        , it showed bad results in the case of 14 genes. This arrangement occurred because in cases of data discrimination on the SOM, such as is shown in Fig. 3, the centroids of the two classes on the SOM are close to one another.

We have compared the proposed feature selection methods with chi-square statistics [33], which is a filter method according to [3]. We chose this particular method because it can be used in similar situations as our approach, that is, instances of nonlinear class discrimination. A set of three genes, which had the best chi-square values, provided only approximately 70% of correctly classified cases (54th in rating), while the sets identified by our methods provided 76–79% of correctly classified cases (Fig. 8 and Table 2).

The sets of genes that we obtained in this study cannot be considered to be tumor grade signatures because their number (three) was selected arbitrarily. Our initial set of 14 genes was selected almost arbitrarily, as well, from the large number of genes available in the NCBI GEO database. Nevertheless, we can say that these methods can be used as a basis for the gene selection procedure and signature design. The efficacy is assumed to be no less than that achieved in the current study (79.4%) and could reach a higher amount.

Another real world data set (voice features for the PD diagnostics) showed the same advantages and weaknesses of the proposed methods as those shown in the genes data set. The calculation time of αT
                         or αN
                         in this case was approximately 37 and 4.5min respectively, for all possible feature combinations. The bootstrap we performed for the feature combinations listed in Table 3 took approximately 240s for one set. Therefore, for all possible feature sets it will take 240×1023=245,520s; that is, approximately 68.2h.

@&#CONCLUSIONS@&#

In this article, non-time-consuming methods of selecting variables for further training of an artificial neural network are introduced. These methods are based on the self-organizing maps that are implemented for the data clusterization quality evaluation and are applicable in a situation in which there is nonlinear discrimination between classes. Additionally, these methods do not require splitting of the data into the training and test sets; this absence of a splitting requirement makes the methods feasible in situations in which there are small data volumes.

The proposed methods can be implemented in combination with metaheuristics methods for optimal feature set selection, as described in [34] or [35]. This approach will reduce the time cost of the calculations significantly and increase the number of available variables.

This work was supported by the Russian Foundation for Basic Research [12-04-90434-Ukr_a] and the Grant of the Saint-Petersburg Government (2012).

@&#ACKNOWLEDGMENTS@&#

The authors are grateful to Prof. V. Kavsan and Prof. V. Dmitrenko from the Institute of Molecular Biology and Genetics of NASU, Kiev, Ukraine, who provided us with information on the NCBI GEO gene expression database and extracted the data that we used from it. The authors are also grateful to Neil Martin (SUT) for his help in the preparation of the text.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2014.06.001.


                     
                        
                           Supplementary data 1
                           
                        
                     
                  

@&#REFERENCES@&#

