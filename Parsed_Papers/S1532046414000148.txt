@&#MAIN-TITLE@&#Automatic recognition of disorders, findings, pharmaceuticals and body structures from clinical text: An annotation and machine learning study

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Disorders, Findings, Drugs and Body parts were annotated in Swedish clinical text.


                        
                        
                           
                           A conditional random fields model was trained to recognise the annotated entity types.


                        
                        
                           
                           English clinical entity recognition approaches were also suitable for Swedish.


                        
                        
                           
                           Disorder and Finding are more granular categories than those used in most other studies.


                        
                        
                           
                           Results for these two separate categories show that this division is meaningful.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Named entity recognition

Corpora development

Clinical text processing

Disorder

Finding

Swedish

@&#ABSTRACT@&#


               
               
                  Automatic recognition of clinical entities in the narrative text of health records is useful for constructing applications for documentation of patient care, as well as for secondary usage in the form of medical knowledge extraction. There are a number of named entity recognition studies on English clinical text, but less work has been carried out on clinical text in other languages.
                  This study was performed on Swedish health records, and focused on four entities that are highly relevant for constructing a patient overview and for medical hypothesis generation, namely the entities: Disorder, Finding, Pharmaceutical Drug and Body Structure. The study had two aims: to explore how well named entity recognition methods previously applied to English clinical text perform on similar texts written in Swedish; and to evaluate whether it is meaningful to divide the more general category Medical Problem, which has been used in a number of previous studies, into the two more granular entities, Disorder and Finding.
                  Clinical notes from a Swedish internal medicine emergency unit were annotated for the four selected entity categories, and the inter-annotator agreement between two pairs of annotators was measured, resulting in an average F-score of 0.79 for Disorder, 0.66 for Finding, 0.90 for Pharmaceutical Drug and 0.80 for Body Structure. A subset of the developed corpus was thereafter used for finding suitable features for training a conditional random fields model. Finally, a new model was trained on this subset, using the best features and settings, and its ability to generalise to held-out data was evaluated. This final model obtained an F-score of 0.81 for Disorder, 0.69 for Finding, 0.88 for Pharmaceutical Drug, 0.85 for Body Structure and 0.78 for the combined category Disorder+Finding.
                  The obtained results, which are in line with or slightly lower than those for similar studies on English clinical text, many of them conducted using a larger training data set, show that the approaches used for English are also suitable for Swedish clinical text. However, a small proportion of the errors made by the model are less likely to occur in English text, showing that results might be improved by further tailoring the system to clinical Swedish. The entity recognition results for the individual entities Disorder and Finding show that it is meaningful to separate the general category Medical Problem into these two more granular entity types, e.g. for knowledge mining of co-morbidity relations and disorder-finding relations.
               
            

@&#INTRODUCTION@&#

Electronic health records contain valuable information in the form of symptom descriptions, documentation of examinations, diagnostic reasoning and motivations for treatment decisions. Automatic extraction of this information makes it possible to improve applications for patient care documentation, and enables secondary usage of the information in the form of medical knowledge mining.

While a subset of the health record information, e.g. medication lists and diagnosis coding, is documented in a structured format, much important information is only available as free text [1]. An automatic summary of the free text part is therefore called for, providing health personnel with the possibility of forming a quick overview of the patient [2,3]. The information contained in health records can also be used for clinical text mining, i.e. to generate new medical knowledge from a large corpus of electronic health records. Syndromic surveillance [4], comorbidity studies [5] and automatic detection of adverse drug reactions [6] are examples of clinical text mining applications.

An important component in information extraction from health record text is named entity recognition (NER) of relevant entities mentioned in the text, i.e. the automatic detection of spans of text referring to entities of certain semantic categories [7]. This study focuses on recognition of four entity categories that are particularly relevant for constructing a patient overview as well as for studies of co-morbidity [5,8], disorder and finding co-occurrences [9] and adverse drug reactions [6], namely the categories: Disorder, Finding, Pharmaceutical Drug and Body Structure.

There are previous studies on the recognition of clinical entities in English text, but very few studies have been carried out on clinical text written in other languages. The present study was performed on Swedish clinical text, and although both Swedish and English are Germanic languages, NER in Swedish poses additional challenges, as Swedish is more inflective and compounding of words occurs frequently. In addition, medical terminological resources are less extensive for Swedish than for English.

Moreover, previous annotation and NER studies have typically combined the two more granular entity categories Disorder and Finding into one more general category, e.g. called Condition or Medical Problem [10–12], or have focused only on the entity category Disorder [13,14]. To the best of our knowledge, there is only one previous corpus [15] in which the categories Disorder and Finding are annotated as two separate entity categories. The study describing the creation of this corpus does not, however, investigate the effect of this more granular division.

The present study therefore has two specific research questions:
                        
                           •
                           To what extent is it possible to use the NER methods, which have been successful for English clinical texts, on health record texts written in Swedish?

To what extent is it possible to separate the more general entity category Medical Problem into the two more granular entity categories Disorder and Finding?

There are several studies that describe the creation of corpora annotated for named entities and that measure inter-annotator agreement scores between annotators (Table 1
                     ). There are also a number of studies in which the created corpora are used for training and/or evaluating NER systems (Table 2
                     ).

The annotation study by Chapman et al. [16] showed that detailed guidelines for annotating Clinical Conditions resulted in a substantially higher F-score than less detailed, but no significant differences in inter-annotator agreement between pairs of physicians and between physicians and lay people were found (but lay people required more training and had a lower ability to retain their annotation skills over time). Within the CLinical E-Science Framework, Roberts et al. [17] observed a higher F-score for lay people (a biologist/linguist and a computational linguist) than for a clinician, when measuring agreement to a constructed consensus set containing annotations for Condition (symptom and diagnosis), Drug or Device and Locus (e.g. anatomical structure or location). Wang [18] measured the inter-annotator agreement between two computational linguists annotating the categories Finding (corresponding to Medical Problem), Substance and Body, while Ogren et al. [19] measured the average agreement between four clinical data retrieval experts for annotating identical spans of text denoting the category Disorder. For the i2b2 medication challenge 
                     [20,21], inter-annotator agreement was calculated for annotations of Medication Names on pre-annotated data. No statistically significant differences were observed between pairs of NLP community annotators, pairs of expert annotators, or pairs of experts annotating raw text. One participating group [22] annotated an additional subset of the development data provided for the challenge. Pre-annotation was also applied when annotating the MiPACQ corpus [15] for entity categories including Disorder, Anatomy, Sign or Symptom, and Chemical and Drug.

The created corpora have been used as training and evaluation data for machine learning-based NER systems and as evaluation data for rule- and terminology-based systems. An SVM (support vector machine) with uneven margins was trained on a subset of the CLinical E-Science Framework corpus [24], and two studies have been performed on the corpus created by Wang; one using the CRF (conditional random fields) package CRF++ [18] and one combining output from CRF++ with an SVM and an ME (maximum entropy) classifier [10]. All but one of the best performing systems in the i2b2/VA challenge on concepts, assertions, and relations used CRF for concept recognition [23,25]. The best performing system (by de Bruijn et al. [11]) instead used semi-Markov HMM. The second best (by Jiang et al. [12]) found that CRF (CRF++) outperformed SVM, and also managed to improve the results with a rule-based post-processing module. In the i2b2 medication challenge, on the other hand, which included the identification of Medication Names, a majority of the ten top-ranked systems were rule-based [20]. The best performing system (by Patrick and Li [22]) did, however, use CRF++, while the second best (by Doan et al. [26]) was built on terminology matching and a spell checker developed for drug names. This rule-based system was later employed by Doan et al. [27] in an ensemble classifier, together with an SVM and a CRF++ system. On the Ogren et al. [19] corpus, a terminology-based method for recognising disorders by matching to SNOMED CT has been evaluated [14,28], and there is also a terminology-based study for recognising diseases and drugs in Swedish discharge summaries, described by Kokkinakis and Thurin [13], in which the MeSH terminology was used.

Typical features used for training the machine learning models were the tokens (sometimes in a stemmed form), orthographics (e.g. number, word, capitalisation), prefixes and suffixes, part-of-speech information, as well as the output of terminology matching, which had a large positive effect in many studies (e.g. [10,18]). Most studies used features extracted from the current and the two preceding and two following tokens, while Roberts et al. [24] used a window size of ±1. The best performing system in the i2b2/VA concepts challenge used a very large feature set with a window size of ±4, also including character n-grams, word bi/tri/quad-grams and skip-n-grams, as well as sentence, section and document features (e.g. sentence and document length and section headings). In addition, features from semi-supervised learning methods were incorporated, in the form of hierarchical word clusters constructed on unlabelled data [11].

@&#METHODS@&#

A corpus was first annotated and evaluated. Thereafter, suitable features for the NER task were evaluated and a model was trained using these features and evaluated on held-out data. Finally, an error analysis was carried out.

The IOB-encoding [7, pp. 763–764] of the annotated entities was used, as exemplified in Fig. 1
                     . As machine learning algorithm, the CRF (conditional random fields) [29] implementation CRF++ [30] was chosen, which has been used in many previous clinical NER studies. CRF++ was used as linear chain CRF, in which each output variable is dependent on the previous and subsequent output variable.

To study clinical narratives with a variety of disorders, free text sections from clinical notes from an internal medicine emergency unit (from Karolinska University Hospital) were compiled into a corpus.
                           1
                           This research has been approved by the Regional Ethical Review Board in Stockholm (Etikprövningsnämnden i Stockholm), permission number 2012/834-31/5.
                        
                        
                           1
                         Texts with the Assessment sub-heading were chosen as these contain reasoning about findings as well as diagnostic speculations, which means that they contain many mentions of disorders and findings and are particularly suited for a study of these two entity categories. Assessment fields are also interesting in that they form a prototypical example of the difficulties associated with conducting text processing on clinical text, as they are written using a highly telegraphic language, containing many abbreviations and few full sentences. The compiled corpus consisted of 1,148 randomly selected Assessment fields that were extracted from the health record database Stockholm EPR Corpus 
                        [31], which contains patient records written in Swedish from the years 2006 to 2008.

The definition of the four used annotated entity categories can be summarised as follows: (1) A Disorder is a disease or abnormal condition that is not momentary and that has an underlying pathological process. (2) A Finding is a symptom reported by the patient, an observation made by the physician or the result of a medical examination of the patient. This includes non-pathological findings with medical relevance.
                           2
                           The definition for Disorders and Findings is a summary of the definition in the SNOMED CT Style Guide [32].
                        
                        
                           2
                         (3) A Pharmaceutical Drug is a medical drug that is either mentioned with a generic name or trade name, or with other expressions denoting drugs, e.g. drugs expressed by their effect, such as painkiller or sleeping pill. Narcotic drugs used outside of medical care were excluded. (4) A Body Structure is an anatomically defined body part, excluding body fluids and expressions indicating positions on the body.

Annotation guidelines were developed by a senior physician with previous experience of annotating clinical text (PH1) and a computational linguist without previous annotation experience (CL). A test annotation of 664 Assessment fields (not included in the 1,148 fields compiled for the final corpus) was performed by PH1, for annotator training as well as for development of the annotation guidelines. Whenever there was an issue not covered by the annotation guidelines, it was discussed and the guidelines were updated accordingly. At first, there were many modifications of the guidelines, but the need for modifications gradually decreased. The final version of the guidelines were reviewed by a second physician, also with previous experience of annotating clinical text (PH2).

The following are the most important points of the guidelines
                           3
                           The complete guidelines are available at http://dsv.su.se/health/guidelines/.
                        
                        
                           3
                        : The shortest possible expression that still fully describes the entity was annotated. Modifiers that, for example, describe severity were therefore excluded, while modifiers describing the type of an entity were included. In the example: The patient experiences a strong stabbing pain in left knee,
                           4
                           In Swedish: Patienten känner en kraftigt huggande smärta i vänster knä.
                        
                        
                           4
                         the words strong and left were therefore not annotated, whereas stabbing pain was annotated as a Finding, and knee as a Body Structure. All mentions of any of the four selected classes were annotated, regardless of whether, for example, a Disorder was referred to with an abbreviation or acronym or in a negated or a speculative context, or the person experiencing the Finding was someone other than the patient. The guidelines also included rules for handling the frequent occurrence of compound words. Compound words were not split up into substrings, and therefore, for example, diabetes in diabetesclinic
                        
                           5
                           In Swedish: diabetesklinik.
                        
                        
                           5
                         was not annotated as a Disorder, whereas the word heartdisease
                        
                           6
                           In Swedish: hjärtsjukdom.
                        
                        
                           6
                         which is a compound denoting a Disorder, was annotated as such. A compound including a treatment with a pharmaceutical drug was, however, classified as belonging to the entity category Pharmaceutical Drug.

The definition of Finding was broader than the definition used in other annotation studies, e.g. i2b2 [25], and more closely followed the definition in SNOMED CT, as also non-pathological, medically relevant findings were included. The guidelines did, however, agree with the i2b2 guidelines [25] in that only findings that were explicitly stated (e.g. high blood pressure) were included, whereas test measures (e.g. blood pressure 145/95) were not annotated.

PH1 had the role as the main annotator, annotating all notes included in the study. A subset of the notes were independently annotated by PH2, and yet another subset was independently annotated by CL (Table 3
                        ). To become familiar with the annotation task, PH2 and CL carried out a test annotation on 50 notes. Neither these test annotations, nor the texts annotated by PH1 in the guideline development phase, were included in the constructed corpus. The annotation tool Knowtator [33], a plug-in to Protégé, was used for all annotations. The doubly annotated notes were used for measuring inter-annotator agreement (and thereby the reliability of the annotations [34]), as well as for constructing a reference standard to use in the final evaluation.

Inter-annotator agreement was measured in terms of F-score, as e.g. the frequently used inter-annotator agreement measure kappa cannot be applied on this task that lacks well-defined negative cases [35]. Disagreements were: entities annotated by only one of the annotators, differences in choice of entity category, and differences in the length of annotated text spans.

Out of a large subset (25,370 tokens) of the doubly annotated notes shown in Table 3, a sub-corpus was created to use in the evaluation phase of the machine learning study (the Final Evaluation subset). This sub-corpus was compiled by PH1 who resolved each conflicting annotation in the doubly annotated data. A program for presenting and resolving annotations was developed, which presented pairs of conflicting annotations on a sentence level, without revealing who had produced which annotation. PH1 could thereby select one of the presented annotations without knowing who had produced it, thereby minimising bias.

The rest of the annotated corpus (the Development subset), was used for feature selection and as training data for the final model. The properties of each of the two subsets are presented in Table 4
                        .

The most frequently used features among previous studies on English clinical text were evaluated, including terminology matching. MeSH, ICD-10 and SNOMED CT are available in Swedish, as well as FASS, which includes a list of pharmaceutical drugs used in Sweden [36]. One of the challenges of clinical NER in Swedish is, however, that medical terminologies are less extensive for Swedish than for English, as e.g. SNOMED CT only contains the preferred term for each concept and lacks synonyms. We have previously developed terminology-based systems for Swedish clinical text, detecting the entities Disorder, Finding and Body Structure [37] as well as Pharmaceutical Drug [38]. For the present study, these systems were extended by adding a more extensive version of MeSH [39] as well as a vocabulary list of general Swedish, extracted from the Swedish Parole corpus, containing non-medical language compiled from e.g. newspaper texts and fiction [40]. All semantic classes in SNOMED CT and MeSH were used as feature values, and tokens matching the FASS vocabulary [36] were given the feature value Pharmaceutical Drug. Tokens matching (the descriptions of) ICD-10 codes in chapter 1–17 and 19 (except codes T36–T62.9, which list substances) were assigned the feature value ICD-10-disorder and tokens matching codes in chapter 18 (listing symptoms and clinical findings) were assigned the value ICD-10-finding. Tokens not found in any of the medical resources, but in the vocabulary from Parole, were assigned the feature value Parole, and tokens not found in any terminology were given the value Unknown. A token matching several semantic classes was assigned feature value according to the priority stated in the annotation guidelines (e.g. the SNOMED CT category Qualifier having the highest priority, followed by Body Structure, Disorder, Finding and Pharmaceutical). As described in the previous terminology matching study [37], the SNOMED CT list of body structures was stop word filtered.

Another particular challenge for NER in Swedish text compared to English text is the frequent occurrence of compound words. Compound word splitting was therefore added as an additional feature not used in previous clinical NER studies. We implemented a simple compound splitting, dividing words into a maximum of two word constituents if at least one of the constituents was found in one of the vocabulary lists and the other constituent was found when applying fuzzy matching. The fuzzy matching, which used a maximum string distance of one, was motivated by the fact that Swedish compounds are sometimes constructed with an ‘s’ binding the two constituents, and sometimes constructed by the removal or change of the last vowel in the first constituent. The compound splitting favoured two equally long constituents over one short and one long constituent. The minimum length of a constituent was set to four letters. Compound splitting was applied on tokens not found in any of the vocabulary lists used. If it was possible to divide a token, the constituents as well as the semantic classes found by the terminology matching for the constituents were used as features.

A third challenge is that Swedish is more inflective than English, increasing the importance of morphologic normalisation. The Swedish lemmatiser Granska [41] was therefore applied on the texts to obtain lemma forms of tokens to use as features for the machine learning, as well as for being able to use normalised forms for the terminology match. Granska was also used for obtaining part-of-speech information.

The following features were added one by one, in the following order: (1) The current token. (2) The lemma form of the current token. (3) The lemma form of surrounding tokens. (4) Part-of-speech of the current token and surrounding tokens. (5) The output of the terminology-based system for the current token and surrounding tokens. (6) Compound splitting for the current token. (7) Orthographic features, i.e. initial upper case, all upper case or no upper case for the current token (Fig. 1).

The use of an increasingly larger window size was evaluated for features (2)–(5), as well as fuzzy terminology matching. Features and window sizes that lead to an improved result were retained, whereas the others were not used. For the best feature combination, the CRF++ regularisation hyper-parameter for balancing between under-and over-fitting, was also varied. For all experiments, L2-regularisation was used.

We chose to use 30-fold cross validation on the Development subset for selecting features. This is more time-consuming than using the more standard approach of 10-fold cross validation, but has the advantage that a larger proportion of the Development subset is used as training data in each fold, thereby more closely resembling the situation that is being optimised for, that the entire Development subset is available as training data.

As the features were added incrementally, with each feature improving the results being retained, figures from this evaluation could not determine how much each individual feature contributed to the results. Therefore, models were constructed, in which one feature type at a time was removed while retaining all other features. 30-Fold cross validation was used also for the evaluation of these models, which were constructed with the best settings and all the best features, except the feature type whose contribution was to be evaluated.

In order to obtain the final results, we evaluated how well a model trained using the best features would perform in a deployment setting. A CRF model was therefore trained on the entire Development subset using the best features (i.e. the features for which best results were achieved in the feature selection process), and this model was then evaluated on the previously unused Final Evaluation subset.

As a final step, an error analysis was carried out. The error analysis was performed on NER classifications obtained when applying 30-fold cross-validation with the best features on the Development subset. Using the Development subset means that it is also possible to treat the Final Evaluation subset as unseen data in future studies. A manual error analysis was carried out by PH1 for the classes Disorder, Drug and Body Structure, while only errors due to incorrect span and to confusion between classes were measured for Finding (due to the large number of instances for this class).

@&#RESULTS@&#

The results consist of inter-annotator agreement scores for the annotated corpus, as well as results for the feature selection and for the NER model evaluated on held-out data. The results were measured using precision, recall and F-score, all calculated with the CoNLL 2000 script [42].

Inter-annotator agreement between the physicians (PH1 and PH2), between the main physician annotator and the computational linguist (PH1 and CL), and the average results are shown in Table 5
                         for the four categories, as well as for Disorder and Finding merged into one class. Annotations for the entity Pharmaceutical Drug had the highest agreement for both pairs of annotators, and there was also a relatively high agreement for annotations of Body Structure and Disorder, while the agreement for Finding was lower. Finding was also the only category for which there was a large difference between the two pairs of annotators, with higher agreement between PH1 and CL than between PH1 and PH2. When automatically removing all annotated Findings containing a number for PH2 and CL (to make them better comply with the guidelines), the agreement between the two physicians increased from 0.58 to 0.61, but still remained lower than the agreement between PH1 and CL.

In the Development subset, there were a total of 27 unique entity types sometimes annotated as a Disorder and sometimes as a Finding: this was 3% of the total number of unique entity types among annotated Disorders and Findings. Among these, some were equally frequently annotated as a Disorder and as a Finding (e.g. stress and muscle pain), whereas some more often were classified as a Finding (e.g. dizziness/vertigo and tachycardia). In accordance with the annotation guidelines, these entities were annotated as a Finding when they were symptoms of another disorder, and as a Disorder when they were the main medical problem described in the Assessment field.

Feature selection was performed through 30-fold cross-validation on the Development subset, and by selecting features that maximised the macro-averaged F-score over the four entity categories. The following features and window sizes gave the best average results: (1) Lemma forms for the current token and the previous token. (2) Part-of-speech for the current token, the following token and the two previous tokens. (3) Terminology match for the current token and the previous token. (4) The compound splitting features for the current token. (5) Orthographic features for the current token.

The features used when obtaining the best results are illustrated with boldface in Fig. 1. Fuzzy terminology matching, using a Levenshtein distance of one, as well as larger window sizes for lemma, part-of-speech and terminology match, were evaluated, which, however, gave slightly lower results.

The best average results using the selected features are shown in boldface in Table 6
                        , while the boldfaced figures in Tables 7–10
                        
                        
                        
                         show the results for each individual category. Results were high above the baseline for all categories, with the smallest difference for Pharmaceutical Drug, which had the best baseline. Tables 6–10 also show the extent to which each individual feature type contributed to the best results, by presenting results when one feature type at a time was removed, while all other features were retained. That is, e.g. Best settings – Orthographics shows the results when all the best features except orthographics were used. Without lemmatisation shows the results when using the current and previous token instead of the current and previous lemma. The evaluation showed that the current lemma was the most important feature for all categories, while terminology matching was the second most important feature, except for the category Finding, for which lemmatisation was more important. All other features played a minor role.

Results are presented for a CRF++ hyper-parameter of 6, for which the best results were achieved when giving the parameter integer values between 1 and 11 (with the best average F-score of 0.794 at 6 and the lowest F-score of 0.779 at 1).

The best parameter and feature settings were also used when training a model on the entire Development subset. The results when evaluating this model on the held-out Final Evaluation subset are shown in Table 11
                        . All categories were recognised with an F-score that is in line with the average inter-annotator agreement scores, and the final results were also very close to those achieved on the Development subset during feature selection, which shows that the results obtained during development generalise well on unseen data.

The results of the error analysis are shown in Tables 12 and 13
                        
                        . Confusions between entity categories (especially between Disorder and Finding), manual annotation errors, an incorrect span and borderline cases (i.e. when it was not evident whether the CRF classifier or the human annotator was correct) were identified error types among false positives as well as among false negatives. An entity span that was longer than the annotated span was counted as a false positive, whereas a too short span was classified as a false negative. Abbreviations were also a source of both false positives and negatives, as were compound words. A compound word that was incorrectly classified as belonging to an entity category had either as one of its constituents a word belonging to that category (e.g. lyme-disease-test
                        
                           7
                           In Swedish: borreliaprov.
                        
                        
                           7
                         and liver-values
                        
                           8
                           In Swedish: levervärden.
                        
                        
                           8
                        ), or had as one of its constituents a word that frequently occurred in compound words of this category (e.g. COPD-treatment,
                           9
                           In Swedish: KOL-behandling.
                        
                        
                           9
                         which was misclassified as a Drug because of the constituent treatment). Compound words were also frequent among false negatives, both compounds of full-length words and compounds with an abbreviation as one of its constituents. Other false negatives were inflected words, which the lemmatiser had failed to lemmatise, misspellings or spelling variants and entities expressed with jargon or non-standard language. Among the false negatives for the category Drug grouped into Incorrect: No other reason were expressions for groups of medicines, for instance Cortisone.

@&#DISCUSSION@&#

A comparison between the final results of the created NER model and results of previous clinical NER studies answers the research question of whether previous approaches for English are applicable on Swedish clinical text. The achieved NER results, as well as an analysis of the confusion between categories for different annotators and for the NER classifier, also answer the research question as to what extent it is possible to separate the category Medical Problem into the two more granular entity categories Disorder and Finding.

Apart from the fact that guidelines for handling compound words are needed, entity annotation of Swedish clinical text does not pose any evident challenges additional to those posed by entity annotation of English text. It is, therefore, not surprising that there are no systematic differences between the inter-annotator agreement figures reported here and the results from previous English studies. Our reported inter-annotator agreement for Pharmaceutical Drug (average F-score 0.90) was better than the average agreements for the category Drug or Device reported by Roberts et al. (F-scores 0.84, 0.32 and 0.59 for three different document types) but slightly lower than the agreement reported by Wang and Patrick for the comparable category Substance (F-score 0.95). Also for Body Structure (average F-score 0.80), the agreement presented here was slightly higher than the figures reported for the category Locus by Roberts et al. (F-scores 0.78, 0.75 and 0.71), but lower than the agreement for Body presented by Wang and Patrick (F-score 0.85). For the combined category Disorder+Finding, our agreement (F-score 0.78, or 0.80 when numerical values were removed) was lower than the agreement for the corresponding categories Clinical Condition reported by Chapman et al. (F-score 0.92) and Finding reported by Wang and Patrick (F-score 0.91). The agreement figures for Condition, reported by Roberts et al. (F-scores 0.81, 0.77, 0.67), however, closely match or are lower than our agreement for Disorder+Finding. Also our inter-annotator agreement for the separate category Disorder (F-score of 0.79) was slightly higher than the agreement reported by Ogren et al. for Disorder (F-score 0.76). Finally, it can be noted that the similarity in inter-annotator agreement figures between the pairs of physicians, and the physician versus the computational linguist, is also in accordance with results from previous studies [16,17].

For NER of the annotated entities on the other hand, differences between English and Swedish and between available terminological resources might affect the results. Fig. 2
                         gives an overview of NER results from a number of previous studies and compares them to the results achieved here. The results achieved in these previous studies vary, which e.g. could be attributed to variations in the exact definitions of entities, to the type of clinical text that is studied, as well as to the size of the used training data. The best results among these studies for recognising the categories Disease and Drug were achieved by Kokkinakis and Thurin [13] with a rule- and terminology-based method using a restricted vocabulary list. These results, which are better than the results obtained here, can probably be partly explained by their study being carried out on discharge summaries, which are the opposite in terms of style to e.g. Assessment fields. The other rule- and terminology-based system evaluated on discharge summaries (by Doan et al. [26]) obtained slightly lower results for Medication Names than we obtained for the entity Pharmaceutical Drug, and the rule- and terminology-based system evaluated on the corpus created by Ogren et al. (which contains several different clinical text types [14,28]) achieved lower results for Disorder than what was obtained here.

There is also a machine learning study by Patrick and Li [22], obtaining results for Medication Names that are very similar to the results we achieved for Pharmaceutical Drug, despite the fact that the study by Patrick and Li was conducted solely on discharge summaries. Doan et al. [26] showed, however, that better results can be achieved using the same corpus; results that exceed those achieved here. Also the i2b2 2010 challenge corpus consists to a large extent of discharge summaries, and the entity Medical Problem was recognised by Jiang et al. [12] with higher precision and recall than we obtained for the comparable category combination Disorder+Finding. The fact that the i2b2 2010 challenge corpus contains a considerably larger set of annotated entities probably had additional positive effects on the results.

The two English NER studies that are most similar to the present study, in terms of number of available annotated entities and also in terms of clinical text types, were conducted by Wang and Patrick [10] and by Roberts et al. [24]; Wang and Patrick using intensive care service progress notes and Roberts et al. using a number of different clinical text types. For entity categories similar to Disorder+Finding and Pharmaceutical Drug, Wang and Patrick had a somewhat larger set of annotated entities and also achieved somewhat better results than presented here, while the opposite holds true for Roberts et al., who used a smaller set of annotated data and achieved lower results. For Body Structure, however, both Wang and Patrick and Roberts et al. present lower results than those achieved here, for Wang and Patrick possibly since nested annotations were allowed, making the Body entity more difficult to recognise.

It can also be noted that, similar to previous studies, terminology matching proved to be an important feature, while in contrast to most previous studies, a small window size gave the best results. That a small window size was most optimal might be explained by the training data not being large enough for the number of features that is generated when many feature types with a large number of possible values are included.

In summary, the results presented here are in general in line with or slightly lower than results for presented previous studies. This difference is, however, probably not primarily caused by the study being conducted on Swedish texts, but could be attributed to the fact that we used a wide definition of entities (as non-pathological findings and pharmaceuticals expressed in general terms were included), a clinical text type for which a highly telegraphic language is used and in which a large variety of disorders and findings occur, and that we had a smaller number of entities in our training data compared to some previous studies. The error analysis shows, however, that some of the identified errors belong to error types that are unlikely to occur in English text. Some of the false negatives caused by entities being expressed with jargon, spelling variants and abbreviations might have been avoided with more extensive terminology resources, which are available for English. There were also some errors caused by the lemmatiser failing to lemmatise inflected words, which indicates that an adaption of the lemmatiser to the medical domain is needed. Compound words were frequent among false negatives, including compounds with an abbreviation as one of its constituents. Compound splitting improved the average recall by 1.7 percentage points, but the implemented compound splitter was dependent on finding a constituent in either the Parole corpus or in a medical terminology, which had the effect that compounds containing abbreviations or medical terms not included in a terminology were not split. Also the heuristics of having a minimum length of four letters for a constituent prevented compound splitting of words containing an abbreviation. However, compound splitting also reduced the average precision by 0.5 percentage points, indicating that an improved splitting is not enough, but has to be combined with, e.g. a larger training set. The false positives for compound words also show that the entity recognition task defined here might be somewhat more difficult than the task defined for previous English studies, as e.g. the word liver in liver-values, would be defined as a Body Structure in previous English studies, whereas we did not define constituents of compound words as belonging to an entity category.

The inter-annotator agreement scores, as well as the results from the NER model, show that the two categories Disorder and Finding are more difficult to differentiate than the other categories. The agreement for the category Finding was low for both pairs of annotators, 0.58 for PH1-PH2 and 0.73 for PH1-CL, while the agreement for Disorder was higher with an F-score of 0.77 for PH1-PH2 and 0.80 for PH1-CL. Merging the two classes Disorder and Finding, resulted in higher agreement between the two physicians (F-score 0.72) than would be the case for a weighted average of the two classes (F-score 0.65). For the physician and the computational linguist, the result of merging the classes had an even larger effect, with an F-score of 0.84. These figures show that disagreements between annotators were often with regard to which of these two categories to use. However, in the Development subset, annotated by the main annotator, only 3% of the unique entity types among annotated Disorders and Findings were annotated as Disorders in some contexts and as Findings in other contexts, which supports the meaningfulness of this more granular division.

The error analysis of the NER model shows that in many of the cases for which the system failed to detect a Disorder, it had instead classified the entity as a Finding (31% of the false negatives for Disorder), and vice versa (9% of the false negatives for Finding). Whether it is meaningful to divide Medical Problem into the two more granular categories Disorder and Finding is dependent on the intended application of the NER system. An automatically generated high-level patient summary might for instance place high demands on recall of the category Disorder. This means that recognised entities classified as belonging to the category Finding (or entities for which the model is uncertain of whether to classify it as a Finding or a Disorder) also ought to be included in the summary, thereby boosting recall for included Disorders. The results achieved, i.e. recognising the category Disorder with a precision of 80% and Finding with a precision of 72% are, however, likely to be high enough for it to be meaningful to make a distinction between these categories when mining for new medical knowledge (e.g. since known co-morbidities have been successfully extracted from structured data that contains inaccuracies [8,44]). Having access to a NER system that distinguishes between disorders and findings makes it possible to separately mine for co-morbidity relations and disorder-finding relations.

@&#LIMITATIONS@&#

The inter-annotator agreement shows how reliable the evaluation data is for measuring the performance of the evaluated system, and gives an indication of the difficulty of recognising the four entities, especially of their relative difficulty. The agreement scores cannot, however, be used as an absolute upper ceiling for the performance of the NER system, since the system mimics the behaviour of one single annotator, which might be easier than agreeing on how to annotate given the annotation guidelines.

Another limitation concerns the involvement of the authors of this paper in the annotation and feature experiments. PH1 and PH2 might have been biased towards producing annotations which they suspected would be easier for the NER system to detect. However, as neither of these two annotators were involved in the development of the NER system, it is unlikely that they knew enough of the system for this possible bias to have any large effects on the results. CL, on the other hand, who annotated half of the evaluation data, was the main person responsible for the NER experiments. However, as only PH1 was involved in the construction of the final version of the evaluation data, the risk that CL biased the evaluation data is likely to be a minor one.

Future work includes further improving the NER system (e.g. by further adapting it to clinical Swedish by improved compound splitting and lemmatisation) as well as adapting the system to other clinical text types. As the relatively small size of the training data might have influenced the results, a possible future direction could be to provide more annotated data. However, since annotating data is costly, a more important contribution would be to study how the results can be improved, or how the constructed model can be applied to another clinical domain, with a minimum of additional data. This could, for instance, be achieved by using features from unsupervised methods [11] or by cleverly selecting the data to annotate by using active learning [45].

@&#CONCLUSION@&#

This study has shown that clinical NER methods previously applied on English are successful also on Swedish clinical text. The category Disorder was recognised with an F-score of 0.81; Finding with an F-score of 0.69; Drug with an F-score of 0.88; Body Structure with an F-score of 0.85; and the combination Disorder+Finding with an F-score of 0.78. These results are in line with, or slightly lower than, published results for similar studies on English texts. The slightly lower results achieved here could probably to a large extent be explained by the smaller size of the training data and by the fact that our study was conducted on clinical text extracted from Assessment fields and that wide definitions of the entity categories were used. A small proportion of the errors made by the NER system were, however, errors less likely to occur in an English text, such as the lemmatiser failing to lemmatise medical terms, errors caused by compounding and by compounds containing an abbreviated constituent. The smaller size of the available Swedish vocabularies might also have affected the results.

The study has also shown that a distinction between the two more granular categories Disorder and Finding is sometimes difficult to make, but that the NER results for the two separate categories are high enough for this separation to be meaningful for some applications. A NER system separating disorders and findings could, for instance, be used for knowledge mining of co-morbidity relations and of disorder-finding relations.

MS was responsible for the overall design of the study and for the NER part, while MK was responsible for the annotation part. MK developed the annotation guidelines with assistance from MS, and GN revised them. MK was the main annotator, while MS and GN annotated subsets of the data. MK also carried out the error analysis. MS designed and carried out the NER experiments, with feedback from HD. HD drafted parts of the background, while MS drafted the rest of the manuscript with feedback from the other authors. All authors read and approved the final manuscript.

@&#ACKNOWLEDGMENTS@&#

This work was partly supported by the Swedish Foundation for Strategic Research through the project High-Performance Data Mining for Drug Effect Detection (Ref. No. IIS11-0053) at Stockholm University, Sweden. It was also partly supported by Vårdal Foundation. We are very grateful to the reviewers for their many detailed and constructive comments, and we would like to thank Aron Henriksson and Magnus Ahltorp for fruitful discussions on the design of the study.

@&#REFERENCES@&#

