@&#MAIN-TITLE@&#Rough set based rule induction in decision making using credible classification and preference from medical application perspective

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Discover certain regularities at the primitive concept level.


                        
                        
                           
                           Improve the efficiency of the discovery process and express the user's preference.


                        
                        
                           
                           Develop high quality attribute-oriented and rule-based models.


                        
                        
                           
                           Find the causes for solitary pulmonary nodule and results of the long term treatment.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Rough set theory

Credible index

Rule induction

Medical prediction

@&#ABSTRACT@&#


               
               
                  This paper presents a new heuristic algorithm for reduct selection based on credible index in the rough set theory (RST) applications. This algorithm is efficient and effective in selecting the decision rules particularly the problem to be solved in a large scale. This algorithm is capable to derive the rules with multi-outcomes and identify the most significant features simultaneously, which is unique and useful in solving predictive medical problems. The end results of the proposed approach are a set of decision rules that illustrates the causes for solitary pulmonary nodule and results of the long term treatment.
               
            

@&#INTRODUCTION@&#

Information technology has become an integral part of the medical and health care systems in today's world. Clinicians (doctors, nurses, and allied health professionals) rely on mission critical systems to help them make correct judgments on diagnoses. These systems collect, store, and provide access to patients’ data. As patient's data and medical information continue to be accumulated, information overload becomes an issue. For example, data generated from medical diagnoses (e.g., patients’ blood pressure, body mass index, age, etc.) can be voluminous, making it difficult for doctors to judge the disease correctly. Large amount of medical information available on the Internet can easily overwhelm a patient who seeks to use such information, leading to undue anxiety or incorrect self-diagnosis [1]. Similarly, advances in scientific research have led to an exponential growth in medical and biomedical data that hinder understanding of the underlying phenomena.

Automated decision support for clinicians has been proposed in recent years to assist medical diagnosis and to provide evidence-based patient care [2]. A study of 55,000 clinicians in public hospitals on how they use an online clinical information retrieval system shows that their use of online evidence increases significantly with patient admissions. This demonstrates the potential of automated decision support systems (DSS) in influencing clinical decisions. However, as Ruland et al. [2] point out, there has been little work devoted to the development of computer-supported systems that assists clinicians’ judgments and diagnoses [2]. In particular, data mining and machine learning, though widely used in businesses, are not commonly applied to automation of clinical decision support.

As a generalization-based information modeling method, attribute-oriented induction provides an efficient way to extract different kinds of knowledge rules in decision making. The development of the attribute-oriented induction method for decision making is motivated by (1) certain regularities can be discovered and expressed at the primitive concept level; (2) the availability of certain background knowledge can improve the efficiency of the discovery process and express the user's preference. The knowledge discovery literature [3–5] indicates that attribute-oriented induction based discovery often generates too many rules without focus. Furthermore, conventional induction approaches cannot guarantee that the classification of a decision table is credible. In other words, an attribute-oriented induction based approach could be insufficient for extracting reliable knowledge from a data set. To resolve the problem, several techniques (concepts) should be employed cooperatively to support this application, such as a rough set approach, conceptual hierarchies, and condition constraints (e.g., weight functions). In this situation, an aggregated methodology for determining credible decision rules is important.

The main reason for using the rough sets approach for rule induction is due to the qualitative nature of the data which makes them difficult to analyze by standard statistical techniques [6,7]. The applications of rough set model in a variety of problems have demonstrated its usefulness [7–10]. In addition, to improve the efficiency of the discovery process and the quality of classification, the availability of certain background knowledge, such as conceptual hierarchies should be involved in decision making. Concept hierarchies, not only improves the efficiency of the discovery process but also expresses the user's preference for guided generalization, which may lead to a desirable generalization process. The conventional rough set approach to induct attributes-oriented rules did not incorporate with the conceptual hierarchies [11] and the weight of each input attribute. It is desired to develop the measures to evaluate the induced rules, for example, the strength and length of the rule. Generally, discovering the shortest and strongest rules is of most interest in decision making.

This paper focuses on how rough set theory, conceptual hierarchies, and condition constraints (e.g., weight functions) can be used cooperatively to develop high quality attribute-oriented and rule-based models in decision making. In this paper, several indices related to determining the optimal level of each attribute for credible classification and attribute preference are introduced. The issues involved in the determination of the reducts of attributes are discussed. A structural approach for the discovery of credible generalized and preference-based decision rules is developed. The discovery process is performed by the heuristic approaches. Evaluation of each credible generalized medical rule is illustrated and discussed.

This paper is organized as follows. In Section 2, the related literatures in attribute-oriented induction and the rough set theory are reviewed. In Section 3, the solution approach is proposed. A hospital case study is presented to demonstrate the significance of the proposed approach in Section 4, and Section 5 concludes the paper.

@&#LITERATURE REVIEW@&#

In this section, the essential background knowledge (concept hierarchies) and rough sets theory, which represents the essence of attribute-oriented induction, are introduced.

As a generalization-based information modeling method, attribute-oriented induction provides an efficient way to extract different kinds of knowledge rules in decision making. An attribute-oriented induction method [12] has been developed as a technique for discovering knowledge from data. The method integrates learning-from-examples techniques, extracts generalized rules from a data set of interest, and discovers high level data regularities [13,14].

The development of the attribute-oriented induction method in decision making is motivated by the following reasons. First of all, certain regularities (e.g., association rules) can be discovered and expressed at the primitive concept level by information modeling techniques [15–18], while stronger and more interesting regularities can be discovered at high concept levels and expressed in concise terms. Thus it is often necessary to generalize low level primitive data in decision making from relatively high level concepts for effective data modeling. Secondly, the availability of certain background knowledge, such as concept hierarchies, not only improves the efficiency of the discovery process but also expresses the user's preference for guided generalization, which may lead to a desirable generalization process.

An attribute-oriented induction method has been developed which collects an interesting set of data as an initial data relation, performs on it the induction process (including attribute removal and concept hierarchy ascension), and derives generalized relations and rules. Overall, attribute-oriented induction is a technique for the generalization of any subset of data in a relational database and the extraction of knowledge from the generalized data. The generalized data may also be stored in a database in the form of a generalized relation and be updated incrementally with database updates [19]. Potentially this approach would be able to be extended to generalization-based data mining in object-oriented database [20], and other kinds of databases in decision making.

Next, the essential background knowledge of the concept hierarchies and rough set theory, which represents the essence of attribute-oriented induction, are introduced next.

The most important background knowledge applied in attribute-oriented induction is the concept hierarchy (or lattice) associated with each attribute [4,19,21]. Most concept hierarchies are stored implicitly in databases. For example, a set of attributes of a patient (name, sex, race, age, address) in a database schema represents the concept hierarchies of the attribute patient. Rules and view definitions can also be used as the definitions of concept hierarchies [22–24]. Conceptual hierarchies for numerical or ordered attributes can be generated automatically based on the analysis of the data distribution in the set of relevant data [25,26]. Moreover, a given hierarchy may not be best suited for a particular knowledge discovery task. Therefore, in many cases such hierarchies should be adjusted dynamically based on the analysis of the data distributions of the corresponding sets of data [25].

Concept hierarchies represent background knowledge necessary to control the generalization process. Different levels of concepts can be organized into a taxonomy of concepts which is partially ordered according to general-to-specific ordering. The most general concept is the null description, described by a reserved word “ANY”, and the most specific concepts correspond to the specific values of attributes in the database [27]. Using a concept hierarchy, the rules learned can be represented in terms of generalized concepts and stated in a simple and explicit form, which is desirable to most users. A concept hierarchy of a university student database is shown in Fig. 1
                        .

In the data mining domain, there are numerous literature incorporated with the concept hierarchies, for example, hierarchical integrated association rules [28], hierarchical clustering [29], and clustering attributes in the search space that is not limited by user defined hierarchies [30]. Mining can also be improved by employing a hierarchical view of the data [31]. Basically, each data mining approach is used due to its unique solutions corresponding to the research issues. Rough set theory (RST) is one effective approach in data mining. Particularly, it is capable to handle qualitative and quantitative information effectively. Note that the cost to develop the concept hierarchies should be associated with the charge from domain experts.

The rough sets method [7,32] has been developed as an interesting technique for discovering knowledge from data. One of the reasons for using the rough set as an approach to mine data is due to the qualitative nature of the data being analyzed which makes it difficult for standard statistical techniques [6,33]. Basically, the rough set based approach integrates learning-from-example techniques, extracts rules from a data set of interest, and discovers data regularities [34,35].

Pawlak (1982) first introduced the theory of rough sets as an extension of set theory for the study of intelligent systems characterized by incomplete information [36–39]. It is motivated by the practical needs in concept formation [40]. One may consider the rough sets theory to be complementary to other generalizations of set theory (e.g., fuzzy sets) [41,42]. The applications of rough set model in a variety of problems have demonstrated its usefulness [8,32,43–45].

Basically, the concept of the rough set is a new mathematical approach to uncertainty and imprecision in data analysis. The starting point of the rough set theory is the assumption that with every object of interest we associate some knowledge (e.g., if the objects are patients suffering from a certain disease, symptoms of the disease form the data about patients). Objects are indiscernible if they are characterized by the same information. The generated indiscernibility relation is the mathematical basis of the rough sets theory.

In general, a set of all similar objects is called elementary and forms a basic atom of knowledge. Any union of some elementary sets is referred to as a precise set – otherwise a rough (imprecise) set. However, as an outcome of the above definition, each rough set has boundary-line elements, i.e., elements which cannot be classified with certainty as members of the set or its complement. In other words boundary-line cases cannot be properly classified employing the available knowledge. Thus, rough sets can be viewed as a model of vague concepts.

Any vague concept is characterized by a pair of precise concepts – called the lower and the upper approximation. The lower approximation consists of all objects which surely belong to the concept and the upper approximation contains all objects which possibly belong to the concept. Approximation constitutes two basic operations in the rough set approach [32,46].

Recently, RST has been investigated in certain areas such as fault diagnosis [47–50], interval data clustering [51–54], supply chain management [55–58], image analysis [50,59,60], knowledge acquisition [61–63], manufacturing quality control [64,65], customer relationship management [66–68]. However, these conventional rough set approaches cannot guarantee that the categorization of a decision table is credible. Fortunately, the availability of certain background knowledge, such as conceptual hierarchies, could improve the efficiency of the discovery process and the quality of categorization. For example, stronger regularities can be discovered at high concept levels and expressed in concise terms. Thus, it is often necessary to generalize low level primitive data in databases to relatively high level concepts for effective data mining.

This section focuses on how rough set theory, conceptual hierarchies, and condition constraints (e.g., weight functions) can be used cooperatively to create high quality rule-based models.

In this section, a general decision table for the representation of the relationship between condition attributes and decision attributes is used. In Table 1
                        , the element (e
                        
                           ij
                        ) denotes the value of attribute (A
                        
                           j
                        ) that an object (tuple) (X
                        
                           j
                        ) contains, where i
                        =1 to m and j
                        =1 to n. O
                        
                           k
                         depicts the different decision attributes of the corresponding tuple. In addition, the number of values (value set V
                        
                           m
                        ) and weights (weight set W
                        
                           m
                        ) are incorporated in this table. Since there are different types of attribute, discrete or non-discrete, the numbers of values are assumed as scaled (integer) numbers in this study. The heuristic approach that converts real numbers to scaled (integer) numbers with involving tolerance can be referred to Tseng et al. [69]. The weight highly correlated to attribute characteristics can be derived through domain experts and the Analytic Hierarchy Process (AHP) approach [70] (Table 2
                        ).

Since attributes contain the concept of hierarchy in the decision table is the focus, the concept of the hierarchy framework and types of attributes are introduced next. Concept hierarchies are discussed in Kim [71], Ziarko [72] and Chen et al. [73]. Two examples of concept trees are introduced in this section: the university employee status (see Fig. 2
                        ) and the stone size tree (see Fig. 3
                        ). The values on the left, e.g., level 0, label the depth of the tree, and the leaf values in a tree cover all possible values of the corresponding attribute.

To determine the most possibility of the value combination, the maximum value of the degree of categorization (classificatory complexity) for a subset E of condition domain C, is defined as:
                           
                              
                                 
                                    DCmax
                                    =
                                    
                                       ∏
                                       
                                          j
                                          ∈
                                          E
                                       
                                    
                                    
                                       card
                                        
                                       (
                                       
                                          V
                                          j
                                       
                                       )
                                    
                                 
                              
                           
                        where V
                        
                           j
                         is the number of values of condition attributes and the function card yields set cardinality DCmax indicates the upper bound of classificatory complexity.

A categorization is a partition of the instance space into equivalence classes based upon the condition attributes. Pawlak [32] and Quinlan [74] applied inductive algorithms to categorize data. To be credible, a categorization must provide decisions which should be adequately supported by evidence. Three relevant factors of credibility are used in the literature: (1) coverage, (2) consistency, and (3) evidence of each decision.

In general, it is obvious that selecting a higher level concept of each attribute should increase coverage and support of each decision but not valuable rules since the rules are too general. Moreover, the decision table may be inconsistent since some data collected may be conflicting. Basically, the categorization measurement in terms of the credibility index is based on consistent data. Therefore, all of the inconsistent data should be eliminated.

The following three credibility indexes are defined to determine desired categorization.
                           
                              1.
                              Credibility index (CI(i)) and total credibility index (TCI):
                                    
                                       (1)
                                       
                                          
                                             C
                                             I
                                             (
                                             i
                                             )
                                             =
                                             
                                                
                                                   
                                                      1
                                                      
                                                         D
                                                         C
                                                         −
                                                         q
                                                      
                                                   
                                                
                                             
                                             ×
                                             (
                                             1
                                             −
                                             |
                                             P
                                             (
                                             D
                                             |
                                             C
                                             (
                                             j
                                             )
                                             −
                                             F
                                             (
                                             
                                                D
                                                ¯
                                             
                                             )
                                             |
                                             )
                                          
                                       
                                    
                                 
                                 
                                    
                                       (2)
                                       
                                          
                                             T
                                             C
                                             I
                                             =
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                C
                                                I
                                                (
                                                i
                                                )
                                             
                                          
                                       
                                    
                                 where q is the number of inconsistent classes; DC is the degree of categorization of the decision table; P(D|C(j)) is the probability of the decision “D” specified by the class C(j); 
                                    
                                       P
                                       (
                                       
                                          D
                                          ¯
                                       
                                       )
                                    
                                  is the theoretical fraction of D.

These two indexes are used to confirm each categorization's credibility. The smaller the difference between C(j) and D will cause the higher the value of the credibility index.

Modified credibility index (MCI(j)) and total modified credibility index (TMCI):
                                    
                                       (3)
                                       
                                          
                                             M
                                             C
                                             I
                                             (
                                             i
                                             )
                                             =
                                             C
                                             I
                                             (
                                             i
                                             )
                                             ×
                                             
                                                
                                                   (
                                                   no
                                                   _
                                                   of
                                                   _
                                                   object
                                                   )
                                                
                                                i
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       (4)
                                       
                                          
                                             T
                                             M
                                             C
                                             I
                                             =
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                M
                                                C
                                                I
                                                (
                                                i
                                                )
                                             
                                          
                                       
                                    
                                 where (no_of_object)
                                    i
                                  is the number of objects which support the decision rule i.

As the concept hierarchical is decreasing, the value of modified credibility index will change and sequently the coverage is enlarged. The higher MCI results in the higher evidence of the categorization. Later, the credibility reduct is determined based on that the highest categorization are identified.

Resolution index (RI):
                                    
                                       (5)
                                       
                                          
                                             R
                                             I
                                             =
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                m
                                             
                                             
                                                L
                                                (
                                                j
                                                )
                                             
                                          
                                       
                                    
                                 where L(j) is the level of jth attribute; m is the number of condition attributes.

Whenever the level of concept hierarchical is generalized of specified, the resolution index must to be recalculated since the lower RI has the strong indicates stronger support to credibility.

Based on the three indexes, the procedure presented next is applied to determine credible categorizations. Condition attributes are generalized by climbing their concept tree. One attribute is selected for generalization at each iteration. In this approach, the TMCI directs the algorithm. The decision table with its TMCI greater than the threshold value is defined as a credible categorization. The results of all combinations at the level of each attribute for credible categorization are determined by this procedure.
                           
                              
                           
                        
                        
                           
                              
                           
                        
                     

Since the decision-makers are more comfortable with 100% scale in determining the threshold value, therefore, an alternative approach to determining categorizations is to normalize the TMCI in the range [0,1]. For computing the resolution index (RI), an alternative approach is to assign a different weight for each attribute and recalculate the RI. If several categorization credibility indexes (TMCI) are equivalent, the highest RI should be selected.

In this section, the decision rules based on credible classifications are presented (Fig. 4
                        ). If the condition attributes do not contain their concept hierarchies, the solution approach which determines the rules will only include steps (1), (2), (8), (9) in Fig. 4.

A reduct is defined as a minimal sufficient subset of a set of attributes which has the same ability to discern concepts as when the full set of attributes is used [32,43]. Basically, the reducts represent necessary condition attributes to make a decision. Furthermore, a subset of attributes has more than one reduct, hence the simplification of decision rules does not yield unique results. Thus decision rules can be optimized according to the preassumed criteria related to the user's problem domain.

According to rough set theory, I
                           ={U, A} is an information system, where U is a finite set of objects and A is a finite set of attributes. With every attribute a
                           ∈A, a set of its values V
                           
                              a
                            is associated. Assume A
                           =
                           C
                           ∪
                           D, B
                           ⊂
                           C, where B is a subset of C; the positive region POSB(D)={x
                           ∈
                           U: [x]
                              B
                           
                           ⊂D} can be defined. The positive region POSB(D) includes all objects in U which can be with classified into classes of D, in the knowledge B. The degree of dependency between B and D can be defined as K(B, D)=
                           card(POS
                           
                              B
                           (D))/card(POS
                           
                              C
                           (D)), where card yields the set cardinality. In general, if K(B, D)=
                           K(C, D), and K(B, D)≠
                           K(B-{a}, D), for any a
                           ∈
                           B are hold; then B is a reduct of C. Since a reduct (B) preserves the degree of dependency with respect to D and a reduct (B) is a minimal subset, any further removal of condition attributes will change the degree of dependency. The following procedure for determining the reducts and cases is adopted from the Pawlak [32].
                              
                                 
                              
                           
                        

Note that the represents the objects where each A
                           
                              j
                            attribute contains V
                           
                              ij
                           , while 
                              
                                 
                                    
                                       [
                                       
                                          V
                                          
                                             i
                                             k
                                          
                                       
                                       ]
                                    
                                    
                                       
                                          O
                                          
                                             k
                                             i
                                          
                                       
                                    
                                 
                              
                            includes the objects with each O
                           
                              k|i
                            outcome (decision) attribute containing V
                           
                              ik
                           . In order to find dispensable attributes, the examination of each attribute of the object is required. One might also have to drop one attribute at a time and check whether the intersection of the remaining attributes is still included in the decision attribute.

In this section, identification of the reducts with desired condition attributes from a data set is discussed. The direct use of the result provided by reduct generation algorithm may lead to many reducts that contain condition attributes that are not meaningful. A strength index is introduced in order to identify meaningful reducts. A reduct with a higher value of the strength index is preferred over a reduct with a lower value index. Note that the comparison of the reducts is restricted to the same decision attribute and the number of attributes selected in the reducts.

The strength index of reduct f is defined as follows:
                              
                                 (6)
                                 
                                    
                                       S
                                       I
                                       (
                                       f
                                       )
                                       =
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          m
                                       
                                       
                                          
                                             v
                                             j
                                          
                                          
                                             W
                                             j
                                          
                                          ×
                                          
                                             n
                                             f
                                          
                                       
                                    
                                 
                              
                           where f is the reduct number, f
                           =1,…,
                           n; v
                           
                              j
                           
                           =1 if condition attribute j is selected, 0 otherwise (A
                           
                              j
                           
                           =“x”); n
                           
                              f
                            is the number of identical reducts f.

In general, a rule is a combination of the values of some attributes such that the set of all objects matching it is contained in the set of objects labeled with the same class. In order to simplify the decision table, value-reducts of the attributes illustrated in the previous section should be determined. Denote rule r
                           
                              i
                            as an expression:
                              
                                 (7)
                                 
                                    
                                       
                                          r
                                          i
                                       
                                       :
                                       (
                                       
                                          A
                                          
                                             i
                                             1
                                          
                                       
                                       =
                                       
                                          u
                                          
                                             i
                                             1
                                          
                                       
                                       )
                                       ∧
                                       (
                                       
                                          A
                                          
                                             i
                                             2
                                          
                                       
                                       =
                                       
                                          u
                                          
                                             i
                                             2
                                          
                                       
                                       )
                                       ∨
                                       …
                                       ∧
                                       (
                                       
                                          A
                                          
                                             i
                                             n
                                          
                                       
                                       =
                                       
                                          u
                                          
                                             i
                                             n
                                          
                                       
                                       )
                                       →
                                       (
                                       O
                                       =
                                       
                                          u
                                          d
                                       
                                       )
                                    
                                 
                              
                           where u
                           
                              i1, u
                           
                              i2,…,
                           u
                           
                              in
                           , and u
                           
                              d
                            are the value contents of the attributes; set {∨, ∧, →} of connectives are disjunction, conjunction, implication, respectively.

Basically, a set of specific decision rules (reducts) forms a reduced information system. Each rule corresponds to exactly one equivalence class of the original system. In other words, a set of those decisions rules (reducts) can be represented in a concise form (rule).

The proposed rule-extraction algorithm provides an effective tool for the generation of concise decision rules. To facilitate the rule-extraction process, the concept of a case is introduced. A case represents a set of reducts with the same number of attributes and the same outcome. The same case number might be assigned to more than one object. In general, the reducts of the same case are merged. More details are provided in the next section. Furthermore, the concept of extracting the best reducts is incorporated into this algorithm. The quantitative information associated with each object is used to confirm the rules. The “weak” rules, i.e., supported by few examples only, are considered here as less important. Lastly, the final generalized relations are transformed into decision rules and the rule aggregation is also performed by the rule-extraction algorithm (REA) algorithm (see Appendix A). In practice, the REA is very easy to use and understand, in particular, it is capable to handle a big data set and generate the desired decision rules given pre-existed constraints. Note that the proposed methodology takes the initial data set after data cleaning and generates candidate reducts. Then the proposed approach selects the best reducts based on strength index and preference and finalizes the final decision rules.

In this section, the performance of the rule extraction algorithm is evaluated using randomly generated data sets with a varying number of objects and attributes. The REA requires that the entire data set be read before its execution.

Four performance measures are introduced: accuracy index, coverage index, length index, and robustness ratio. Accuracy index α
                        
                           R
                        (D) and coverage index ψ
                        
                           R
                        (D) are defined as following [75]:
                           
                              (8)
                              
                                 
                                    
                                       α
                                       R
                                    
                                    (
                                    D
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      [
                                                      x
                                                      ]
                                                   
                                                   R
                                                
                                                ∩
                                                D
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      [
                                                      x
                                                      ]
                                                   
                                                   R
                                                
                                             
                                          
                                       
                                    
                                    ,
                                     
                                    and
                                     
                                    0
                                    <
                                    
                                       α
                                       R
                                    
                                    (
                                    D
                                    )
                                    ≤
                                    1
                                 
                              
                           
                        
                        
                           
                              (9)
                              
                                 
                                    
                                       ψ
                                       R
                                    
                                    (
                                    D
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      [
                                                      x
                                                      ]
                                                   
                                                   R
                                                
                                                ∩
                                                D
                                             
                                          
                                       
                                       
                                          
                                             D
                                          
                                       
                                    
                                    ,
                                     
                                    and
                                     
                                    0
                                    <
                                    
                                       ψ
                                       R
                                    
                                    (
                                    D
                                    )
                                    ≤
                                    1
                                 
                              
                           
                        where |A| denotes the cardinality of set A, α
                        
                           R
                        (D) denotes the accuracy of R (e.g., e
                        
                           ij
                        
                        =
                        v
                        
                           ij
                        , in a decision table) as to categorization of D, and ψ
                        
                           R
                        (D) denotes a coverage, respectively. α
                        
                           R
                        (D) measures the degree of the sufficiency of a proposition, R
                        →D, while ψ
                        
                           R
                        (D) measures the degree of its necessity of a proposition, D
                        →
                        R.

The accuracy index is used to identify categorization of the original data set. The coverage index is used to verify the rule from the rule set.

Length index (LI) and robustness ratio (r_ratio) for rule j are defined as following:
                           
                              (10)
                              
                                 
                                    L
                                    I
                                    (
                                    i
                                    )
                                    =
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       m
                                    
                                    
                                       
                                          e
                                          
                                             i
                                             j
                                          
                                       
                                       =
                                       no
                                       _
                                       of
                                       _
                                       attribute
                                        
                                       in
                                        
                                       rule
                                        
                                       i
                                        
                                       and
                                        
                                       
                                          e
                                          
                                             i
                                             j
                                          
                                       
                                       ≠
                                       "
                                       x
                                       "
                                        
                                       in
                                        
                                       the
                                        
                                       decision
                                        
                                       table
                                    
                                 
                              
                           
                        
                        
                           
                              (11)
                              
                                 
                                    r
                                    _
                                    ratio
                                    (
                                    i
                                    )
                                    =
                                    
                                       
                                          no
                                          _
                                          of
                                          _
                                          the
                                          _
                                          rule
                                          _
                                          i
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                −
                                                1
                                             
                                             n
                                          
                                          
                                             (
                                             no
                                             _
                                             of
                                             _
                                             the
                                             _
                                             rule
                                             _
                                             i
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Basically, the length index is constituted by summation of non-empty element (i.e., “x”) in the rule while the robustness ratio is “number of the specific rules” divided by “total number of the specific rules.” In general, rules with a smaller number of attributes are preferred, and the robustness ratio is used to indicate a measure of confidence. Generally speaking, the higher robustness ratio then the higher evidence of the categorization could be concluded.

This case study illustrates the methodology presented in this paper to induct rules for medical decision making in the palpable breast lump (PBL) project in the H hospital. The diagnosis of a palpable breast lump (PBL) can be benign or malignant. Malignant lumps are breast cancer. Palpable breast lumps are usually discovered by tissue examination or breast X-rays. Much of the currently recommended diagnostic pathway for PBLs is based on the work of Boyages [76]. The recommended diagnostic pathway includes the determination of the probability of malignancy in PBLs using a Baysian tool called the likelihood ratio. Del Turco [77] determined the likelihood ratio for malignancy in a PBL is based on the patient's age, lump size and duration of disease. Subsequent work added information on lump growth rate as determined by sequential breast X-rays. Further differentiation of PBLs on the basis of border patterns resulted from the introduction of imaging tests into the diagnostic pathway. A more recent development is the use of the Mammographic Examination (ME) which is reported to have sensitivities as high as 95% and specificities in the 85% range [78–80].

The problem is illustrated as follows: The analysts of the surgery department of the H hospital were responsible for extracting the reliable and concise medical rules from the given breast cancer patient profiles with possible measurement errors. In other words, the rules with strong evidence (i.e., supported by more examples and as few attributes as possible) are the focus of this project in medical decision making. The analysts were also required to determine which attributes are significant to the derived rules and prove that the rules are valid in the determination of the probability of malignancy in PBLs.

Consider the decision table obtained from Hospital H. The condition attributes are presented in Fig. 5
                        . Employee status has 7 possible values and the lumbar region pain has 2 possible values. The maximum degree of classification DCmax=1792. In this classification, 6 equivalence classes are illustrated with the coverage of 0.33%.

Consider the exemplified classification shown in Table 3
                         that is based on condition/decision attributes in Fig. 5. Using statistics, it can classify the object to each categorization and calculate the credible index subsequently.

Only two attributes involve concept hierarchies, i.e., ES and SS, they are represented in the first and sixth column. Note that a classification is credible if all possible combinations of condition attributes can be covered in the decision table and each decision is supported by as many input instances as possible. The degree of classification (DC) should represent the details of decision rules. For example, the DC of the first object in Table 3 is 1792. The credibility index of the first tuple in the Table 3 is CI
                        =(1/1792)×(1−|(5/12)−(1/6)|), MCI=4.2E−4×6=2.5E−3. TMCI is 0.035 which is not acceptable. The Resolution index (RI)=9. To obtain a better TMCI, ES and SS are generalized next.

During this generalization of ES, the instances (7, 2, 2, 2, 1, 2, (2,1)), (5, 2, 2, 2, 1, 1, (2,1)) and (5, 2, 2, 2, 1, 2, (2,1)) from Table 3 were combined to form (2, 1, 2, 2, 2, 1, (2,1)) instance in Table 4
                        . Since the degree of categorization (DC) decreased and no inconsistent decisions occurred, the TMCI increased from 0.035 to 0.233. Note that the RI of Table 4 is equal to 7.

During this generalization of SS, the decision does not remain consistent. For example, (2, 1, 2, 2, 2, 1, (2,1)) and (2, 1, 2, 2, 2, (1,1)) in Table 4 are in conflict. These two tuples should be removed from Table 5
                        . The value of DC in Table 5 becomes 127 (=128−1). The increase of TMCI is not guaranteed as its value should be determined by the number of consistent tuples and the degree of categorization (DC). In this case, the number of consistent tuples and the value of DC decrease. However, the TMCI increases due to the weak impact of inconsistent instances on its value. Here, the value of RI is 6.

To obtain the resulting credible categorization (TMCI) of all combinations of the levels for each attribute, the level-search algorithm is implemented. The results are illustrated in Table 5. The first row represents the state of generalization from the concept hierarchy contained attributes (e.g., (a) and (f)); and the first column includes the state of non-hierarchy contained attributes (e.g., (b)–(e)). Note that the original table (Table 4) corresponds to the bottom of the first column of Table 6
                        .

As with the above algorithm, the credibility (TMCI) can assessed for each categorization and the combinations are listed in Table 6. For a threshold value equal to 1, the shadowed cells in Table 6 are the elements in CAN, for example, a2b0c0d0e0f2, a1b0c0d1e1f1, and so on. Obviously, a1b0c0d0e0f1 corresponds to the highest value but it is not a valuable categorization as the value of RI is 2.

After the requirements for each attribute have been determined, the elements of CAN are restricted to row (2), (3), (4), (5), (10), (11), (12) and column (6), (7), (8). After the comparison of the RI of each element was performed, seven categorizations were selected {a1b0c0d0e0f3, a1b1c0d0e0f2, a1b0c1d0e0f2, a1b0c0d0e1f2, a1b1c1d0e0f1, a1b1c0d0e1f1 and a1b0c1d0e1f1.} The value of RI equal to 4 is associated with all seven categorizations. Note that generalizing to the root of the concept tree is equivalent to removing the corresponding condition attribute.

An alternative approach to determining categorizations is to normalize the TMCI in the range [0,1] since the decision-makers are more comfortable with this scale in determining the threshold value. For computing the resolution index (RI), an alternative approach is to assign a different weight for each attribute and recalculate the RI. If several categorization credibility indexes (TMCI) are equivalent, the highest RI should be selected.

In the previous section, seven categorizations were selected. Here, expertise is required to select the final categorization to generate the rules. Based on the domain expert's judgment, attribute b is highly correlated to attribute e and attribute d has a less significant impact on decision attributes. Therefore, the categorization – a1b0c1d0e1f1 (see Table 7
                        ) is selected. Note that attributes b and d are removed since they are at level 0 of the concept tree. Finally, the reducts of attributes equivalent to a set of generalized rules are illustrated next.

The decision table in Table 7 represents the final categorizations. After performing the reduct search procedure, the list of resulting value reducts for Table 7 is obtained (see Table 8
                        ). The heuristic procedure is applied to determine the reducts.

Consider Table 9
                         which includes the data from object 1 in Table 8 expanded with a column indicating the number of objects and a row containing weights associated with the attributes.

The strength index for the three reducts with outcome (2, 3) is as follows:

Reduct 1, SI(1)=32.5, Reduct 2, SI(2)=33.8, and Reduct 3, SI(3)=37.7. The higher value of the strength index implies that the corresponding reduct is more preferred. In this case reduct 3 with SI(1)=37.7 is preferred over reduct 1 and reduct 2.

Before execution the proposed rule-extraction algorithm. Create a case number in the Table 10
                        . S is the case number determined by outcome and total number of attributes contained in the reduct. The represent a set of reducts with same number of attributes and the same outcome. The same case number might be assigned to more than one object. In general, the reducts of the same case are merged.

Consider the value reducts in Table 7. After the rule-extraction algorithm was performed, the list of resulting concise rules was derived (see Tables 11 and 12
                        
                        ). The heuristic algorithm was used to determine the concise rules (Fig. 6
                        ).

In this section, the performance of the rule extraction algorithm is evaluated using randomly generated data sets with a varying number of objects and features. The REA requires that the entire data set should be read before its execution. The four performance measures computed for the data in Tables 7 and 11 are summarized in Table 12 and the final four rules are presented in Fig. 7
                        .

As shown above, generalized medical rules generated by the heuristic algorithm are much shorter than the original data set. Those generalized rules are fully examined to be credible and robust. Several observations can be made based on the content of Table 12 and Fig. 7. First of all, those four rules are equally robust since the r_ratio of each rule is similar. The values of the accuracy, coverage, length index, and the combination of value of rule 1 are prevailing (optimal), since the other rules contain lower coverage and longer length. One can conclude that rule 1 is the most concise and preferable over the other rules. However, all rules are valuable because the four rules cover different outcomes which provide various information.

In this case study, computational results will be illustrated with the data set collected at the ABC Hospital and Clinics for 1180 Solitary Pulmonary Nodule (SPN) patients with known diagnoses, confirmed by pathology tests. Ten randomly selected features for each patient were used in the computational study. The 1180 patients’ records were checked for completeness and reduced to 952. Each of the 228 records rejected was missing at least one of the 10 features used in our study. Moreover, due to validation concern, the hospital also collected the other set of data which includes 889 completed patient records. The selected 10 features are listed next: F1 Age (Patient's age), F2 CT max area (Computed Tomography maximum area), F3 Calcification type (1=central calcification, 2=laminated, 3=dense), F4 Location in thorax (1=central, 2=mediastinal, 3=peripheral), F5 Nodes (0=none, 1=less than 1cm, 2=larger than 1cm without calcification), F6 Other sus lesions, 0=No, 1=Yes (Other suspected=lesions), F7 1=M, 0=F (1=Male, 0=Female), F8 PET PN image bg, 1=Yes, 2=No (Positron Emission tomography Pulmonary Nodule image is greater than background), F9 Pk/yrs (packet - years), F10 BMI (Body Mass Index), D-1 Diagnosis-1 (B=Benign, M=Malignant), D-2 Diagnosis-2 (1=recovery, 2=no recovery, 3=lack of effects). According to the aforementioned features, three out of ten features (i.e., F1, F2 and F10) get involved with the concept hierarchies while the other seven features are only corresponding to “one” level of contents.

A solitary pulmonary nodule (SPN) is defined as a discrete pulmonary opacity less than 3cm in size that is surrounded by normal lung tissue and is not associated with atelectasis or adenopathy. Approximately 150,000 SPNs are identified in the U.S. per year. Benign processes, such as inflammation, fibrosis, or granulomas can manifest as SPNs. Some 40–50% of SPNs are malignant. Therefore, an early and correct diagnosis is critical for optimal therapy of malignant SPNs and avoidance of unnecessary diagnostic tests, treatment costs, and risks for benign SPNs.

In the current medical practice, more than one thousand data points from noninvasive tests might be considered during the diagnosis of an SPN. For an individual patient, it is almost impossible for a human decision-maker to determine which of these data points are critical for the correct diagnosis. Physicians have difficulty in differentiating benign from malignant cases based on clinical features, including symptoms, physical examination, and laboratory results. It is clear that implementing the proposed algorithms could avoid invasive tests and save the patient diagnosis cost. Therefore, there is a need to verify the proposed approach is high accurate in diagnosis and validation.

The “Rough Set Application – Credible and Preference-Based Rule Induction” software is developed by University of Texas at El Paso. This Rough Set Based Decision Support System is implemented using Apache 1.3 web server to result in a prototype system for remote use. The system is developed with C language and the Common Gateway Interface (CGI) is used as a communication protocol between the server and client ends.

Several functions can be observed in this software. First, the software allows the user to identify if the feature contains the Concept Hierarchy (CH) or not. If so, the user is able to enter “number of levels associated with the CH of the feature.” Note that when the user uploads the original data file, the data set is defaulted to comprise the data associated to the highest level of the CH of each feature (if any). The second function shows the data, which can be integers or real numbers based on selection of the levels of the CH from the features (if any). The third function illustrates the capacity to convert the continuous data type into discrete type. The continuous type of data is divided into several ranges starting from the minimum value and if the tolerance is provided, and terminates at the maximum value. With the forth function, the software is able to eliminate inconsistent data. For example, identical conditional features generate opposite outcomes. The selected data is based on high number of supports for that data. With the fifth function, the TCMI values corresponding to different selections (combinations) of levels of the features are illustrated. The decision maker should select the data set corresponding to the highest TMCI value to derive the decision rules. The fifth function shows the final results when the previous options are implemented. The outcomes produced from the previous option are able to show which specific (individual) objects contributes to the rule. The last function depicts the cross validation results.

In this case study, three features (F1, F2 and F10) includes CHs. F1 and F10 contain 0, 1, 2 levels while F2 contains 0, 1, 2, 3 levels. Moreover, the discrete outcomes corresponding to F1, F2 and F10 are as follows: 5, 8 and 4. After implementing the software, the best combination which is able to generate the best TMCI was identified: F1-level 2, F2-level 3 and F10-level 1 with level 1 associated with the features without CHs (i.e., F3-F9). The TMCI value is 0.402.

In order to investigate the effect/impact of weight and CH to the features, four cases have been considered: (1) Equal weight with no HC, (2) Equal weight with HC, (3) Unequal weight with no HC and (4) Unequal weight with HC. Cross-validation based on two data sets is performed in each case. The average results of five-fold are the final accuracy and coverage. Table 13
                            shows weight table for each feature while Table 14
                            depicts crossed validation (i.e., five-fold) results based on four different cases. Furthermore, the exhaustive approach [81] and LEM2 [82] incorporated with and without CH is also used to compare the proposed approach (see Table 15
                           ).

Two data sets have been used to validate four different cases (i.e., equal weight, equal weight with concept hierarchy, unequal weight and unequal weight with concept hierarchy). Note that the sample size of each data set after data cleaning are 952 and 889 respectively. In Table 14, from the first data set, one can observe that coverage seems performing better than accuracy except case III and unequal weight cases (no matter with or without concept hierarchy) are almost prevailing over equal weight cases in cross validation. From the values of categories of “number of rules generated” and “the highest frequency in the rule set,” they are describing characteristics of the outputs and not directly to impact the performance. Certainly, one can argue that more simple rules are definitely better than more compound rules. For instance, in case IV, 81 simple rules have been generated vs. 23 compound rules. The data set which includes the rule(s) with highest frequency is better than others. However, the aforementioned statements should be more informative rather than decisive. In Table 14, performance evaluation based on the second data set seems to provide similar conclusions to the first one. One can observe that variation among all four cases in the second data set seems smaller than the first data set. Overall, the case of un-equal weight with concept hierarchy performs better than others. Note that the derived decision rules have been verified by the lung cancer specialists/physicians.

In order to compare performance of the proposed approach with other RST based methodologies, the exhaustive and LEM2 approaches have been used. Table 15 illustrated the findings of this crossed validation comparison. To concise this comparison, only the best performed case (i.e., Case IV) is selected. Moreover, the data shown in the table is used average values instead of individual one. Note that there is no weight function in either the exhaustive approach or the LEM2 method. Therefore, the weight of each feature is assumed to one in both approaches.

The results of comparison are very encouraging, since the proposed approach prevails over the cases of the exhaustive and LEM2 approaches with and without concept hierarchy and proves the usefulness of the proposed method for medical prediction. In addition, our heuristic algorithm performed more efficiently while handling additional number of features.

@&#CONCLUSION@&#

In this paper, a methodology to determine generalized rules based on credible classification was proposed in decision making. Furthermore, a rule extraction algorithm was developed for the induction of rules from data sets. The relatively small number of induced rules allows for reasonable domain interpretation. The computational results indicate that the algorithm could handle large-scale data sets.

Future research could focus on the following two fields:
                        
                           1.
                           The multi-outcome oriented rules induction: Evaluation of multi-outcome is particularly important for problems aiming at the selection of actions (e.g., medical treatments) based on outcomes. The Multi-Attribute Value (MAV) model [78,83,84] is capable of handling the ordinal scale issue and the uniform weight of evaluating of the multi-outcome. The MAV methodology is an approach with the ability to capture the interval scale and to solicit the uneven weight of the problem in this paper.

The fuzzy characteristic rules in an attribute-oriented database: Current knowledge discovery techniques are not able to efficiently elicit the decision (characteristic) rules from the imprecise or uncertainty data model [85,86]. Therefore, systematic methodology for this topic needs to be developed.


                     
                        
                           
                        
                     
                     
                        
                           
                        
                     
                     
                        
                           
                        
                     
                  

@&#REFERENCES@&#

