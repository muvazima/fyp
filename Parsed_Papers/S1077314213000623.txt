@&#MAIN-TITLE@&#Unified Dictionary Learning and Region Tagging with Hierarchical Sparse Representation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Hierarchical structure among feature points, regions, and images are explored.


                        
                        
                           
                           We add a hierarchical structure to the process of sparse coding.


                        
                        
                           
                           Region sparse reconstruction is guided by the hierarchical structure.


                        
                        
                           
                           Dictionary learning and region tagging are unified into a new framework.


                        
                        
                           
                           Achieve better performance compared with state-of-the-art methods.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sparse coding

Region tagging

Sparse reconstruction

Unified hierarchical structure

Tree-guided dictionary learning

@&#ABSTRACT@&#


               
               
                  Image patterns at different spatial levels are well organized, such as regions within one image and feature points within one region. These classes of spatial structures are hierarchical in nature. The appropriate integration and utilization of such relationship are important to improve the performance of region tagging. Inspired by the recent advances of sparse coding methods, we propose an approach, called Unified Dictionary Learning and Region Tagging with Hierarchical Sparse Representation. This approach consists of two steps: region representation and region reconstruction. In the first step, rather than using the 
                        
                           
                              
                                 ℓ
                              
                              
                                 1
                              
                           
                        
                     -norm as it is commonly done in sparse coding, we add a hierarchical structure to the process of sparse coding and form a framework of tree-guided dictionary learning. In this framework, the hierarchical structures among feature points, regions, and images are encoded by forming a tree-guided multi-task learning process. With the learned dictionary, we obtain a better representation of training and testing regions. In the second step, we propose to use a sub-hierarchical structure to guide the sparse reconstruction for testing regions, i.e., the structure between regions and images. Thanks to this hierarchy, the obtained reconstruction coefficients are more discriminate. Finally, tags are propagated to testing regions by the learned reconstruction coefficients. Extensive experiments on three public benchmark image data sets demonstrate that the proposed approach has better performance of region tagging than the current state of the art methods.
               
            

@&#INTRODUCTION@&#

Image archives on the Web are growing rapidly. According to the report released in August 2011, Facebook has 700million users and 150billion photos. This brings along a great challenge in the research of image retrieval. A considerable number of automatic image annotation (AIA) approaches used in image retrieval have been proposed to meet this challenge [1–4]. However, these image-level tagging methods tend to become less effective because a global image matching approach can hardly handle the diversity and arbitrariness of Web image content. Tagging images in a more fine-grained level can be helpful to improve the performance of image retrieval. Image tag localization, i.e., assigning tags to image regions (namely, region tagging), has become a research direction to solve this problem [5].

The first step of region tagging is to obtain the representation of regions (note this paper aims at solving the region tagging problem, therefore, we assume the ground truth of region segmentations for the images have been provided in the dataset and the proposed method focuses on assigning the candidate tags to the ground truth of regions. For this reason, we say that the first step of region tagging is region representation rather than segmentation for an image). Several related methods have been proposed. The bag-of-words (BoWs) model treats an image as a set of unordered appearance descriptors extracted from local patches. Then these descriptors are quantized into discrete “visual words”, thus generating a compact histogram representation for every image. The BoW approach discards the spatial order of local descriptors, which limits the descriptive power of the image representation. To overcome this problem, the spatial pyramid matching (SPM) [6] is proposed. This method partitions an image into 
                        
                           
                              
                                 2
                              
                              
                                 l
                              
                           
                           ×
                           
                              
                                 2
                              
                              
                                 l
                              
                           
                        
                      segments at different scales l
                     =0,1,2. Then the BoW histogram within each of the 2l segments is computed. Finally the method concatenates all the histograms to form a vector representation of the image. However, it is time consuming to compute the kernel matrix for classification. For this reason, Yang et al. [7] proposed the ScSPM method, which integrates the sparse coding technique into the SPM. Instead of the K-means vector quantization in the traditional SPM, this method computes a spatial-pyramid image representation based on the sparse codes of SIFT features. Furthermore, unlike the original SPM that performs spatial pooling by computing histograms, the ScSPM computes the max spatial pooling that is more robust to local spatial translations. The above three methods assume that the feature points extracted from images are independent from each other when training the dictionary (or visual words), and as such they ignore the relationship among them.

However, in real world, patterns at different spatial levels are well organized, such as regions within one image, feature points within one region. These classes of spatial structures are hierarchical in nature, i.e., every region in images contains feature points, some regions constitute the image they belong to, and all the images constitute a root. This hierarchical structure can be naturally represented as a tree, where structured semantic information is encoded. For example, images in the same scene are usually composed of fixed region tags. Correspondingly, two images in one scene should have the same child nodes in a tree. As shown in Fig. 1
                     , both the first image and third image are mainly composed of objects like tree, building and sky. In the past few years, a variety of hierarchical structures have been used for practical applications. For instance, patterns among object parts are used to form compositional models for object recognition [8–10]. The relationship of objects are used to capture semantic contextual information for robust object detection or image labeling [11–13]. The hierarchy of low-level features have been shown to be more discriminative than single features for object recognition [14,15]. These research efforts demonstrate that the utilization of this hierarchically encoded spatial and semantic information can provide rich information to facilitate vision tasks. For this reason, we believe that integrating the hierarchical structure in Fig. 1 into region representations will help us obtain more accurate representation for regions and thus can effectively facilitate region tagging.

To this end, we firstly learn a dictionary via sparse coding technique like that in ScSPM, what is different lies that the procedure of dictionary learning here is guided by the hierarchy. Then, we use this tree-guided dictionary to encode feature points to get more accurate region representations. Specifically, sparse coding first solves a regression problem to obtain the regression coefficients on the base of an initialized dictionary. Then, a new dictionary is trained to update the initialized dictionary by minimizing a least squared error function. These two steps are repeated until the optimal dictionary is obtained. In our method, we integrate the hierarchical structure existing among feature points, regions, and images into the regression problem. Thus the regression coefficients are computed, which is guided by this hierarchy. This procedure is called as tree-guided regularization and can be solved by the tree-guided multi-task group lasso [16]. Because the regression coefficients are computed under guidance by the hierarch, the dictionary trained by these hierarchy-guided coefficients also encodes a hierarchical structure information. After that, we use this tree-guided dictionary to encode each feature point to get the sparse codes. To further improve the discrimination of region features, we also compute a spatial-pyramid region representation based on the learned sparse codes like ScSPM. In this way, more accurate region representations can be obtained.

With the learned representation of each region, the second step is to assign tags to image regions. Though several related learning based methods [17–20] can be used to predict tags for testing regions, we propose to use the sparse reconstruction method [21–23] to attain such a goal. When using the training regions to reconstruct the testing regions, the traditional sparse reconstruction methods also ignore the hierarchical structure of training regions, i.e., the structure above the red line in Fig. 1. This structure provides context information as it encodes the spatial and semantic information between regions and images. In our method, we also use the tree-guided group lasso to formulate this hierarchical structure into the framework of sparse reconstruction. With this hierarchy-guided sparse reconstruction process, we can get more discriminate reconstruction coefficients. Finally, tags are propagated to testing regions by the learned reconstruction coefficients.

One thing to note is that the two hierarchical structures used in two steps correspond to different depths of a unified hierarchical structure, respectively. In region representation, the whole hierarchical structure existing among feature points, regions, and images is integrated to obtain more accurate representations. In region reconstruction, a sub-hierarchical structure between regions and images (the part above the red line in Fig. 1) is used to guide sparse reconstruction to obtain a better performance of region tagging. Fig. 1 illustrates this unified hierarchical structure, and the red line depicts the different depths in the tree.

In conclusion, this paper has two main contributions. The first one is to propose the idea that using a unified hierarchical structure to guide the process of region tagging. The second contribution is to propose a region tagging framework of integrating this unified hierarchical structure into dictionary learning and region reconstruction, respectively. In the experiments, we evaluate the proposed framework on three public benchmark image data sets and compare it with the state-of-the-art methods. Experimental results demonstrate that the proposed framework has better performance of region tagging.

The remainder of this paper is organized as follows. We first introduce related works in Section 2. Then some preliminaries used in this paper are given in Section 3. The framework of this paper is introduced in Section 4. In Section 4.3, we discuss some computational issues of the proposed method. The experimental analysis and conclusion are given in Sections 5 and 6, respectively.

@&#RELATED WORKS@&#

In this section, we will introduce the related works about hierarchical structure and region reconstruction as well as the tree-guided group lasso.

In the past years, numerous hierarchical methods have been proposed to solve various tasks in computer vision. In [24], Epshtein and Ullman proposed the semantic hierarchy for the recognition of objects and their parts. Its advantages include improved classification performance, accurate localization of object parts, and explicit identification of the different appearances in each object part. In addition, Ref. [25] uses multiscale conditional random fields to combine local classifiers with regional and global features for semi-supervised labeling and classification. Sudderth et al. [26] model the hierarchy of scenes, objects and parts using hierarchical Dirichlet processes, which encourage scenes to share objects, objects to share parts, and parts to share features. Hierarchical models are also common within grammar models for scenes [27,28] and they have been shown to be very flexible to represent complex relationships.

The above works are a variety of hierarchical models. Some other approaches aim to learn a hierarchical structure. Parikh et al. [29] present a unified approach to unsupervised learning of hierarchical spatial structures from a collection of images. The hierarchical spatial structures exist among objects in a scene, object-parts in an object, and low-level features in object-parts. Our work is primarily motivated by this paper, which inspires us to integrate a unified hierarchical structure existing among feature points, regions, and images into dictionary learning and region reconstruction to guide the process of region tagging. To our knowledge, there is no existing methods to do the same thing like this.

Sparse reconstruction method has been demonstrated to be effective in region-level annotation in the past years. Liu et al. [21] proposed the Bi-Layer sparse coding (BLSC) for encoding image regions and propagating labels at region level. In this work, images were first segmented into training regions. Then the Bi-Layer model was applied to reconstruct each testing region from a dictionary formed by other training regions. The common tags of images containing the target region and sparsely selected regions are re-assigned to the target region according to the reconstruction coefficients. Note that in this paper training regions in the dictionary are implicitly assumed to be independent from each other. Contextual relationships among these semantic regions are ignored. To overcome this problem, Liu et al. incorporate the group structure of region-in-image relationship into the sparse reconstruction framework by adding a group lasso penalty [30] in the extended version [31], which takes regions within the same image as a group (we call it as EBLSC (extended BLSC) in the following paragraphs). Similarly, the Spatial Group Sparse Coding (SGSC) [22] also uses the group lasso to facilitate region tagging (an extended version can be found in [32]). Different from EBLSC, SGSC extends the group lasso with spatial correlations among image regions. However, due to the non-overlapping definition of groups, EBLSC and SGSC only consider contextual correlations of training regions within the same image. In order to obtain a better performance of region tagging, Han et al. [23] propose the Graph Guided Sparse Reconstruction for Region Tagging (
                           
                              
                                 
                                    G
                                 
                                 
                                    2
                                 
                              
                           
                        SRRT). This method uses a graph model to boost the performance, which describes the context constraints of the low-level visual similarity and high-level semantic correlations co-existed in the training regions across images rather than only within the same image.

Similar to the above three methods, our approach also aims to take into account the contextual relationship among training regions when reconstructing testing regions. However, our proposed hierarchical structure between regions and images not only takes the regions within the same image as a group (group in low level) but also takes all the training regions as a bigger group (group in high level). Moreover, for the groups in different levels, we assign different weights to tune its importance in region tagging. This two-level-group method is more advanced than the single-level-group EBLSC and SGSC as well as different from 
                           
                              
                                 
                                    G
                                 
                                 
                                    2
                                 
                              
                           
                        SRRT. The tree structure depicts the inner hierarchical relationship among training regions and can be helpful to improve the performance of region tagging.

Recently, sparse learning via 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    1
                                 
                              
                           
                         regularization [33] and its various extensions has received increasing attention in many areas including machine learning, signal processing, and statistics. In particular, the group lasso [30] utilizes the group information of the features, and yields a solution with grouped sparsity. The traditional group lasso assumes that the groups are non-overlapping. However, in many applications the features may form more complex overlapping groups, even some certain tree structures. Tree-guided group lasso is proposed to handle these cases. As an extension of group lasso, the tree-guided group lasso defines groups on each node in a tree, where each leaf node represents an individual output variable and each internal node indicates the cluster of the output variables that correspond to the leaf nodes of the subtree rooted at the given internal node. Several research efforts have been proposed to discuss the tree-guided group lasso. Kim and Xing [16] studied the tree structured group lasso for multi-task learning, where multiple related tasks follow a tree structure. Liu and Ye [34] developed an efficient optimization algorithm for the tree structured group lasso based on the Moreau-Yosida regularization. Generally speaking, tree-guided group lasso can be viewed as an example of the structured sparsity-inducing norms proposed in [35], where more general set-functions are investigated to incorporate prior knowledge or structural constraints. According to this paper, by selecting specific submodular functions, we can get a new interpretation to the current known norms in regularization of regression problem, like group lasso or tree-guided group lasso. In our method, we use the framework of tree-guided group lasso to formulate the hierarchical structure into dictionary learning and region reconstruction, respectively. And thus a Unified Dictionary Learning and Region Tagging with Hierarchical Sparse Representation is proposed.

In this section, we first provide notations used in the rest of this paper. Then, we will introduce the definition of traditional sparse coding and sparse reconstruction techniques, respectively. After that, the structured sparsity will be discussed.

Given an image dataset 
                           
                              I
                              =
                              (
                              
                                 
                                    I
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    I
                                 
                                 
                                    g
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    I
                                 
                                 
                                    N
                                 
                              
                              )
                           
                         with 
                           
                              N
                           
                         images. Suppose there are 
                           
                              
                                 
                                    R
                                 
                                 
                                    g
                                 
                              
                           
                         regions for each image 
                           
                              
                                 
                                    I
                                 
                                 
                                    g
                                 
                              
                              ,
                              
                              g
                              =
                              1
                              ,
                              …
                              ,
                              N
                           
                        , thus the total number of regions in 
                           
                              I
                           
                         is 
                           
                              R
                              =
                              
                                 
                                    ∑
                                 
                                 
                                    g
                                    =
                                    1
                                 
                                 
                                    N
                                 
                              
                              
                                 
                                    R
                                 
                                 
                                    g
                                 
                              
                           
                        . We extract 
                           
                              
                                 
                                    F
                                 
                                 
                                    r
                                 
                              
                           
                         feature points from the rth region, 
                           
                              r
                              =
                              1
                              ,
                              …
                              ,
                              R
                           
                         and let 
                           
                              F
                              =
                              
                                 
                                    ∑
                                 
                                 
                                    r
                                    =
                                    1
                                 
                                 
                                    R
                                 
                              
                              
                                 
                                    F
                                 
                                 
                                    r
                                 
                              
                           
                         denote the total number of feature points extracted from images in dataset 
                           
                              I
                           
                        . Now we obtain the feature matrix 
                           
                              X
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    D
                                    ×
                                    F
                                 
                              
                           
                        , where D is the dimension of each feature point and F denotes the number of feature points. Given an unlabeled testing region, we extract Q feature points from this region, and use 
                           
                              
                                 
                                    x
                                 
                                 
                                    q
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    D
                                    ×
                                    Q
                                 
                              
                           
                         to represent it. 
                           
                              V
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    D
                                    ×
                                    M
                                 
                              
                           
                         is defined as the dictionary targeted by sparse coding, M is the size of dictionary. 
                           
                              U
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    M
                                    ×
                                    F
                                 
                              
                           
                         is defined as the sparse codes of 
                           
                              X
                           
                        . Correspondingly, the sparse codes of 
                           
                              
                                 
                                    x
                                 
                                 
                                    q
                                 
                              
                           
                         are represented by 
                           
                              
                                 
                                    u
                                 
                                 
                                    q
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    M
                                    ×
                                    Q
                                 
                              
                           
                        . Suppose there are 
                           
                              K
                           
                         labeled tags for all the R regions in dataset and the ground-truth label for each region is available, we let 
                           
                              Y
                              ∈
                              {
                              0
                              ,
                              1
                              
                                 
                                    }
                                 
                                 
                                    K
                                    ×
                                    R
                                 
                              
                           
                         denote the label indicator matrix for all the regions in dataset, where 
                           
                              
                                 
                                    Y
                                 
                                 
                                    (
                                    i
                                    ,
                                    j
                                    )
                                 
                              
                              =
                              1
                           
                         if the 
                           
                              j
                           
                        th image region has the 
                           
                              i
                           
                        th tag and 
                           
                              
                                 
                                    Y
                                 
                                 
                                    (
                                    i
                                    ,
                                    j
                                    )
                                 
                              
                              =
                              0
                           
                         otherwise.

We use 
                           
                              C
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    M
                                    ×
                                    R
                                 
                              
                           
                         and 
                           
                              
                                 
                                    c
                                 
                                 
                                    q
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    M
                                    ×
                                    1
                                 
                              
                           
                         to represent the region-level features for training regions and testing region. In sparse reconstruction, we intend to reconstruct 
                           
                              
                                 
                                    c
                                 
                                 
                                    q
                                 
                              
                           
                         from the training regions 
                           
                              C
                           
                         by a learned reconstruction coefficient vector 
                           
                              β
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    R
                                 
                              
                           
                        .

In sparse coding, only the feature matrix 
                           
                              X
                           
                         is available. Our goal is to train a dictionary 
                           
                              V
                           
                         and then get the sparse codes of the original features, i.e., matrix 
                           
                              U
                           
                        . The conventional way for such a problem is to solve it iteratively by alternately optimizing over 
                           
                              V
                           
                         or 
                           
                              U
                           
                         while fixing the other [7]. Fixing 
                           
                              V
                           
                        , the function can be solved by optimizing over each column 
                           
                              
                                 
                                    U
                                 
                                 
                                    f
                                 
                              
                           
                         individually:
                           
                              (1)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          U
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          f
                                          =
                                          1
                                       
                                       
                                          F
                                       
                                    
                                 
                                 ‖
                                 
                                    
                                       X
                                    
                                    
                                       f
                                    
                                 
                                 -
                                 
                                    
                                       VU
                                    
                                    
                                       f
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 γ
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          f
                                          =
                                          1
                                       
                                       
                                          F
                                       
                                    
                                 
                                 ‖
                                 
                                    
                                       U
                                    
                                    
                                       f
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       1
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    U
                                 
                                 
                                    f
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    M
                                    ×
                                    1
                                 
                              
                           
                         denotes the 
                           
                              f
                           
                        th column in 
                           
                              U
                           
                        , and 
                           
                              
                                 
                                    X
                                 
                                 
                                    f
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    D
                                    ×
                                    1
                                 
                              
                           
                         denotes the 
                           
                              f
                           
                        th column in 
                           
                              X
                           
                        . 
                           
                              ‖
                              ·
                              
                                 
                                    ‖
                                 
                                 
                                    1
                                 
                              
                           
                         and 
                           
                              ‖
                              ·
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                           
                         denote the 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    1
                                 
                              
                           
                         and 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                 
                              
                           
                         norm of vectors, respectively. γ
                        ⩾0 is a tuning parameter that controls the amount of sparsity in the solution. Setting 
                           
                              γ
                           
                         to a large value leads to a smaller number of non-zero regression coefficients. Fixing 
                           
                              U
                           
                        , the problem reduces to a least square problem with quadratic constraints:
                           
                              (2)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          V
                                       
                                    
                                 
                                 ‖
                                 X
                                 -
                                 VU
                                 
                                    
                                       ‖
                                    
                                    
                                       F
                                    
                                    
                                       2
                                    
                                 
                                 ,
                                 
                                 s.t.
                                 
                                 ‖
                                 
                                    
                                       V
                                    
                                    
                                       m
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       1
                                    
                                 
                                 ⩽
                                 1
                                 ,
                                 
                                 ∀
                                 m
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 M
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    V
                                 
                                 
                                    m
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    D
                                    ×
                                    1
                                 
                              
                           
                         denotes the 
                           
                              m
                           
                        th column in 
                           
                              V
                           
                        . The optimization in Eq. (2) can be done efficiently by the Lagrange dual as used in [36].

Sparse reconstruction methods have been widely used in the computer vision to learn basic visual patterns [37,38] or to select the most discriminant visual features [39,40]. The sparse reconstruction formulation is defined as:
                           
                              (3)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          β
                                       
                                    
                                 
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 ‖
                                 
                                    
                                       c
                                    
                                    
                                       q
                                    
                                 
                                 -
                                 C
                                 β
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 λ
                                 ‖
                                 β
                                 
                                    
                                       ‖
                                    
                                    
                                       1
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              λ
                              ⩾
                              0
                           
                         is the regularization parameter for the lasso penalty. 
                           
                              
                                 
                                    c
                                 
                                 
                                    q
                                 
                              
                           
                         is the testing regions needed to be reconstructed and 
                           
                              C
                           
                         is the training regions. 
                           
                              β
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    R
                                    ×
                                    1
                                 
                              
                           
                         is the reconstruction coefficient. This optimization problem can be efficiently solved by the lasso algorithm [41].

Clearly, the standard lasso used in Eqs. (1) and (3) offers no mechanism to explicitly cope with the estimates of the regression coefficients for correlated output variables (i.e. the relevance across columns in 
                           
                              U
                           
                         and variables in 
                           
                              β
                           
                        ). However, in many real-world applications, different outputs are related in a complex manner. In other words, they have a structure. Owing to this, the structured sparse technique was proposed during the past several years.

The 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    1
                                 
                              
                              /
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                 
                              
                           
                         penalty [42] is such one method, taking advantage of the relevance of the output variables. A 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                 
                              
                           
                         norm is applied to the regression coefficients 
                           
                              
                                 
                                    U
                                 
                                 
                                    m
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    1
                                    ×
                                    F
                                 
                              
                           
                         for each row m in 
                           
                              U
                           
                        , and all the 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                 
                              
                           
                         norms are combined through an 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    1
                                 
                              
                           
                         norm to encourage sparsity across rows. The 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    1
                                 
                              
                              /
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                 
                              
                           
                        -penalized regression is defined as the following optimization problem:
                           
                              (4)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          U
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          f
                                          =
                                          1
                                       
                                       
                                          F
                                       
                                    
                                 
                                 ‖
                                 
                                    
                                       X
                                    
                                    
                                       f
                                    
                                 
                                 -
                                 
                                    
                                       VU
                                    
                                    
                                       f
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 γ
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          m
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 ‖
                                 
                                    
                                       U
                                    
                                    
                                       m
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 .
                              
                           
                        
                     

The 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    1
                                 
                              
                           
                         part 
                           
                              
                                 
                                    
                                       ‖
                                       ·
                                       
                                          
                                             ‖
                                          
                                          
                                             1
                                          
                                       
                                       =
                                       
                                          
                                             ∑
                                          
                                          
                                             m
                                          
                                       
                                    
                                 
                              
                           
                         of the penalty plays the role of selecting rows, and the 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                 
                              
                           
                         part combines information across columns. Since the 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                 
                              
                           
                         penalty does not have the property of encouraging sparsity, if the mth row is selected as relevant, all of the elements of 
                           
                              
                                 
                                    U
                                 
                                 
                                    m
                                 
                              
                           
                         take non-zero values. Thus, the estimate 
                           
                              U
                           
                         is sparse only across rows but not across columns.

In the same way, we can apply the structure information of 
                           
                              β
                           
                         into the sparse construction. For example, we can take the regions that belong to one image as a group, and all the 
                           
                              R
                           
                         regions in 
                           
                              C
                           
                         can be partitioned into 
                           
                              N
                           
                         non-overlapping groups. We let 
                           
                              
                                 
                                    β
                                 
                                 
                                    g
                                 
                              
                           
                         denotes the sub-vector of reconstruction coefficients, let 
                           
                              
                                 
                                    C
                                 
                                 
                                    g
                                 
                              
                           
                         denotes the corresponding sub-vector training regions. Then the reconstruction coefficient vector 
                           
                              β
                           
                         is partitioned into 
                           
                              N
                           
                         sub-vectors, such that 
                           
                              β
                              =
                              (
                              
                                 
                                    β
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    β
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    β
                                 
                                 
                                    N
                                 
                              
                              )
                           
                        . Here we can formulate a sparse reconstruction framework with group lasso penalty [30] as follows:
                           
                              (5)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          β
                                       
                                    
                                 
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 ‖
                                 
                                    
                                       c
                                    
                                    
                                       q
                                    
                                 
                                 -
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          g
                                          =
                                          1
                                       
                                       
                                          N
                                       
                                    
                                 
                                 
                                    
                                       C
                                    
                                    
                                       g
                                    
                                 
                                 
                                    
                                       β
                                    
                                    
                                       g
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 λ
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          g
                                          =
                                          1
                                       
                                       
                                          N
                                       
                                    
                                 
                                 ‖
                                 
                                    
                                       β
                                    
                                    
                                       g
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 .
                              
                           
                        
                     

As shown in Fig. 1, there exists a tree structure among feature points, regions, and images, i.e., the tree structure exists across the columns in 
                           
                              X
                           
                        . Specifically, the feature points belonging to the same region form one low-level group, the feature points belonging to the same image form one high-level group and all the feature points in dataset form a root-level group. These groups represent the relevance between columns of 
                           
                              X
                           
                         (feature points) in different levels. Because each column in 
                           
                              U
                           
                         is the sparse codes of each column in 
                           
                              X
                           
                         (see Eq. (1)), the tree structure in 
                           
                              X
                           
                         can be reflected in 
                           
                              U
                           
                         (this correspondence is illustrated in Fig. 2
                        . For this reason, we add a regularization that can reflect this tree structure in 
                           
                              U
                           
                         to Eq. (1) to attain our goal. This can be solved by the tree-guided group lasso. Because here we consider the relevance between multiple columns 
                           
                              
                                 
                                    U
                                 
                                 
                                    f
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    M
                                    ×
                                    1
                                 
                              
                           
                         (tasks), the tree-guided group lasso falls into the multi-task regression problem, where the output is a multivariate vector with an internal structure, the estimation of the regression parameters can potentially benefit from taking into account this structure in the estimation process [16]. In the next paragraphs, we will discuss how to generalize the above 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    1
                                 
                              
                              /
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                 
                              
                           
                         regularization to tree-guided multi-task regularization.

We use 
                           
                              T
                           
                         to represent this tree with the set of vertices Ve of size 
                           
                              |
                              Ve
                              |
                           
                        . Then, we expand the 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                 
                              
                           
                         part of the 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    1
                                 
                              
                              /
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                 
                              
                           
                         penalty into a group-lasso penalty, where the group is defined based on 
                           
                              T
                           
                        . Each node 
                           
                              ν
                              ∈
                              Ve
                           
                         of 
                           
                              T
                           
                         is associated with a group 
                           
                              
                                 
                                    U
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          ν
                                       
                                    
                                 
                                 
                                    m
                                 
                              
                           
                         whose members consist of all the leaf nodes in the subtree rooted at node 
                           
                              ν
                           
                        . Each group in leaf nodes contains a variable in 
                           
                              
                                 
                                    U
                                 
                                 
                                    m
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    1
                                    ×
                                    F
                                 
                              
                           
                        , and thus each group in internal nodes contains the variables located at the leaves of the subtree rooted at this given internal node.


                        Fig. 2 shows an example of a tree containing totally 10 nodes with 4 leaf nodes and 6 internal nodes. Each group in leaf nodes 
                           
                              
                                 
                                    U
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          ν
                                       
                                    
                                 
                                 
                                    m
                                 
                              
                              ,
                              
                              ν
                              ∈
                              (
                              1
                              ,
                              2
                              ,
                              3
                              ,
                              4
                              )
                           
                         is associated with a variable 
                           
                              
                                 
                                    u
                                 
                                 
                                    i
                                 
                                 
                                    m
                                 
                              
                              ,
                              
                              i
                              ∈
                              (
                              1
                              ,
                              2
                              ,
                              3
                              ,
                              4
                              )
                           
                         in 
                           
                              
                                 
                                    U
                                 
                                 
                                    m
                                 
                              
                           
                         and each group in internal nodes 
                           
                              
                                 
                                    U
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          ν
                                       
                                    
                                 
                                 
                                    m
                                 
                              
                              ,
                              
                              ν
                              ∈
                              (
                              5
                              ,
                              6
                              ,
                              7
                              ,
                              8
                              ,
                              9
                              ,
                              10
                              )
                           
                         is the cluster of corresponding variables located in leaf nodes rooted at this internal node.

Given these groups that arise from 
                           
                              T
                           
                        , tree-guided multi-task group lasso can be written as [16]:
                           
                              (6)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          U
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          f
                                          =
                                          1
                                       
                                       
                                          F
                                       
                                    
                                 
                                 ‖
                                 
                                    
                                       X
                                    
                                    
                                       f
                                    
                                 
                                 -
                                 
                                    
                                       VU
                                    
                                    
                                       f
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 γ
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          m
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          ν
                                          ∈
                                          Ve
                                       
                                    
                                 
                                 
                                    
                                       w
                                    
                                    
                                       ν
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   U
                                                
                                                
                                                   
                                                      
                                                         G
                                                      
                                                      
                                                         ν
                                                      
                                                   
                                                
                                                
                                                   m
                                                
                                             
                                          
                                       
                                    
                                    
                                       2
                                    
                                 
                                 ,
                              
                           
                        Each group of regression coefficients 
                           
                              
                                 
                                    U
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          v
                                       
                                    
                                 
                                 
                                    m
                                 
                              
                           
                         is weighted by 
                           
                              
                                 
                                    w
                                 
                                 
                                    ν
                                 
                              
                           
                         that reflects the strength of correlation with the group.

The weight 
                           
                              
                                 
                                    w
                                 
                                 
                                    ν
                                 
                              
                           
                         is defined as [16]:
                           
                              (7)
                              
                                 
                                    
                                       w
                                    
                                    
                                       ν
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         g
                                                      
                                                      
                                                         ν
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            ∏
                                                         
                                                         
                                                            m
                                                            ∈
                                                            Ancestor
                                                            (
                                                            ν
                                                            )
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         m
                                                      
                                                   
                                                   ,
                                                
                                                
                                                   if
                                                   
                                                   ν
                                                   
                                                   is an internal node,
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            ∏
                                                         
                                                         
                                                            m
                                                            ∈
                                                            Ancestor
                                                            (
                                                            ν
                                                            )
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         s
                                                      
                                                      
                                                         m
                                                      
                                                   
                                                   ,
                                                
                                                
                                                   if
                                                   
                                                   ν
                                                   
                                                   is a leaf node,
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where the two quantities 
                           
                              
                                 
                                    s
                                 
                                 
                                    ν
                                 
                              
                              (
                              
                                 
                                    s
                                 
                                 
                                    m
                                 
                              
                              )
                           
                         and 
                           
                              
                                 
                                    g
                                 
                                 
                                    ν
                                 
                              
                           
                         are associated with the node 
                           
                              ν
                           
                         in 
                           
                              T
                           
                        , which satisfy 
                           
                              
                                 
                                    s
                                 
                                 
                                    ν
                                 
                              
                              +
                              
                                 
                                    g
                                 
                                 
                                    ν
                                 
                              
                              =
                              1
                           
                        . Furthermore, 
                           
                              
                                 
                                    s
                                 
                                 
                                    ν
                                 
                              
                           
                         represents the weight for selecting each child variable associated with node 
                           
                              ν
                           
                         separately, and 
                           
                              
                                 
                                    g
                                 
                                 
                                    ν
                                 
                              
                           
                         represents the weight for selecting them jointly. If 
                           
                              
                                 
                                    s
                                 
                                 
                                    ν
                                 
                              
                              =
                              1
                           
                         and 
                           
                              
                                 
                                    g
                                 
                                 
                                    ν
                                 
                              
                              =
                              0
                           
                         for all nodes, then only separate selections are performed, the tree-guided group lasso penalty reduces to the lasso penalty in Eq. (1). On the other hand, if 
                           
                              
                                 
                                    s
                                 
                                 
                                    ν
                                 
                              
                              =
                              0
                           
                         and 
                           
                              
                                 
                                    g
                                 
                                 
                                    ν
                                 
                              
                              =
                              1
                           
                         for all nodes, the penalty reduces to the 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    1
                                 
                              
                              /
                              
                                 
                                    ℓ
                                 
                                 
                                    2
                                 
                              
                           
                         penalty in Eq. (4) 
                        [16]. By this weighting scheme, we can set different importance for each group according to the variables’ relevance within this group. If the variables within groups are strongly correlated, 
                           
                              
                                 
                                    g
                                 
                                 
                                    ν
                                 
                              
                           
                         is set to a high value to encourage a joint covariate selection, otherwise 
                           
                              
                                 
                                    g
                                 
                                 
                                    ν
                                 
                              
                           
                         is set to a small value. In addition, Ref. [16] proves that 
                           
                              
                                 
                                    ∑
                                 
                                 
                                    ν
                                    ∈
                                    Ve
                                 
                              
                              
                                 
                                    w
                                 
                                 
                                    ν
                                 
                              
                              =
                              1
                           
                         by this weighting scheme.

After sparse coding, a tree-guided multi-task dictionary 
                           
                              V
                           
                         has been computed. We can encode the feature points 
                           
                              
                                 
                                    x
                                 
                                 
                                    q
                                 
                              
                           
                         and 
                           
                              X
                           
                         using the dictionary, obtaining the corresponding sparse codes 
                           
                              
                                 
                                    u
                                 
                                 
                                    q
                                 
                              
                           
                         and 
                           
                              U
                           
                        . Then, in order to obtain the region-level features, we calculate the max pooling function
                           1
                           The max pooling function of a matrix is defined to compute the max value of each row in the matrix and then concatenates these values into a vector. Please refer to [7] for the detailed definition.
                        
                        
                           1
                         of 
                           
                              
                                 
                                    u
                                 
                                 
                                    q
                                 
                              
                           
                         and 
                           
                              U
                           
                         in every region like [7], obtaining the spatial pyramid representation of regions 
                           
                              
                                 
                                    c
                                 
                                 
                                    q
                                 
                              
                           
                         and 
                           
                              C
                           
                        . In sparse reconstruction, we will use the training regions 
                           
                              C
                           
                         to reconstruct the testing region 
                           
                              
                                 
                                    c
                                 
                                 
                                    q
                                 
                              
                           
                        . Because there exists a tree structure across the training regions 
                           
                              C
                           
                        , i.e., the part above the red line as shown in Fig. 1, the variables in reconstruction coefficients 
                           
                              β
                           
                         also have this relationship. We use symbol 
                           
                              
                                 
                                    T
                                 
                                 
                                    ¯
                                 
                              
                           
                         to represent this subtree with the set of vertices 
                           
                              
                                 
                                    Ve
                                 
                                 
                                    ¯
                                 
                              
                           
                         of size 
                           
                              |
                              
                                 
                                    Ve
                                 
                                 
                                    ¯
                                 
                              
                              |
                           
                        , where 
                           
                              |
                              
                                 
                                    Ve
                                 
                                 
                                    ¯
                                 
                              
                              |
                              =
                              |
                           
                        
                        Ve
                        
                           
                              |
                           
                         – the number of leaf nodes in 
                           
                              T
                           
                        . The subtree will be used to guide reconstruction process.

Similar to the tree 
                           
                              T
                           
                         in Section 4.1, each node in 
                           
                              
                                 
                                    T
                                 
                                 
                                    ¯
                                 
                              
                           
                         is also associated with a group. However, each group in leaf nodes here contains a variable in 
                           
                              β
                           
                        , rather than the variables in 
                           
                              
                                 
                                    U
                                 
                                 
                                    m
                                 
                              
                           
                        . Fig. 3
                         is an example of subtree 
                           
                              
                                 
                                    T
                                 
                                 
                                    ¯
                                 
                              
                           
                        . The six groups in leaf nodes 
                           
                              
                                 
                                    β
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          ν
                                       
                                    
                                 
                              
                              ,
                              
                              ν
                              ∈
                              (
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              6
                              )
                           
                         are corresponding to six variables in 
                           
                              β
                           
                        , respectively. The four groups in internal nodes 
                           
                              
                                 
                                    β
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          ν
                                       
                                    
                                 
                              
                              ,
                              
                              ν
                              ∈
                              (
                              7
                              ,
                              8
                              ,
                              …
                              ,
                              10
                              )
                           
                         contain the variables located at the leaves of the subtree rooted at this four nodes. Based on the defined group, we formulate the sparse reconstruction framework with the tree-guided group lasso penalty [34]:
                           
                              (8)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          β
                                       
                                    
                                 
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 ‖
                                 
                                    
                                       c
                                    
                                    
                                       q
                                    
                                 
                                 -
                                 C
                                 β
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 λ
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          ν
                                          ∈
                                          
                                             
                                                Ve
                                             
                                             
                                                ¯
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       w
                                    
                                    
                                       ν
                                    
                                 
                                 ‖
                                 
                                    
                                       β
                                    
                                    
                                       
                                          
                                             G
                                          
                                          
                                             ν
                                          
                                       
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 ,
                              
                           
                        because we apply the tree structure to guide the sparse reconstruction, we call this method as the tree group lasso in the experiments. The definition of 
                           
                              
                                 
                                    w
                                 
                                 
                                    ν
                                 
                              
                           
                         is the same to that in Section 4.1.

Suppose 
                           
                              
                                 
                                    β
                                 
                                 
                                    ˆ
                                 
                              
                           
                         is the solution of Eq. (8) and 
                           
                              Y
                              ∈
                              {
                              0
                              ,
                              1
                              
                                 
                                    }
                                 
                                 
                                    K
                                    ×
                                    R
                                 
                              
                           
                         is the label indicator vector of training regions, the predicted tag index 
                           
                              i
                              ∈
                              {
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              K
                              }
                           
                         for testing region 
                           
                              
                                 
                                    c
                                 
                                 
                                    q
                                 
                              
                           
                         is obtained by the following label propagation process:
                           
                              (9)
                              
                                 i
                                 =
                                 argmax
                                 {
                                 Y
                                 
                                    
                                       β
                                    
                                    
                                       ˆ
                                    
                                 
                                 
                                    
                                       }
                                    
                                    
                                       i
                                    
                                 
                                 .
                              
                           
                        
                     

We summarize the proposed Unified Dictionary Learning and Region Tagging with Hierarchical Sparse Representation in Algorithm 1.
                           Algorithm 1
                           Unified Dictionary Learning and Region Tagging with Hierarchical Sparse Representation 
                                 
                                    
                                       
                                       
                                          
                                             
                                                Input: Matrix of feature points extracted from N training images 
                                                   
                                                      X
                                                      ∈
                                                      
                                                         
                                                            R
                                                         
                                                         
                                                            D
                                                            ×
                                                            F
                                                         
                                                      
                                                   
                                                , where D is the dimension of feature points and F is its numbers. 
                                                   
                                                      Y
                                                      ∈
                                                      {
                                                      0
                                                      ,
                                                      1
                                                      
                                                         
                                                            }
                                                         
                                                         
                                                            K
                                                            ×
                                                            R
                                                         
                                                      
                                                   
                                                 is the matrix of label indicator for all the training regions, where K is the number of labels and R is the number of training regions. 
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            q
                                                         
                                                      
                                                      ∈
                                                      
                                                         
                                                            R
                                                         
                                                         
                                                            D
                                                            ×
                                                            Q
                                                         
                                                      
                                                   
                                                 is the matrix of feature points extracted from the testing region.
                                          
                                          
                                             
                                                Output: Label’s index i of the testing region
                                          
                                          
                                             1: Initialize dictionary 
                                                   
                                                      V
                                                   
                                                ;
                                          
                                          
                                             2: repeat
                                             
                                          
                                          
                                             3: Update 
                                                   
                                                      U
                                                   
                                                 using Eq. (6);
                                          
                                          
                                             4: Update 
                                                   
                                                      V
                                                   
                                                 using Eq. (2);
                                          
                                          
                                             5: untilconvergence.
                                          
                                          
                                             6: compute the max pooling function of 
                                                   
                                                      U
                                                   
                                                 to obtain 
                                                   
                                                      C
                                                   
                                                ;
                                          
                                          
                                             7: foreach testing region 
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            q
                                                         
                                                      
                                                      ∈
                                                      
                                                         
                                                            R
                                                         
                                                         
                                                            D
                                                            ×
                                                            Q
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             8: encode testing region 
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            q
                                                         
                                                      
                                                   
                                                 to obtain 
                                                   
                                                      
                                                         
                                                            u
                                                         
                                                         
                                                            q
                                                         
                                                      
                                                   
                                                 using the dictionary 
                                                   
                                                      V
                                                   
                                                ;
                                          
                                          
                                             9: compute the max pooling function of 
                                                   
                                                      
                                                         
                                                            u
                                                         
                                                         
                                                            q
                                                         
                                                      
                                                   
                                                 to obtain 
                                                   
                                                      
                                                         
                                                            c
                                                         
                                                         
                                                            q
                                                         
                                                      
                                                   
                                                ;
                                          
                                          
                                             10: reconstruct 
                                                   
                                                      
                                                         
                                                            c
                                                         
                                                         
                                                            q
                                                         
                                                      
                                                   
                                                 with 
                                                   
                                                      C
                                                   
                                                 using Eq. (8);
                                          
                                          
                                             11: output 
                                                   
                                                      
                                                         
                                                            β
                                                         
                                                         
                                                            ˆ
                                                         
                                                      
                                                      =
                                                      (
                                                      
                                                         
                                                            β
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                      ,
                                                      
                                                         
                                                            β
                                                         
                                                         
                                                            2
                                                         
                                                      
                                                      ,
                                                      …
                                                      ,
                                                      
                                                         
                                                            β
                                                         
                                                         
                                                            R
                                                         
                                                      
                                                      )
                                                   
                                                ;
                                          
                                          
                                             12: compute y using 
                                                   
                                                      y
                                                      =
                                                      Y
                                                      
                                                         
                                                            β
                                                         
                                                         
                                                            ˆ
                                                         
                                                      
                                                   
                                                ;
                                          
                                          
                                             13: output the label’s index i of the testing region using Eq. (9).
                                          
                                          
                                             14: end for
                                             
                                          
                                       
                                    
                                 
                              
                           

Some computational issues are discussed in this section. We mainly discuss the computational complexity of the proposed tree-guided dictionary learning in Section 4.1 (i.e. the line 3 in Algorithm 1) and the hierarchical sparse reconstruction method in Section 4.2 (i.e., the line 10 in Algorithm 1).

We use Eq. (6) to learn a tree-guided dictionary. [16] selects the smoothing proximal gradient descent (SPG) method to solve Eq. (6). The convergence rate of SPG is 
                           
                              O
                              (
                              1
                              /
                              ε
                              )
                           
                         iterations, where 
                           
                              ε
                           
                         is the given desired accuracy. The time complexity per iteration of SPG for Eq. (6) is 
                           
                              O
                              
                                 
                                    
                                       
                                          
                                             M
                                          
                                          
                                             2
                                          
                                       
                                       F
                                       +
                                       M
                                       
                                          
                                             ∑
                                          
                                          
                                             ν
                                             ∈
                                             Ve
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      U
                                                   
                                                   
                                                      
                                                         
                                                            G
                                                         
                                                         
                                                            ν
                                                         
                                                      
                                                   
                                                   
                                                      m
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        , where 
                           
                              
                                 
                                    
                                       
                                          
                                             U
                                          
                                          
                                             
                                                
                                                   G
                                                
                                                
                                                   ν
                                                
                                             
                                          
                                          
                                             m
                                          
                                       
                                    
                                 
                              
                           
                         denotes the length of vector 
                           
                              
                                 
                                    U
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          ν
                                       
                                    
                                 
                                 
                                    m
                                 
                              
                           
                        . For the details, please refer to [16].

In order to solve the Eq. (9), Liu et al. proposed an algorithm based on the Moreau-Yosida regularization [34]. This algorithm needs to find the minimizer of Moreau-Yosida Regularization. The time complexity for this is 
                           
                              O
                              (
                              R
                              log
                              R
                              )
                           
                        , and the algorithm achieves the global convergence rate of O(1/k) for k iterations.

@&#EXPERIMENTS@&#

We use three open benchmark image datasets in our experiments: two versions of MSRC dataset,
                              2
                              
                                 http://research.microsoft.com/en-us/projects/objectclassrecognition/.
                           
                           
                              2
                            i.e., the MSRC-v1 and MSRC-v2, and the SAIAPR TC-12 [43] image dataset. Regions of each image in all these datasets have been segmented and the ground truth of region mask is provided. Therefore, the tagging ground truth for each region is available so that we can use it to evaluate the performance of region tagging. We show some example images from the three datasets in Fig. 4
                           .

The MSRC-v1 dataset contains 240 images, and the segmented 562 regions are from 13 categories (or 13 labeled tags). MSRC-v2 is an extended version of MSRCv1. MSRC-v2 contains 591 images, and the segmented 1482 regions are associated with 23 tags. Both of these two versions of MSRC datasets provide the ground truth of region segmentation and region tagging. The SAIAPR TC-12 dataset extends the widely used IAPR TC-12
                              3
                              
                                 http://www.imageclef.org/photodata.
                           
                           
                              3
                            image dataset by adding the region segmentation masks and the ground truth of region-level annotation. The SAIAPR contains 20,000 images and 99,535 regions in total, and its 276 tags are well chosen according to an annotation hierarchy.

We extracted the dense SIFT features as the feature points for each of the image regions in all three image datasets. The dimensionality of feature is 128 for each feature point.

For MSRC-v1, we randomly sampled 200 images and the corresponding regions are used as the training regions. For MSRC-v2, we randomly sampled 471 images to form the training set. The remaining images are left for testing. Since there exists an annotation hierarchy of tags in SAIAPR TC-12 dataset, some tags in the high level of this hierarchy describe the visual content at the image level and thus are not the localized tags. In this experiment, we only selected the localized tags and randomly selected 2500 regions whose tags are within the selected subset of tags as the training regions, and another 500 regions for testing. For each dataset, the randomly sampling was repeated for five times and the average performance was reported.

We use 
                              
                                 Accuracy
                              
                            to evaluate the region tagging performance. Let 
                              
                                 
                                    
                                       N
                                    
                                    
                                       u
                                    
                                 
                              
                            denote the total number of testing regions to be labeled, and 
                              
                                 
                                    
                                       N
                                    
                                    
                                       r
                                    
                                 
                              
                            is the number of regions that are assigned the right tags according to the tagging ground truth, the 
                              
                                 Accuracy
                              
                            is defined as: 
                              
                                 Accuracy
                                 =
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             r
                                          
                                       
                                    
                                    
                                       
                                          
                                             N
                                          
                                          
                                             u
                                          
                                       
                                    
                                 
                              
                           . Since there are multiple tags in our experiment, we refer to 
                              
                                 Accuracy
                              
                            as the accuracy of the overall performance of tagging. When 
                              
                                 
                                    
                                       N
                                    
                                    
                                       u
                                    
                                 
                              
                            indicates the number of testing regions for the 
                              
                                 i
                              
                           th tag, 
                              
                                 i
                                 ∈
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 K
                              
                           , and 
                              
                                 
                                    
                                       N
                                    
                                    
                                       r
                                    
                                 
                              
                            is the number of testing regions that are annotated correctly with the 
                              
                                 i
                              
                           th tag, 
                              
                                 Accuracy
                                 =
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             r
                                          
                                       
                                    
                                    
                                       
                                          
                                             N
                                          
                                          
                                             u
                                          
                                       
                                    
                                 
                              
                            is used to evaluate the performance of tagging for a certain tag.

Two parameters need to be set in our method: 
                              
                                 γ
                              
                            in Eq. (6) and 
                              
                                 λ
                              
                            in Eq. (8). In this experiment, we use a 5-fold cross validation on the training dataset to tune the value of this two parameters.

The ranges of 
                              
                                 γ
                              
                            for MSRC and SAIAPR TC-12 datasets are {5e−5,1e−4,5e−4,1e−3,5e−3,1e−2,5e−2,1e−1,1}. The ranges of 
                              
                                 λ
                              
                            for MSRC and SAIAPR TC-12 datasets are {1e−3,1e−2,1e−1,2e−1,3e−1,4e−1,5e−1,6e−1}. The parameter tuning results are shown in Figs. 5 and 6
                           
                           . We can see that when 
                              
                                 γ
                                 =
                                 0.001
                              
                            the dictionary shows the best performance on both the MSRC-v1 and MSRC-v2, and when 
                              
                                 γ
                                 =
                                 0.005
                              
                            the dictionary shows the best performance on SAIAPR TC-12.

For the weights in Eqs. (7) and (8), we simply let 
                              
                                 
                                    
                                       g
                                    
                                    
                                       ν
                                    
                                 
                                 =
                                 0.1
                              
                            and 
                              
                                 
                                    
                                       s
                                    
                                    
                                       ν
                                    
                                 
                                 =
                                 0.9
                              
                            for all nodes in our experiments. In this way, we successfully let the groups located in the same level in tree have the same weights 
                              
                                 
                                    
                                       w
                                    
                                    
                                       ν
                                    
                                 
                              
                            and groups in different levels have different weights (see Eqs. (7) and (8)).

In order to investigate the advances of the tree-guided dictionary, we conduct some experiments in this section. As we know, the dictionary learned by sparse coding plays the same role of visual words in BoW, which uses the K-means algorithm to get the visual words. Each column in the dictionary is corresponding to a visual word and every visual word stands for one cluster centroid of some feature points. On the one hand, K-means algorithm partitions feature points into few visual words according to the texture of patches around these feature points. As a result, feature points with similar texture more probably form a cluster. On the other hand, the hierarchical structure means some groups in different levels, i.e., feature points located in the same region form a low-level group and feature points located in the same image form a high-level group. Therefore, the feature points belonging to one group more probably form a cluster. When this hierarchical structure is added into the process of dictionary learning, the above two factors act together, resulting in partitioning the feature points belonging to one group into fewer visual words. This means few more discriminate visual words are selected to represent the regions. In this way, more accurate and more sparse representation of regions can be obtained.


                        Fig. 7
                         lists this property of tree-guided dictionary. From top to bottom are the results on MSRC-v1, MSRC-v2 and SAIAPR TC-12 datasets, respectively. X-coordinate denotes the index of regions in the dataset. Y-coordinate denotes the total number of visual words that feature points in one region are partitioned into. Red line represents the results generated by tree-guided dictionary, and blue line represents the results generated by the traditional dictionary (we call it lasso dictionary). For each dataset, we select 50 regions, 100 regions and total number of regions in the dataset to illustrate the results, respectively. From the figures we can see that on all three datasets, the feature points located in the same region are partitioned into fewer visual words by tree-guided dictionary than lasso dictionary. Let us take an example: the feature points located in the first region in MSRC-v1 are partitioned into 19 visual words by tree-guided dictionary, but partitioned into 61 visual words by the lasso dictionary.

More intuitive version can be found in Figs. 8–10
                        
                        
                        . Taking Fig. 8 as an example, we randomly select three regions (each row corresponds to one region) in MSRC-v1 to show the results. For each row, from left to right are original images, feature points located in one region, the results generated by tree-guided dictionary and the results generated by lasso dictionary. The number of visual words that feature points located in the same region are partitioned into is depicted by the bar graph. Specially, the number of bars equals to the number of visual words generated by tree-guided dictionary or lasso dictionary. The tag above every bar denotes how many feature points are partitioned into this visual words and the sum of all tags in one figure equals the total number of feature points located in the left region. For example, in the first row of Fig. 8, there are 70 feature points located in the grassland region. these 70 feature points are partitioned into 21 visual words by tree-guided dictionary, but partitioned into more than 60 visual words by lasso dictionary. both the sum of tags in the third figure and forth figure equal to 70. Y-coordinate in the third and forth figure denotes the index for each visual word, we sort these visual words according their index so they display an increased trend in every figure.

@&#EXPERIMENTAL RESULTS@&#

Two traditional methods and five representative sparse reconstruction methods, KNN [44], Linear SVM [44], Lasso [41], Group Lasso (GL) [30], Sparse Group Lasso (SGL) [45], Spatial Group Sparse Coding (SGSC) [22] and the Graph-Guided Sparse Reconstruction for Region Tagging (
                           
                              
                                 
                                    G
                                 
                                 
                                    2
                                 
                              
                           
                        SRRT) [23] are compared with the Tree Group Lasso (TGL). For Lasso and Group Lasso, the reconstruction coefficient vector 
                           
                              β
                           
                         is obtained by solving Eqs. (3) and (5), respectively. For Sparse Group Lasso, 
                           
                              β
                           
                         is obtained by adding an 
                           
                              
                                 
                                    ℓ
                                 
                                 
                                    1
                                 
                              
                           
                        -norm penalty of 
                           
                              β
                           
                         to the objective function of Eq. (5). We use the SLEP package [46] to implement Lasso, Group Lasso and Sparse Group Lasso in this experiment. The range of tuning parameter of these three methods is the same to that of TGL. For SGSC and 
                           
                              
                                 
                                    G
                                 
                                 
                                    2
                                 
                              
                           
                        SRRT, we use the same values of parameters according to their corresponding papers.

The overall performance of region tagging on three datasets is reported in Tables 1–3
                        
                        
                        , respectively. We also list the experimental results on tree-guided dictionary and lasso dictionary, respectively. From the tables, we can draw the following three conclusion: firstly, tree-guided dictionary outperforms lasso dictionary for all the methods on three datasets. This demonstrates the tree-guided dictionary can provide more accurate representation of regions than lasso dictionary. Second, the performance of traditional method (KNN, Linear SVM) is generally lower than the sparse reconstruction method, which shows the advantage of sparse techniques. Third, for all the datasets, the proposed algorithms obtain the best performance of region tagging.
                           4
                           After finishing our experiments, we find that [47] achieves more than 0.86 average accuracy on MSRC-v2 dataset. Note [47] aims at solving the segmentation problem, ours aims at solving the region tagging problem. Moreover, the evaluation metric accuracy in this paper is different from ours. The accuracy in [47] equals to the overall percentage of correctly classified image pixels. However, the accuracy in our paper equals to the overall percentage of correctly tagged regions and the definition is different.
                        
                        
                           4
                         For example, in Table 1, the improvement varies from 1.03% to 4.75%, and averages at 3.265%. As previously mentioned, by the weighting scheme defined in Eqs. (7) and (8), tree-guided group lasso can vary from Lasso to Group Lasso, which is more flexible.

In order to evaluate the performance of learned tree-guided dictionaries, we show the results of region tagging on every single label on tree-guided dictionary and traditional lasso dictionary using the proposed tree group lasso method. Figs. 11–13
                        
                        
                         are the bar graphs of results. Note in the experiments the accuracy of some labels are zero, we don’t list these labels in Figs. 11–13. From the figures, we can see that the performance based on tree-guided dictionary is better than that on lasso dictionary for majority of the listed labels.

@&#CONCLUSION@&#

We have proposed a Unified Dictionary Learning and Region Tagging with Hierarchical Sparse Representation algorithm. The method proposes to integrate the region representation and region reconstruction into a framework, where a unified hierarchical structure among feature points, regions, and images is applied on these two tasks. The two hierarchical structures used in region representation and reconstruction correspond to different depths of this unified hierarchical structure, respectively. Compared with other five state-of-the-art sparse reconstruction algorithms and two traditional methods, our framework shows a promising performance improvement of region tagging. In addition, our method has an appealing scalability ability of constructing graph models to flexibly integrate other meaningful contextual correlations into the process of sparse reconstruction.

@&#ACKNOWLEDGMENTS@&#

This paper was partially supported by the 100 Talents Programme of The Chinese Academy of Sciences, the NSFC (under Grant 61202166) and Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center (Contract Number D11PC20068). The US Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the US Government.

@&#REFERENCES@&#

