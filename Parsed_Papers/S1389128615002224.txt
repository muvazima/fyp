@&#MAIN-TITLE@&#Motivating participation and improving quality of contribution in ubiquitous crowdsourcing

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We report 4 case studies on motivation to participate in ubiquitous crowdsourcing.


                        
                        
                           
                           We demonstrate that motivation can affect levels of participation and performance.


                        
                        
                           
                           We provide recommendations on designing ubiquitous crowdsourcing applications.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Ubiquitous Crowdsourcing

Motivation

Participation

Performance

Engagement

@&#ABSTRACT@&#


               
               
                  Ubiquitous crowdsourcing, or the crowdsourcing of tasks in settings beyond the desktop, is attracting interest due to the increasing maturity of mobile and ubiquitous technology, such as smartphones and public displays. In this paper we attempt to address a fundamental challenge in ubiquitous crowdsourcing: if people can contribute to crowdsourcing anytime and anyplace, why would they choose to do so? We highlight the role of motivation in ubiquitous crowdsourcing, and its effect on participation and performance. Through a series of field studies we empirically validate various motivational approaches in the context of ubiquitous crowdsourcing, and assess the comparable advantages of ubiquitous technologies' affordances. We show that through motivation ubiquitous crowdsourcing becomes comparable to online crowdsourcing in terms of participation and task performance, and that through motivation we can elicit better quality contributions and increased participation from workers. We also show that ubiquitous technologies' contextual capabilities can increase participation through increasing workers' intrinsic motivation, and that the in-situ nature of ubiquitous technologies can increase both participation and engagement of workers. Combined, our findings provide empirically validated recommendations on the design and implementation of ubiquitous crowdsourcing.
               
            

@&#INTRODUCTION@&#

Crowdsourcing work and the associated distribution of micro-tasks across large numbers of individuals are becoming increasingly popular in settings beyond the desktop, thus enabling a wide range of applications. Ubiquitous technologies, such as smartphones and public displays, are now mature enough to allow users to contribute to crowdsourcing tasks wherever and whenever. While the increased ease with which it is now possible to participate in crowdsourcing work raises new possibilities, it also raises an important question of motivation: if people can contribute to crowdsourcing anytime and anyplace, why would they choose to do so?

While the issue of motivation has been a long-standing concern in the design of computer systems and online services, new technologies require that new motivational approaches are developed, adapted, and validated. In terms of crowdsourcing, research in psychology, sociology, management and marketing provide a solid theoretical basis on human motivation [36]. However, these theoretical approaches typically have to be adapted and fine-tuned for a crowdsourcing setting. At the same, by motivating workers to contribute more, task requesters can unwillingly make them more susceptible to quality control issues [36] so careful motivational considerations have to be taken into account.

In addition to accounting for human behaviour, a motivational approach also needs to account for the technologies and context of use. Ubiquitous technologies are particularly challenging as they are typically in the hands of users, away from the controls of a lab setting, and normally lack any identification mechanisms. Therefore, crowdsourcing using these technologies may produce “noisy” results due to unpredictable behaviour or misappropriation from users [28,59]. Thus, prior work further emphasizes the importance of appropriate motivational approaches to address these challenges.

In this paper we adapt, present and validate motivational approaches for ubiquitous crowdsourcing by drawing on extensive literature and leveraging our own results from four case studies. These approaches draw on prior literature in human behaviour, account for ubiquitous technologies, and are validated in field trials to establish their effect on participation and quality of contribution. We answer four important questions on the motivational aspects of ubiquitous crowdsourcing regarding the possibility of eliciting altruistic contributions, the effectiveness of various motivational approaches, the potential advantage that ubiquitous technologies can offer for crowdsourcing, and finally on the situated nature of ubiquitous crowdsourcing:

                        
                           1.
                           
                              Can ubiquitous crowdsourcing work altruistically? We answer this through a case study where we experimentally compare results obtained between ubiquitous and online crowdsourcing. Specifically, we establish a baseline assessment showing that performance in ubiquitous settings without payment is comparable to online settings with payment, and worthy of further investigation.


                              Can psychological empowerment motivate ubiquitous crowdsourcing? We answer this by validating psychological empowerment approaches as motivators for participation in ubiquitous crowdsourcing. Specifically, we validate how 3 types of psychological empowerment affect participation and contribution in crowdsourcing on mobile phones.


                              Can contextual cues motivate ubiquitous crowdsourcing? To answer this we test the effect of presenting contextual information on engagement. Specifically, we evaluate how the presentation of location cues affects participation in a “crowd-mapping” setting.


                              Can situatedness motivate ubiquitous crowdsourcing? Specifically, we investigate how the situatedness of ubiquitous technology can motivate people to participate in in-situ feedback collection.

We conclude the paper with the lessons learned throughout our case studies, discussion of other potential motivational approaches in ubiquitous crowdsourcing and empirically validated recommendations on the design and implementation of ubiquitous crowdsourcing.

@&#RELATED WORK@&#

Crowdsourcing with ubiquitous technologies is increasingly gaining researchers' attention [64,65], especially on mobile phones. This has allowed researchers to push tasks to the workers, anywhere and anytime. Most of these platforms have been deployed in developing countries targeting low-income workers providing them with simple tasks, e.g. [15,25]. Recent advances in mobile technologies have also allowed for more intricate and creative tasks. For instance, Wallah, a mobile crowdsourcing platform for Android OS implements caching for offline situations and aims to minimize the impact of different screen sizes of smart phones [40]. More broadly, the location-based distribution of crowdsourcing tasks has allowed its workers to perform real-world tasks in a peer-to-peer fashion. Some examples of this include providing location-aware recommendations for restaurants [1], providing instant weather reports [1], or authoring news articles by requesting photographs or videos of certain events from workers [63].

Recently, another community has developed around the topic of crowdsourcing measurements and sensing. This participatory sensing movement is also referred to as “Citizen Science” [49] and relies on mobilizing large parts of the population to contribute to scientific challenges via crowdsourcing. Often this involves the use of smartphones for collecting data [6] or even donating computational resources while the phone is idle [2].

Despite the appeal of mobile phones, using them for crowdsourcing requires workers' implicit deployment, configuration and utilising users' own hardware. For example, in SMS-based crowdsourcing, participants need to explicitly sign up for the service, at the cost of a text message exchange. This challenges recruitment of workers, as a number of steps need to be performed before a worker can actually start contributing using their device. Alternatively, a passive approach of crowdsourcing tasks to workers is to embed public displays into a physical space and leveraging workers' serendipitous availability. Crowdsourcing using public displays requires little effort from the worker to contribute [19,24], lowering the barriers to contribution from a workers' perspective by minimising the initial effort. Furthermore, it allows for a geofenced and more contextually controlled crowdsourcing environment [24], thus enabling targeting certain individuals [19,20], leveraging people's local knowledge [21,28] or simply reaching an untapped source of potential workers [27,29].

A reflection on the effective facilitation mechanism for public displays to motivate users to deliver reliable and meaningful feedback is lacking but also imperative. Most prior research has reported the use of public displays for hedonic services (e.g., games, opinion disclosure) or information-based services (e.g., information boards) that offer instant benefits to users [5,38]. There is a lack of deliberation on the possibility of using public displays in an altruistic manner, such as for non-paid crowdsourcing. Pragmatically, a successful demonstration of the potential of public displays for altruistic services implies a possible future direction for public displays research and practice.

Despite the various benefits of public displays for crowdsourcing, there are some serious drawbacks. For instance, the walk-up-and-use nature of public displays can result in limited usability and accessibility of tasks, with less rich interface controls than a standard desktop environment or a mobile phone. This means that not all types of tasks can be crowdsourced on a public display. Another drawback is that the maintenance of public displays is more difficult than maintaining an online server and can incur higher initial costs. The scalability of crowdsourcing using public displays is directly dependent on the number of displays allocated to the system and their audiences. Even so, displays are becoming increasingly affordable, and researchers have systematically attempted to identify novel applications for this technology. Also, research on interactive displays in public spaces has often noted that users typically demonstrate playful and exploratory behaviour when using this technology. As a result, the collection of feedback on such displays is difficult, and results tend to be rather noisy [28].

Why do people participate in crowdsourcing markets, and what predicts their performance?  A traditional “rational” economic approach to eliciting higher quality work would be to increase extrinsic motivation, e.g., an employer can increase how much they pay for the completion of a task [17]. Some evidence from traditional labor markets supports this view: Lazear [41] found workers to be more productive when they switched from being paid by time to being paid by piece.

An experiment by Deci [14] found a “crowding out” effect of external motivation such that students paid to play with a puzzle later played with it less and reported less interest than those who were not paid to do so. In the workplace, performance-based rewards can be “alienating” and “dehumanizing” [16]. If the reward is not substantial, the performance is likely to be worse than when no reward is offered at all; insufficient monetary rewards can act as a small extrinsic motivation that tends to override the possibly larger effect of the task's likely intrinsic motivation [18]. Given that crowdsourcing markets such as Mechanical Turk tend to pay very little money and involve relatively low wages [48], external motivations such as increased pay may have less effect than requesters may desire. Indeed, research examining the link between financial incentives and performance in Mechanical Turk has generally found a lack of increased quality in worker output [42]. The relationship between price and quality has also had conflicting results in other crowdsourcing applications such as answer markets (e.g., [26]). Although paying more can get work done faster, it has not been shown to get work done better.

Another approach to getting work done better could be increasing the intrinsic motivation of the task. Under this view, if workers find the task more engaging, interesting, or worth doing in its own right, they may produce higher quality results. Unfortunately, evidence so far has not fully supported this hypothesis. For example, while crowdsourcing tasks framed in a meaningful context motivate individuals to do more, they are no more accurate [8]. On the other hand, work by Rogstadius et al. [56] suggests that intrinsic motivation has a significant effect on workers' performance. One has to note however a number of questions and methodological questions that are yet to be settled. First, prior studies have methodological problems with self-selection, since workers may see equivalent tasks with different base payment or bonuses being posted either in parallel or serially. Second, very few studies besides [56] have looked at the interaction between intrinsic and extrinsic motivations; Mason & Watts [42] vary financial reward (extrinsic), while Chandler and Kapelner [8] vary meaningfulness of context (intrinsic) in a fixed diminishing financial reward structure. Finally, the task used in Chandler and Kapelner [8] resulted in very high performance levels, suggesting a possible ceiling effect on the influence of intrinsic motivation.

Here we describe the motivational approaches that were adapted in our case studies. These were chosen based on previous work demonstrating their success in motivating participation as well as improving contribution quality.

One way to achieve higher levels of motivation in ubiquitous crowdsourcing is through psychological empowerment. Its role in improving citizen participation has been highlighted in the past [68]. Psychological empowerment integrates perceptions of personal control, a proactive approach to life and a critical understanding of the socio-political environment [69]. Generally speaking, commitment in achieving personally relevant aims strengthens agency [7] and consequentially generates psychological empowerment through these actions. Some of the factors of psychological empowerment that have been repetitively found to motivate citizen participation that are normally used in collective good settings are:

                           
                              •
                              
                                 perceived self-efficacy: the degree to which individuals believe they have the capabilities to achieve the desired goals [46],


                                 sense of community: the relationship between the individual and the social structure [52], and


                                 causal importance: an individual's beliefs about the relationship between actions and outcomes. [50].

Another way to motivate users' participation in ubiquitous crowdsourcing is through the use of contextualized information. Contextualizing information has been shown to help tap into individual's episodic memories [61] by allowing mentally “re-living” specific life experiences and improve recollection by thinking back in detail to past personal experiences [60]. Reflection can be used to examine patterns of past experiences, which may provide useful information about general level of physical activity or emotional states in different situations, allowing the person to relate to other data [60]. Furthermore, literature suggests the use of digital cues as appropriate contextual cues because our memory is a reconstructive process mediated by triggers from everyday events [4,11]. The most used types of digital cues are visual cues [39,66] or location cues [67], which can trigger everyday recall, promote attentiveness and lead to increased participation.
                     

Another approach to leveraging ubiquitous crowdsourcing is through supporting opportunistic participation. Prior work has shown that allowing passersby to understand situated and contextually relevant information can lead to genuinely insightful contribution [3]. For instance, public displays have become a viable medium for such opportunistic contribution. Supporting this, De Cindio et al. [9] observed that people leave feedback often during so called peak or protest moments, when the circumstances for public discourse or disapproval are right. These results together raise the question of whether situated feedback mediums could be leveraged to reach people during these key moments for discourse.

One may expect these moments to occur when citizens confront a public display in a city and are given the possibility to leave instant feedback about a locally remarkable and topical issue that invades their territory. Public displays also foster sociality and group use by nature [37], and eliciting contribution from groups of users is often easier than from individuals [28]. Further, the well-known honeypot effect 
                        [5] can be leveraged to our advantage in spreading awareness about the feedback channel among nearby potential users.

Archetypal feedback applications on public displays utilize typing in some form as their main input modality. Brignull and Rogers reported Opinionizer 
                        [5], a system that combined a projected screen with a laptop to type feedback and converse about the everyday contexts it was deployed in. They introduced the honeypot effect and emphasized social pressure and awkwardness that users often feel when interacting publicly. A playful feedback application, connected to social networking services and utilizing a virtual keyboard and a web camera for feedback was introduced by Hosio et al. [30]. Studies with Ubinion also highlighted situated public displays being fit for acquiring contextually relevant feedback.

In this case study we summarise our work on directly comparing crowdsourcing conducted in an online and a ubiquitous setting. Specifically, we contrast the crowdsourcing of an identical task that was conducted in Mechanical Turk, as well as on public interactive displays. This work has been previously reported in [19,56] and here we summarise the points that are most pertinent to our discussion.

This case study was the first (to our knowledge) attempt to investigate altruistic use of interactive public displays in natural usage settings as a crowdsourcing mechanism. Our goal was to explore the possibility of workers contributing on a non-personal device thus highlighting the “ubiquitous” nature of this task while also not actively recruiting participants. Furthermore, we investigated what effect motivation can have in such a setting.

The experiment consisted of testing a crowdsourcing service on public displays and on Mechanical Turk. The task used in our experiment is the counting task described by Rogstadius et al. [56] in which workers are asked to count malaria-infected blood cells on images of a petri dish generated algorithmically (Fig. 1). We deployed this task on both Mechanical Turk, as well as a set of public interactive displays deployed throughout our university campus.  Because the task was identical across these two deployments, we were able to make a direct comparison between the two. No money was given to the participants who used the public displays, while participants on Mechanical Turk were rewarded with 0, 3 or 10 cents per task completed.
                     

Focusing on our public display deployment we validated two motivational approaches. The first approach was based on intrinsic motivation in which we identified from literature two types: enjoyment-based and community-based 
                           [34].  We used one construct per motivator, to enable reliable testing on a public display.

                              
                                 •
                                 
                                    Task identity: A worker performs a task because he knows that his work will be used (e.g., writing a product description for a website) [34].


                                    Community identity: A worker who only accepts tasks from requesters with a good reputation because they are known as a valuable supporter of the community [34].

Using these we derived 4 conditions of motivation for our experiment. These were conveyed through the introductory text on the instruction page, which was manipulated based on the condition:

                              
                                 •
                                 
                                    Control (no motivation): “We invite you to identify blood cells infected with malaria parasites.”


                                    Enjoyment-based (task identity): “We invite you to identify blood cells infected with malaria parasites. This in turn will help produce better software to improve malaria cell detection.”


                                    Community-based (community identity): “We invite you to identify blood cells infected with malaria parasites. This in turn will help Oulu medical scientists on their research.”


                                    Both enjoyment & community-based (task & community identity): “We invite you to identify blood cells infected with malaria parasites. This in turn will help produce better software to improve malaria cell detection and help Oulu medical scientists on their research.”

The second motivational approach we tested was fact-check, with two levels: present and absent. Fact-check simply consists of asking users a question whose answer is very simple to answer. This can be an effective way to filter out non-serious answers and improve the overall quality of the answers given [35]. An important characteristic of this step is that the question must be easy to answer, and it must be clear to the respondent that the experimenters also know the answer. In those conditions where the fact-check was present, we showed users a question just before starting their first task. The question was: “What is the name of our planet?” Users would then have to select their answer out 4 possibilities (Earth, Saturn, Mars, and Jupiter) ordered randomly to avoid bias.

On Mechanical Turk, a total of 158 workers completed at least one task while on the public displays there were 482 unique sessions in which at least one task was completed. Our analysis showed that when it comes to crowdsourcing using public displays, intrinsic motivators, but not fact-check, had a significant effect on accuracy, completion time and tasks completed. Workers that were presented with any of the 3 psychological constructs (i.e. excluding the control condition) were more accurate (F(3,1192) = 13.93, p < .01), spent more time doing each task (F(3,1192) = 6.07, p < .01) and completed more tasks per session (F(3,474) = 3.45, p = .02), thus paying overall more effort in performing the tasks. While previous work has found that unusually long task completion time indicates distraction and poor commitment [54], in this case those in the control conditions ended up spending significantly less time performing the tasks, particularly the more complex ones. Furthermore, the presence of the fact-check also acted as an important quality control mechanism. Those that answered this question correctly were far more likely to perform the following tasks more accurately and in greater number, while not having a significant influence in the time they would be willing to spend.

Furthermore, we compared our two datasets (Public display vs. Mechanical Turk) in terms of: accuracy and rate of uptake of tasks. The rate of uptake is indicative of how quickly the tasks were completed, and how much time it takes to have a large number of tasks completed. Overall, our analysis showed that accuracy results obtained on Mechanical Turk were about 10% higher when compared public display. However, participants' performance was significantly higher (p < .01) in conditions where a motivator and the fact-checking question were present when compared to Mechanical Turk. In addition, workers on the public display were more likely to “give up” after a certain point of complexity highlighting the need for careful task assignment in this medium. Finally, the rate of uptake of tasks on the public display was much higher than on Mechanical Turk reaching 1200 tasks completed in 25 days compared to the non-paid version on Mechanical Turk that only reached 100 tasks completed in over 45 days.

Understanding the social dynamics around situated technologies is crucial for the improvement of crowdsourcing in these mediums. By understanding the potential emergent worker behaviours in ubiquitous crowdsourcing, tasks can be designed to cater for the more productive worker archetypes. In online and mobile crowdsourcing studies where personal devices are used, workers are normally treated as a black box. There are only a few exceptions in the literature [57,58] in which task requesters can infer worker performance from the way they conduct the tasks. Ubiquitous crowdsourcing offers the possibility to directly observe people completing crowdsourcing tasks. Similar but simpler approaches provide requesters more visibility into worker behaviour, such as oDesk's Worker Diary, which periodically takes snapshots of workers' computer screens.

Here, we had the unique opportunity to observe participants' attitudes and social context when completing tasks. We used video recording to capture all interactions with one of the public displays during the study (Fig. 2). Our video observations included 123 instances of interaction. Our content analysis coded these videos concurrently by a group of researchers using open and axial coding, thus identifying emerging themes of behaviour. As reported in [19], this analysis confirmed instances of the behaviours that we initially noted in our in-situ observations, but also revealed several new behaviours that people exhibited when using the display. The six identified behaviours were:

                              
                                 •
                                 
                                    Ignorer: passers-by that ignored the display, exhibiting what is often referred as display blindness [44], and


                                    Unlocker: those that actually unlocked the screen but completed no tasks. These account for the high number of curiosity clicks mentioned previously.


                                    Herder: individuals would approach the display with a group of people, complete some tasks and then leave with the group. The other members would adopt a passive position behind the herder, in a way that suggested they were not applying social pressure but rather observing,


                                    Loner: individuals that approached the display alone and typically spent more time than others completing tasks.


                                    Attractor: attracted others to join them on the display, commonly referred as the honeypot effect [5], and complete tasks jointly.


                                    Repeller: applied social pressure to try to make the worker leave the display. Instances of repellers also happened when groups of two or more people approached the display.

The relative frequency of the behaviour patterns that actively interacted with the display was: Loner 19%, Attractor 11%, Herder 6%, Repeller 14%, Unlocker 44%. The remaining 6% of interactions did not fit the description of the aforementioned behaviour patterns. Loners completed on average a higher amount of tasks (M = 4.91, SD = 1.04), followed by attractors (M = 3.71, SD = 4.53), herders (M = 3.43, SD = 1.29) and finally repellers (M = 1.29, SD = .59). A Kruskal–Wallis test showed that there was a significant difference in average number of tasks completed between the different behaviours (χ
                           2(4) = 22.18, p < .01). Post-hoc analysis using the Mann–Whitney tests showed that there were only a significant difference between loners and repellers in terms of average number of completed tasks (U = 26.04, p < .01). As for accuracy, a Kruskal–Wallis test showed that there was no significant difference in accuracy between the different behaviours (χ
                           2(4) = 7.99, p = .09). These results suggest that appealing to certain workers can improve the task uptake without having a significant impact on accuracy. At the same it also highlights the importance of motivation when collecting crowd work.

The latter two behaviours (attractor and repeller) ultimately led to a disturbance and delay in the completion of the tasks. In other words, this resulted in the opposite of peer pressure in that workers instead of being pressured to do well, they would engage in performative acts [27] resulting in non-serious completion of tasks. Previous work has reported that in some cases the engagement with these interactive public artefacts emerges only when the overall social context provides a “license to play” [32]. In the case of playful applications or games, this does not matter and can even act as a catalyst to use [37], but when collecting meaningful data from the public, it may be beneficial to attract more loners than groups.

We found that through the controlled use of motivational design performance can be significantly improved, and that community-based and enjoyment-based motivational approaches can be successful. We also found that when crowdsourcing using public displays, fact-checking is not as effective as in online settings, but it still acts as a reliable mechanism to identify non-serious respondents. Finally, we found that crowdsourcing on public displays without explicit recruitment can produce comparable performance even to paid studies on Amazon's Mechanical Turk. This highlights the difference in the compared mediums, which was expected since they tap into very different populations. Online workers typically expect to get paid for their work. By completing non-paid tasks they are practically losing money by not using the time to complete higher-paying tasks. Thus, work motivated by altruism is not particularly suitable for labor markets such as MTurk. On the other hand, ubiquitous crowdsourcing offers a setting that is rather suitable for enticing volunteers and eliciting altruism in the context of crowdsourcing.

An important limitation of the previous case study was that due to its “in-the-wild” nature we were not able to identify and keep track of all people who took part in the ubiquitous crowdsourcing part, and therefore it is impossible to reliably assess individuals' performance. To address this limitation in attributing performance and findings to individual participants, we conducted a study where participants were recruited. Here we were interested in collecting rich data on the effect of psychological empowerment on individual participants, and how that may affect their performance, behaviour, and perceptions.

In this case study we evaluated the impact of three principles of psychological empowerment, namely perceived self-efficacy, sense of community and causal importance, on public transport passengers' motivation to report issues and complaints while on the move. We chose SMS as the communication medium for recruitment and enabling participation and interaction with bus riders. For more details on this study please refer to [23].

When participants signed up for the study, they were invited to report problems or make suggestions for the improvement of the bus service. They were informed via SMS that the study was conducted and controlled by the University, and their comments would eventually be shared with the bus company. They were also informed that they could submit an SMS with ‘Help’ to receive further tips, and an SMS with ‘Unregister’ to opt out of the study. Following their registration, participants were randomly allocated to one of the four conditions: one control condition and three representing the factors of psychological empowerment that we wanted to affect.

At the end of each day (8 p.m.) participants received a single motivational SMS reflecting the condition they were allocated to. The participants in the Control condition also received a message that did not involve psychological empowerment but simply thanked them for their participation. Some examples of motivational SMS sent to participants for each condition are:

                              
                                 •
                                 
                                    Perceived self-efficacy: “Your contributions have been great. Please continue contributing whenever you feel it is necessary.”


                                    Sense of community: “Thanks for being part of this movement to make public transportation more enjoyable to everyone.”


                                    Causal importance: “Thank you for your comments. All your messages will be taken into account.”


                                    Control: “Thank you for participating.”
                                 

We hypothesized that increased psychological empowerment will lead to:

                              
                                 •
                                 Increased participation, which can be measured by the number of submitted reports [46,50,52].

Improved quality of contribution, which can be analysed through the submitted reports.

As we have reported previously [23], we took measures to ensure that every SMS we sent to each participant reflected a single empowerment strategy. In addition we ensured that each participant received a variety of messages (rather than the exact same message every day), and each message should only reflect the empowerment strategy of the participant's condition. To satisfy these requirements, we constructed a pool of 14 messages for each of the 3 manipulated conditions (7 messages for active participants and 7 messages for passive participants), plus 2 messages for the control condition (1 message for active participants and 1 message for passive participants). We chose to have only two messages for the control condition as it models current systems and it intentionally does not change. This resulted in 44 distinct messages.

To verify that the messages reflected the intended empowerment strategy, we recruited 10 colleagues and briefed them on each psychological construct. We then asked them to use card sorting to assign each of the 44 messages to one type of psychological empowerment or to the control condition. Overall, 92.1% of the assignments were accurate. In addition, we interviewed five colleagues who had not participated in the card sorting, discussing how our messages made them feel and what their thoughts were on them. The responses confirmed that nearly all messages instilled the feelings of psychological empowerment we intended. We used this feedback to further iterate on the messages until we were satisfied they reflected the intended psychological empowerment.

@&#RESULTS@&#

In total we had 65 participants and received 354 reports from participants. Of all reports, 109 (30%) were in the Perceived self-efficacy condition, 88 (25%) in Sense of community, 94 (27%) in Causal importance and 63 (18%) in the Control condition. All reports were subjected to a qualitative content analysis [31]. This process consisted of open and axial coding, and was conducted independently by two researchers. The resulting coding scheme was discussed and iterated, and all reports were classified in one of six categories. Interrater reliability was satisfactory (Cohen's K = 0.85). The six categories were identified:

                              
                                 •
                                 
                                    Delays in bus arrivals (N = 41), e.g. “Bus 39 at 8:20 a.m. was 10 min late”,


                                    Driver behaviour (N = 84) such as being impolite, driving in a dangerous manner, or showing no respect for customers, e.g. “The driver was driving too fast with this rain”,


                                    Other passengers' behaviour (N = 6), e.g. “Bus full of kids constantly shoving me”,

Quality of infrastructure (N = 46) such as non-operational vending machines or inappropriate bus stops and shelters, e.g. “The electronic schedules have constant errors, I do not trust them anymore”,


                                    Quality of service (N = 85) referring to the cost, or the quality of the overall service, e.g. “Never enough change when I buy tickets on board” and


                                    Suggestions (N = 92) either for the improvement of existing services or development of new ones, e.g. “There should be a bus only for students”.

Overall, no significant relationship was found between motivation and category of report. However, the suggestions category was the most popular for participants in the Self-efficacy and Causal importance conditions (30% and 32% of their total reports, respectively), but not for participants of Sense-of-community and the Control condition (19% and 18%). When distinguishing reports between suggestions and complaints (i.e., the remaining five categories) we found a significant relationship between the motivational approach and the category of reports (χ
                           2 = 9.05, df = 3, p = .03).

We also found a significant effect of motivation on the total number of reports participants submitted (F(3,61) = 5.44, p < .01). Participants in the Perceived self-efficacy and Causal importance conditions contributed significantly more reports when compared to those in the Control condition (p < .01 and p < .05, respectively), but this was not true for those in the Sense of community condition (p > .05).  Some participants mentioned that our feedback messages acted as a reminder while adding to motivation to provide more feedback.

Finally, we observed no significant effect of the motivation approach on the length of the reports in terms of total characters (F(3,357) = 2.056, p = .11) or the time of day the report was sent (F(3,357) = 1.24, p = .30). Reports were overall spread throughout the day starting ranging from 7am to 11pm with the spikes occurring during rush hours (morning rush hour: 8am-9am, evening rush hour: 7–8 p.m.).

Our analysis showed that our manipulation of the motivational approach had two significant effects. First, those participants who received motivational messages were more likely to provide suggestions rather than complaints. Second, those participants provided more reports and participated more frequently. Overall, our 65 participants revealed that Self-efficacy and Causal importance increased participation while also improving the quality of contribution.
                        

An important limitation of the two case studies we have presented so far is that while they rely on ubiquitous and mobile technology, they do not take full advantage of the ubiquitous capabilities of these technologies. For instance, no contextual information was presented to participants, either in response to their actions or actions of others. For this reason, in this case study we seek to assess whether the potential of ubiquitous technologies in providing contextual information can act as a motivator for users. This case study is extensively described in [22], and here we summarise the motivational aspects of this work.

To investigate the effect of contextual cues on ubiquitous crowdsourcing, we built an online map-based platform where participants could report inaccessible spots throughout the city.

Rather than create a map-based application for venues and events – which already exist commercially – we opted to build an online “crowd-mapping” service, which would ultimately result in an accessibility map of the city (Fig. 3).

This case study aimed to assess how displaying different contextual information can affect the persuasive power of a message, and contribute to changing the attitudes the participants regarding cities' accessibility.

We hypothesized that the presence of contextual information will lead to:

                              
                                 •
                                 Increased participation (more reports submitted) through recollection and reflection as suggested by previous studies [33].

Increased awareness of environmental barriers and inaccessible spots [51].

We manipulated two variables:

                              
                                 •
                                 the presence of location cues (i.e., ability to zoom-in to an exact location on a map), and

the presence of visual cues (i.e., showing in-map photos when browsing a map).

This led to a 2 × 2 design with 4 conditions: Control, Zoom, Picture, Zoom&Picture. We instructed participants to take pictures of inaccessible locations around town to serve as “proof” and then later login to our online service to upload their reports. Each participant was allocated to one of four conditions that manipulated the user interface design and interaction mechanisms available in our web application. After logging in participants were presented with a Google Maps interface which had all reports from all conditions. Participants could:

                              
                                 •
                                 see their own and others' reports: depending on the condition, the participant was shown or not shown a photograph of the inaccessible location, and could or could not zoom into the map to get granular information about the exact location. Participants could always see the address and comments added to any given inaccessible spot (Fig. 3a); and

add a report using a form which was identical for all participants. To add a report participants could zoom to guarantee accurate pinpointing of the marker, rate the severity of the inaccessible spot (low – green marker, medium – yellow marker, high – red marker), leave a message, and upload a picture of the location, which all these were mandatory in all conditions for consistency (Fig. 3b).

Following the findings from Case Study 2, we wanted to make sure that participants were able to visualize their contributions and hopefully motivate them to further participate. Those in the Control and Zoom conditions were able to see their own pictures but never those submitted by other participants. We decided to allow this as to avoid participants to begin to question the value of their photographs and efforts in the system. This was particularly important, as uploading a picture was one of the requirements to be able to submit a report. Finally, the Control condition served as an example of system that provides feedback and information out of context.

@&#RESULTS@&#

In total, we had 24 participants and received 154 reports, 23 (14.9%) in the Control condition, 26 (16.9%) in the Zoom condition, 50 (32.5%) in the Picture condition and 55 (35.7%) in the Zoom&Picture condition. In terms of the severity of inaccessible spots of the reports we received there were 39 (25.3%) low, 62 (40.3%) medium, and 53 (34.4%) high.

We found a significant effect of motivation on the total number of reports submitted by participants (F(3,20) = .52, p = .67). Participants in the Picture and Zoom&Picture conditions contributed significantly more reports when compared to those in the Control condition (p = .04 and p = .02, respectively). Participants in the Zoom condition did not submit significantly more reports than those in the Control condition.

To submit a report, participants had to rate the severity of the spot's inaccessibility (low, medium, high). We found a significant relationship between condition and the severity level participants reported (χ
                           2 = 21.35, df = 6, p < .01). Low severity inaccessible spots (green markers) were more popular among participants in the Zoom condition and the Control condition. In contrast, we found that the two conditions with pictures had a greater inclination to report medium and high severity inaccessible spots (yellow/red markers).
                        

In this case study we demonstrate that contextual cues can significantly enhance user participation in a “crowd-map” platform to report inaccessible spots in a city. Our analysis showed that our manipulation of different levels of contextual information had two significant effects. First, visual cues led to enhanced user participation through increased number of reports. Second, visual cues lead to a bigger sense of urgency, which motivated users to find more severe accessibility issues that require more immediate attention.

A significant affordance of ubiquitous crowdsourcing is the ability to recruit and attract people in-situ. For instance, it is possible to collect opinions on an issue directly co-located with the used technology. In this case study we investigate this affordance in the scope of a major renovation of a city centre, which included building new pavement and underground heating systems for two of the busiest pedestrian streets (Fig. 4). This renovation heavily affected pedestrian flows and everyday business in all the surrounding areas, was a heated topic in the city, and was reported in dozens of stories in local newspapers where it garnered heavy attention in the discussion sections both for and against the project. During this renovation we explored the use of public interactive displays for collecting feedback and ideas from citizens on the renovation project. Specifically, we looked at what type of feedback was collected by the displays that were physically close to the renovation (i.e., “in-situ”) as opposed to the displays that were farther away.

In this case study, we used 57″ full-HD touch screen displays fitted in weather-proof casings. Many of the displays had been located in the vicinity of the renovation area already for several years and as such have gone beyond novelty to be an accepted part of the city infrastructure itself [47]. We used 12 displays: 5 located on the renovated streets (e.g., Fig. 4), and 7 farther away but still in popular locations. As providing contextual information worked well in Case Study 2, we hypothesized that by allowing citizens to provide input on an issue that they can actually see on the spot would work as an important added motivator for contribution.

The tested system was an application for the public displays that allowed citizens to rate the progress of the renovation, and to provide open-ended feedback.  The input was given directly on the public displays using an on-screen keyboard. We also included a stream of recent submitted comments following findings from Case Study 2 regarding the motivational importance of presenting users with the results of their contributions (Fig. 5
                           ). This practice has been suggested for enhancing communication between community members on public displays [62]. We hypothesized this would motivate feedback submission because letting users observe others' messages enhances sense of community, a strong motivator for participation in urban settings [9].
                        

@&#RESULTS@&#

During the deployment we collected a total of 246 text-based feedback. The five public displays located next to the reconstruction were significantly more popular (F(1,11) = 13.07, p <.01) than the seven that were not (N = 196 vs. N = 50). This difference is more accentuated if we look at the average amount of comments collected per display (39.2 vs. 7.1).

Some of the comments collected in the public displays next to the reconstruction highlight the importance of having the ability to participate in-situ after experiencing or while observing the reconstruction such as: “More employees are needed, 
                           
                              this
                            
                           needs to be done faster.”, “
                              Looking good
                           
                           , also the new stage looks nice!” and “It's great to 
                           
                              see
                            
                           the City developing!”.

Finally, the comment stream allowed people to pick up previous comments and discuss them, often by agreeing and supplementing them. An example sequence consists of the following messages: “Wasting years because of this small renovation is way too long.”, “Also, please add more working hours, it is taking too long.”, and “Yea, I also really agree on that”. This suggests that adding the comment stream led to further engagement with other citizens.

This case study evaluates the impact of situatedness in feedback collection. Our results highlight the potential of context to motivate participation in an urban context with the public displays located right next to the reconstruction eliciting more contribution. Furthermore, showing users each other's contribution can foster discussion and further increase participation.

To compare the effectiveness of motivation across our case studies, we further calculated task uptake (average number of tasks performed per day) and noise (% of bad quality contributions) when motivation was present and not present (Table 1). We then calculated the difference between these metrics across the conditions that either entailed motivation or did not (Table 1).

The results show that motivation increased task uptake for Case Studies 2 (90%), 3 (54%) and 4 (297%). However, for Case Study 4 we mostly attribute the substantial increase to the fact that displays co-located near the renovation are situated in a central area of the city. As for Case Study 1, task uptake remained the same due to the design of the experiment which entailed having the same amount of tasks completed on each condition at the end of the study. Furthermore, motivation reduced the amount of noise in most cases studies (1, 2 and 3). While the absolute number of valid comments was higher when motivation was present in Case Study 4 (32 vs. 22), their physical location ended up attracting a much larger number of users that misappropriated the prototype. We reflect more on these results in our discussion.

@&#DISCUSSION@&#

The case studies we presented have all been validated and tested in field trials. Such a methodology is particularly important when investigating motivational aspects of ubiquitous technologies. The urban space itself is a rich yet challenging environment to deploy ubiquitous infrastructure and applications in [43]. Several considerations, including the intertwined social practices of the location, robustness of the technology, abuse, vandalism, balance between the different stakeholders, and even weather conditions may cause constraints when deploying in the wild [13]. However, to gain an understanding of how technology is used and appropriated by the general public, deployment in authentic environments, or living laboratories, is highly beneficial [55] .

In Table 2
                      we summarise all case studies and their findings in terms of motivation and performance in ubiquitous crowdsourcing. Each case study addresses one of the questions we posed at the start of this paper as follows:

                        
                           1.
                           
                              Can ubiquitous crowdsourcing work without payment and explicit recruitment? Yes, we demonstrate that with appropriate motivation ubiquitous crowdsourcing without explicit recruitment and payment can compete with online crowdsourcing in terms of quantity and quality of contributions as well as attracting new workers.


                              Can psychological empowerment motivate ubiquitous crowdsourcing? Yes, it elicits more positive types of contribution and increased participation.


                              Can contextual cues motivate ubiquitous crowdsourcing? Yes, they can increase participation and improve the attitudes of workers.


                              Can situatedness motivate ubiquitous crowdsourcing? Yes, it elicits increased participation and engagement.

While altruism should be enough of a motivator since it appeals to people's desire to help [53], in our case studies we show that typically it is not. Across all our case studies we show that the control condition (i.e. the condition with no motivational manipulation) suffers severely in terms of participation: people are simply less willing to participate.

For example, in Case Study 1 we found that appropriate motivational and fact-checking mechanisms in the design are an important prerequisite for collecting accurate responses from users. Specifically, when considering only those people who gave a correct response to the fact-check question, the system achieved an average accuracy of 88%. This accuracy is comparable and even higher than that of workers on Amazon's Mechanical Turk – even those rewarded with money – highlighting the potential for ubiquitous crowdsourcing.  Without a motivational approach and fact-check, altruistic crowdsourcing can only work if participants actually think the problem being solved is interesting and important [53], which is in most cases hard to achieve. In these cases a careful design is required with appropriate motivational approaches and worker quality signaling through verifiable questions or analysis of worker behaviour.

In Case Study 2 we showed that using a motivational strategy has a significant effect on increasing the level of participation of individuals on the go. Specifically, we found that participants in two of the three intervention conditions (Perceived self-efficacy and Causal importance) had significantly more reports submitted than the control group. Our finding that Sense of community does not substantially increase participation is in contrast to prior research [52].

However, the level of participation is not the only aspect of concern: quality can be just as important.  Our case studies showed there was a significant effect on the quality and type of feedback provided by participants who were motivated by our manipulations with the exception of Case Study 4 (Table 1). For instance, in Case Study 2 we found that participants in the Perceived self-efficacy and Causal importance conditions did not complain as much, but rather focused more on providing constructive criticism and ideas on improving the service quality. Furthermore, the amount of noise was lower (−15%) in the conditions with a motivational approach when compared to the control group. These findings strongly suggest that for contribution on the go, motivational approaches are important to mitigate non-serious use. An additional take-away point is that while SMS can be used “anytime, anywhere”, its affordances are more appropriate for eliciting “flash-contribution”. In this sense, this medium is better suited for “as-it-happens” reporting or for quick crowdsourcing interactions. Also, highlighting the individual's belief in his own ability to perform the tasks and the usefulness of their contribution can significantly improve the rate and quality of contribution. A drawback we wish to highlight in Case Study 2 was that participants could never see the results of their contributions with the SMS gateway, which acted like a black-box. This in turn could have acted as a “de-motivator” being partly responsible with the rapid decrease in submitted feedback.

For this reason, in Case Study 3 we opted for a task that would allow participants to see the results of their contribution through an aggregated map of inaccessible spots around the city. Here, our results showed a clear and strong impact of contextual cues on participants' behaviour.  Our finding that pictures, but not location, had an effect on our participants agrees with prior theory that argues for the evocative power of images but not location per se [4,11], while others have reported the importance of images in everyday memory [4,12]. In addition, our results showed that participants from the two conditions with pictures had a tendency to report inaccessible spots as being of higher severity than the reports from the other two conditions. We believe this was caused in part by the fact that these participants became more aware of the issues at hand and therefore attributed a higher level of severity to their own reports. Hence, we argue that when presenting users with information that is more closely related to their life can greatly impact their willingness to contribute to ubiquitous crowdsourcing.

While the broader term “ubiquitous crowdsourcing” has been used throughout this paper, we wish to highlight one fundamental nuance. There is a clear distinction between ubiquitous crowdsourcing using one's own personal device (e.g., mobile phone) versus a non-personal device that is embedded in the urban space (e.g., public displays). As highlighted in our case studies, while some broader motivational approaches can apply to both cases (i.e., the use of contextual information or presenting users their contributions) there are various considerations to take into account depending on the technology used. For instance, the daily motivational SMS used in Case Study 2 or the collection of data around the city as in Case Study 3 can only apply to personal devices. Hence, the broader adaptation of motivational approaches to ubiquitous crowdsourcing must also consider the technology and its affordances.

Based on Case Studies 1 and 4, as well as literature, we provide two recommendations for researchers planning to orchestrate longitudinal studies to elicit contribution on public interactive devices/displays. First, one should expect social use of this kind of publicly deployed technology. For instance, social and performative uses are intrinsic motivators that have been found to drive the use of public displays [37,45]. Further, the findings in [5] suggest that people feel a certain awkwardness and external pressure when interacting alone with such displays. This often leads to displays being used by multiple users (most likely friends) at the same time [28]. However, for crowdsourcing deployments that leverage publicly available technologies Case Study 1 demonstrated (as emphasized in Section 4.2) that one should design for “loners”, as groups of people will ultimately lead to a higher uptake of tasks without having a significant impact on accuracy when completing crowdsourcing tasks.

One should also set realistic goals for the kind of deployments where ubiquitous crowdsourcing is conducted on non-personal devices: the collected data can be noisy and sparse. It is has often been reported that various social needs such as self-expression or documenting rule breaking emerge in the use of new communication channels [62]. If a crowdsourcing application is deployed in the field, and for example allows free-form submissions such as the one in Case Study 4 did, these needs are likely to lead to appropriation and increased noise in the collected data as seen in Table 1. In terms of the volume of collected data, previous literature certainly suggests that ample amounts of data can be collected in field trials [3,20,27,28]. However, a common factor in all these reports is a highly advertised and disruptive deployment that offers informal or even amusing topics for participation. Long-term deployments that leverage public technology for crowdsourcing may suffer from sparse data, but at the same time they allow for sustained participation. Because sustained participation, according to Clary and Snyder [10], is one of the key components of meaningful community engagement, it is desirable for crowdsourcing as well. Therefore, while public technologies are recognized as potential mediums for crowdsourcing, it is advised not to expect in naturalistic settings the same quantity and quality of participation from the crowds as reported in literature with controlled and/or short-term studies.

One positive affordance of public devices particularly in regards to crowdsourcing is that they are used opportunistically. For instance, Müller et al. argue that public displays do not invite people for a single reason, but users come across and start to use them with no clear motives in mind [43]. Therefore, they reach users that could otherwise be hard or borderline impossible to reach. This has been demonstrated in the past by bridging citizens and city officials through public displays [28]. In that study 67% of the users who used the display to communicate with officials had never before had any kind of contact with them. The physical settings, target audience, used feedback mechanisms, and the topic of participation were all found to affect the success of public displays in reaching the crowd.

Opportunistic participation was also evident in our own Case Study 4. The fact that some public displays were directly co-located with the reconstruction led to increased interest from the crowd. Citizens who could witness the construction and suffer its annoyances (e.g., air pollution) were more inclined to leave more specific comments. The case study also highlights the potential of public technology in reaching otherwise hard-to-reach masses. The crowdsourced feedback on the renovation was the only feedback that the city officials ever received from the citizens. The other offered feedback mechanisms, phone, Web, and email, were indeed not used by the crowd. Thus, while crowdsourcing with public technologies is still just an emerging opportunity, our findings are encouraging and motivate further exploration.

Another fundamental nuance explored in our case studies and of interest to ubiquitous crowdsourcing as a whole is how the recruitment of workers is conducted. Specifically, in Case Studies 1 and 4 we did not actively recruit workers while in Case Studies 2 and 3 the opposite happened.

Each of these approaches to recruitment has their advantages and disadvantages in ubiquitous crowdsourcing. For instance, by not explicitly recruiting workers there is a raise the awareness of crowdsourcing among the local community acting as a self-renewable “workforce” by constantly attracting new workers. In the particular technology we used in our case studies (i.e., public displays) passersby get attracted to complete tasks and leveraging workers' serendipitous availability [43]. In a study by Gupta et al. [25] it became evident how forms of crowdsourcing that require explicit recruitment, in this case on mobiles, have difficulties in sustaining participation. While their participation started strong, when they reduced the payments the number of active users dropped 53% within a day. Most users who left reported that they could not invest more time and work for lower compensation, even though they were still getting paid. In other words, there were no new workers coming in when the old ones become fed up with the task at hand. While this may have been partially caused by the study design, recent evidence strongly suggests that even after lowering rewards, participation rates stay high on e.g. crowdsourcing on public displays [29]. Again the explanation can be found in the affordances of the medium itself: (new) users of public displays often have nothing else to do, and "killing time" is often as powerful a motivation as the offered rewards seem to be [29].

On the other hand, by not explicitly recruiting workers it also means that much less is known about them. In addition, the number of participants is hard to scale up: it is dependent on the number of displays utilised in the platform and on the people in the spaces where the displays are. The conventional procedure in studies where there is explicit recruitment is to assemble an appropriate sample in terms of largeness and diversity depending on what is being investigated. This obviously means a higher control from the researchers allowing them to differentiate their findings in terms of demographics, getting on-going feedback and following-up with workers through interviews. Furthermore, by having the workers' contact details it is possible to obtain additional information on the fly or take a more pro-active approach to motivation like the sent SMS we report in Case Study 2. In addition, since we had no way of identifying individuals in Case Studies 1 and 4, this lead to increased misuse of the platforms. This was particularly evident in Case Study 4 due to the fact that the contribution was through free-form textual submission.

In conclusion, the decision to conduct a study on ubiquitous crowdsourcing in a more controlled or in an “in-the-wild” manner will ultimately depend on what the researchers' goals are. If identifying workers specifically is crucial to better understand the findings or to allow an on-going dialog then an explicit recruitment approach should be considered. However, if the goal is to reach a higher number of people and therefore a higher amount of responses where knowing the background of workers is not a significant concern then a passive approach without explicitly recruiting workers should be considered.

@&#CONCLUSION@&#

In this paper we address a fundamental challenge in ubiquitous crowdsourcing: if people can contribute to crowdsourcing anytime and anyplace, why would they choose to do so? Here we highlight the role of motivation in ubiquitous crowdsourcing, and its effect on participation and performance. We show that through motivation ubiquitous crowdsourcing becomes comparable to online crowdsourcing in terms of participation and task performance, and that through motivation we can elicit more positive contributions and increased participation from workers. We also show that ubiquitous technologies' contextual capabilities can increase participation and improve the attitude of workers, and that the in-situ nature of ubiquitous technologies can increase both participation and engagement of workers. Combined, our findings provide empirically validated recommendations on the design and implementation of ubiquitous crowdsourcing.

@&#ACKNOWLEDGEMENTS@&#

This work is partially funded by the Academy of Finland (Grants 276786, 285062, 286386), and the European Commission (Grants PCIG11-GA-2012-322138 and 645706-GRAGE).

@&#REFERENCES@&#

