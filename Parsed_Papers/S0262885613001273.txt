@&#MAIN-TITLE@&#Spatiotemporal bag-of-features for early wildfire smoke detection

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We select key frames from a video and detect candidate blocks only in key frames.


                        
                        
                           
                           We prepare 3D spatiotemporal volumes by combining the candidate blocks.


                        
                        
                           
                           We introduce a new weighting scheme for generating a more reasonable BoF.


                        
                        
                           
                           The random forest classifier is built during the training phase by using the BoF.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Wildfire smoke detection

Spatiotemporal feature

Bag-of-features

Histogram of oriented gradient

Histogram of oriented optical flow

Random forest

@&#ABSTRACT@&#


               
               
                  Wildfire smoke detection is particularly important for early warning systems, because smoke usually rises before flames arise. Therefore, this paper presents an automatic wildfire smoke detection method using computer vision and pattern recognition techniques. First, candidate blocks are identified using key-frame differences and nonparametric smoke color models to detect smoke-colored moving objects. Subsequently, three-dimensional spatiotemporal volumes are built by combining the candidate blocks in the current key-frame with the corresponding blocks in previous frames. A histogram of oriented gradient (HOG) is extracted, and a histogram of oriented optical flow (HOOF) is extracted as a temporal feature based on the fact that the direction of smoke diffusion is upward owing to thermal convection. From spatiotemporal features of training data, a visual codebook and a bag-of-features (BoF) histogram are generated using our proposed weighting scheme. For smoke verification, a random forest classifier is built during the training phase using the BoF histogram. The random forest with the BoF histogram can increase the detection accuracy performance when compared with related methods and allow smoke detection to be carried out in near real time.
               
            

@&#INTRODUCTION@&#

As public interest in natural disasters increases, research on wildfire detection is gaining attention because wildfire is one of the major reasons for global warming and is resulting in not only extensive loss of life and property but also ecological problems. According to a survey report [1], an average of 586 wildfires occurs each year in South Korea, with damaged areas averaging 6620ha. Therefore, an early warning of wildfires is crucial to protecting the ecological environment and reducing the potential for casualties and property damage.

Wildfire management requires large budgets and extensive human resources for prevention and suppression within private forests. In particular, it is not easy to monitor and control wildfires in remote areas. Therefore, the construction of a systematic, scientific, and integrated system for wildfire control with risk forecasting is necessary [1].

With the rapid development of information technology, automatic wildfire detection has evolved to become a new research field. Wildfire detection can be divided into two research categories: smoke detection and flame detection. Smoke detection is particularly important for early warning systems, because smoke usually rises before flames arise. The conventional approaches to wildfire smoke use either infrared (IR) sensors that identify the heat flux of firelight or light detection and ranging (LIDAR) systems that measure the laser light backscattered by smoke particles. These types of smoke detectors are more common than others because they are better at detecting the smaller amounts of smoke produced by flaming fires. However, the optical sensors have limitations: they are expensive to manage and they generate many false alarms due to atmospheric conditions, light reflections, and the vast distance between the sensor and the burning point [2,3]. Moreover, because these sensors are point sensors and each covers a limited area, many sensors are necessary to cover a wide area. These sensors cannot provide additional information such as the locations of burning points, degree of smoke, and direction of burning.

In contrast to optically based systems, visual sensors such as charge-coupled device (CCD) cameras that produce visible spectrum images have been used for several years to watch for wildfire. This is because a visual sensor has several advantages: it functions as a volume sensor and thus can monitor a wider area than an optical sensor, its equipment and management costs are lower, and it can provide the status regarding smoke without visiting the location.

The conventional approach is to watch for signs of fire or smoke on a monitor using a camera that is installed atop a mountain. However, because this method needs many human participants, several recent studies [3–12] based on computer vision techniques have attempted to detect wildfire automatically so that dependable smoke detection results can be achieved.

Detection using visual sensors is more difficult for wildfire smoke than for indoor or short-range smoke, because the main characteristics of wildfire smoke are low spreading speed, indistinct shape, vague color patterns, and constant uncertainty [3]. Therefore, we focus on developing a robust smoke detection algorithm by analyzing the indefinite characteristics of wildfire smoke. The main recent investigations are summarized below.

Vicente and Guillemant [4] proposed an automatic system for early wildfire smoke detection consisting of two parts: In the first part, a temporal algorithm is performed at the pixel level and a spatial analysis is performed to insert connected pixels into the same envelope. The second part deals with a classification method for discriminating between distant smoke and various other phenomena.

Krstinić et al. [5] presented a comparative evaluation of the histogram-based smoke classification methods. To find an efficient combination of color space and pixel-level smoke segmentation, several color space transformations were evaluated by measuring the separability of smoke and non-smoke classes. Then, they proved that good color spaces for smoke detection include HSI and its derivatives.

Töreyin and Cetin [6] proposed a partitioned smoke detection algorithm using four sub-algorithms: 1) slow-moving video object detection, 2) gray region detection, 3) rising object detection, and 4) shadow elimination. These four sub-algorithms individually detect the presence or absence of smoke, and the decisions of the sub-algorithms are combined by an adaptive weighted majority algorithm.

Ham et al. [7] proposed a fire-smoke detection method that analyzes temporal patterns in smoke and uses fuzzy finite automata (FFA). To consider the smoke characteristics over time, the temporal patterns of intensity entropy, wavelet energy, and direction of motion are used for generating multivariate probability density functions and FFA are applied for smoke verification. The proposed FFA comprise a set of fuzzy states (Very High, High, Low, and Very Low) and a transition mapping that describes what event can occur in which state and what the resulting state will be.

Habiboglu et al. [8] used background subtraction and color thresholds to find the smoke-colored slow-moving regions in video. Candidate regions are divided into spatiotemporal blocks and correlation features are extracted from the blocks. Then, a binary support vector machine (SVM) classifier with spatiotemporal correlation descriptors is used to classify smoke-colored and non-smoke-colored objects.

Genovese et al. [9] proposed an image processing system for the detection of wildfire smoke on the basis of computational intelligence techniques. The detection process focuses on the extraction of specific features such as the movement, color, and edge of wildfire smoke. Then, two-layer feedforward neural networks are used to estimate the areas that describe smoke regions in the different frames.

Benazza-Benyahia et al. [10] proposed wildfire smoke detection by measuring the local fractal feature of smoke areas based on the discrete cosine transform. First, moving blocks are extracted as candidate smoke-containing areas. At the next stage, a candidate block is declared a smoke block if it has a roughness characterized by a specific range of the Hurst exponent.

Stula et al. [11] presented the iForestFire system for protection against forest fires. To detect smoke with reasonably low error rates, several algorithms based on different visual characteristics of smoke are implemented one by one. Post-processing algorithms based on the merging of meteorological and video data are applied, and a decision about raising the alarm is made through voting based on the output of each detection algorithm.

Ko et al. [3] proposed a wildfire smoke detection algorithm that uses spatiotemporal visual features and an ensemble of decision trees (i.e., a random forest). To detect wildfire smoke using a video camera, spatiotemporal characteristics such as the color, wavelet coefficients, direction of motion, and histogram of oriented gradient (HOG) are extracted from candidate blocks. Two different random forests are trained and tested using independent spatial and temporal feature vectors. This method yields good performance for a variety of smoke. However, it needs additional computation time because two classifiers are used.

Even though early wildfire smoke detection is clearly important, detecting wildfire smoke is more difficult than detecting indoor or short-range smoke for the following reasons:
                        
                           •
                           The main characteristics of wildfire smoke are low spreading speed, indistinct shape, vague color patterns, and constant uncertainty [3].

The whirls and spirals typical of fluid behavior are not detectable, and many other details are lost [9].

Smog, cloud, and fog have color and textural characteristics similar to those of wildfire smoke.

In an initial study [12], we briefly introduced a wildfire smoke detection method based on a spatiotemporal bag-of-features (BoF) and a random forest classifier. However, in this study, we take steps toward more accurate wildfire smoke detection. First, we have analyzed more of the related work to reflect the current research trends in our system. Second, we change the color probability model to cover a wider range of smoke. Third, we change the structure of the ensemble trees of the random forest to improve the detection performance. Fourth, we propose a new weighting scheme for generating a more reasonable BoF. Fifth, we perform several experiments on the influence of codebook size for a BoF, the histogram binning for a BoF, the number of trees in a random forest for smoke classification, and the performance compared with that of other recent methods.

In brief, the main contributions and overall procedures of our work are as follows:
                        
                           1.
                           We select key frames from a video sequence and detect candidate blocks only in key frames rather than in every frame.

We prepare 3D spatiotemporal volumes by combining the candidate blocks in the current key frame with the corresponding blocks in previous frames. A histogram of oriented gradient (HOG) is extracted from the current block as a spatial feature, and a histogram of oriented optical flow (HOOF) is extracted as a temporal feature based on the fact that the direction of smoke diffusion is upward owing to thermal convection.

A visual codebook is built using a combination of the spatiotemporal features, and we introduce a new weighting scheme for generating a more reasonable BoF based on the visual codebook.

The random forest classifier, which is an ensemble of decision trees, is built during the training phase by using the BoF histogram. The random forest with the BoF histogram can increase the detection accuracy and allow smoke detection to be carried out in near real time.

The remainder of this paper is organized as follows: Section 2 describes the candidate smoke block detection algorithm as the first step. Section 3 introduces our feature extraction method using a spatiotemporal BoF. Section 4 introduces the smoke verification using a random forest classifier. Section 5 presents an experimental evaluation of the accuracy and applicability of our proposed wildfire smoke detection method. Section 6 presents our conclusions and discusses the scope for future work.

One of the main characteristics of wildfire smoke is the relatively low apparent spreading speed, as the surveillance cameras are installed at vast distances. Therefore, a general frame difference, background subtraction, and optical flow calculation may fail to extract all smoke pixels or may generate false smoke regions due to a dynamic background. In this study, whenever the frame difference is over a certain threshold, we adopt the method of [3] to select key frames from a video sequence instead of using all frames.

First, the input video sequences are divided into 32×24 blocks according to the aspect ratio in the MPEG standard, and all subsequent procedures are applied to these units. Because wildfire smoke has a relatively slow spreading speed unlike indoor smoke or moving object, a general frame difference cannot detect moving smoke regions effectively. Thus, to overcome this problem and distinguish wildfire smoke from moving object, key frames are selected from a video sequence using gray image whenever the frame difference is over a certain threshold (θ
                     1) [3].
                        
                           (1)
                           
                              
                                 
                                    
                                       if
                                       
                                       
                                          
                                             
                                                
                                                   Kframe
                                                   
                                                      k
                                                   
                                                   −
                                                   frame
                                                   
                                                      i
                                                   
                                                
                                             
                                             >
                                             θ
                                          
                                       
                                    
                                 
                                 
                                    
                                       then
                                       
                                       Kframe
                                       
                                          
                                             k
                                             +
                                             1
                                          
                                       
                                       =
                                       frame
                                       
                                          i
                                       
                                       ;
                                    
                                 
                                 
                                    
                                       
                                       k
                                       +
                                       +
                                       ;
                                       i
                                       +
                                       +
                                       ;
                                    
                                 
                                 
                                    
                                       else
                                       
                                       i
                                       +
                                       +
                                       ;
                                    
                                 
                              
                           
                        
                     
                  

where the threshold θ has a lower value for slow wildfire smoke as it has a slower movement speed than indoor smoke. In this study, we set θ as 0.9% of the overall pixels as the same method of [3]. Using the new key frame Kframe [k
                     +1] and previous key frame Kframe [k], a block (Bb
                     ) at position b in the current key frame is declared as a moving block using the following formula [3]:
                        
                           (2)
                           
                              
                                 
                                    
                                       
                                          B
                                          b
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   δ
                                                   
                                                      
                                                         Kframe
                                                         
                                                            k
                                                         
                                                         
                                                            
                                                               x
                                                               y
                                                            
                                                            b
                                                         
                                                         ,
                                                         Kframe
                                                         
                                                            
                                                               k
                                                               +
                                                               1
                                                            
                                                         
                                                         
                                                            
                                                               x
                                                               y
                                                            
                                                            b
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   otherwise
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                       
                                          
                                             k
                                             ≥
                                             1
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                  

where δ(·)is the Kronecker delta function. This function returns “1” if the two corresponding blocks of key frames have difference, which implies a candidate smoke block and “0” otherwise.

If the current frame has a significant frame difference from the previous key frame using Eqs. (1) and (2), we detect slow-moving blocks between key frames and then confirm the candidate blocks by using a smoke color model for real-time processing.

In general, the smoke color is widely distributed in the RGB color space, depending on the burning material. However, it has been proved that the HSI color space can improve the accuracy of smoke detection over that of the RGB color space, because the HSI color space increases the differentiation between smoke and non-smoke [13]. Moreover, Ham [7] proved that smoke color in the HSI color space is distributed with a wide range of hue (H), a low level of saturation (S), and a high level of intensity (I). Thus, to remove non-smoke-colored blocks, we design the probability density functions (PDFs) of a smoke color using only S and I without H.

The parameters of the PDFs are learned from training data. In typical supervised learning the underlying distribution functions are known [14], but smoke color does not follow a known distribution or parametric densities. Therefore, we construct the PDFs to have multimodal densities, rather than unimodal densities, by using a nonparametric method. In this study, we use a smoother kernel function in the form of a Gaussian in order to obtain the following smoother kernel density model:
                        
                           (3)
                           
                              
                                 p
                                 
                                    x
                                 
                                 =
                                 
                                    1
                                    N
                                 
                                 
                                    
                                       ∑
                                       
                                          n
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       
                                          1
                                          
                                             
                                                
                                                   2
                                                   π
                                                   
                                                      h
                                                      2
                                                   
                                                
                                             
                                             
                                                
                                                   d
                                                   2
                                                
                                             
                                          
                                       
                                       exp
                                       
                                          
                                             −
                                             
                                                
                                                   
                                                      
                                                         x
                                                         −
                                                         
                                                            x
                                                            n
                                                         
                                                      
                                                   
                                                   2
                                                
                                                
                                                   2
                                                   
                                                      h
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        
                     
                  

where N is the number of data and h is the parameter that determines the width of the effective Gaussian window in each dimension d. Once PDFs are generated, the likelihood of a pixel bj
                     , denoted byp(b
                     
                        j
                     |Smoke), can be estimated from the PDFs defined.


                     Fig. 1
                      shows the resultant PDFs for wildfire smoke. These PDFs are then used to configure real candidate blocks.

From the PDF, a candidate block is determined on the basis of a pixel likelihood p(b
                     
                        j
                     |Smoke) in a block (bi
                     ) using the following formula:
                        
                           (4)
                           
                              
                                 
                                    b
                                    i
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                1
                                             
                                          
                                          
                                             
                                                0
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                if
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         j
                                                         =
                                                         1
                                                      
                                                      n
                                                   
                                                   
                                                      I
                                                      
                                                         
                                                            p
                                                            
                                                               
                                                                  
                                                                     b
                                                                     j
                                                                  
                                                                  |
                                                                  Smoke
                                                               
                                                            
                                                            ≥
                                                            0.01
                                                         
                                                      
                                                      >
                                                      
                                                         T
                                                         1
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                otherwise
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                  

where I(·) is an indicator function. That is, I(·) returns unity if the argument equals or exceeds the minimum probability of 0.01, and zero otherwise. The adjustable parameter T
                     1 determines the extent of candidate blocks. In this study, on the basis of several experiments, the most appropriate minimum threshold T
                     1 was found to be 30% of the total number of pixels (n) included in a box. If the result exceeds the threshold T
                     1, the block bi
                      is declared a candidate smoke block.


                     Fig. 2(a) shows candidate smoke blocks after key-frame differencing. The blue blocks in Fig. 2(b) are the result of filtered non-smoke-colored blocks. Once the candidate smoke blocks are detected, the image is scanned to group the blocks into clusters based on block connectivity.

Once candidate smoke blocks are detected, we extract an HOG for a spatial feature and an HOOF for a temporal feature. Then, we combine these two features into one feature vector, and we construct a BoF model in order to transform the feature vector into a BoF histogram owing to the simplicity, robustness, and good practical performance [15].

Sparse spatiotemporal features have recently exhibited good performance in video-based action recognition [15–18]. To obtain a descriptor for interest points, the spatial feature and temporal feature are calculated for each spatiotemporal cube surrounding an interest point and concatenated to form a vector. This descriptor is then projected to a lower-dimensional space using a BoF model. In this study, we use candidate blocks that are selected from previous steps instead of from interest points.

In a manner similar to that for action recognition in video, we first prepare 3D spatiotemporal volumes by combining the candidate blocks with t corresponding blocks in previous frames, as shown in Fig. 3
                        . Each volume (Δ
                           x
                        , Δ
                           y
                        , Δ
                           t
                        ) has the same width and height, with candidate blocks (10, 10), and the time duration Δ
                           t
                         is 100. From each volume, we compute the spatial appearance and temporal motion as the local spatiotemporal features, as shown in Fig. 3.

For a spatial feature, an HOG is generated from the current block. Since the direction of smoke diffusion is upward owing to thermal convection, the gradient distribution of a smoke boundary has a distinguishable pattern. By using this characteristic, we extract an HOG as a spatial feature. In order to extract an HOG from a candidate block, gradient orientations are estimated at each pixel and a histogram of each orientation in a candidate block is calculated.

For a temporal feature, an HOOF is generated from 100frames within the same volume. To generate the proposed HOOF, the optical flow is computed at every frame of the video. Each flow vector is binned according to its primary angle from the horizontal axis and weighted according to its magnitude [16]. The observed HOOF is the same histogram whether the smoke is moving from the left to the right or vice versa. Since smoke usually drifts continually upwards due to hot airflows, we estimate the direction of motion from the 100frames in each volume. The range of motion is discretized into nine directions including zero motion, and each discrete direction is binned according to magnitude. Then, all HOOF histograms are accumulated into one histogram and normalized to 0–1 using an L1-sqrt normalization step. The final dimension of the spatiotemporal feature is 18 (=9+9).

After extracting a set of spatiotemporal features from training data, we construct a spatiotemporal BoF. Such BoF models have received much attention owing to their simplicity, robustness, and good practical performance [15]. A BoF is designed to represent each image by an orderless collection of local features calculated from a set of small sub-images called patches [19]. The feature descriptors of all patches are clustered using k-means clustering and each cluster is treated as a visual codeword. Then, a visual codebook that is composed of k visual words is built to describe a BoF using training descriptors. By mapping the local features of an image to the visual codebook, we can describe the image as a BoF histogram of the visual codewords according to the presence (or count) of each visual codeword [20].

Given a random subset of volumes including smoke and non-smoke regions from the training set, the visual words are learned by performing k-means clustering. In our experiments, the size of the visual codebook was determined as k
                        =100. The experimental results for deciding the appropriate number of codewords are described in Section 5.

Once the visual codebook of the BoF is built, two kinds of BoF histograms should be estimated for the smoke and non-smoke classes. The BoF histogram assigns each feature to the closest visual word and computes the histogram of visual word occurrences over a spatiotemporal volume [21]. In general, the K-dimensional BoF histogram is estimated from patches by means of binary weighting, which indicates the presence or absence of a visual word by the values 1 and 0, respectively. All of the weighting schemes perform the nearest-neighbor search in the visual codebook, in the sense that each patch is mapped to the most similar visual word. However, binary weighting causes an aliasing effect on the BoF histogram, which may lead to sudden changes in the computed feature vector.

Jiang et al. [20] proposed a soft-weighting approach to weight the significance of visual words. Instead of searching only for the nearest visual word of each patch in an image, they selected the N nearest visual words and assigned different weights according to the sum of distances. By using the soft-weighting scheme, they avoided the fundamental disadvantages of the conventional weighting schemes. This method uses a visual vocabulary of K visual words and a K-dimensional vectorT
                        =[t
                        1,…,
                        t
                        
                           k
                        ,…,…
                        t
                        
                           K
                        ], with each component tk
                         representing the weight of a visual word k such that
                           
                              (5)
                              
                                 
                                    
                                       t
                                       k
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          N
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                
                                                   M
                                                   i
                                                
                                             
                                             
                                                
                                                   1
                                                   
                                                      2
                                                      
                                                         i
                                                         −
                                                         1
                                                      
                                                   
                                                
                                                sim
                                                
                                                   j
                                                   k
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        
                     

where Mi
                         represents the number of patches whose ith nearest neighbor is visual word k. The measure sim(j,k) represents the similarity between patch j and visual word k.

In spite of its good performance, soft weighting has three limitations. First, even if the nearest N patches have long distances from visual word k, they influence how much weight is placed on visual word k because of the factor 1/2
                           i−1. Second, even if the nearest N patches have similar short distances from visual word k, the lower-order patches give relatively low weight to visual word k regardless of the similarity. Third, this method needs additional memory because every visual word has to memorize the order of all nearest patches.

To solve the first problem of soft weighting, three steps are taken. First, we set the number of nearest visual words to N like in soft weighting but define the minimum threshold to control the number N. Therefore, the number N can be changed, depending on the visual distance. Second, our method estimates the weight using only the visual distance itself, regardless of the order of nearness. Third, each patch memorizes only the N nearest visual words, regardless of the order. Each patch will therefore contribute not only to its visual word k of a BoF histogram but also to a maximum of N neighboring visual words.

Suppose that we have a visual vocabulary of K visual words and we use a K-dimensional BoF histogram. The proposed weighted BoF histogram is generated as follows:
                           
                              (1)
                              Set all components of the K-dimensional BoF histogram T
                                 =[t
                                 1,⋯,t
                                 
                                    k
                                 ] to zero.Each component tk
                                  represents the weight of a visual word k in image I.

Extract a patch P from an image I.

Compute the L1-distance D between P and the visual words V by using the following formula:
                                    
                                       (6)
                                       
                                          
                                             
                                                D
                                                
                                                   i
                                                   =
                                                   1
                                                   …
                                                   k
                                                
                                             
                                             =
                                             
                                                
                                                   P
                                                   −
                                                   
                                                      v
                                                      i
                                                   
                                                
                                             
                                             .
                                          
                                       
                                    
                                 
                              

Sort distances D
                                 
                                    i
                                    =1…k
                                  in ascending order and find visual word vi
                                 
                                 ∈
                                 V that has a distance below the minimum threshold T2
                                  by searching from higher order to lower order:


                                 
                                    
                                       (7)
                                       
                                          
                                             Near
                                             
                                                V
                                             
                                             =
                                             
                                                
                                                   
                                                      v
                                                      i
                                                   
                                                   |
                                                   
                                                      D
                                                      
                                                         i
                                                         =
                                                         1
                                                         …
                                                         k
                                                      
                                                   
                                                   <
                                                   
                                                      T
                                                      2
                                                   
                                                
                                             
                                             .
                                          
                                       
                                    
                                 
                                 i
                                 =0

while (i
                                 ≤
                                 N or visual words exist in a set Near (V)), do
                                    
                                       (5-1)
                                       Add the contribution of P to the component of tk
                                           corresponding to Di
                                          :
                                             
                                                (8)
                                                
                                                   
                                                      
                                                         t
                                                         i
                                                      
                                                      =
                                                      
                                                         t
                                                         i
                                                      
                                                      +
                                                      W
                                                      
                                                         
                                                            D
                                                            i
                                                         
                                                      
                                                      .
                                                   
                                                
                                             
                                          
                                       

The weighting function W(·) for the distance Di
                                           is chosen to obey the following exponential weighting scheme:
                                             
                                                (9)
                                                
                                                   
                                                      W
                                                      
                                                         
                                                            D
                                                            i
                                                         
                                                      
                                                      =
                                                      
                                                         1
                                                         
                                                            exp
                                                            
                                                               
                                                                  F
                                                                  ⋅
                                                                  
                                                                     D
                                                                     i
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                      ,
                                                      F
                                                      >
                                                      0
                                                      .
                                                   
                                                
                                             
                                          
                                          i
                                          =
                                          i
                                          +1

End.

In the above, the minimum distance threshold (T
                        2) is selected as the median value between 
                           
                              
                                 T
                                 2
                              
                              ∈
                              
                                 
                                    
                                       min
                                       i
                                    
                                    
                                       D
                                       i
                                    
                                 
                              
                           
                         and 
                           
                              
                                 T
                                 2
                              
                              ∈
                              
                                 
                                    
                                       max
                                       i
                                    
                                    
                                       D
                                       i
                                    
                                 
                              
                           
                        . In addition, a weighting factor F can be chosen to maximize (minimize) the influence of Di
                         on W(·). When F equals 1, a change in Di
                         will be exponentially reflected in W(·). The exponential weighting is more sensitive to changes in local feature relevance and gives rise to a greater performance improvement [22]. In this study, we set F at 0.5 in accordance with experiments.


                        Fig. 4
                         shows the overall procedure for generating a BoF histogram. Then, BoF histograms generated from training data are used for learning a random forest classifier as shown in Fig. 4(d).

As opposed to previous heuristic methods and simple pattern classifiers for smoke verification, this study uses a random forest classifier [23] to determine whether the candidate smoke clusters represent real smoke. Recently, the random forest has been employed in many computer vision applications such as action recognition [24], human detection [25], image retrieval [26], and smoke detection [3]. A random forest is a decision tree ensemble classifier, with each tree grown using some type of randomization. Random forests have a capacity for processing large amounts of data with high training speeds. The structure of each tree in the random forest is binary and is created in a top-down manner, as shown in Fig. 4(d).

In general, a random forest with Tn
                      trees is trained offline from the database. Once a training set is collected from the training data, the random forest is started by choosing a random subset I′ from the training data including the local BoF histograms I and placing that at the root node. At node n, the training data In
                      is iteratively split into left and right subsets Il
                      and Ir
                      by using Eq. (10) with the decision threshold t and the splitting function f(vi
                     ) for the feature vector v. The threshold t is randomly chosen in the range 
                        
                           
                              T
                              3
                           
                           ∈
                           
                              
                                 
                                    min
                                    i
                                 
                                 f
                                 
                                    
                                       v
                                       i
                                    
                                 
                                 ,
                                 
                                    max
                                    i
                                 
                                 f
                                 
                                    
                                       v
                                       i
                                    
                                 
                              
                           
                        
                      by the splitting function f(vi
                     ) [23].
                        
                           (10)
                           
                              
                                 
                                    
                                       
                                          I
                                          l
                                       
                                       =
                                       
                                          
                                             i
                                             ∈
                                             
                                                I
                                                n
                                             
                                             |
                                             f
                                             
                                                
                                                   v
                                                   i
                                                
                                             
                                             <
                                             
                                                T
                                                3
                                             
                                          
                                       
                                       ,
                                    
                                 
                                 
                                    
                                       
                                          I
                                          r
                                       
                                       =
                                       
                                          I
                                          n
                                       
                                       \
                                       
                                          I
                                          l
                                       
                                       .
                                    
                                 
                              
                           
                        
                     
                  

The growth for training one decision tree of a random forest continues within the maximum tree depth. After decision tree T is completed, each training datum must correspond to one of the leaf nodes k. To assign a class label ci
                      at each leaf node k, we count the number of samples in a leaf node and assign a class that has a higher frequency:
                        
                           (11)
                           
                              
                                 k
                                 =
                                 arg
                                 
                                    max
                                    i
                                 
                                 P
                                 
                                    
                                       
                                          c
                                          i
                                       
                                       |
                                       T
                                    
                                 
                                 .
                              
                           
                        
                     
                  

There are two conditions that can end the iterative training. The first condition occurs when no more information gain is possible. From among the candidate thresholds for each spit function, the candidate threshold T
                     3 that maximizes the gain in information about the corresponding node is selected. The information gain, ∆E, is defined at each spit node as follows:
                        
                           (12)
                           
                              
                                 ΔE
                                 =
                                 −
                                 
                                    
                                       
                                          I
                                          l
                                       
                                    
                                    
                                       
                                          I
                                          n
                                       
                                    
                                 
                                 E
                                 
                                    
                                       I
                                       l
                                    
                                 
                                 −
                                 
                                    
                                       
                                          I
                                          r
                                       
                                    
                                    
                                       
                                          I
                                          n
                                       
                                    
                                 
                                 E
                                 
                                    
                                       I
                                       r
                                    
                                 
                              
                           
                        
                     where E(Il
                     ) and E(Ir
                     ) are the entropies of subclasses Il
                      and Ir
                      in the set of training data and In
                      denotes the total input data at the splitting node. The second condition occurs when the training process reaches a leaf node that is at the maximum depth of the tree. We set the maximum tree depth at 20 according to the experiment of [27].

Once the individual decision trees are trained, the ensemble of trees is assembled into a random forest classifier as shown in Fig. 4(d). The number of trees T is set to 60, which has been shown empirically to yield good results and computation times comparable with those for related methods. The experimental results used to decide the appropriate number of trees and compare the processing times are described in Section 5.

Once the random forest classifier has been learned, the BoF histogram of the test blocks is created and is distributed into the trained random forests. Then, each feature corresponds to one leaf of a decision tree in the random forest. In order to compute the final class distribution, we sum the probabilities of all trees, L
                     =(l
                     1, l
                     2,…, lr
                     ), as follows:
                        
                           (13)
                           
                              
                                 P
                                 
                                    
                                       
                                          c
                                          i
                                       
                                       |
                                       L
                                    
                                 
                                 =
                                 
                                    1
                                    
                                       T
                                       n
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          t
                                          =
                                          1
                                       
                                       
                                          T
                                          n
                                       
                                    
                                    
                                       P
                                       
                                          
                                             
                                                c
                                                i
                                             
                                             |
                                             
                                                l
                                                t
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where Tn
                      is the number of trees. We choose ci
                      as the final class of an input feature if p(c
                     
                        i
                     |L) has the maximum value.

In Fig. 5
                     , the test block is placed into the smoke class because that has the maximum posterior probability.

@&#EXPERIMENTAL RESULTS@&#

We performed experiments using the KMU Fire & Smoke Database, the database of Töreyin, and YouTube data. The KMU Fire & Smoke Database (http://cvpr.kmu.ac.kr/Dataset/ Dataset.htm) includes 38 diverse fire videos, such as “Indoor–outdoor smoke,” “Wildfire smoke,” and “Smoke & fire like moving object.” The frame rate of the video data is 30Hz, while the size of each input image was 320×240 pixels.

To perform the training, 517 blocks were randomly selected from ten videos: 130 blocks of dense smoke, 131 blocks of tenuous smoke, 120 blocks of tenuous smoke-colored clouds and fog, and 136 blocks of dense smoke-like clouds and fog. To check the ability of the proposed system to classify real wildfire smoke, the testing video sequences consisted of eight videos that included wildfire smoke and eight videos that included smoke-colored clouds and fog, as shown in Tables 1–2
                     
                      and Fig. 10. Three wildfire smoke movies (1, 3, and 4) were collected in the experiments of Töreyin and Cetin [6]. Movies 6, 7, and 8 were collected from YouTube. The other wildfire and smoke-colored test sequences were taken from our database.

In this study, we used the global evaluation measures for wildfire smoke detection that were proposed by Krstinić et al. [5]:
                        
                           •
                           Average true positive rate (ATPR or sensitivity), the average observer quality in terms of correct detections.

Average false positive rate (AFPR or false detection), the average observer quality in terms of false detections.

Average true negative rate (ATNR or specificity), the average observer quality in terms of correct rejections.

Average false negative rate (AFNR or missed detection), the average observer quality in terms of missed detections.

First, we compared the performances of the three different weighting schemes for generating a BoF histogram: binary weighting, soft weighting, and the proposed weighting. For these performance evaluations, we used five videos of smoke (Movies 1–5) and five videos of smoke-colored clouds or fog (Movies 9–13). As shown in Fig. 6
                        , the proposed weighting scheme exhibited the best performance as regards ATPR, AFPR, ATNR, and AFNR, with 96.6%, 0.3%, 99.7%, and 3.4%, respectively. From the results, we can infer that binary weighting is not a highly distinguishing scheme for BoF, especially in ATPR (89.1%) and AFNR (10.9%). Moreover, soft weighting produced 4.4% worse ATPR and AFNR than the proposed algorithm.

The size of the codebook is the main parameter for BoF generation, and it is known that performance tends to improve steadily as the codebook grows [15]. Even though a small codebook reduces the dimension of the BoF, it may lack discriminative power because two patches may be assigned to the same cluster even if they are not similar. Conversely, a large codebook is less generalizable, sensitive to noise, and needs extra processing time [28].

Therefore, it is essential to find the proper size of codebook by considering the computational cost. Fig. 7
                         shows the results of experiments using nine possible codebook sizes and the same ten datasets when the number of trees is 100. As shown in Fig. 7, even though a codebook of 50 clusters (codewords) gave the shortest processing time of 34.6ms per frame, the detection performance was worse than others. The performance became saturated or worsened when we used more than 400 clusters. Moreover, the processing time increases linearly as the number of codewords increases. In contrast, 100 clusters gave the best performance and a relatively good processing time of 36.3ms per frame, so 100 was adopted as the number of codewords in the codebook.

Since the main purpose of the proposed method is to detect wildfire smoke in real time, prompt verification of smoke is very important. Even though the random forest is known to be very fast in learning and testing as compared to other classifiers such as the SVM [3,27], the classification speed of a random forest is influenced by the number of trees. To determine the proper number of trees for a random forest, we used the same test data and compared only ATPR, AFPR, and computation time by changing the number of trees. As shown in Fig. 8
                        , the classification performance became almost constant when the number of trees was over 60. Moreover, because the processing time increases linearly as the number of trees increases, we fixed the number of trees at 60.

We compared the performance of the proposed algorithm with that of previous algorithms. Among the existing methods, those of Töreyin and Cetin [6], Ham et al. [7], and Ko et al. [3] had performed well and were tested using the same 16 test videos containing eight smoke samples and eight smoke-colored samples.


                        Fig. 9
                         shows the comparison of the results of the four methods. Note that the proposed method outperformed the methods of Töreyin and Cetin, Ham et al., and Ko et al., with an ATPR of 97.1% versus 78.4%, 94.1%, and 96.6%, an AFPR of 0.2% versus 7.4%, 16.6%, and 9.8%, an ATNR of 99.8% versus 92.6%, 83.4%, and 90.2%, and an AFNR of 2.9% versus 21.6%, 5.9%, and 3.4%, respectively.

In particular, the method of Töreyin and Cetin produced the lowest ATPR and highest AFNR, since it detected candidate smoke regions in every frame by using the frame difference. However, many true smoke regions were missed because the wildfire smoke, especially that in Movies 1, 3, and 4, appears to move very slowly due to the vast distance between the camera and the location of the smoke. The method of Ham et al. gave the highest AFPR of 16.6% for the test videos. The main reason for this higher error was that smoke was confused with moving smoke-colored objects and swaying trees. Similarly, the method of Töreyin and Cetin gave a higher AFPR and AFNR for Movies 9 and 11 because moving clouds and fog were confused with fire-smoke. Even though the method of Ko et al. reduced the AFNR due to movements of smoke-colored clouds by using spatiotemporal features with two random forest classifiers, it still gave 9.6% higher AFPR and 1% lower ATPR than the proposed method. The reason for the higher performance of the proposed method was that a spatiotemporal BoF represented smoke by a collection of local properties calculated effectively from a set of 3D volumes and so a random forest was able to distinguish real smoke from a dynamic smoke-colored cloud in motion.

The rate of smoke detection was also improved by the proposed method, which yielded an average of 25fps, whereas the methods of Töreyin and Cetin, Ham et al., and Ko et al. yielded 15fps, 22fps, and 10fps under the same system conditions. In particular, the method of Ko et al. requires additional processing time because it uses two types of random forest.


                        Fig. 10
                         shows the smoke detection results obtained with our proposed method using a set of 16 tests. As shown in Fig. 10, our proposed method detected smoke correctly and eliminated false alarms for test videos containing smoke and non-smoke objects with different speeds and colors.

@&#CONCLUSION@&#

Automatic wildfire detection is important to protecting the ecological environment and reducing the potential for casualties and property damage. It has been getting attention in the research field of computer vision, because the camera is one easy way to monitor wildfires in remote areas.

In this paper we introduced a wildfire smoke detection method based on a spatiotemporal BoF and a random forest classifier. Once candidate blocks are detected, the 3D spatiotemporal volumes are prepared by combining the candidate blocks with 100 corresponding blocks in previous frames. An HOG is extracted from the current block as a spatial feature, and an HOOF is extracted as a temporal feature based on the fact that the direction of smoke diffusion is upward owing to thermal convection. We also proposed an efficient weighting scheme for BoF histogram generation by considering the distance between a patch and the N nearest codewords.

For classification, the random forests that comprise the ensemble of decision trees are built during the training phase using the BoF histogram. Compared with related methods, our algorithm can indeed provide improved smoke detection performance.

In future work, we plan to install our system on top of a watch tower to monitor a wide area in a real field and optimize some parameters of our systems to improve the detection performance. In addition, to reduce false alarms caused by upward movement of smoke-colored clouds or fog, we plan to use humidity information as a new feature and generate a new BoF by combining spatiotemporal and humidity information.

@&#REFERENCES@&#

