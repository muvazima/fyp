@&#MAIN-TITLE@&#A cross-platform solution for light field based 3D telemedicine

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We propose a novel framework for a world's first LF-based 3D telemedicine system.


                        
                        
                           
                           We develop an algorithm to convert the LF into 3D models with high levels of details.


                        
                        
                           
                           We design a cross-platform solution for different systems and platforms.


                        
                        
                           
                           We develop a demo platform with multidiscipline 3D telemedicine applications.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

3D

Telemedicine

Light field

Cross-platform

@&#ABSTRACT@&#


               
               
                  Current telehealth services are dominated by conventional 2D video conferencing systems, which are limited in their capabilities in providing a satisfactory communication experience due to the lack of realism. The “immersiveness” provided by 3D technologies has the potential to promote telehealth services to a wider range of applications. However, conventional stereoscopic 3D technologies are deficient in many aspects, including low resolution and the requirement for complicated multi-camera setup and calibration, and special glasses. The advent of light field (LF) photography enables us to record light rays in a single shot and provide glasses-free 3D display with continuous motion parallax in a wide viewing zone, which is ideally suited for 3D telehealth applications. As far as our literature review suggests, there have been no reports of 3D telemedicine systems using LF technology. In this paper, we propose a cross-platform solution for a LF-based 3D telemedicine system. Firstly, a novel system architecture based on LF technology is established, which is able to capture the LF of a patient, and provide an immersive 3D display at the doctor site. For 3D modeling, we further propose an algorithm which is able to convert the captured LF to a 3D model with a high level of detail. For the software implementation on different platforms (i.e., desktop, web-based and mobile phone platforms), a cross-platform solution is proposed. Demo applications have been developed for 2D/3D video conferencing, 3D model display and edit, blood pressure and heart rate monitoring, and patient data viewing functions. The demo software can be extended to multi-discipline telehealth applications, such as tele-dentistry, tele-wound and tele-psychiatry. The proposed 3D telemedicine solution has the potential to revolutionize next-generation telemedicine technologies by providing a high quality immersive tele-consultation experience.
               
            

@&#INTRODUCTION@&#

The demand for quality healthcare continues to grow, especially in regional areas, where the lower health status of the population remains a concern. Telehealth has the potential to enable better healthcare services for regional residents. Conventional 2D video-based tele-consultations have been widely explored in the past to support remote healthcare. Current standard-definition 2D telemedicine systems are limited in their capabilities to provide a satisfactory tele-consultation experience due to low picture quality and non-guaranteed quality of service [1]. Two pronounced limitations have been identified as hindering the widespread adoption of 2D telemedicine: (a) the difficulty in obtaining the desired 2D camera views, and (b) the lack of depth perception. With the continuing progress in bandwidth and protocols for the information highway, new healthcare structures are becoming feasible. Coupled with recent standardization efforts in video compression, high-definition 3D video telemedicine is now feasible.

Prior studies report on strong evidence that the shared sense of presence enabled by 3D video technology is superior to its 2D counterpart, substantially improving communication quality and trust between geographically separated clinicians and their patients by creating a higher level of realism [2]. However, conventional stereoscopic 3D technologies are deficient in many aspects, including multi-camera setup and calibration, low resolution and the need to wear special glasses. In some circumstances, e.g., tele-oncology, eye contact between oncologists and patients is important for reassurance and connection. The cumbersome glasses worn by oncologists can restrict eye contact and present as a counterproductive reminder of the demarcation between oncologists and patients.

In the past decade, several researchers have attempted to develop real-time 3D acquisition systems. Two primary methods for 3D model reconstruction are: (1) active methods, which create a complete 3D model by aggregating 3D streams from time-of-flight (TOF) cameras (e.g. Microsoft Kinect) all of which can cover a 360 degree view. However, the performance of structured light sensors, such as the Kinect, severely degrades when multiple cameras are pointing at the same scene. There is crosstalk when the dot patterns of multiple devices interfere with one another. Many researchers have tried to find solutions to reduce interference for overlapping structured light depth cameras. However, this depth sensing method still suffers from a considerable amount of missing data at occlusions, and is obviously not suitable for 3D telemedicine application, which requires high precision and resolution, such as tele-surgery in telehealth; (2) compared to depth sensing via IR cameras, image-based methods, which are also known as passive methods, are able create 3D representations solely from photographs and easily capture the scene from different viewing positions.

Recent advances in computational photography have enabled us to explore “light fields (LFs)” from a digital perspective. A LF is a field (much like the magnetic field), where a light ray in space can be parameterized by three coordinates {x, y, z} and two angles {θ, ϕ} in a five-dimensional function L(x, y, z, θ, ϕ). LF photography [3] is able to capture both spatial and angular information and enables new possibilities for digital imaging [4,5]. Light field displays (LFDs), on the other hand, are far beyond current stereoscopic and multi-view displays, and are able to render the input 3D mesh data and reconstruct the 3D LF into a set of light rays [6]. LFDs can reproduce a LF having both horizontal and vertical parallax [7]. LFDs are capable of providing 3D images with continuous motion parallax in a wide viewing zone, without wearing glasses. With the advantages in capture and display, LF technology is ideally suited for a broad range of telehealth applications, e.g., tele-wound, tele-psychiatry and tele-surgery [8].

With the advent of commercial LF cameras, LF photography has been largely adopted by professional photographers to create animated and multi-focus digital photos. Moreover, the depth perception from LF photography has enabled LF photography to be adopted in a wide range of conventional industry applications, e.g., automated optical 3D inspection in industry, measurement of 3D flow with a single access point in fluid mechanics research, 3D Plant analysis for breeding, picking or weeding, 3D Microscopy for industrial parts, etc. There have been no reports of 3D telemedicine systems based on LF technology in the literature. In view of this fact, we propose a 3D telemedicine system based on LF technology in this paper. Built upon the state-of-the-art LF rendering and streaming techniques, this system is able to capture the LF of patients and provide an immersive 3D experience for health professions. More specifically, LF cameras (Lytro Illum) were used to capture the LF of the patient, and a sophisticated 3D model construction algorithm was developed to convert the captured LF into a 3D model. The 3D data is then compressed, encoded, and transmitted to the server station for storage, management and distribution. A multi-platform design was developed to realize 3D tele-consultation under different platforms and displays, i.e., a LFD with Leap motion controller [9], an autostereoscopic 3D multi-view display, a 3D model display with webcam head tracking and a mobile phone platform with motion tracking. Moreover, a cross-platform solution was proposed for the software implementation of the 3D telemedicine system, which provides a unified user experience with the developed modular design. A demo software platform was designed to realize 2D/3D video conferencing, 3D model display and edit, blood pressure and heart rate monitoring, and patient data display functions, which can be further extended for potential multi-discipline telehealth applications in tele-dentistry, tele-wound and tele-psychiatry.

The contributions of this paper are multifold, and are summarized as follows:
                        
                           •
                           Proposal of a novel framework for a LF-based 3D telemedicine system;

Development of a 3D model construction algorithm via LF reconstruction and rendering;

Design of a cross-platform solution for LF-based 3D telemedicine under different operating systems and platforms; and

Development of a demo software platform with unified user experience and potential multi-discipline 3D telemedicine applications.

The remainder of the paper is organized as follows. The architecture of the proposed LF-based 3D telemedicine system is illustrated in Section 2. A novel 3D model construction algorithm is presented in Section 3. The cross-platform solution for the software implementation is presented in Section 4. Results and discussion are given in Section 5. Concluding remarks are provided in Section 6.

The architecture for the proposed LF-based 3D telemedicine system is illustrated in Fig. 1
                     , which consists of a telemedicine unit, a base unit, a server unit, and a mobile unit.

The telemedicine unit mainly consists of four modules: (1) a 3D capture unit (consisting of three Lytro Illum cameras), which is responsible for capturing the LF of the patient; (2) a processing unit, which implements 3D model construction, compression, encoding and transmission; (3) biosignal monitoring devices, which collect vital signs of the patient, e.g., blood pressure and heart rate; and (4) a 2D monitor, which is for video conferencing with the doctor.

The 3D capture unit is designed to capture the LF of the patient with both RGB and depth information. In the proposed system, Lytro Illum LF cameras are distributed to capture a complete LF of the patient. The LF capture technique can record various aspects of a light ray such as color, intensity, and direction [10]. Contrary to traditional cameras, a LF camera has a microlense array in front of the image sensor. Such an array consists of hundreds of thousands of microscopic lenses, which splits up what would have become a 2D-pixel into individual light rays just before reaching the sensor. The resulting raw image is a composition of as many tiny images as there are microlenses.

A sophisticated software algorithm, in the processing unit, is implemented to find matching light rays across all these images. It collects a list of (1) matching light rays, (2) their positions in the microlense array, and (3) their positions within the sub-image. A 3D model construction algorithm is implemented to convert the recovered LF to a 3D model, which is then compressed and encoded before transmission. In the meantime, the biosignals collected via the biosignal monitoring devices are also transferred to the processing unit for transmission.

The base unit is composed of a processing unit, a video camera and 3D display units. The processing unit is responsible for (1) receiving, decoding and decompressing the data from the server unit and (2) encoding and transmitting the video from the camera. As shown in Fig. 1, three 3D display and interactive designs were developed in this system to realize a 3D tele-consultation service at the doctor's site,
                           
                              •
                              LFD with Leap motion controller: the LFD reconstructs the LF as a set of light rays, which is achieved by using an array of projection modules emitting light rays and a holographic screen. The light rays generated in the projection module hit the holographic screen at different points, and the holographic screen makes the optical transformation to compose these light rays into a continuous 3D view. Each point of the holographic screen emits light rays of different color in various directions. Light rays leaving the screen spread in multiple directions, as if they were emitted from points of 3D objects at fixed spatial locations. LFDs are capable of providing 3D images with continuous motion parallax in a wide viewing zone, without wearing glasses. Objects appear behind or even in front of the screen just like holograms. Furthermore, a Leap motion controller [11] is connected to the LFD as an interactive unit providing a touchless gesture control [12];

Autostereoscopic 3D multi-view display: this monitor is able to display stereoscopic images without the need for special glasses by applying parallax barriers or lenticular arrays in front of conventional display panels. A sophisticated software algorithm was designed to render the 3D model to multi-view images. The display is able to provide 3D images with a limited amount of vertical parallax without wearing glasses to provide a stereoscopic viewing experience.

2D monitor with webcam head tracking: to achieve a 3D tele-consultation service where the above two solutions are not available, we designed an alternative solution which integrates OpenGL 3D rendering and webcam head tracking on the conventional 2D monitor. Head movements of the doctor are tracked via the webcam, while the captured signals are translated to provide different perspective views of the 3D model.

The server unit consists of a data server, a web server and a wireless module. The data server is responsible for data storage, management and distribution. The web server provides system access for the web-based client at the base unit, and the wireless module offers wireless access for mobile units.

The mobile unit allows the doctor to communicate with the patient in real-time and access the patient's history over mobile networks. The mobile unit can carry out 2D/3D video conferencing with the telemedicine unit. 3D rendering and display are applied on the mobile phone platform to generate and display the received 3D model, while the motion sensor on the mobile phone is used to track the movement and change the perspective of views. Moreover, through the cloud data stored at the server, the doctor is able to access the medical history of the patient on the mobile unit, e.g., historical 3D model data, health data, treatment data, blood pressure, heart pressure data, etc.

In the most crucial component of the 3D system, the digitized 3D data, either in the form of a 3D video stream, point cloud, or mesh, are then transmitted for rendering, display and analysis. Among these 3D formats, the 3D mesh model, that has been widely used in computer modeling and vision areas, can be adopted for both 3D display and interaction. Hence, the first and most crucial stage of the system is how to digitize the patient into a 3D format, and construct a 3D mesh model from a LF. In general, a 3D mesh is made of vertices, which can be generated from point clouds such as the depth maps, and triangular faces which display the color of the object's surface at that location. Reconstructing a depth map from correspondences, also known as stereo matching, is a traditional challenging computer vision task, which has been studied for more than three decades [13]. However, unlike dense depth estimation from stereo or multiple views, dense depth estimation from light fields has not been heavily investigated until recently. While depth reconstruction from light fields can be based on traditional stereo matching techniques, recent works exploit the structure of light fields and epipolar plane images (EPIs) directly to estimate disparity, which can be considered as 2D slices of constant angular and spatial coordinates through the Lumigraph. There is a large amount of on-going research in this space. Schechner and Kiryati [14] and Vaish et al. [15] extensively discuss the advantages and disadvantages of each cue. In this paper, we embrace the method using structure tensors based on local gradients to estimate the direction of lines on the EPIs [16]. In this section, we will describe the algorithm for 3D model construction from the captured LF. A diagram of the algorithm is shown in Fig. 2
                     .

Raw LF images consist of many micro-images, the number of which is as many as that of the number of microlenses inside the LF sensor (about 100,000 microlenses in front of the 40 MPixel imaging sensor of the Lytro Illum camera). A cropped region of the raw image shown on the right of Fig. 3
                         shows that the lenselet grid is hexagonally packed. The method presented in [17] is implemented to decode, calibrate and rectify the raw LF image. An array of sub-aperture images representing the 4D LF can be obtained by taking the values at the same pixel location in each micro-images, as shown in Fig. 4
                        .

In contrast to sparser and less structured input images from stereo cameras, densely sampled 4D LFs exhibit a very specific internal structure: every captured scene point corresponds to a linear tract in a so called EPI, where the slope of the tract reflects the scene point's distance to the camera.

In this paper, we consider a 2D slice through the LF. We fix a horizontal line of constant u* (a vertical line of constant 
                           
                              v
                              *
                           
                        ) in the image plane and a constant camera coordinate s* (t*). The resulting map is called an EPI. Each camera location in the image plane Π yields a different pinhole view of the scene. The variation of s leads to a change of u, where the relationship can be derived as [18]
                     


                        
                           
                              (1)
                              
                                 Δ
                                 u
                                 =
                                 
                                    f
                                    Z
                                 
                                 Δ
                                 s
                                 ,
                              
                           
                        where f is the distance between the image and camera planes, and Z is the distance between the camera plane and the object. It is noted that the slope of the line is related to its depth, meaning that the density of the LF should not change along such a line. Therefore, computing depth is equivalent to computing the slope of level lines in the EPIs [18–20].

The slopes can be extracted using many known orientation estimation methods [21,22]. In this paper, we implement a method using the structure tensor to compute the disparity on slice {u*, s*} with the following rule [23,16]
                     


                        
                           
                              (2)
                              
                                 
                                    d
                                    
                                       
                                          u
                                          *
                                       
                                       ,
                                       
                                          s
                                          *
                                       
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 tan
                                 
                                    
                                       
                                          
                                             1
                                             2
                                          
                                          arctan
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            J
                                                            x
                                                            2
                                                         
                                                         −
                                                         
                                                            J
                                                            y
                                                            2
                                                         
                                                      
                                                      
                                                         2
                                                         
                                                            J
                                                            x
                                                         
                                                         
                                                            J
                                                            y
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where J
                        
                           x
                         and J
                        
                           y
                         are the derivatives of the EPI along directions x and y, respectively,


                        
                           
                              (3)
                              
                                 
                                    J
                                    x
                                 
                                 ≈
                                 
                                    
                                       I
                                       (
                                       x
                                       +
                                       1
                                       ,
                                       y
                                       )
                                       −
                                       I
                                       (
                                       x
                                       −
                                       1
                                       ,
                                       y
                                       )
                                    
                                    2
                                 
                                 ,
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    J
                                    y
                                 
                                 ≈
                                 
                                    
                                       I
                                       (
                                       x
                                       ,
                                       y
                                       +
                                       1
                                       )
                                       −
                                       I
                                       (
                                       x
                                       ,
                                       y
                                       −
                                       1
                                       )
                                    
                                    2
                                 
                                 .
                              
                           
                        
                     

The reliability estimate 
                           
                              r
                              
                                 
                                    u
                                    *
                                 
                                 ,
                                 
                                    s
                                    *
                                 
                              
                           
                           (
                           x
                           ,
                           y
                           )
                           ∈
                           [
                           0
                           ,
                           1
                           ]
                         is defined as the coherence of the structure tensor


                        
                           
                              (5)
                              
                                 
                                    r
                                    
                                       
                                          u
                                          *
                                       
                                       ,
                                       
                                          s
                                          *
                                       
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 :
                                 =
                                 
                                    
                                       
                                          
                                             (
                                             
                                                J
                                                y
                                                2
                                             
                                             −
                                             
                                                J
                                                x
                                                2
                                             
                                             )
                                          
                                          2
                                       
                                       +
                                       8
                                       
                                          J
                                          x
                                          2
                                       
                                       
                                          J
                                          y
                                          2
                                       
                                    
                                    
                                       
                                          
                                             (
                                             
                                                J
                                                y
                                                2
                                             
                                             +
                                             
                                                J
                                                x
                                                2
                                             
                                             )
                                          
                                          2
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Note that (2) and (5) are for horizontal EPIs, whereas analogous expressions can be obtained for vertical EPIs. The derivatives used in (2) can be estimated with the optimal 3×3 discrete filter proposed by Farid and Simoncelli [24], expressed by the following one-dimensional filters,


                        
                           
                              (6)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             p
                                             →
                                          
                                          =
                                          [
                                          0.229879
                                          
                                          0.540242
                                          
                                          0.229879
                                          ]
                                          ,
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             d
                                             →
                                          
                                          =
                                          [
                                          −
                                          0.425287
                                          
                                          0
                                          
                                          0.425287
                                          ]
                                          ,
                                       
                                    
                                 
                              
                           
                        where d and p are applied to the derivatives corresponding to perpendicular directions.

The local depth estimation only takes into account the immediate local structure of the LF. However, the depth values within a slice need to satisfy global visibility constraints across all cameras. After obtaining depth estimates 
                           
                              d
                              
                                 
                                    u
                                    *
                                 
                                 ,
                                 
                                    s
                                    *
                                 
                              
                           
                         and 
                           
                              d
                              
                                 
                                    v
                                    *
                                 
                                 ,
                                 
                                    t
                                    *
                                 
                              
                           
                         from the horizontal and vertical slices, respectively, we need to integrate those estimates into a consistent single depth map for each view 
                           u
                           :
                           Ω
                           →
                           
                              
                                 ℝ
                              
                           
                        . A sophisticated way to integrate the horizontal and vertical slices is to employ a global optimization scheme in the domain Ω, as proposed in [16], to minimize a functional of the form


                        
                           
                              (7)
                              
                                 E
                                 (
                                 u
                                 )
                                 =
                                 
                                    ∫
                                    Ω
                                 
                                 g
                                 |
                                 D
                                 μ
                                 |
                                 +
                                 ρ
                                 (
                                 μ
                                 ,
                                 u
                                 ,
                                 v
                                 )
                                 d
                                 (
                                 u
                                 ,
                                 v
                                 )
                                 ,
                              
                           
                        where 
                           g
                           (
                           u
                           ,
                           v
                           )
                           =
                           1
                           −
                           
                              r
                              
                                 
                                    s
                                    *
                                 
                                 ,
                                 
                                    t
                                    *
                                 
                              
                           
                           (
                           u
                           ,
                           v
                           )
                         is a measure of the edge strength that is used to weight local smoothness for encouraging discontinuities of u to lie on edges of the original input image.

In the data term, the solution should be close to either 
                           
                              d
                              
                                 
                                    u
                                    *
                                 
                                 ,
                                 
                                    s
                                    *
                                 
                              
                           
                         or 
                           
                              d
                              
                                 
                                    v
                                    *
                                 
                                 ,
                                 
                                    t
                                    *
                                 
                              
                           
                        , while suppressing impulse noise. Also, the two estimates 
                           
                              d
                              
                                 
                                    u
                                    *
                                 
                                 ,
                                 
                                    s
                                    *
                                 
                              
                           
                         or 
                           
                              d
                              
                                 
                                    v
                                    *
                                 
                                 ,
                                 
                                    t
                                    *
                                 
                              
                           
                         shall be weighted according to their reliabilities 
                           
                              r
                              
                                 
                                    u
                                    *
                                 
                                 ,
                                 
                                    s
                                    *
                                 
                              
                           
                         or 
                           
                              r
                              
                                 
                                    v
                                    *
                                 
                                 ,
                                 
                                    t
                                    *
                                 
                              
                           
                        . This is achieved by setting


                        
                           
                              (8)
                              
                                 ρ
                                 (
                                 μ
                                 ,
                                 u
                                 ,
                                 v
                                 )
                                 =
                                 min
                                 (
                                 
                                    r
                                    
                                       
                                          v
                                          *
                                       
                                       ,
                                       
                                          t
                                          *
                                       
                                    
                                 
                                 |
                                 μ
                                 −
                                 
                                    d
                                    
                                       
                                          v
                                          *
                                       
                                       ,
                                       
                                          t
                                          *
                                       
                                    
                                 
                                 (
                                 u
                                 ,
                                 
                                    s
                                    *
                                 
                                 )
                                 |
                                 ,
                                 
                                 
                                    r
                                    
                                       
                                          u
                                          *
                                       
                                       ,
                                       
                                          s
                                          *
                                       
                                    
                                 
                                 |
                                 μ
                                 −
                                 
                                    d
                                    
                                       
                                          u
                                          *
                                       
                                       ,
                                       
                                          s
                                          *
                                       
                                    
                                 
                                 (
                                 v
                                 ,
                                 
                                    t
                                    *
                                 
                                 )
                                 |
                                 )
                                 .
                              
                           
                        
                     

To this end, the globally optimal solution to function (7) is computed using the technique of functional lifting described in [25]. The globally consistent depth map computed for the LF shown in Fig. 4 is illustrated in Fig. 5
                        (a).

The acquired depth map is composed of both the foreground (the patient) and background (the scene), while only the foreground is of interest for 3D model construction. Therefore, the depth map of the salient region needs to be extracted from the globally consistent depth map before 3D mesh construction. In this paper, a RGB-D saliency region detection algorithm [26] is used to extract the salient region from the background. More specifically, the input color and depth images are first abstracted into a set of regular super-pixels using an improved depth-aware over-segmentation algorithm. Secondly, an accurate background measure is computed by taking the depth cue into account and further refined using an edge-preserving smoothing method. The foreground is then obtained through a specialized multi-level RGB-D model, which is later computed through optimization. The depth map of the salient region (the fist in this experiment) is extracted from the scene, with the background removed, as shown in Fig. 5(b).

The depth image is a composition of regularly placed pixels. We can construct a triangle mesh that connects the centers of neighboring pixels. In 3D mesh construction, if the input point cloud is noisy, the constructed mesh can be too bumpy for practical use. Vertex normals, which are computed from the generated noisy mesh, are smoothed with a low pass filter first. A bilateral mesh denoising algorithm is applied to refine noisy vertices, while preserving important features such as salient edges. The constructed 3D mesh is shown in Fig. 6
                        (a).

Depth and RGB images have the same world coordinate and the same view point with respect to the coordinate. The 3D mesh model is built by wrapping the faces of the mesh with the texture images, as shown in Fig. 6(b).

In Section 2, a multi-platform display and interactive design was presented. Hence, a cross-platform solution for the proposed design is presented in this section to achieve software implementation on those platforms. More specifically, the information flows between the units are presented first, followed by the design of the cross-platform software applications.

As described in Fig. 1, there are four units in the system, i.e., the telemedicine unit, the base unit, the server unit and the mobile unit. The server unit is responsible for the data storage, management, distribution, and more importantly the communication among all the units. The information flow between the three units and the server unit are illustrated in Fig. 7
                        .


                        
                           
                              •
                              Telemedicine unit and server unit: (a) Uplink: raw LF images are processed through LF reconstruction, 3D mesh construction, and compression to obtain compressed 3D data. The compressed 3D data is packed with biosignal data and transmitted to the server unit, and (b) Downlink: the 2D video data from the base unit is processed by the server unit, and transmitted to the telemedicine unit, where the video data is decompressed and displayed on the 2D monitor;

Base unit and server unit: (a) Uplink: the 2D video captured from the camera and the edited 3D model at the base unit are processed, compressed, encoded and transmitted to the server unit for storage and distribution, and (b) Downlink: the data received at the base unit is decoded and decompressed to recover the 3D model and biosignals from the telemedicine unit. The 3D model is rendered by either LF rendering, multi-view rendering or OpenGL 3D rendering for LFD, autostereoscopic multi-view 3D display and 3D model display on the 2D monitor, respectively.

Mobile unit and server unit: (a) Uplink: the 2D video captured from the mobile phone camera and the edited 3D model are processed, compressed, encoded and transmitted to the server unit to be forwarded to the telemedicine unit, and (b) Downlink: the data received at the mobile unit is decoded and decompressed to recover the 3D model and biosignals from the telemedicine unit. The 3D model is then rendered for 3D display on mobile phones.

In this work, we aimed to design a software platform that can realize basic 3D telemedicine services, such as 3D video conferencing, 3D model display and edit, and telehealth services such as blood pressure and heart rate monitoring. Moreover, this platform was designed to be extendable to future potential multi-discipline telehealth applications, e.g., tele-dentistry, tele-wound, tele-oncology, tele-psychiatry and tele-surgery.

More specifically, a software platform was designed with the following functions:
                           
                              •
                              User authentication: it provides an interface for the doctor to login to his/her account and load the user data, including personal information, patient history, contact list, calling history, etc.;

Video conferencing: it provides a 2D/3D video conferencing service between doctors and patients, while 2D and 3D modes can be switched for different user and tele-consultation preferences. Moreover, the interactive control functions, such as motion control, webcam head tracking and Leap motion control are integrated with the 3D video conferencing to realize interactions;

3D model display and edit: it loads and displays the historical 3D model data. In addition, this function is designed to be compatible with 3D mesh models from external files in .stl format, such as magnetic resonance imaging (MRI) and 3D scanning data;

Blood pressure and heart rate (BP/HR): it provides a real-time and historical display of the BP/HR data;

Patient data: it displays the patient's personal and medical data;

Settings: it allows the user to set the preferences of the platform, such as video quality, connection status, data usage, connection test and calibration.

Help: it provides the software manual and version information.

To realize the above functions on different platforms, we embrace a modular design framework with nine different modules, as shown in Fig. 8
                        .
                           
                              1.
                              Communication module: as shown in the figure, this module is responsible for the data communication between the server unit and the five function modules in the platform, including (1) outgoing user authentication data from the authentication module, and incoming verified user data, such as personal information, patient list and contact list; (2) two-way 2D/3D video conferencing data for the video conferencing module; (3) incoming 3D model data of the patient, and outgoing edited 3D model data for the 3D model module; (4) incoming real-time and history BP/HR data for the BP/HR module; and (5) incoming patient data for the patient data module;

User authentication module: this module reads the input user name and password from the login page, and passes them to the server via the communication module for verification;

Video conferencing module: this module communicates with the communication module and realizes the 2D/3D video conferencing function, while the 2D video conferencing function is realized by the 2D video conferencing module, and the 3D video conferencing function is realized by the 3D video conferencing module which embraces the mesh display module for real-time 3D model display;

Mesh display module: this module embeds the MeshLab [27] open source system to handle the mesh display and edit functions, including mesh vertices, solid mesh, and color rendered mesh display, zooming, marking, distance measurement, rotation, etc.;

3D model module: this module retrieves the model list of the patient from the server via the communication module, and embraces the mesh display module for mesh display and edit functions;

BP/HR module: this module receives and displays the real-time and historical BP/HR data from the server, and is able to give a statistical summary of the data, including their average, maximum and minimum values;

Patient data module: this module simply receives and displays the patient data stored in the server;

Interactive control module: this module translates the input signals from interactive control units (including motion sensor control from mobile phones, webcam head tracking for the desktop platform, and Leap motion control for the desktop and LFD platforms) to mesh display controls, such as rotating, zooming and marking; and

Data output module: this module is responsible for the data export from the other modules, such as exporting screen shots to JPEG files, recording a video from video conferencing to an.avi file, and exporting the patient data or measurement data to.txt/.xls files.

Corresponding to the information flow and function design, the database design of the system is presented in Fig. 9
                        . The database of the demo consists of two categories, i.e., the doctor data and patient data. The doctor data includes doctor's login information user name and password, and doctor's personal information name, affiliation and number. For the patient data of the doctor, it includes contact's name, number and the time of last call. Each contact ID is affiliated to a patient ID, which connects to the patient data.

The patient data consists of two parts, (1) patient's personal data including name, number, location, email, etc., and (2) patient's 3D model information, including the model's location and time, the number of frames, and the mesh geometric information of each frame, etc.

@&#RESULTS AND DISCUSSION@&#

In this section, the constructed 3D models from a group of experiments and the developed cross-platform demo software applications are presented. Moreover, we discuss the potential applications of the developed system in multi-discipline telehealth applications, such as tele-dentistry, tele-dermatology, tele-psychiatry and tele-surgery.

The experimental setup, shown in Fig. 10
                        , was comprised of a LF camera, a chair and a platform for the test object. A commercially available Lytro Illum LF camera [28] was employed to capture static LFs of the scene, and the chair (or platform) was about 60cm to the camera. The scene was captured under homogeneous ambient illumination so that specular reflections were minimized. Five different experiments with different objects and scenes were carried out, including face, ear, hand, finger, and Lindie scenes. Each LF was decoded by using the MATLAB LFToolbox (Version 4) [17] and the processing time of each LF was approximately 60s.

Multiple experiments under different lighting, scene and object settings were carried out to demonstrate the performance of the 3D model construction algorithm described in Section 3. As shown in Fig. 11
                        , the constructed 3D models show high levels of details. Moreover, Table 1
                         shows the comparison of RAW LF files, RAW LF images, image arrays and 3D meshes of the experimental samples shown in Fig. 11. The results in Table 1 show that the sizes of the constructed 3D meshes are significantly smaller than those of RAW LF images and image arrays. In addition, we compare the sizes of 3D meshes with the image arrays consisting of only the constructed region. As illustrated by the table, the size of the 3D mesh is approximately 16% of the size of the image array with only the constructed region. It should be noted that, the constructed 3D meshes illustrated in the figure and table are without compression. Therefore, the constructed 3D mesh is more suited for storage and transmission in real-time 3D streaming for telemedicine applications.

In this section, we present the results for the developed applications on different platforms. The design of the demo applications follows a simple and ease-of-use principle, and uses the same design language across different platforms. In Fig. 12
                        , we show the developed App on the IOS mobile operating system, which shows the user interfaces of all the designed modules and their components. Fig. 13
                         shows the application user interfaces on a web-based client, desktop client and Android platforms. It can be seen that the developed applications under different operating systems show a unified user interface and functions, which provides a unified user control and navigation experience.

@&#DISCUSSION@&#

Conventional tele-consultation or telemedicine services can be generally grouped into two types: the store and forward system, and the 2D video conferencing type system, which both can only provide doctors with plain 2D images or videos. The doctor might be able to diagnose the type of the disease via the captured 2D images or videos. However, it is always difficult or impossible for a doctor to remotely identify the level of the disease by purely observing the image of the infected area. The depth information which identifies the level of the disease is lost in conventional 2D image photography. The proposed system provides a solution for 3D telemedicine, which can offer a 3D model of the interested area. Although the functions in the developed demo applications are relatively simple, the 3D capture and display features can be integrated to various telemedicine applications. Here, we will discuss some of the potential applications of the proposed system in multiple disciplines.
                           
                              •
                              Tele-dentistry: conventional tele-dentistry is based on 2D video conferencing and intraoral cameras, where the doctor is able to observe the patient's oral area via the image taken by the intraoral camera. However, diseases like tooth decay and tooth cavities are unable to be identified via the 2D image. In this case, a 3D scanning model of the teeth from devices such as 3D intra-oral scanners is much more suitable for identifying the level of these teeth diseases. The proposed system is able to embrace the 3D scanning data file of the teeth and provide a 3D teeth model display (as shown in Fig. 14
                                 ), which can potentially improve the tele-dentistry service provided when compared to conventional 2D image-based tele-dentistry;

Tele-wound/Tele-dermatology: conventional wound healing care requires the patient to visit hospital frequently in order to accurately identify the healing process of the wound area, which is not time and cost efficient. On the other hand, the conventional 2D video conferencing is unable to provide an inside view of the wound healing process. With the LF-capture and 3D model construction functions in the proposed system, the specific area of the wound can be rebuilt as a 3D model. By providing the 3D model in real time and allowing the doctor to interact with the 3D wound model, the healing process can be identified accurately and efficiently;

Tele-psychiatry: in a tele-psychiatry consultation, movement and expression are two of the most important factors used to identify the patient's psychological status. Due to the low resolution and the lost perspective of view in the conventional 2D video conferencing based tele-consultation, psychiatrists cannot catch the concerned movements or expressions effectively. With the 3D capture and recording features provided in this system, the psychiatrist is able to choose the specific view of the patient, highlight and zoom in on the specific area to catch the required movements and expressions. Moreover, the high definition 3D model provided in the system can be embraced in multiple face recognition and expression recognition methods to improve the recognition performance and used for automatic symptom identification [29]; and

Tele-surgery: in a higher level of telemedicine application, a geographically distanced surgeon can perform an operation remotely with the immersive 3D LFD. Moreover, using the real-time rebuilt 3D model of the patient, the proposed system can give surgeons the ability to superimpose anatomic images right on their patients while they are being operated on.

@&#CONCLUSION@&#

This paper proposed a novel next-generation autostereoscopic 3D telemedicine system, which is the first telemedicine system using state-of-the-art LF technology. A novel framework of the system was established to realize 3D capture and display. A new 3D model construction algorithm from raw LF images was proposed in this paper, which was demonstrated to be able to provide 3D models with high levels of details. Furthermore, a cross-platform solution for the LF-based 3D telemedicine system under multiple platforms was proposed. Demo applications were developed under multiple operating systems, with 2D/3D video conferencing, 3D model display and edit, blood pressure and heart rate monitoring, and patient data display functions. Moreover, it was demonstrated that the proposed system can be further extended to multi-discipline telehealth and tele-consultation services, such as tele-dentistry, tele-wound/tele-dermatology, tele-psychiatry and tele-surgery.

The proposed framework and components in this paper will pave a foundation for future LF-based 3D telemedicine systems, which pose more challenges in various directions, e.g., real-time LF video capture, compression and conferencing, and the diverse requirements for different clinical applications. On one hand, real-time LF video conferencing relies on future commercialized LF video cameras, while, how to store and transmit this enormous amount of LF video data in real-time poses true technical research challenges. In our future work, we will first conduct research on LF video compression and transmission by exploring the 3D correlation in a LF video to achieve real-time LF video streaming in bandwidth-limited environments. Furthermore, upon the finalization of the system prototype, we will carry out end-user trials with industry partners to evaluate the clinical performance of the system and solve the different requirements for various telemedicine applications. It is foreseeable that the proposed 3D telemedicine system has the potential to revolutionize the next-generation 3D video communication technology, which can significantly improve the current tele-consultation experience through providing a high level of detail, and low-latency communications.

@&#ACKNOWLEDGEMENT@&#

This work was supported in part by a Smart Futures Fellowship funded by the Queensland Government of Commonwealth Australia.

@&#REFERENCES@&#

