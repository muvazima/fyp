@&#MAIN-TITLE@&#An approach to locate optic disc in retinal images with pathological changes

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           An approach to locate OD in retinal image with pathological changes is proposed.


                        
                        
                           
                           Features are vessel direction, intensity, OD edges, and size of bright regions.


                        
                        
                           
                           It can locate OD robustly in both normal and diseased retinal images.


                        
                        
                           
                           Comparison shows it can handle the challenging images better.


                        
                        
                           
                           Total 340 retinal images from four public databases were tested.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Optic disc

Localization

Retinal images

@&#ABSTRACT@&#


               
               
                  Automatic optic disc (OD) detection is an essential step for screening of eye diseases. An OD localization method is proposed in this paper, which aims to locate OD robustly in retinal image with pathological changes. There are mainly three steps in this approach: region-of-interest (ROI) detection, candidate pixel detection, and confidence score calculation. The features of vessel direction, intensity, OD edges, and size of bright regions were extracted and employed in the proposed OD locating approach. Compared with the OD locating method based on vessel direction only, the proposed method could handle the following cases better: OD partially appears in retinal image, retinal vessels are not obvious in retinal image, or there are bright lesions in retinal images. Four public databases with total 340 retinal images were tested to evaluate the performance of our method. The proposed method can achieve an accuracy of 100%, 95.8%, 99.2%, 97.8% for DRIVE database, STARE database, DIARETDB0 database, DIARETDB1 database respectively. Comparison studies showed that the proposed approach is especially robust in the retinal images with diseases.
               
            

@&#INTRODUCTION@&#

The detection of retinal structure is an important prerequisite to diagnose retinal diseases automatically. The retina contains several important structures, such as optic disc (OD), fovea, and vessels. Optic disc is the region where blood vessels and optic nerves converge. There are no light-sensing cells in OD area, so optic disc is also called blind spot. In the normal retinal images, OD is the brightest part and looks like a pale, nearly circular or vertically slightly oval disk. Correct OD detection is the basic step for computer aided diagnosis of different eye diseases, such as diabetic retinopathy [1] and glaucoma [2]. The changes of OD shape can reflect the extent of vision loss, thus various parameters of OD are used to diagnose eye diseases. For example, diameter and edge of cup, radius of OD, rim area, mean cup depth, and cup-disc-ratio are usually employed to detect glaucoma. OD detection is also a key step for the localization and segmentation of other anatomical structures such as fovea and retinal vessels. Since retinal vessels usually converge at the center of OD, vessel tracking can start from the rim of OD.

The challenges of robust OD detection includes: individual differences, fundus ocular diseases, uneven illumination, and retinal vascular occlusion. Many algorithms have been proposed for OD localization. Usually satisfactory results can be achieved in healthy retinal images. However, for the images with pathological changes, there is still no good solution. Fig. 1
                      shows some examples of these challenges. In Fig. 1(b), there are large areas of lesions in the image. The intensity of OD area is much darker than surrounding regions due to uneven illumination in Fig. 1(c). Only part of OD is shown in some retinal image as in Fig. 1(d). Most of the OD locating methods is effective for only one type of these difficulties.

In this paper, an improved method based on edge information is proposed to locate OD robustly in normal retinal images as well as images with pathological changes. The main contribution of this work can be summarized as three aspects. (1) Vessel edge information and OD edge are combined to detect candidate OD pixels, which successfully detect the candidate pixel of OD center in images with poor illumination or few vessel information. (2) A confidence score is proposed to locate OD center, in which a new compensation value is proposed. For images with poor illumination or few vessel information in OD region, this score will compensate the value for missing vessel edge information. (3) Comparing with the method using vessel edge information only, the improvement on candidate pixel detection and the confidence value achieves promising results in the following situations: the images with dark OD due to uneven illumination and low contrast; the retinal images with incomplete OD; the images with bright exudates, whose size and intensity are similar to OD.

@&#RELATED WORK@&#

Many methods have been proposed to detect OD, which can be generally classified into three categories: methods based on intensity, methods based on template matching, and methods based on vascular structure.

In early studies, the methods to locate OD center are mainly based on intensity. In these methods, OD is located by finding the largest bright area [3], the pixel with highest average intensity in a certain neighborhood area [4], or the bright region with maximum variance [5–7]. These methods can usually obtain correct location in normal fundus images. In the images with pathological changes, lesions such as hemorrhages, drusen, and exudates may influence the localization results, because the intensity and shape of these lesions may be similar to OD. Sometimes uneven illumination makes the OD's features look less prominent. Hence, an illumination correction operation was performed by Hsiao [7]. It is hard to acquire correct OD location by solely using the feature of intensity. Many improved methods have been proposed. For example, a line operator was designed to detect circular brightness structure by Lu [8]. In [9], a circular transformation was proposed to describe circular shape of OD and image variation across OD boundary to detect OD location and OD boundary. Besides, OD center was located by a voting to the candidate pixels detected by the maximum difference, maximum variance and maximum gray-level in low-pass filtered image [10].

Template matching is another widely used method. This method commonly consists of two steps: selecting candidate pixels (usually according to intensity, vascular structure etc.) and matching the given templates. In [11] candidates of OD center were determined by pyramidal decomposition and Hausdorff distance was employed for circular template matching to edge image. A template of Laplacian of Gaussian with a vertical channel in the middle was proposed in [12], in which the vertical channel corresponds to the major vertically oriented vessels. A PCA-based model called “disc space” was proposed by Li [13] to locate OD. For every pixel in the candidate region, the model is applied in different scales (0.8–1.2) and Euclidian distance to its projection onto the “disc space” was calculated. The point with the smallest value was determined as the OD center. In [14], binary circular template and Pearson correlation coefficient were employed to detect the OD candidate matching in CIElab lightness image. Approximate Nearest Neighbour Field [15] was used to find the correspondence between a chosen OD reference image and an input image.

Due to uneven illumination or pathology, there is large variation in the shape, color, intensity, and size of OD. It is hard to achieve stable results using the above intensity-based or template-based methods. It is observed that optic disc is the entrance region of blood vessels. The vascular geometric structures are utilized to locate the OD center [16–20]. Fuzzy convergence was proposed by Hoover [16] to determine the origination of retinal vessel network. Fuzzy segment model is set to create convergence image via voting type. A geometrical model of vessel structure was presented by Foracchia [17], which suggests that vessel path follows a similar directional parabolic course in all fundus images, and the vertex is defined as the origin of retinal vessels, namely OD center. A point-distribution-model (PDM) was trained to describe the position of the main anatomical structures including optic disc, macula, and vascular arch by Niemeijer [18]. The OD center was detected by fitting the PDM model to a testing image. In [19], OD center was determined by information of vessel which is extracted by curvelet transform. Two main vessels were detected using adaptive mathematical morphology in [20] and the OD center was located on the point of intersection.

Among the OD locating methods based on vessel information, there is a group of algorithm which detects OD according to vessel directions. It is noted that the retinal vessels are mainly oriented vertically when they start from OD. A vessel direction matched filter is proposed by Youssif [21] to match the direction of vessels at OD vicinity. A vessel direction map (VDM) recorded the maximal response direction at each pixel on the vessel segment. Then the filter was resized in different scales and the least accumulated difference was selected as the OD center. A fast localization method was presented by Mahfouz [22] in which two projections of certain image features (vertical and horizontal vessel feature) were employed to find the OD center. In [23], three image features were used to locate OD. The retinal background was estimated and the difference between the retinal image and the estimated retinal background was employed as one feature. The other two features are the mean and standard deviation of vertical projection, which are directional retinal vessel features. Based on the vessel density, thickness, orientation as well as retinal luminance, Bayes classifier was further applied to determine the location of OD in [24].

There are some other approaches of OD localization, which includes ant colony optimization [25], or combining state-of-the-art OD detector [26]. It is still difficult to detect OD robustly and fast in retinal image with lesions. In this paper, an algorithm is proposed to detect OD robustly in normal retinal images as well as in retinal image with pathological changes.

@&#METHODOLOGY@&#

In this section, an approach of OD localization in retinal image is proposed. The main steps in this approach are region-of-interest (ROI) mask generation, candidate pixel detection, and confidence score calculation. The OD locating method is based on four features, which include vessel direction, intensity, OD edges, and size of bright region. These features were extracted and a confidence score is calculated to combine these features. The OD is finally located at the pixel with the highest confidence score.

In order to reduce the redundant computation brought by background and boundary effect. An ROI mask is generated in every retinal image to save computing time.

Mask generation was performed according to the method proposed in [21]. First, a threshold t is applied on the image's red channel. t is set to 35 in this paper. Then the morphological operators of opening, closing, and erosion are applied in turn using a 3×3 square kernel. As the pixels near the edge of ROI mask may affect the final OD localization, the ROI mask is shrunk by 5pixel. Due to uneven illumination, there are black holes in the ROI region in some images. Hole-filling operation is performed to solve this problem in this paper. Fig. 2
                         shows the process of ROI mask generation with hole-filling.

In order to handle retinal images with different resolutions, the diameter of the ROI mask is employed to resize an input image. The image size can be adjusted automatically to a given ROI diameter and the proposed algorithm can process retinal images from different resources.

The candidate pixels of OD center are detected by two steps: horizontal and vertical position determination. Three features including vertical component of vessel edges, horizontal component of vessel edges, and OD edges are used to search candidate locations. The horizontal positions of candidate pixels are determined by vertical component of vessel edges, while vertical positions of candidate pixels are determined by horizontal component of vessel edges and OD edges. A sliding window is scanned horizontally and the horizontal position of candidate pixel is determined by the features within the sliding window. Similarly, the vertical locations of candidate pixels are decided by the features in the sliding window, which moves vertically on each horizontal location.

The horizontal positions of candidate pixels of OD center are detected based on vessel direction. It is observed that blood vessels originate from OD and exit mainly in vertical directions. When the distance from OD is further away, the vascular direction tends to be horizontal. The horizontal edge and vertical edge are detected via Eqs. (1) and (2). The edge extraction is performed in green channel because contrast between vessels and background is most obvious in green channel.
                              
                                 (1)
                                 
                                    
                                       
                                          E
                                          h
                                       
                                       
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       =
                                       
                                          I
                                          g
                                       
                                       
                                          
                                             i
                                             +
                                             1
                                             ,
                                             j
                                          
                                       
                                       −
                                       
                                          I
                                          g
                                       
                                       
                                          
                                             i
                                             −
                                             1
                                             ,
                                             j
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (2)
                                 
                                    
                                       
                                          E
                                          v
                                       
                                       
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       =
                                       
                                          I
                                          g
                                       
                                       
                                          
                                             i
                                             ,
                                             j
                                             +
                                             1
                                          
                                       
                                       −
                                       
                                          I
                                          g
                                       
                                       
                                          
                                             i
                                             ,
                                             j
                                             −
                                             1
                                          
                                       
                                    
                                 
                              
                           where I
                           
                              g
                            denotes the intensity in green channel, i,j represents the row and column coordinate of a pixel respectively. 
                              
                                 
                                    E
                                    h
                                 
                                 
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                              
                            and 
                              
                                 
                                    E
                                    v
                                 
                                 
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                              
                            represent horizontal and vertical edge.

Two features were extracted according to Eqs. (3) and (4), respectively, which mainly follows the method proposed in [22].
                              
                                 (3)
                                 
                                    
                                       f
                                       e
                                       a
                                       t
                                       u
                                       r
                                       e
                                       _
                                       1
                                       (
                                       i
                                       ,
                                       j
                                       )
                                       =
                                       
                                          
                                             
                                                
                                                   
                                                      E
                                                      v
                                                   
                                                   
                                                      
                                                         i
                                                         ,
                                                         j
                                                      
                                                   
                                                
                                             
                                             −
                                             
                                                
                                                   
                                                      E
                                                      h
                                                   
                                                   
                                                      
                                                         i
                                                         ,
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                I
                                                g
                                             
                                             
                                                
                                                   i
                                                   ,
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (4)
                                 
                                    
                                       f
                                       e
                                       a
                                       t
                                       u
                                       r
                                       e
                                       _
                                       2
                                       
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   
                                                      E
                                                      v
                                                   
                                                   
                                                      
                                                         i
                                                         ,
                                                         j
                                                      
                                                   
                                                
                                             
                                             +
                                             
                                                
                                                   
                                                      E
                                                      h
                                                   
                                                   
                                                      
                                                         i
                                                         ,
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       ×
                                       
                                          I
                                          g
                                       
                                       
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                    
                                 
                              
                           
                        

Most of the edges in retinal image are vessel edges. Other edges in retinal image include the edges of OD and retinal lesions. The feature_1 describes the vertical component of edge. The region with larger feature_1 has higher possibility to be OD region. The feature_2 describes both the vertical and the horizontal component of edge as well as the pixel intensity. The examples of feature_1 and feature_2 are shown in Fig. 3(b) and (d), respectively. It can be noted from retinal images that main retinal blood vessels oriented vertically from OD center. Therefore, feature_1 and feature_2 will be large around the location of OD.

Horizontal position is determined using feature_1. In order to find the horizontal location of OD center, a window with the size of image height×2 main vessel width is scanning horizontally in feature_1 image (Fig. 3(b)). The sum of the feature_1 in the window is calculated as Eq. (5). Fig. 3(c) illustrates the obtained H
                           proj after smoothing. Ideally, the global maximum of H
                           proj (red dot in Fig. 3(c)) should correspond to the horizontal location of OD center, which is illustrated using the red line in Fig. 3(b). Experiments showed that the OD horizontal location does not always correspond to the global maximum of H
                           proj. Every local peak in H
                           proj is detected as the potential horizontal positions.
                              
                                 (5)
                                 
                                    
                                       
                                          H
                                          
                                             proj
                                          
                                       
                                       
                                          j
                                       
                                       =
                                       
                                          ∑
                                          
                                             n
                                             =
                                             j
                                             −
                                             vessel
                                                
                                             width
                                          
                                          
                                             j
                                             +
                                             vessel
                                                
                                             width
                                             −
                                             1
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                m
                                                =
                                                1
                                             
                                             
                                                image
                                                   
                                                height
                                             
                                          
                                          
                                             f
                                             e
                                             a
                                             t
                                             u
                                             r
                                             e
                                             _
                                             1
                                             
                                                
                                                   m
                                                   ,
                                                   n
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Every local peak of H
                           proj is added to the candidate list of horizontal OD position. N horizontal positions are determined by the calculation of H
                           proj. Based on these N horizontal locations, the vertical position of potential OD center is determined by feature_2 as well as OD edge.


                           Feature_2 describes the feature of vessel edge and intensity. On each horizontal position, sum of feature_2 in the vertical sliding window with the size of OD diameter×OD diameter is calculated via Eq. (6). Fig. 3(e) shows an example of vertical projection V
                           proj. The point with the maximal V
                           proj (red dot in Fig. 3(e)) is identified as the vertical position of potential OD center, which should correspond to the OD vertical position according to Fig. 3(d). For N horizontal positions, N vertical positions are decided by V
                           proj. Hence N candidate pixels are identified. An example is illustrated in Fig. 5, in which blue dots denote the candidate pixels determined using V
                           proj.
                              
                                 (6)
                                 
                                    
                                       
                                          V
                                          
                                             proj
                                          
                                       
                                       
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       =
                                       
                                          ∑
                                          
                                             m
                                             =
                                             i
                                             −
                                             D
                                          
                                          
                                             i
                                             +
                                             D
                                             -
                                             1
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                n
                                                =
                                                j
                                                −
                                                D
                                             
                                             
                                                j
                                                +
                                                D
                                                −
                                                1
                                             
                                          
                                          
                                             f
                                             e
                                             a
                                             t
                                             u
                                             r
                                             e
                                             _
                                             2
                                             
                                                
                                                   m
                                                   ,
                                                   n
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where D denotes diameter of OD.

The OD in some retinal image is not complete and there are few vessels across the OD. Fig. 1(d) shows such an example. The value of vertical and horizontal edge feature in such cases is low around OD. Therefore, correct vertical location cannot be identified using V
                           proj only. In another case, the value of computed V
                           proj will be large in exudate region if it is brighter than OD on the same horizontal position. In these cases, OD center will be missed in the list of candidate pixels. To solve these two issues, another feature which describes the edges of OD is extracted to find more candidate pixels.

Considering that OD is usually brighter than its surroundings in normal retinal images as well as retinal images with dark OD, morphological operators are employed to detect the edges of OD. The detail steps are described as follows.
                              
                                 1.
                                 A 8×8 median filter is used for preprocessing in green channel.

Dilation operations are performed using structuring element S
                                    1 and S
                                    2, which is defined by the following equations:
                                       
                                          (7)
                                          
                                             
                                                
                                                   C
                                                   1
                                                
                                                =
                                                
                                                   I
                                                   g
                                                
                                                ⊕
                                                
                                                   S
                                                   1
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          (8)
                                          
                                             
                                                
                                                   C
                                                   2
                                                
                                                =
                                                
                                                   I
                                                   g
                                                
                                                ⊕
                                                
                                                   S
                                                   2
                                                
                                             
                                          
                                       
                                    where S
                                    1 and S
                                    2 are circle structuring elements with radius of 8 and 10pixel respectively. I
                                    
                                       g
                                     represents the intensity of green channel after preprocessing.

A subtraction operation is performed between C
                                    1 and C
                                    2 as in the following equation:
                                       
                                          (9)
                                          
                                             
                                                D
                                                =
                                                
                                                   C
                                                   1
                                                
                                                −
                                                
                                                   C
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 

10% brightest pixels in image D obtained using Eq. (9) are thresholded. The connected area with the pixel number less than 250 is deleted.

An example of edge detection result is shown in Fig. 4(b). In this boundary detection, the edges of vessels will not be extracted because both S
                           1 and S
                           2 are larger than vessel radius and vessels are eliminated via step 2. When an area is brighter than its surroundings, its edge will be detected. The detected edges include edges of OD and edges of lesions.

In order to localize more candidate pixels of OD center, a window with size of OD diameter×OD diameter is scanning vertically on each horizontal position. The point with the maximal sum inside this window is selected as an additional candidate point. It is worth mentioning that if there are no edges detected in the sliding windows, there are no additional candidate pixels detected by this step. To avoid the wrong detection caused by other lesions with sharp contrast which is far away from OD in the same horizontal position, a candidate pixel is added to the candidate list only if the distance between this candidate pixel obtained by OD edge and the candidate pixel detected using V
                           proj is less than one OD diameter. In Fig. 5
                           , the candidate pixels detected using OD edges are represented by yellow dots and blue dots denote the candidate pixels determined using V
                           proj. It can be observed that the OD center is not on the list of candidate pixels if only V
                           proj is employed to find the vertical positions. The correct candidate pixel is added to the candidate list using OD edges. The final candidate pixels contain the pixels detected by either of the feature, which means both blue dots and yellow dots in Fig. 5 are selected as candidate pixels of OD center.

In order to identify the OD location from the list of candidate pixels, a confidence score is calculated to combine all the features. The candidate pixel with maximal score is determined as the final location of OD center.

It is observed that OD is a large round area which is usually brighter than its surroundings. Besides, OD is always surrounded by main vessels in retinal images. In order to distinguish OD from other lesions such as exudates, the size of bright object and vertical vessel information are both taken into consideration in the design of the confidence score in this paper.

To extract the feature of OD size, bright pixels are segmented firstly. Considering that OD is not always the brightest area in retinal images with uneven illumination or pathologies, the brightest P pixels in green channel in a local rectangular region with the size of image height×W1 centered at each horizontal position is segmented. The value of P is set according to the estimated size of OD. In order to eliminate the segmented region due to bright lesions, further processing is performed as the following: (1) small areas (less than 150pixel) are deleted; (2) bright regions with the height much more than OD diameter around ROI mask boundary are excluded; (3) If the major axis of segmented region is much longer than OD diameter, the region is eliminated. Examples of the detected bright region are shown in Fig. 6
                        .

For each candidate pixel, the number of segmented bright pixels N inside a window with size W
                        2 (refer to Table 2) centered on the pixel is calculated. A proportion z is obtained by dividing this number by P to describe the size of bright region, which is expressed as the following equation:
                           
                              (10)
                              
                                 
                                    z
                                    
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    =
                                    
                                       N
                                       P
                                    
                                    ;
                                 
                              
                           
                        
                     

A confidence score s is proposed to combine all the proposed features.
                           
                              (11)
                              
                                 
                                    s
                                    
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    =
                                    
                                       H
                                       
                                          proj
                                       
                                    
                                    
                                       j
                                    
                                    +
                                    z
                                    
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    ×
                                    max
                                    
                                       
                                          
                                             H
                                             
                                                proj
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 H
                                 
                                    proj
                                 
                              
                              
                                 j
                              
                           
                         is the value acquired via Eq. (5), max{H
                        proj} represents the maximal value of H
                        proj. The pixel with the maximal s is determined as the final location of OD center. The size of bright object is extracted as the feature z in OD localization.

Because OD is usually a large bright region with surrounding main vessels in retinal images, the candidate pixel with large value of z and H
                        proj indicates OD center. In Eq. (11), the second term z(i, j)×max{H
                        proj} is actually a compensation value for H
                        proj which is determined by z. When OD is the area with few vessels and low intensity, the value of H
                        proj will not be the maximum. The higher the value of z in this area is, the higher the compensation value is, which will lead to a higher value of s.

Comparing with method in [22] that uses eccentricity as weighting factor, the proposed method can avoid wrong OD localization due to round-shaped lesions or few vessel information in OD.

@&#RESULTS@&#

Four retinal image databases were tested to evaluate the performance of our algorithm. The four databases include: (1) STARE database [27] (81 images, image size: 605×700pixel); (2) DRIVE database [28] (40 images, image size: 565×584pixel); (3) DIARETDB1 database [29] (89 images, image size: 1500×1152pixel); (4) DIARETDB0 database [29] (130 images, 1500×1152pixel). The detail properties of these four databases are described in Table 1
                      
                     [15]. The four databases contain many images with uneven illumination and images with large areas lesions such as exudates, hemorrhages, or drusens.

In this paper, the value of OD diameter and main vessel width is obtained according to the values proposed by Mahfouz [22]. All parameters are set according to the images from STARE database. The settings of all parameters are presented in Table 2
                     . Since the size of retinal images varies among different databases, images will be rescaled in pre-processing by adjusting its ROI mask size to a certain size. In this work, the diameter of this ROI is set to 647×647, which is determined according to images from STARE database. The proposed OD localization approach can thus handle retinal images with different sizes automatically.

In the evaluation, the detected OD center is considered as the correct location if it is inside the contour of OD. Our approach could achieve an average success rate of 98% for all the four databases. The proposed method achieved a success rate of 100% in DRIVE database, 95.8% in STARE database, 99.2% in DIARETDB0 database and 97.8% in DIARETDB1 database, which are summarized in Table 3
                     . Some examples of localization from different databases are illustrated in Fig. 7
                     , in which OD location is represented as a black “+”. It can be seen from Fig. 7 that Fig. 7(a) and (b) are normal retinal images. Fig. 7(c)–(l) are challenging images which contain large bright exudates (Fig. 7(c) and (d)), low contrast and dark OD (Fig. 7(e)–(j)), or incomplete OD (Fig. 7(g) and (h)). Correct OD localization is achieved for the above images by the proposed method. Fig. 7(k) and (l) show two examples among seven images that the proposed method still fails to locate OD correctly.


                     Table 4
                      compares the performance of the proposed method with other existing methods on four public databases of retinal images. It can be noted that the proposed method is among the top performers. The methods in [17,21] are both based on vessel detection. They can achieve satisfactory results, but it takes 2 to 4min to process one image, which is time consuming. The method in [22] is computation efficient (less than 1s for each image) because it is based on vessel edge information and vessel segmentation is not performed. Experiments show that method in [22] cannot handle retinal image with pathological changes (please refer to the performance on STARE database). Researchers have put efforts on improving detection accuracy based on the method in [22] in order to locate OD both accurately and efficiently. Method in [22] was employed to select candidate pixels and a circular transformation was proposed to locate optic disk center and disk boundary simultaneously in [9], which performs well in speed as well as accuracy. In this paper, an improved method is also proposed based on the method in [22] and it can perform better than the method in [22] for retinal images with pathological changes. Detail comparison between our method and method in [22] is described in the section of Discussion.

@&#DISCUSSION@&#

The proposed approach is initiated by the method in [22], which is a fast OD locating algorithm based on the vessel edge. In both methods, candidate pixels detection and confidence score calculation are two essential steps for OD localization. Comparison analysis is performed between these two methods.

Locating candidate pixel of OD using vessel edges and vessel direction only as in [22] may lead to wrong locating results. For example, Fig. 8(b) and (c) are V
                     proj and H
                     proj of Fig. 8(a) respectively. It can be seen that the value of V
                     proj and H
                     proj inside OD region (OD region is marked as a red box in Fig. 8(b) and (c)) is low due to the lack of vessel edges. Although candidate horizontal position of OD can be detected using vessel edge information only, vertical position of OD will be missed when there is few vessels across OD using the method in [22]. With the proposed improvement, OD center can be identified as candidate pixel as shown in Fig. 9(b). Blue dots denote the candidate pixels detected by method in [22], and yellow dots denote the candidate pixels added by the proposed algorithm. It can be seen that the proposed method can improve the accuracy of candidate pixel detection.

In [22], the final OD position is determined as the candidate pixel with maximal product of H
                     proj and eccentricity of largest bright object. According to our experiment, the value of H
                     proj at OD center is not always to be maximal. Due to the complex variety in diseased images, the method proposed in [22] will fail when the eccentricity (the ratio of minor axis to major axis of bright object) of other bright object is similar to that of OD. Fig. 10
                      shows such an example. It is can be seen from Fig. 10(a) that the eccentricities of two clusters are similar (the red box represents the cluster region), and value of eccentricity for the non-optic disc region is even larger. Fig. 10(b) shows that the value of H
                     proj in non-OD region is just slightly lower than that of OD area. Hence, the OD localization scheme in [22] leads to the wrong result in such cases. The OD location result using method in [22] is shown as the cross in Fig. 10(a). Besides, when the OD is incomplete, its eccentricity will be much less than 1, which will result in wrong location. Examples of location result using method in [22] are presented in Fig. 12(a) and (c).

To avoid this influence, a confidence score is proposed in our approach to detect the OD location. Because OD is usually a large bright region compared with its surroundings and surrounded by main vessels, the proposed score takes both size of bright object and the value of H
                     proj into consideration. The feature of bright object size decides the compensatory value in this confidence score. When there are few vessel edges in OD region, this confidence value will be compensated using the feature of OD size. The method of extracting bright region in [22] only selected the bright pixels in a small window centered on each candidate region. In our method, the bright region is segmented in the widow with size of image height×
                     W
                     1, which means that vertical region of the whole image height is considered. As shown in Fig. 11
                     , there are two candidate pixels (blue dots) at the same horizontal position. It is obvious that the size of bright area in a certain window W
                     2 (it is masked as a red box) centered on OD center is larger than that centered on non-OD center.

The proposed confidence score can achieve more robust OD localization than the decision scheme in paper [22]. Some comparison examples are shown in Fig. 12
                     . The statistical comparison of three methods, which contain the OD location using edge feature only [22], using the method by calculating eccentricity of bright cluster [22], and our proposed method, are presented in Table 5
                     . All these methods can achieve satisfactory localization results for normal retinal images. Performance of these methods is only compared on STARE database in this table, because there are a large amount of challenging and various diseased images in STARE database. The results of other databases can be found in Table 4.

Comparison study indicates that the method in [22] can achieve robust results in normal retinal images. But the method in [22] cannot perform well for the following cases: (1) only part of OD is presented in retinal images; (2) retinal vessels are not obvious in retinal images; (3) there are round-shape lesions with bright colour in retinal images. An improved approach is proposed in this paper, which aims to achieve better OD localization results in retinal images with pathological changes. Our improvement consists of three parts: (1) a ROI mask is generated to save computing time; (2) edges of OD are enhanced to detect the additional candidate pixels. (3) the size of the largest bright object is extracted as a feature in the confidence score to decide final OD location.

Many OD localization methods are based on vessel segmentation [16,21]. Segmenting the vessels will cost longer processing time and it is difficult to achieve accurate vessel segmentation in the low contrast area. When OD is in dark and low contrast region, vessel segmentation often fails and OD localization will fail as well. The proposed method is based on vertical and horizontal edge extraction, OD edge extraction, and size of bright object. It is more robust than the methods based on vessel segmentation only. Experimental results showed that the proposed method can achieve satisfactory OD location in the above described situations and the processing is very fast.

@&#CONCLUSION@&#

An improved method to locate OD center robustly in the retinal image with pathological changes is proposed in this paper. Two main improvements have been proposed to increase accuracy of OD localization, which consist of candidate pixels detection and final OD center locating scheme. Vertical and horizontal edges, intensity, OD edge, and size of bright object are extracted as features in the proposed algorithm. The computational complexity is more efficient than the methods based on the vessel segmentation and it performs better than the methods based on vessel direction only.

The advantage of the proposed method is its robustness, especially for challenging images. These challenging images include image with poor illumination and retinal image with pathological changes, which can be concluded as the following three situations: (1) the images with dark OD due to uneven illumination and low contrast; (2) the retinal images with incomplete OD; (3) the images with bright exudates, whose size and intensity are similar to OD. To our knowledge, most of the available algorithms can achieve satisfactory OD locating result for normal retinal image, while their performance decreases for image with diseases.

@&#ACKNOWLEDGEMENT@&#

This research work is partially supported by NSFC (No. 81271650) and NCET-10-0041.

@&#REFERENCES@&#

