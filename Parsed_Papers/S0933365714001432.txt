@&#MAIN-TITLE@&#Development of electroencephalographic pattern classifiers for real and imaginary thumb and index finger movements of one hand

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Decoding accuracy of real and imaginary finger movements was explored by ANN and SVM.


                        
                        
                           
                           Real and imaginary movement of thumb/index fingers of one hand was used for decoding.


                        
                        
                           
                           The SVM was better for trial accumulation, ANN - for single-trial discrimination.


                        
                        
                           
                           Decoding of imagined movements through individual time intervals is promising for BCI.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Electroencephalography

Symbolic regression

Support vector machine

Artificial neural network

Motor imagery

Finger movements

Brain–computer interface

@&#ABSTRACT@&#


               
               
                  Objective
                  This study aimed to find effective approaches to electroencephalographic (EEG) signal analysis and resolve problems of real and imaginary finger movement pattern recognition and categorization for one hand.
               
               
                  Methods and materials
                  Eight right-handed subjects (mean age 32.8 [SD=3.3] years) participated in the study, and activity from sensorimotor zones (central and contralateral to the movements/imagery) was recorded for EEG data analysis. In our study, we explored the decoding accuracy of EEG signals using real and imagined finger (thumb/index of one hand) movements using artificial neural network (ANN) and support vector machine (SVM) algorithms for future brain–computer interface (BCI) applications.
               
               
                  Results
                  The decoding accuracy of the SVM based on a Gaussian radial basis function linearly increased with each trial accumulation (mean: 45%, max: 62% with 20 trial summarizations), and the decoding accuracy of the ANN was higher when single-trial discrimination was applied (mean: 38%, max: 42%). The chosen approaches of EEG signal discrimination demonstrated differential sensitivity to data accumulation. Additionally, the time responses varied across subjects and inside sessions but did not influence the discrimination accuracy of the algorithms.
               
               
                  Conclusion
                  This work supports the feasibility of the approach, which is presumed suitable for one-hand finger movement (real and imaginary) decoding. These results could be applied in the elaboration of multiclass BCI systems.
               
            

@&#INTRODUCTION@&#

Interest in developing non-invasive brain–computer interfaces (BCIs) has grown remarkably due to the application of new methods of brain activity registration and analysis and the development of new hardware and software for BCI systems. Non-invasive BCIs allow subjects to communicate without using their peripheral nervous system. This feature is crucial for patients with severe brain and brain stem injuries, for whom normal communication is difficult or even impossible. Non-invasive BCIs also might be implemented for various uses, including rehabilitation, artificial limbs and wheelchair control. The development of these systems for clinical applications is being conducted by different research groups worldwide [1–4]. Furthermore, non-invasive BCI applications for non-medical purposes are also possible, including the control of technical devices under extreme environmental conditions and the real-time estimation of subject states [5].

The amount of research devoted to BCIs has grown considerably over the last few years. More than 1500 articles on the development of BCIs have been indexed in the USA National Library of Medicine at the National Institutes of Health (www.pubmed.com) during the last 10 years. Non-invasive BCI systems may be based on various types of brain activity recordings, including electroencephalography (EEG) [6], magnetoencephalography (MEG) [7], functional magnetic resonance imaging and near-infrared spectroscopy [8,9]. Among these applications, EEG seems to be the most affordable and available.

One of the promising measures for EEG-based BCIs is the brain activity associated with imagery movement acts. Different signals have been used during BCI studies to classify EEG patterns associated with the imaging of certain movements. These include event-related desynchronization/synchronization of mu or beta rhythms in the sensorimotor cortex and evoked potential parameters [10].

The primary challenge during EEG-based BCI development is extraction of the classifying signal, which provides high discriminative accuracy in motor imagery pattern classification [11]. For example, mu and beta rhythms are used to extract the classifying signal [12].

Several BCIs have been developed based on sensorimotor rhythm, e.g., the Wadsworth BCI [13] in which users control a cursor on the computer screen through imagined movements of their hands and entire body. The Graz BCI [14,15] is based on the discrimination of simple imagined movements of the hands and legs; it can be used to manage a cursor, virtual keyboard or orthopedic device for a paralyzed arm.

The Berlin BCI [5] is based on the discrimination of imagined movements of the right and left hands for computer application management. Doud and colleagues [16] used imagined movements of the hands, legs and tongue for continuous BCI control of a three-dimensional helicopter model. Similar BCI systems are used by research groups to manage robotized artificial limbs and control wheelchairs [17].

More recent studies are dedicated to the imagined movements of large body parts, such as hands, legs, wrists [18] and elbows [19]. However, the ability to use the imagined movements of fine body parts increases the degrees of freedom for BCIs and supports the likelihood of practical use.

Signals corresponding to different finger movements can be extracted from electrocorticographic (ECoG) signals despite distributed and overlapped representations in the motor cortex [20,21]. Miller et al. [21] described broadband signals recorded on appropriate cortex areas that were specific to single finger movements. Analysis of the ECoG potentials recorded during the real finger movements revealed that just before the initiation of finger movement, there is a characteristic decrease in power at lower frequencies (alpha/beta range) and a spectrally broad but spatially focal increase in power at higher frequencies (above 40Hz). A principal component analysis of these changes revealed robust, characteristic motifs, which were consistent and reproducible across all subjects and electrodes in the hand motor cortex during finger movements. The relative strength of the main principal spectral component varied according to the neighboring cortical location, indicating separable individual finger somatotopy among the subjects [21].

Few reports have been dedicated to BCIs based on the EEG patterns of imagined and real movements of fine human body parts. Xiao and Ding [22] evaluated the discrimination performance of three types of EEG features: spectral features obtained from PCA, alpha/beta band power changes, and EEG temporal data for decoding the individual finger movements of one hand. The experimental results demonstrate the feasibility of applying a broadband EEG feature to the discrimination of individual fingers. The highest decoding accuracy was 45% among all investigated EEG features, and EEG temporal data and spectral principal component features demonstrate similar decoding accuracies. However, alpha/beta power changes (regularly used for large body part movement discrimination) do not contain sufficient information for decoding fine individual finger movements [22].

The EEG signals corresponding to the real finger movements of one hand were classified with a similar accuracy by [23], with MEG data providing more robust classification of the same cases [23]. The study demonstrated that the information available from simultaneously recorded EEG was not sufficient for robust classification. Furthermore, it showed that the slow amplitude modulations of the time series were more predictive than was the power of the time–frequency representation of different frequency bands when decoding single finger taps. This finding indicates that important information is captured in the precise, spatiotemporal pattern of movement-related magnetic field dynamics. However, implementation of the MEG method is restricted by the complexity of the recording process required.

The principal barrier to the practical implementation of BCI systems is the lack of robust, fast and accurate EEG pattern classification methods. Most of studies in the field of EEG-based BCI systems have been dedicated to the development of these methods.

Another problem is signal nonstationarity [24]. Nonstationarity of the EEG signal is observed at the transition from the calibration stage to real-time application [25] and can be caused by fatigue and other uncontrolled factors. Explanations for nonstationarity can include external noises, a high-dimension EEG signal [26] and changes in the background activity of the user [27]. The problem of EEG signal nonstationarity during the implementation of BCI systems is the subject of future BCI development [24].

This study aimed to find effective approaches for EEG signal analysis to resolve problems with pattern recognition and categorization. Feature generation methods based on distinct signal regression (numerical and symbolic) approaches across various time domains can avoid the loss of information due to the time localization of features. The artificial neural network (ANN) classifier and support vector machine (SVM) classifiers were chosen for EEG pattern recognition.

The experiments were performed during motor execution and imagination to assess the effectiveness of the classifiers. The EEG signals obtained during imagined and real movements of the fingers of one hand were used as the initial data. The results assessed the possibility of recognizing EEG patterns corresponding to real and imagined finger movements. The results were also used to estimate the ANN and SVM classifier accuracy.

The methods of EEG data acquisition, feature extraction and classification are described in the ensuing section. The classification results and discussion are presented in Sections 3 and 4. Finally, a brief conclusion is given.

A total of 8 healthy, right-handed participants (3 male, 5 female, mean age: 32.8 [SD=3.3] years) participated in the study. The subjects participated in a different number of repeated sessions, and 17 separate EEG registrations were performed on different days. No participant reported a history of any medical or neurological disorder, and all had normal or corrected vision. All procedures were carried out in accordance with the Helsinki declaration (1974). The participants gave their written consent after receiving a detailed explanation of the experimental procedure.

Participants had to perform 4 types of tasks: 2 motor and 2 mental (imagining motor activity). In the motor tasks, subjects had to press a button with the thumb (1st motor task) and index finger (2nd motor task) of their right hand. In the imagery tasks, they had to use their kinesthetic sense [15] to imagine moving the thumb and index finger of their right hand. All tasks were performed with open eyes. The subjects fixated their vision on a point in the center of the screen and voluntarily “reacted” with movement/imagining when a cross appeared. The tasks were performed with a block design (100 trials per block, 2350ms duration per trial). The block-based design was used due to several reasons. First, it was applied to eliminate the influence of the presented stimulus type on the ensuing EEG activity. In the case of imaginary movements, the type of the movements might be encoded by the stimulus cue during the training trial set (increasing the trial and entire session length) or by the instruction to the subjects in the block-based approach. Second, the block design was used for the future accumulation of the sequenced trials for real-time applications when subjects might imagine movements several times to receive device feedback.

The model of the voluntary/imaginary tapping is presented in Fig. 1
                           .

The blocks of trials of imaginary and real movements were counterbalanced both across subjects and across different sessions with the same subjects. Between-trials intervals inside each block of trials were randomized from 700 to 2500ms.

Monopolar EEG was registered from 19 derivations (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, and O2 according to the 10–20 system) and a reference at the earlobe with a Mitsar electroencephalograph (St. Petersburg, Russia). The bandpass filter was set to 0.53–30Hz with a notch filter set to 45–55Hz using WinEEG software (Ponomarev, V.A., Kropotov, Ju.D., registration no. 2001610516 at 08.05.2001). The electrode impedance was less than 5kΩ. The sampling rate per channel was 2000Hz. The raw EEG data were transformed to a weighted average reference montage (WAR montage) and digitized at sampling rate 500Hz. The WAR montage produces a spatial filtering effect and diminishes the influence of a common reference in the EEG signal. In comparison to common averaged reference montage, the WAR montage greatly reduces the topographic displacement of a widespread potential, such as alpha rhythms [28]. To control a situation of involuntary movements, electromyography (EMG) was registered from the thumb and index finger during imaginary task, and trials with any movements during imaginary tasks were excluded from further analysis.

The EEG signal was bandpass filtered (0.53–30Hz) for comparative ANN and SVM classification. Because we used real motor and motor imagery tasks, there was a possibility that the distinguished features of the finger movements and movement imagery could be represented in both low and high frequency bands in the sensorimotor zones. A real button press is accompanied by desynchronization in the sensorimotor cortex of the contralateral hemisphere at a frequency of more than 10Hz [29,30] preceded by a slow negative wave (800–500ms duration) [31,32]. Evidence from the field of BCI research also supports the value of high frequency signals [23] for motor imagery discrimination.

Extra high and slow frequency activities were automatically marked as artifacts and excluded from further analysis. The thresholds were set as follows: (1) 50μV for slow waves in the 0–2Hz band and (2) 35μV for fast waves filtered in the 20–35Hz band. Those corrections were applied to mark artifacts without changes in the native EEG signals and could be successfully applied in future real-time experiments. After artifact correction, the data from each person were merged into one time series and analyzed in sequence using the trial time beginning data.

Because people exhibit rather rhythmic patterns of activity, we hypothesized that differences may exist for imagery and motor tasks during the nearby time period (1600ms length interval, 450ms after the cross presentation and 750ms after the trial started). The structure of a trial is shown in Fig. 1. The data from the first 450ms following the cross presentation were excluded from analysis to eliminate the possibility of event-related potentials connected with the visual presentation of the cross. Although we analyzed distinct zones than those involved with visual perception, we wanted to minimize the influence of any possible components of visual perception and processing. The subjects were asked to generate motor behaviors and kinesthetic imagined movements [15]. The areas of interest were in the sensorimotor (Cz and C3 derivations) cortex, which are important to motor and kinesthetic imagery task discrimination [15,33].

Most BCIs based on various approaches have similar feature extraction steps:
                           
                              1.
                              allocating areas in the observed EEG channels where the signal is expected;

transition to spectral analysis using a Fourier transform;

categorization of signal patterns using ANN or SVM classifiers [17,34,35].

The disadvantage of transitioning to a spectral analysis is the loss of information regarding the time localization of the features. Therefore, in this study, the signal analysis methods (based on various signal regressions (numerical and symbolic)) were implemented to generate space in time domains.

The difficulty of optimizing a parametric regression model remains a highly relevant problem in the field of pattern recognition despite a long history of research. The group method of data handling is widely used and according to this method, is the model that delivers the best approximation in a variety of scenarios [36].

In the present paper, a symbolic regression method is used. Symbolic regression is a method of automatic regression models constructed by iterating through arbitrary superpositions of functions from a given data set. Genetic programming is used to construct a superposition of functions and to find the optimal regression model. The search for models is implemented under the iterative scheme “generation – choice” in accordance with certain rules of model generation and criteria for their selection. The sets of competing models are consistently generated, with each model acting as a superposition of a given set of smooth parametric functions. The best models are selected from a set for subsequent modification.

Let us pose the problem of finding a symbolic regression model from several independent variables. Let there be n independent variables, 
                              
                                 
                                    X
                                    i
                                 
                                 ,
                                 i
                                 =
                                 
                                    
                                       1
                                       ,
                                       n
                                    
                                    ¯
                                 
                              
                           , and one dependent variable, Y. Additionally, a sample of values, X
                           ∈
                           R
                           
                              N×n
                           , of independent random variables is given, 
                              
                                 
                                    X
                                    1
                                 
                                 =
                                 
                                    x
                                    
                                       k
                                       ,
                                       1
                                    
                                 
                                 ,
                                 
                                    X
                                    2
                                 
                                 =
                                 
                                    x
                                    
                                       k
                                       ,
                                       2
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    X
                                    n
                                 
                                 =
                                 
                                    x
                                    
                                       k
                                       ,
                                       n
                                    
                                 
                                 ,
                                 k
                                 =
                                 
                                    
                                       1
                                       ,
                                       N
                                    
                                    ¯
                                 
                              
                           :
                              
                                 
                                    
                                       X
                                       =
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            x
                                                            
                                                               1,1
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            x
                                                            
                                                               1,2
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      ⋯
                                                   
                                                   
                                                      
                                                         
                                                            x
                                                            
                                                               1
                                                               ,
                                                               n
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            x
                                                            
                                                               2,1
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            x
                                                            
                                                               2,2
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      ⋯
                                                   
                                                   
                                                      
                                                         
                                                            x
                                                            
                                                               2
                                                               ,
                                                               n
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      ⋮
                                                   
                                                   
                                                      ⋮
                                                   
                                                   
                                                      ⋮
                                                   
                                                   
                                                      ⋮
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            x
                                                            
                                                               N
                                                               ,
                                                               1
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            x
                                                            
                                                               N
                                                               ,
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      ⋯
                                                   
                                                   
                                                      
                                                         
                                                            x
                                                            
                                                               N
                                                               ,
                                                               n
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       ,
                                    
                                 
                              
                           The vector, Y
                           ∈
                           R
                           
                              N
                           , of corresponding values of the dependent random variable is 
                              
                                 Y
                                 (
                                 
                                    X
                                    1
                                 
                                 =
                                 
                                    x
                                    
                                       k
                                       ,
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    X
                                    n
                                 
                                 =
                                 
                                    x
                                    
                                       k
                                       ,
                                       n
                                    
                                 
                                 )
                                 =
                                 
                                    y
                                    k
                                 
                                 ,
                                 k
                                 =
                                 
                                    
                                       1
                                       ,
                                       N
                                    
                                    ¯
                                 
                              
                           : 
                              
                                 Y
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      y
                                                      1
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      y
                                                      2
                                                   
                                                
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      y
                                                      N
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           .Additionally, a set of smooth functions is specified: G
                           ={g(x, y, z, …)|g
                           :
                           R
                           ×⋯×
                           R
                           →
                           R}.

The functions, 
                              
                                 g
                                 
                                    
                                       x
                                       ,
                                       y
                                       ,
                                       z
                                       ,
                                       …
                                    
                                 
                                 ∈
                                 G
                              
                           , have a finite non-zero number of arguments. Both the values of the independent variables from the matrix, X, and the resulting values of another function, g′∈
                           G, can be used as arguments for g
                           ∈
                           G.

Consider the set of all possible superpositions of not more than r
                           ∈
                           R functions g
                           ∈
                           G: 
                              
                                 
                                    Ω
                                    r
                                 
                                 =
                                 {
                                 ω
                                 (
                                 x
                                 )
                                 =
                                 
                                    
                                       
                                          g
                                          1
                                       
                                       ∘
                                       
                                          g
                                          2
                                       
                                       ∘
                                       ⋯
                                       ∘
                                       
                                          g
                                          k
                                       
                                    
                                 
                                 (
                                 x
                                 )
                                 |
                                 
                                    g
                                    i
                                 
                                 ∈
                                 G
                                 ,
                                 i
                                 =
                                 
                                    
                                       1
                                       ,
                                       k
                                    
                                    ¯
                                 
                                 ,
                                 k
                                 ≤
                                 r
                                 }
                              
                           . Each element of the set, ω
                           ∈
                           Ω
                           
                              r
                            is the smooth function of the vector of independent variables: 
                              
                                 ω
                                 =
                                 ω
                                 (
                                 x
                                 )
                                 ,
                                 x
                                 =
                                 
                                    
                                       
                                          
                                             
                                                x
                                                1
                                             
                                             ,
                                             …
                                             ,
                                             
                                                x
                                                n
                                             
                                          
                                       
                                    
                                    T
                                 
                              
                           .

The regression model 
                              
                                 
                                    f
                                    
                                       r
                                       ,
                                       d
                                    
                                 
                                 (
                                 w
                                 ,
                                 x
                                 )
                              
                            is specified as follows:
                              
                                 (1)
                                 
                                    
                                       
                                          f
                                          
                                             r
                                             ,
                                             d
                                          
                                       
                                       (
                                       w
                                       ,
                                       x
                                       )
                                       =
                                       
                                          w
                                          T
                                       
                                       ⋅
                                       
                                          
                                             
                                                1
                                                
                                                   
                                                      
                                                         Ω
                                                         ¯
                                                      
                                                      d
                                                      r
                                                   
                                                
                                             
                                          
                                       
                                       =
                                       
                                          w
                                          0
                                       
                                       +
                                       
                                          w
                                          1
                                       
                                       
                                          ω
                                          1
                                       
                                       
                                          x
                                       
                                       +
                                       ⋯
                                       +
                                       
                                          w
                                          d
                                       
                                       
                                          ω
                                          d
                                       
                                       (
                                       x
                                       )
                                       ,
                                    
                                 
                              
                           where d
                           ∈
                           R; 
                              
                                 w
                                 =
                                 
                                    
                                       (
                                       
                                          w
                                          0
                                       
                                       ,
                                       …
                                       ,
                                       
                                          w
                                          d
                                       
                                       )
                                    
                                    T
                                 
                                 ∈
                                 
                                    R
                                    
                                       d
                                       +
                                       1
                                    
                                 
                              
                            are the vector of parameters of the model; x
                           =(x
                           1, …, x
                           
                              n
                           )
                              T
                           
                           ∈
                           R
                           
                              n
                            is the independent variables vector; 
                              
                                 
                                    
                                       Ω
                                       ¯
                                    
                                    d
                                    r
                                 
                                 =
                                 
                                    
                                       [
                                       
                                          ω
                                          1
                                       
                                       (
                                       x
                                       )
                                       ,
                                       …
                                       
                                          ω
                                          d
                                       
                                       (
                                       x
                                       )
                                       ]
                                    
                                    T
                                 
                                 ,
                                 
                                    ω
                                    i
                                 
                                 (
                                 x
                                 )
                                 ∈
                                 
                                    Ω
                                    r
                                 
                                 ,
                                 i
                                 =
                                 
                                    
                                       1
                                       ,
                                       d
                                    
                                    ¯
                                 
                              
                            is the vector whose components are superpositions of smooth functions in the set Ω
                           
                              r
                           .

Finally, we define the set of all regression models:
                              
                                 (2)
                                 
                                    
                                       
                                          Φ
                                          
                                             r
                                             ,
                                             z
                                          
                                       
                                       =
                                       {
                                       
                                          f
                                          
                                             r
                                             ,
                                             d
                                          
                                       
                                       (
                                       w
                                       ,
                                       x
                                       )
                                       |
                                       d
                                       ≤
                                       z
                                       }
                                       .
                                    
                                 
                              
                           We define the residual function of the regression model 
                              
                                 
                                    f
                                    
                                       r
                                       ,
                                       d
                                    
                                 
                                 (
                                 w
                                 ,
                                 x
                                 )
                              
                            as follows:
                              
                                 (3)
                                 
                                    
                                       p
                                       (
                                       
                                          f
                                          
                                             r
                                             ,
                                             d
                                          
                                       
                                       )
                                       =
                                       
                                          
                                             
                                                1
                                                N
                                             
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                
                                                   
                                                      (
                                                      
                                                         f
                                                         
                                                            r
                                                            ,
                                                            d
                                                         
                                                      
                                                      (
                                                      
                                                         w
                                                         ˜
                                                      
                                                      ,
                                                      
                                                         X
                                                         k
                                                      
                                                      )
                                                      −
                                                      
                                                         y
                                                         k
                                                      
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where X
                           
                              k
                            is a string of independent variables values matrix X; y
                           
                              k
                            is a component of vector Y, which is the corresponding value of the dependent variable; 
                              
                                 w
                                 ˜
                              
                            is a vector of optimal parameters for linear regression model 
                              
                                 
                                    f
                                    
                                       r
                                       ,
                                       d
                                    
                                 
                                 (
                                 w
                                 ,
                                 x
                                 )
                              
                           , which is found using the least squares method:
                              
                                 (4)
                                 
                                    
                                       
                                          w
                                          ˜
                                       
                                       =
                                       
                                          
                                             arg
                                             min
                                          
                                          
                                             w
                                             ∈
                                             
                                                R
                                                
                                                   d
                                                   +
                                                   1
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                1
                                                2
                                             
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                
                                                   
                                                      (
                                                      
                                                         f
                                                         
                                                            r
                                                            ,
                                                            d
                                                         
                                                      
                                                      (
                                                      w
                                                      ,
                                                      
                                                         X
                                                         k
                                                      
                                                      )
                                                      −
                                                      
                                                         y
                                                         k
                                                      
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           Finally, it is necessary to find a regression model 
                              
                                 
                                    f
                                    
                                       r
                                       ,
                                       d
                                    
                                 
                                 (
                                 
                                    w
                                    ˜
                                 
                                 ,
                                 x
                                 )
                                 ∈
                                 
                                    Φ
                                    
                                       r
                                       ,
                                       z
                                    
                                 
                              
                            that minimizes the residual functional: 
                              
                                 p
                                 
                                    
                                       
                                          f
                                          
                                             r
                                             ,
                                             d
                                          
                                       
                                    
                                 
                              
                           : 
                              
                                 
                                    f
                                    
                                       r
                                       ,
                                       d
                                    
                                 
                                 (
                                 
                                    w
                                    ˜
                                 
                                 ,
                                 x
                                 )
                                 =
                                 arg
                                 
                                    
                                       min
                                    
                                    
                                       
                                          f
                                          
                                             r
                                             ,
                                             d
                                          
                                       
                                       ∈
                                       
                                          Φ
                                          
                                             r
                                             ,
                                             z
                                          
                                       
                                    
                                 
                                 p
                                 (
                                 
                                    f
                                    
                                       r
                                       ,
                                       d
                                    
                                 
                                 )
                              
                           .

The symbolic regression method was applied to the analysis of EEG signals recorded during imagined and real finger movements. The automatic evolutionary algorithm was shown to be capable of regression analysis of the complicated time series of EEG signals. Consequently, the regression models were developed in the analytical form. Examples of EEG signals corresponding to the imagined movements of the thumb and index fingers and obtained regression models are shown in Figs. 2 and 3
                           
                           .

The obtained regression model of the imagined movement of the thumb had the following structure: 0.049x
                           +1.39sin(√(−1.718x))+1.086sin(1.449−0.116x)+0.617sin(12.49x)−1.129√(1.718x
                           1
                           −32.7)−1.15sin(−0.09154x
                           1
                           −1.442)+10.14.

The obtained regression model of the imagined movement of the index finger had the following structure: 0.045x
                           +2.79sin(0.86x/√x)−2.084sin(sin(0.124x))+1.557sin(2x/√x)−0.006x√x
                           +0.00016x
                           2
                           −1.132.

The obtained models (in analytical form) demonstrate the significance of the sinus component in the regression analysis of bioelectrical signals. The experiments demonstrated the poor accuracy of classical polynomial regression models. The fixed structure of regression models is necessary for applying their coefficients as classifying features. In this case, a concrete form of the model is defined by the coefficients of the fixed terms. A description of the selected structure is provided in the next section.

Following experiments on the selection of a regression models structure, the minimum required number of terms for a model was evaluated using the sum of sinus functions: a
                           1
                           *sin(b
                           1
                           *
                           x
                           +
                           c
                           1)+
                           a
                           2
                           *sin(b
                           2
                           *
                           x
                           +
                           c
                           2)+···+
                           a
                           8
                           *sin(b
                           8
                           *
                           x
                           +
                           c
                           8), where a
                           
                              i
                           , b
                           
                              i
                           , c
                           
                              i
                            are the regression model coefficients, i
                           =1:8.

An example of an EEG signal corresponding to an index finger movement and the obtained regression model in the form of the sum of sinus functions is shown in Fig. 4
                           .

Signal decomposition in the form of the sum of sinus functions presents opportunities for EEG signals to feature space generation.

Data analysis in the time domain was shown to be an efficient means of EEG pattern evaluation. The symbolic regression of EEG signals corresponding to mental commands is considered to represent the transition from numerical data to analytical, functional analysis. The key advantage of the proposed method is the automatically generated regression model in an analytic form that represents the structure of brain activity patterns.

During the first step, a symbolic regression method was implemented to the entire EEG data sets of imaginary and real movements to find the models of the signals in analytical form. The obtained models indicate the significance of the sinus component in the regression analysis of bioelectrical signals.

The regression model of the complicated time series was evaluated in the form of the sum of sinus functions. The resulting coefficients were used as classifying features or as indicators for selecting an optimal time window size.

The optimal regression model structure and coefficients were used as indicators for the selection of the time window size for feature calculation. The time window must have a size that is sufficiently large to contain a significant image (at least two local extrema of the superposition of eight sinus functions) and sufficiently small to isolate features of this image.

Time windows are required for characterizing the time localization of EEG features. In this study, feature space was generated exclusively within the time domain. This approach was required to isolate the time intervals of the signal, which contributed (more than the other parameters) to the classification accuracy. In this manner, an individual approach can be applied for feature generation within the time domain for each subject.

The classification accuracy of a complicated time series, such as EEG signals, depends on the combination of the generated features and the applied classifier. To perform a comparative analysis of various approaches, two variants of feature generation (single-trial and accumulated-trial) and two classifiers (ANN and SVM) were applied.

Trial accumulation was generated by summing several trials of the same type, which increased the signal-to-noise ratio (weakened the signal). Therefore, this operation required the extraction of a weak signal. The approach was based on the facts that the evoked potential was repeated during the trial series and that the mathematical expectation of the background signal spread converges to zero. In this study, several variants of trial accumulation were applied (accumulations of 3, 5, 10, 20 trials, as well as a single-trial approach). To this end, it should be noted that trial accumulation exhibits regression properties, which level the individual features of a signal and leave only a common trend.

The features for transfer to the classifier were calculated in three steps. In the first step, the pre-processed signals of the C3 and Cz sites, which accumulated according to the variant of the experiment, are divided into overlapping 160ms time windows. The size of the overlapping windows was equal to half that of the time window. In the second step, the area under the curve of the signals in each window is calculated. In the third step, the obtained results are concatenated into one feature vector, which is transferred to the classifier. The first part of the vector was formed with the features of the C3 site signal, and the second part was formed with the features of the Cz site signal.

Currently, ANNs are widely implemented for practical analysis in statistics and signal processing. ANNs are based on the principle of non-linear, distributed, parallel and local processing and adaptation.

Our study employed ANNs with the backward propagation of errors, one input layer, two hidden layers and one output layer. The sigmoid (hyperbolic tangent) function was used as an activation function for the neurons in the hidden layers, and a linear function was used for neurons in the output layer. ANN topology with two hidden layers was chosen by taking into account the large number of EEG examples that must be accumulated into an associative memory of the network. However, according to Kolmogorov's theorem, we know that every function of n variables can be described as the superposition of 2n
                           +1 one-dimensional functions. Consequently, it is not necessary to use more than 2n
                           +1 hidden elements, where n is the number of input elements. In our case, the input vector consists of features calculated during overlapping time windows in two channels. Thus, we tried to strike a balance between the generalization capability and the goal of taking into account the maximum possible number of curve features.

The learning process was performed until the assigned classification accuracy was achieved for the learning sample. The ANN training process was carried out once for each type of movement and for each number of accumulations. The summarized results demonstrated the applicability of the method for complicated time series classification.

The original SVM algorithm was invented by Vapnik in 1963 and then further developed with the help of Cortes in 1995 [37]. The SVM was proposed to be a method of linear classification that constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can then be used for classification and regression. The optimal hyperplane can be described by the following equation: f(x)=(ω, φ(x))+
                           b, where 
                              
                                 ω
                                 =
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    N
                                 
                                 
                                    
                                       λ
                                       i
                                    
                                    
                                       y
                                       i
                                    
                                    φ
                                    (
                                    
                                       x
                                       i
                                    
                                    )
                                 
                              
                           , where the coefficient λ
                           
                              i
                            depends on a vector of classes y
                           
                              i
                            and on the scalar products (φ(x
                           
                              i
                           ), φ(x
                           
                              j
                           )). Consequently, it is necessary to know the values of the scalar products to identify the decision function.

Data transformations depend on the kernel function: K(x, y)=(φ(x), φ(y)). When considering linear classifications, the kernel function is 
                              
                                 K
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 )
                                 =
                                 
                                    x
                                    i
                                    T
                                 
                                 
                                    x
                                    j
                                 
                              
                           .

However, a method to create nonlinear classifiers by applying the kernel function was proposed in 1992 [38]. The linear kernel function can be replaced by a nonlinear kernel function. This step allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space.

In our study, a Gaussian radial basis function was used as the kernel. The corresponding feature space was a Hilbert space of infinite dimensions.

The Gaussian radial basis function is indicated by the following: 
                              
                                 K
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 )
                                 =
                                 exp
                                 (
                                 −
                                 γ
                                 
                                    
                                       
                                          
                                             
                                                x
                                                i
                                             
                                             −
                                             
                                                x
                                                j
                                             
                                          
                                       
                                    
                                    2
                                 
                                 )
                              
                           , for γ
                           >0.

In our work, the SVM classifier was implemented using the LIBSVM package [39]. The regularization parameter and γ were chosen using a “grid-search” with a cross-validation method. The “one-against-one” approach was implemented to realize multiclass classification.

Classifiers based on the SVM have become the gold standard in brain activity pattern classification. The main goal of the SVM classifier application in this study was to compare its classifying accuracy with the results of other research groups and with that of the ANN classifier.

@&#RESULTS@&#

In this section, behavioral data are presented first, followed by the data for decoding the accuracy of EEG signals using ANN and SVM, as presented for one session and several sessions of imaginary and real movements. We also explored the decoding accuracy of ANN and SVM classifications in different time windows of imaginary and real movement processes.

Analysis of the voluntary latency to button press demonstrated that all subjects used the full 1600ms interval, 450ms after the stimulus onset (750ms after the start of trial), for their real button press. The minimum time to voluntary button press was exhibited by subject (subj) 8 with the index finger and occurred approximately 750ms after the stimulus onset (1050ms after the start of trial). The averaged data for all trials across various sessions for each subject are presented in Fig. 5
                        . The Wilcoxon matched pairs test revealed significant differences (p
                        <0.05) between real button presses with the thumb and index finger for subjs 4, 5, 7, 8. The average response times in the repeated sessions are also presented in Fig. 5.

The high variability in the voluntary response times in our study is indicative of the real conditions that result when a subject voluntarily decides to press the button or imagine the movement. The time interval selected for analyses included either all or part of the stages of motor action preparation and fulfillment (planning of movement, readiness potential, motor potential and post-motor positivity).

The participants carried out one hundred trials of each type within each session: real button pressing with the thumb/index finger and imagined button pressing with the thumb/index finger, both of the right hand. The trials that contained artifacts were automatically excluded from the analysis. In total, 60% of the remaining trials were included in the training sample, and 40% were included in the test sample. Single trials and trial accumulations (3, 5, 10, 20 trials) were obtained from non-interacted training and test samples via bootstrap sampling. The decoding accuracy results according to the ANN and SVM classifiers are presented in Table 1
                           . Each value was obtained by averaging 20 independent tests.

Thus, the sample size was equal to 800, and the confidence limits of chance that result in a four-class problem for a significance level of α
                           =1% are 22.1–28.5%. Each value was obtained by averaging 20 independent tests.

Analysis of variance (repeated-measures ANOVA) revealed a significant effect for the factor “number of trials” (5) on the percent of true recognition for both classification algorithms with opposite effects: decreased classification according to ANN (F[4,24]=8.1, p
                           <0.01) and increased classification according to SVM (F[4,24]=6.4, p
                           <0.01). The participants were divided into two subgroups based on their response time stability: stable participants with no differences between their thumb and index finger response times (4 subjects) and unstable participants with significant differences between their thumb and index finger response times (4 subjects). The grouping factor “stability” (2 level) had no effect on the decoding accuracy for either approach.

Estimation of the nonstationarity effect was performed for two subjects who participated in four repeated EEG sessions (the time interval between sessions was approximately 7 days) that involved the same tasks of real and imagined movements. In this case, all four sessions for each subject were merged into a common data pool. Ultimately, 60% of the trials were used as the training set, and 40% were used as the testing set. The ANN and SVM classification results are presented in Table 2
                            and Fig. 6
                           .

To estimate the information values of the EEG signals within narrow time windows, the common time interval was divided into 20 time windows (80ms long) and then used for ANN and SVM classification during single-trial and accumulated trial approaches. Table 3
                         presents the individual results of ANN and SVM classification with the single-trial approach. Table 4
                         presents the individual results of ANN and SVM classification for ten accumulated trials. The averaged values obtained during 10 tests are presented for each subject.

@&#DISCUSSION@&#

In search of a non-invasive and effective BCI approach, we explored real and imagined voluntary finger movements and the possibility of performing EEG data discrimination using ANN and SVM classification. The study revealed the decoding accuracy of various discrimination algorithms and the effects of various conditions on EEG signal classification.

The high variability of voluntary response times in our study accurately represented real conditions in which an individual voluntarily decides to press a button or imagine a movement. The participants were instructed to press the button at any time. Following this instruction, we observed that some participants were rather stable, such as subjs 2 or 3 and did not exhibit any significant differences in their button-pressing times with their thumb or index finger. There were also no significant differences (Fig. 5) in the response times between the two sessions in these subjects. Other participants (subjs 5, 7, 8) pressed the button with each finger with markedly different response times. The behavior of these subjects between the two EEG sessions was also not consistent. When more sessions were included, some participants with previously stable response times changed their behavior and increased their time responses in the following sessions. The reverse was also observed; during the last session, a participant (subj 5) who had previously been non-stable over the three previous EEG sessions began to press the button with his thumb and index finger with markedly similar response times. These observations could be explained by changes of the functional state of these subjects during various sessions that occurred over several days. Under real-time conditions, it is difficult to predict physiological events without any additional information or synchronization stimuli. For this reason, when there was a high variability in response times, we considered wide intervals (1600ms) for signal discrimination. The following data discrimination analysis confirmed that this approach was reasonable.

The time window selected for the analyses included either all or part of the stages of motor action preparation and fulfillment: the planning of movement, readiness potential, motor potential and post-motor positivity. Despite the lack of exact time synchronization of the button press through instruction, the time course of motor preparation and fulfillment (for imagined movement as well) includes consecutive stages that can be identified through a time-line analysis. It must be noted that the extent of distinction between motor and imagined motor signals vary within the subjects, with regard to both time and spatial domains, and therefore should be considered individually.

Our main results show that the accuracy of the ANN classifier in one session approach was higher than the upper confidence limits of a chance level in recognition of a single trial and of 10- and 20-trial accumulations. The SVM classifier demonstrated an accuracy higher than the upper confidence limits of a chance level in all types of accumulation in the one-session approach.

The applied classifiers yielded distinct results, which are shown in Fig. 7
                        . The ANN classification algorithm provides the best decoding accuracy with single-trial classification (average value: 38%, optimal rate: 42%). The true recognition rate was considerably decreased with the use of accumulated trials (regardless of the number of accumulated trials), indicating that the decoding accuracy values of the single-trial approach were not achieved. The average decoding accuracy of SVM classification grows linearly with the number of summarized trials (average value of 45% with 20 trials; the best rate was 62%). We can assume that ANNs might be more sensitive to subtle characteristics of the signal, which diminish with signal summarization. These results demonstrate that the ANN algorithm performs better with single-trial classification, and the SVM algorithm exhibits a higher sensitivity for signal discrimination when using an accumulation of trials. It should also be mentioned that there was high variability in the response times of several participants (4, 5, 7, 8), whereas other participants (1, 2, 3, 6) exhibited stable response times during the real movement task of button pressing. This variability did not influence the accuracy of EEG activity discrimination using either the ANN or SVM algorithm. As mentioned above, the grouping factor “stability” (2) had no effect on the decoding accuracy of either approach.

As observed from individual graphs (Fig. 6), ANN classification gives less inter-subject dispersion of the decoding accuracy, especially with single-trial classification. Classification with the SVM algorithm exhibited more variability, and for certain subjects (subjs 5, 6 and 3), the decoding accuracy was reduced, regardless of the number of accumulated trials, whereas the data from these subjects were more successfully discriminated using the ANN algorithm. However, the ANN and SVM algorithms are each sensitive to distinct features of the EEG signal.

We can also hypothesize that for ANN classification, the decoding accuracy is associated with the high frequency component of the EEG signal, which diminishes with the accumulation of data. The SVM classifier is more sensitive to the low frequency component of EEG, which improves with data accumulation. To increase the accuracy of data discrimination for future studies, it could be advantageous to analyze the value of the low-frequency and high-frequency components of the signal for imagined movement tasks using both classification algorithms.

Our results are comparable with those obtained by [23] in the single-trial classification of brain activity elicited by real finger movements of one hand during non-invasive MEG and EEG recordings. That research group investigated the feasibility of decoding which among four fingers of one hand (the thumb, index, middle and little finger) performed a slight button press. The raw time series were digitally low-pass filtered at 0.15–16Hz and down-sampled to 32Hz. Additionally, the interval length was initially restricted to 500ms from −50 to 450ms around the button press. Consequently, each of the 500ms intervals used for single-trial classification included 3968 features (248 sensors by 16 samples) in the MEG data and 464 features (29 sensors by 16 samples) in the EEG data. The linear SVM was chosen to classify single finger movements because of the robustness of this method. The results demonstrated that the MEG data could be used to reliably discriminate a single button press performed with the thumb, index, middle or little finger (average performance over all subjects and fingers: 57%, best subject performance: 70%, empirical guessing level: 25.1%). The EEG decoding performance was less robust (average performance over all subjects and fingers: 43%, best subject performance: 54%, empirical guessing level: 25.1%) [23].

Similar accuracy between single-trial imagined and real finger movement classification was achieved in our study with ANN (average performance over all subjects: 38%) with data from two electrode positions. The individual spatial features of the EEG signal will be further explored. The higher accuracy obtained using the SVM classifier is associated with trial accumulation (average value of 45% with 20 trial accumulations; the best rate was 62%).

Comparative analysis of one session versus four combined sessions demonstrated significant differences between the two approaches. As shown in Table 2 and Fig. 7, the trial grouping of four sessions from subj 2 noticeably decreased the discrimination percent by both ANN and SVM algorithms. One session trial from subj 5 was characterized by a low decoding accuracy for both classifiers. In this case, the grouping of four sessions stabilized and slightly increased the average decoding accuracy over all the variants of accumulation. The analysis of four sessions demonstrated the insensibility to trial accumulation. This insensibility is most likely due to differences in the functional state of the subject among sessions. Therefore, it might be reasonable to train classifiers at the beginning of each session.

We hypothesize that one way to enhance BCIs is to identify individual informative time windows for each subject. Choosing these informative time windows could serve as a strategy for individual training tempo prior to the application of a BCI together with individual neurodynamic plasticity taken into account [40]. It also serve as a means of reducing the time needed for data accumulation. In this study, we attempted to search for informative time windows for each subject using the ANN and SVM algorithms.

The informative signal values in the narrow time windows vary from subject to subject as shown in Tables 3 and 4. The most informative time interval discrimination accuracy obtained for subjs 5 and 8 (time window from 5 to 8) was not informative for subjs 2 and 7. Subjs 2 and 7 had a much lower percent of correct discriminations within those time windows (for subjs 2 and 7, the informative time interval was bounded by windows 9 and 12). Data from the literature [41,42] support the idea of the highest decoding accuracy for individually chosen time windows. This finding was previously shown for long time periods (5s). In our study, the individuality of motor behaviors and imagined movements was shown for short time periods.

The informative windows for each subject with each classification paradigm (ANN and SVM with single-trial and accumulated-trial approaches) did not coincide (Table 3). The level of correct single-trial discrimination using both the ANN and SVM algorithms within the individual narrow windows were comparable with recognition of the wide time interval (for some subjects, the values were slightly higher). For the 10-trial accumulation approach, the level of true recognition using ANNs with narrow time intervals was comparable to the accuracy of the fully analyzed time interval. For the SVM classifier, the accuracy was slightly lower.

It is necessary to note that informative narrow time windows vary depending on the number of accumulated trials. The classification accuracy during a single trial can be higher in one time interval. With accumulated trials, the classification accuracy can be greater during various time intervals. It should be mentioned that using only the individual, narrow time interval, which is the fifth part of the whole time interval, increased the quality (up to 10%) of signal classification in several subjects (with various combinations of qualifiers and trial accumulations).

@&#CONCLUSIONS@&#

The present study explored the decoding accuracy of EEG signals using real and imagined movements of the thumb and index finger of one hand. The EEG patterns were analyzed using the symbolic regression method to generate automatic regression models in an analytic form. The time windows and feature types were chosen according to the results of the regression analysis.

The applied approaches of EEG signal discrimination demonstrated various classification accuracies with various sensitivities to data accumulation. The decoding accuracy of the SVM algorithm based on the Gaussian radial basis function linearly increased with trial accumulation (mean: 45%, max: 62% with 20 trial summarizations). The decoding accuracy of the ANN algorithm was higher for single-trial discrimination (mean: 38%, max: 42%). The empirical guessing level for both approaches was 25%.

It was also demonstrated that a high variability in response times did not influence the discrimination accuracy of the algorithms used. These results indicated the possibility of imagined and motor task discrimination using ANN and SVM classifiers and identified the strengths of each classifier.

Overall, the classification accuracy of the combination of imagined and real finger movements in our study is comparable to the results of exclusively real finger movements obtained in other EEG studies [23]. The next step will be to use a greater number of fine imagined movements in an attempt to increase the degrees of freedom for noninvasive BCI.

Another aim of this study was to isolate the time intervals of the signals that most contributed to the classification accuracy. Using individual, narrow time interval can reduce the duration of training and raise the quality of signal classification. Data analysis of individual, informative time intervals supports the idea of an individual approach, which we consider to be promising for signal analysis and BCI application.

The decoding of real and imagined movements of the fingers of one hand is a complicated problem. Because the signal-to-noise ratio of the signals is low, trial accumulation is required to improve the classification accuracy. In practice, it is likely that the subject will have to imagine a movement several times in a row to initiate the corresponding movement of the robotic arm or other real or virtual device. One possible implementation of the results of this study would be a rehabilitation system for subjects after a stroke or with disturbances of the movement system. Our study demonstrated that trial accumulation increases the classification accuracy; we therefore plan to use repeated imagined movements based on a block, possibly rhythmic, paradigm in the future. Repeated imaginary movements in the biofeedback paradigm are proposed to have a healing rehabilitation impact on the above-described subjects. The ability to identify and classify imagined movements based on individual, narrow time intervals is a promising approach for developing adaptive algorithms for BCI.

@&#ACKNOWLEDGEMENTS@&#

The study was supported by the RFBR foundation grant no. 13-01-12059 ofi-m.

@&#REFERENCES@&#

