@&#MAIN-TITLE@&#Applications of integrated human error identification techniques on the chemical cylinder change task

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A number of execution procedures and frameworks of HEI techniques were integrated to assess latent human errors.


                        
                        
                           
                           Some latent human errors and error mechanism of a chemical cylinder change task have been identified.


                        
                        
                           
                           The human error probability of the case system has been calculated.


                        
                        
                           
                           Some strategies to prevent latent human errors of the case system have been proposed.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Human errors

Human reliability analysis

Safety management

@&#ABSTRACT@&#


               
               
                  This paper outlines the human error identification (HEI) techniques that currently exist to assess latent human errors. Many formal error identification techniques have existed for years, but few have been validated to cover latent human error analysis in different domains. This study considers many possible error modes and influential factors, including external error modes, internal error modes, psychological error mechanisms, and performance shaping factors, and integrates several execution procedures and frameworks of HEI techniques. The case study in this research was the operational process of changing chemical cylinders in a factory. In addition, the integrated HEI method was used to assess the operational processes and the system's reliability. It was concluded that the integrated method is a valuable aid to develop much safer operational processes and can be used to predict human error rates on critical tasks in the plant.
               
            

@&#INTRODUCTION@&#

Since the industrial revolution, major industrial accidents have become more and more serious. The affected scopes have been broadened gradually due to the development of science and technology and the expansion of factories. Based on investigations of accidents during last few years, it was found that human error is still a critical contributing factor. The Major Accident Reporting System indicates that human error was responsible for 90% of accidents, most of which could have been prevented by management measures; thus, the importance of human factors in industrial safety and accident prevention is quite evident (Leva, 2005). Therefore, it is important to assess human reliability to ensure industrial safety.

With the progress of technology, the design of the human-machine system has become more and more complicated. In response to rapid and complex changes in nowadays work, the study of ergonomics hazards needs to be established (Niu, 2010). The response strategy to minimize the risk of human error is the extensive use of automated processes, taking into account capacity constraints and the security of the operators. Automation improves the performance of operations and reduces the workload of operators, and it has successfully terminated human error at the behavioral level. However, automation introduces other safety issues caused by human cognition and social context that are usually ignored (Cacciabue, 2004). Therefore, in order to make the assessment results more precise and complete in the process of evaluating a system's reliability, it is necessary to realize the human cognitive process in various contexts and in the assessment of the system's design.

Currently, the human error identification (HEI) technique is used extensively to analyze human operational errors. Even though these techniques have been available for decades and there are many methods for assessing human errors, they are not universal for each domain because the development process must match different industrial characteristics, e.g., nuclear power plants, aviation, and the traditional chemical industry. Baysari et al. (2011) have modified one of the HEI tool to include more systemic performance factors contributing to incident occurrence for Australian rail. Since the priorities and objectives of the analysis performed in each method are not entirely the same, the external behavioral responses of operators and the associated psychological reaction mechanisms are not completely understood. Thus, the objectives of this research are as follows:
                        
                           (i)
                           To integrate the existing HEI methods with the methods used in various domains.

To explore the logic of the analysis process to determine the pros and cons of each approach.

To develop an assessment process that can predict human errors more effectively using complementary approaches and by integrating the various HEI approaches.


                     Sheridan (2008) concluded that zero risk is not achievable, but a thorough analysis is bound to effectively make the system safer.

The main objectives of this research were to examine the operational process of changing chemical cylinders in a manufactory and to use the integrated analysis method of the HEI approaches to assess the operational processes and overall reliability of the system.

@&#LITERATURE REVIEW@&#

Human error may occur in any stage of human's information processing, but errors generally are found more frequently in the stages of decision making and executing various actions. In the processes of human-machine interaction, it is difficult to avoid human errors, especially when humans are under pressure or working in a noisy environment (Brigette and Peter, 2006). Therefore, the specific focuses of this research are the use of human factor engineering methods to explore the reasons for human error and the identification of latent human errors that may be easily overlooked.

By investigating and researching past accidents to determine the links between various conditions and situations associated with the accidents, we can identify possible human actions to prevent the recurrence of such accidents (Goossens and Hale, 1997). The Root Cause Analysis (RCA) method has been used to determine the most basic causal relationships associated with operational deviations, so it is useful in identifying the risks and the defects of tasks in order to prevent accidents in the future. However, studies of various systems sometimes tend to focus on the events and components that present the most significant hazards and ignore other minor situations that could also be hazardous (Shan et al., 2011).

Currently, the design of human-machine systems is becoming more and more complex, and the design and widespread use of automated systems have changed people's roles. People's roles have changed gradually from being the active operator and decision-maker to being passive overseers and policymakers of automated processes. This change means there has been a dramatic increase in the cognitive functions and organizational factors required to evaluate the systems effectively (Cacciabue, 2000). However, at the present time, safety assessment and analysis are still conducted using past concepts and methods. The result is that system failures are usually attributed to the operational design of the system, and efforts to prevent such failures involve increasing the protective measures of the machine and the environment. Therefore, the focus of the analysis is still on how to avoid the occurrence of failures rather than on exploring and correcting the causes of the failures. The basic causes of such failures are various environmental factors and the psychological mechanisms of the personnel involved. Thus, the current approach to the analysis of failures neglects many potential errors and may even underestimate latent errors.

Human error identification (HEI) methods are used to identify latent human or operational errors that may arise as a result of human-machine interactions in complex systems and to identify the casual factors, consequences, and recovery strategies associated with the errors (Stanton et al., 2005). The concept of HEI methods emphasizes the analysis and prediction of latent operational errors in human-machine interactions via the understanding of the characteristics of the task and the details of the actions the operators must implement (Dalijono et al., 2006).

HEI methods can be categorized into two types, i.e., qualitative and quantitative approaches. Qualitative approaches typically use the taxonomies of various error modes and apply these error modes to the analysis of the activity in question. Various such qualitative approaches exist, including the Systematic Human Error Reduction and Prediction Approach (SHERPA) (Embrey, 1986), the Human Error Template (HET) (Stanton et al., 2006), the Technique for the Retrospective and Predictive Analysis of Cognitive Errors (TRACEr) (Shorrock, 1997; Shorrock and Kirwan, 2002), and the Cognitive Reliability and Error Analysis Method (CREAM) (Hollnagel, 1998). All of these methods apply some specific error mode taxonomies of the domain to aid the analyst in identifying latent errors. Qualitative approaches are successful in terms of sensitivity, use limited resources, and are simpler and easier to apply than quantitative methods. However, they rely significantly on the subjective judgment of analysts, and, as a result, there are concerns about inter-analyst reliability and the intra-analyst reliability of error predictions (Stanton et al., 2005).

Quantitative methods are used to assign numerical probability values to the associated errors. One of these methods is the human error assessment and reduction technique (HEART) (Williams, 1986, 1988), which predicts and quantifies the likelihood of operational errors and system failure. The main advantage of quantitative methods is that they provide objective numerical data of the occurrence of errors, but they are difficult to use and may require more resources and extensive knowledge of mathematical procedures.

In addition to the two types of HEI methods discussed above, there are some other synthesis methods, such as the human error and recovery assessment framework (HERA) (Kirwan, 1998a, 1998b) that was developed for use with other HEI methods to identify more potential human errors. The summary of a few of the HEI methods that were reviewed is shown in Table 1
                        .

The objectives of this research, which are different from the toolkit concept of HERA, are to integrate the framework and the concepts of the above HEI methods and to collate the error modes determined from various domains.

This research integrates various current HEI techniques to develop a multi-dimensional and structural methodology and to determine a solution that would help operators avoid potential operational errors that they otherwise might make.

In this research, the analysis process was based primarily on implementing the SHERPA process. In addition, other analysis methods and error modes were imported from other HEI techniques to enhance the breadth and depth of the assessment. At the error-identification stage, the performance-shaping factor from CREAM have been included. It could help assessors to understand various factors that may influence operators in the work environment. Other error modes, such as external error modes from HERA, could expand the current error modes to comprehensively predict all of the human error phenomena that may occur. The internal error modes and psychological error mechanisms from TRACEr were used to discuss the human psychological responses that result in operational errors. The quantitative method from HEART was also imported and combined with human a reliability analysis event tree (Swain and Guttman, 1983) to calculate the success and failure rate of each operational error in the process and of the entire task at the probability analysis stage. Finally, it sorts the remedy method into four categories from SHERPA to obtain various improvement methods for effectively avoiding human errors and recovering from failures.


                        Fig. 1
                         shows the research process, including the integrated methods, and the details of the methods and steps followed in this study are presented below.

As shown in Fig. 2
                           , the preliminary steps involve selecting the task under analysis through the interview with operators and the reference of standard operating procedure(SOP) of the task. The analysis begins with the overall goal and all of the subordinate goals of the task. When the analyst approves the level of analysis, the next level can be undertaken. The hierarchical diagram provides the analyst with a useful overview of the entire task and helps analyst clarify the details of the operational process.

In this step, each bottom level task in the HTA is taken in turn and classified into the SHERPA behavior taxonomy, which includes five categories, i.e., action, retrieval, checking, selection, and the communication of information. Each type of action may cause different types of errors and the corresponding error modes. Therefore, task classification helps analyst to thoroughly understand the entire procedure of the system and to identify operational errors more efficiently in the human error identification step (Step 3).

In order to avoid the omission of possible operational errors and to identify the latent human errors in the entire operational process, each bottom level task from HTA is considered the credibility of each error mode. In this research, we integrated a variety of HEI techniques, derived the error modes applied in each technique, and sorted the dimensions of error identification into four categories, i.e., external error modes (EEMs), internal error modes (IEMs), psychological error mechanisms (PEMs), and performance shaping factors (PSFs).

The EEMs list various explicit errors of action in the operational process of the system. The detailed EEMs are summarized in Table 2
                           . The IEMs and PEMs were used to explore the causes of the operational errors and to identify the reasons for explicit errors from each step of the human information process model, including perception, memory, decision, and action steps. Different from the EEMs, the IEMs are focused on the internal causes. For instance, the EEMs only discuss the post-failure assessments, but the IEMs address the reasons for the response delay, i.e., it may be attributed to late detection, a bad decision, or other potential causes of the same explicit behavioral errors. The PEMs discuss in greater depth the incorrect behaviors caused by various kinds of psychological mechanisms. For instance, expectation bias may lead to cognitive errors when people retrieve information; too many messages can cause people to make incorrect decisions because of the high mental workload. Table 3
                            shows the collated IEMs and PEMs. The aim of the PSFs is the context that may affect people's judgments and performance in operating the system. Table 4
                            shows the detailed PSFs.

The analyst should determine the influences and consequences associated with each error identified in the previous step. This step could help the analyst comprehend the consequences of each of the operational errors, including casualties, property damage, and false alarms.

The quantitative analysis in this step is based on the HEART approach, which is a highly structured approach that quantifies human errors. It is combined with the structure and calculation of the human reliability event tree method to analyze the error rate associated with the execution of each step and the entire operational process. This helps the analyst identify the steps in the operational process that are associated with greater risk of error, and it serves as a reference for improvement. Fig. 3
                            shows the implementation framework of the quantitative analysis.

First, the analyst must assign the human unreliability to the major task procedure to identify the generic task categories to be analyzed. The identification should use the form of Nominal Human Unreliability (NHU) from the HEART approach, which is shown in Table 5
                           . The second step is to identify the Error-Producing Conditions (EPCs) associated with the task according to the context of the task and the descriptions of the on-site operation, and each EPC has a multiplicator to adjust its unreliability. The partial EPCs also are provided in Table 6
                           .

After the NHU and EPCs in the task are identified, the assessor must determine the assessed Proportion of Effect (PoA) of each of the EPCs to adjust subjectively the Human Error Probability (HEP) in the task, and the ratings in this research were obtained from a questionnaire and discussions with the engineers who were responsible for the system's equipment and the health and safety of the operators.

To minimize the bias associated with personal subjective judgments, in this research, we designed a questionnaire concerning the assessed proportion effect that was completed by engineers who had actual, operational experience. The results of the questionnaire were averaged and used as the PoA parameter in the calculation process. To calculate the probability of human error, the formula was as follows,
                              
                                 
                                    
                                       H
                                       E
                                       P
                                       =
                                       N
                                       H
                                       U
                                       ×
                                       ∏
                                       A
                                       v
                                       e
                                       r
                                       a
                                       g
                                       e
                                       P
                                       o
                                       A
                                       
                                          (
                                          
                                             E
                                             P
                                             C
                                             −
                                             1
                                          
                                          )
                                       
                                       +
                                       1
                                    
                                 
                              
                           
                        

The subjective judgment term in this formula differs from that used in the original HEART approach. The results of the questionnaire were provided to the engineers for assessment, and the original PoA was replaced by the average PoA.

The calculation results were combined with human reliability analysis and event tree analysis to assess the probability of human error and the probability of success of the entire operational procedure. The formula for calculating the probability of success of the entire task is:
                              
                                 
                                    
                                       
                                          S
                                          
                                             w
                                             h
                                             o
                                             l
                                             e
                                             t
                                             a
                                             s
                                             k
                                          
                                       
                                       =
                                       ∏
                                       
                                          S
                                          
                                             e
                                             a
                                             c
                                             h
                                             s
                                             t
                                             e
                                             p
                                          
                                       
                                    
                                 
                              
                           
                        

Concerning the design of the questionnaire, some of the descriptions of the EPC were not easily understood by the engineers, and these kinds of EPCs were reworded to make them easier to understand. The responses to the questionnaire were provided on a 10-point rating scale so the engineers could provide their feedback quickly and easily. An example of the questionnaire related to the assessed proportion effect is provided in Appendix I.

The final stage was to propose error reduction strategies and prevention recommendations. According to the SHERPA method, the remedial measures were proposed in four categories, i.e., equipment, training, procedures, and organization. There is more than one remedial measure to prevent each error, which enables the system's fault-tolerant features to enhance its reliability and provides an opportunity for managers to select the most appropriate remedy considering economic and operational factors.

The object of this research was the DEZ supply system in a case manufactory. DEZ (chemical formula: (C2H5)2Zn) is a chemical that was used in the manufacturing process in the case plant. At ambient conditions, DEZ is transparent liquid that undergoes spontaneous combustion upon contact with air and reacts strongly with water. Thus, it is apparent that DEZ could cause a fire or an explosion if it leaked (Fthenakis, 2003). Because of DEZ's hazardous properties, most of the operations and actions in the system were designed for automatic execution and control. However, operators still were assigned the tasks of changing the cylinders and maintaining the pipelines and valves in the system. Nevertheless, the reliability of the predicted results was often overestimated because the frequency of maintenance and the probability of a system failure were both very low. The task of changing the cylinders, which was executed routinely, had the potential for leakage of the hazardous chemical (Chen et al., 2006). So, this task had a greater risk of creating a hazardous situation than other tasks in the system, such as monitoring the state of the system.

The task of changing the cylinders usually included the following steps, i.e., 1) turning off the supply valve of the empty cylinder, 2) eliminating the remaining DEZ from the supply pipelines by purging the pipelines with an inert gas, such as nitrogen, 3) switching the supply line to a new cylinder or a buffer cylinder to maintain the supply of DEZ, 4) taking the empty cylinder apart. These steps require the direct involvement of people, so they are most prone to human error. After the installation has been completed, the process pipelines must be purged with inert gas to ensure that there are no impurities present that could cause hazardous chemical reactions or adversely affect the yield rate. This step can also be used to determine if there are leaks in the pipelines and valves by observing any variation in the pressure of the inert gas in the pipeline. After the above steps are completed, the new cylinders can used to supply DEZ to the plant.

In order to reduce the possible hazard caused by DEZ, the DEZ supply system of the case plant was located in a nearby building that was divided into three separate areas, i.e., the system control area, the DEZ supply area, and the cylinder storage area. The control touch screen is in the system control area, and the supply cylinders and other supply pipelines are in the cylinder supply area. The empty cylinder and the new, unused cylinders are placed in the cylinder storage area. In general, people who are involved in the daily inspection and operation of the facility remain in the system control area to monitor the state of the system. Staff members are required to enter the cylinder supplying area for observation or operation only when the cylinders are to be changed or some other operation is required that cannot be done from the control touch panel.

The purposes of this study were to assess each kind of latent human errors in the above procedures, to analyze the causes of the errors, and to develop corresponding recommendations for improvements that can minimize the likelihood of errors.

@&#RESULTS@&#

The task of changing out all of the cylinders in the chemical supply system in the case manufactory were divided into 18 steps. Then, the 18 steps were divided into 55 bottom-level subtasks by HTA, e.g., the “vessels and cylinder depressurization” step could be broken down into “click cylinder figure,” “click vessel exchange,” and “click start.” After eliminating some of the simple subtask that would not produce errors, such as the “movement of personnel” and “removing personal protective equipment”, 40 subtasks remained. Fig. 4
                      shows the partial results of the HTA.

Using SHERPA's taxonomy classification to categorize the 40 subtasks, it was determined that action accounted for 35 of the subtasks, and some of the subtasks were categorized in two classifications. The results showed the highly automated design of the system, and most of the subtasks could be completed only if the operator confirmed the state of system and implemented the action that the system indicated.

In the human error identification step, the errors in each subtask can be organized into a form, and the partial analysis process is shown in Appendix II.

Overall, according to the collation of Appendix II, the major error in the external error modes was omission, which occurred in 27 out of 40 (67.5%) of the subtasks. Either system's signs were not obvious, lack of experience, or lack of a checklist may result in omission. The selection error occurred due to the mismatching of action and objects.

Among the internal error modes, poor decision (45.0%) was the most likely error. Many psychological error mechanisms, including expectation bias, perceptual discrimination failure, similarity interference, insufficient learning and risk recognition failure, may result in incorrect decisions. Misperception (42.5%) was a latent error that is often neglected. Due to causes such as distraction, expectation bias, and perceptual discrimination failure, the operators may have an incorrect perception of some markers, signs, or characteristics and operate with the incorrect object. Compared with poor decisions, misperception is a non-subjective error, and it is difficult to eliminate.

Psychological error mechanisms are the most basic reasons of for the occurrence of errors, and incorrect knowledge (95.0%) was the most likely error. It was assumed that this was because there was no complete training course provided, and the staff had to learn by observation. This resulted in the operators not having adequate knowledge to deal with the various kinds of unexpected situations. It was learned from internal error modes that many errors were caused by distraction (70.0%), and distraction was caused by a variety of environmental factors or influenced by the mental state of the operators.

The greatest impact on performance shaping factors was the procedure factor (90.0%). There was no record of the site conditions and checking of the operational steps when performing a task; rather, there was only one manual in which to look up standard operating procedures, so the operators could easily forget important values or steps.

The secondary factor was experience (77.5%). The execution frequency of the tasks in the plant was approximately 2–3 months, but the operators also had to perform similar operating procedures for other systems more once every 2–3 days. Thus, the operators were influenced by the intrusion of habit in the operation.

Due to the complexity and validation of the calculation and the need to avoid repeated calculations, only the main 18 operational procedures were calculated in the quantitative analysis, and the complete calculation results are shown in Table 7
                        . Since there were only a few engineers who had actual experience operating the system, the questionnaire was completed by three engineers to obtain the subjective assessment of the EPC. The final PoA was derived by averaging the assessments of the three engineers. The variance among the 3 engineers was not significant in this case. If there is an outlier, a discussion meeting has to be conducted to clarify the different opinions.

The results obtained by the HEART formula indicated that, among the 18 operational steps, the “taking out” and “connecting” of cylinders had the highest probability of human error. These two steps involved the most frequent human operations, and the operations were so complex that the operators needed experience to conduct them properly.

The probability of human error in each operational procedure was set in the event tree analysis (Fig. 5
                        ), and there was an 89% probability that the entire task could be executed successfully.

The results of human error identification indicated that the major errors in all operation procedures were attributed to adverse operational procedures and the lack of experience and training of the operators, leading to distraction and similarity interference of the operators and making it difficult for them to detect the tips and warnings of the system. The complexity of the operation task itself was also an obstacle that may increase the operator's mental workload and make inexperienced and untrained personnel more likely to make incorrect decisions. Moreover, the marks of the equipment were not ideal and may cause similarity interference and make personnel to operate on the wrong object.

To solve the above problems, the design of the system could be improved by simplifying the operational procedure and information to enhance the significance of the important information and warnings, such as using eye-catching colors and voice prompts.

Because there were no training sessions provided by the case plant, it is recommended that courses be provided to train at least some of the engineers so they could learn from each other and share supervisory responsibilities.

An independent checklist of the operating procedure should be prepared so the observer can record the operational progress and eliminate omissions. If the checklist could be reviewed by more than one operator, deliberate omissions due to underestimating risk could be prevented (Jou et al., 2009).

Finally, organization could enhance the veteran operators' awareness of the risk involved in the task so emergency response procedures could be established. In addition, the communication system between each unit and the staff should be more convenient and carefully designed to prevent information errors and omissions (Fthenakis, 2003). Organizational factors were also the most frequently cited factors in computer and information security (Kraemer and Carayon, 2007).

In the calculation of HEART, the human error probability increased substantially because the current system does not track progress or other significant effects on important information that could cause the signal-to-noise ratio to be too low to detect. Also, the operators may forget their previous action and other important information. Other issues, such as the lack of independent checks, unclear feedback, and insufficient experience of the operators, can severely affect human reliability in the calculation process. Therefore, it is certain that improvements in the system's interface, providing appropriate training for the staff, and implementing other recommendations would reduce the likelihood of human error. As concluded by Carayon et al. (2014), balancing the work system and workers adaptive role was the key principle for improving patient safety.

@&#DISCUSSION@&#


                     Kirwan (1992b) addressed many of the evaluation criteria for HEI techniques that allow the integrated HEI method to perform well in many aspects, i.e., comprehensiveness, consistency, theoretical validity, usefulness, resource usage, auditability, and acceptability. The comprehensive breadth of coverage of the integrated HEI dealt with all forms of errors. It used four dimensions to identify latent human errors in order to avoid neglecting, whereas others, such as SHERPA and HET, may only be designed specifically for behavioral levels. The outputs from SHERPA are the credible error modes and consequence associated with the errors, but SHERPA does not consider the cognitive components of error mechanisms (Salmon et al., 2002; Malik et al., 2003). Therefore, the initial results for the external errors in the task are promising, while more studies are needed to investigate the different phases (Harris et al., 2005).

A technique that is comprehensive and accurate only some of the time, or only with certain users, is of limited practical use because of its lack of consistency. Therefore, the quantitative analysis in the integrated HEI method, unlike the original HEART (Kirwan, 1994), was assessed by more than one analyst to avoid the influence of different personal judgments, thereby increasing the inter-rater reliability.

The integrated HEI is easier to learn than HERA and CREAM, so it is more useful. Its framework is intuitional and luculent, and no more tools are required to use it. Kirwan (1992a) indicated that such techniques depend, to a large extent, the experts who utilize them have a good understanding of human error. Stanton et al. (2006) also pointed out that using HEI methods is perhaps somewhat difficult for analysts who have little or no experience with a task to make definitive judgments on the probability of an error or its ultimate criticality. When using the integrated HEI method, the analysts only need to know the association among the error modes and understand the psychological mechanisms associated with the errors.

From the perspective of resource usage, the integrated HEI is a comprehensive method of other tools, such as HERA, but it is not a tool-kit that contains all of the tools required to make the analysis more complete. Thus, a HERA team would be needed, and such a team would require a mixed group, made up of operators, human factor specialists, and engineers (Kirwan, 1996, 1997a,b). It is difficult to see how the HERA approach could perform better than the approaches that are far simpler to use and provide results more quickly (Stanton et al., 2005). Unlike HERA, the integrated HEI directly integrates the structure and advantage of various tools rather than importing each kind of tool needed to analyze a task, thereby eliminating a complex, tedious process.

Concerning auditability, the more information one has on the predicted causes of error and their causative mechanisms, the easier it is to understand and evaluate someone else's assessment (Kirwan, 1992b). In this research, the integrated HEI provided more complete causal factors. As to the single error mode-based methods, they would tend to be opaque to future assessors, and so assessments might have to be repeated completely rather than simply being revised or updated (Kirwan, 1998a).

The acceptability of error identification is perhaps the most difficult part for assessors, and they will choose a technique with which they are comfortable. The integrated HEI in this research is intuitional, the results are easy to explain, and its output shows the credible errors of each step and the casual factors.

The integrated HEI in this research was designed for more extensive applications in various domains, and thus the error modes were designed to cover all aspects. The results of its analysis are more complete and detailed, but it is bound to consume more time to execute other techniques that specialize in a specific domain, such as HET (Stanton et al., 2006). Because it imported a large number of error modes, it can determine the possible errors and the associated causes for the error of each task, but the presentation of results seems too multifarious. The assessor can focus on critical operation to make a complete assessment and propose improvement by reviewing the analysis results. However, when observing all the errors of the entire task at the same time and trying to propose a single solution, the lower frequency errors would be neglected, but they might be critical errors in the operation (Shan et al., 2011).

In the process of exploring the errors and causes, a variety of errors could be identified through four error modes. However, the causal relationship as well as dependence among the error modes were not very obvious, and thus the latent causes of some errors would still have to be confirmed. In addition, a large number of error modes and extensive discussion can avoid neglecting errors, but it often happens to discuss the same error repetitively, resulting in lengthening the assessment time.

The purpose of this research was to develop a general human error identification technique to assess the human errors in the operation of various systems in the long run. The research object was the task of changing the chemical cylinders in a case plant. The main conclusions of this research are:
                        
                           (i)
                           The integrated method is feasible; it integrated various kinds of the HEI method, including many qualitative and quantitative approaches, and its feasibility was verified via the case study.

The integrated method can effectively predict more human errors; it applies the same concepts and framework of other HEI methods to deal with the more complete range of latent human errors, and thus it could help in the early identification of errors.

The integrated method can be applied in many domains due to its integration of methods from various kinds of domains, thereby providing more effective information than other individual HEI methods.

The integrated HEI method can provide prevention measures; in this case study, it proposed kinds of prevention measures and suggestions for the case plant to reduce the probability of human error.

It is expected that the method will be developed further and validated in the future. Future developments may analyze and establish more definite causal relationships and dependence between errors and their causes in order to decrease the possibility of neglecting issues and diminish the influence of subjective judgments.

It may also be applied broadly in various domains. The practical examples of task analysis for the test case were limited to the propagation of reliability. Therefore, future development will explore the method's ideas further to analyze human errors for different systems and tasks.

Furthermore, the reliability of machines and components in the system were not considered. Future research may introduce the failure rates of various types of equipment and other unreliable components by taking into account both human and technical degradation factors, thereby establishing and applying a more complete and clear human error database to obtain the actual success rate of the task.

@&#ACKNOWLEDGMENT@&#

Partial of this study was supported by Advanced Manufacturing and Service Management Research Center at National Tsing Hua University through Toward World-Class Universities Project (Project Number: 100N2071E1 & 101N2071E1). The authors would like to express their appreciation to all of the members involved in this project.


                     
                        
                           
                        
                     
                  


                     
                        
                           
                              
                              
                              
                              
                              
                              
                              
                                 
                                    Task Step
                                    Classification
                                    Human error identification
                                 
                                 
                                    EEM
                                    IEM
                                    PEM
                                    PSF
                                 
                              
                              
                                 
                                    1. System state confirmation
                                 
                                 
                                    1.1Click alarm figure to enter alarm page
                                    A
                                    
                                       
                                          
                                             •
                                             Action too late
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Late detection
                                          
                                          
                                             •
                                             Forget to monitor
                                          
                                          
                                             •
                                             Poor decision
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Stimulus overload
                                          
                                          
                                             •
                                             Distraction
                                          
                                          
                                             •
                                             Incorrect knowledge
                                          
                                          
                                             •
                                             Infrequency bias
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Interface
                                          
                                          
                                             •
                                             Experience
                                          
                                          
                                             •
                                             Organization
                                          
                                          
                                             •
                                             Environment
                                          
                                       
                                    
                                 
                                 
                                    1.2Acknowledge all alarm messages
                                    C
                                    
                                       
                                          
                                             •
                                             Omits entire task step
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             No detection
                                          
                                          
                                             •
                                             Late detection
                                          
                                          
                                             •
                                             Misperception
                                          
                                          
                                             •
                                             Forget to monitor
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Stimulus overload
                                          
                                          
                                             •
                                             Distraction
                                          
                                          
                                             •
                                             Incorrect knowledge
                                          
                                          
                                             •
                                             Perceptual discrimination failure
                                          
                                          
                                             •
                                             Prioritisation failure
                                          
                                          
                                             •
                                             Habit intrusion
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Interface
                                          
                                          
                                             •
                                             Experience
                                          
                                          
                                             •
                                             Procedure
                                          
                                       
                                    
                                 
                                 
                                    2. System login
                                 
                                 
                                    2.1Click “Login”
                                    A
                                    
                                       
                                          
                                             •
                                             Omits entire task step
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Misrecall stored information
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Similarity interference
                                          
                                          
                                             •
                                             Memory capacity overload
                                          
                                          
                                             •
                                             Distraction
                                          
                                          
                                             •
                                             Infrequency bias
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Training
                                          
                                       
                                    
                                 
                                 
                                    3. Depressurize vessel and cylinder
                                 
                                 
                                    3.1Click DEZ cylinder figure
                                    A
                                    
                                       
                                          
                                             •
                                             Omits entire task step
                                          
                                          
                                             •
                                             Right action on wrong object
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Misperception
                                          
                                          
                                             •
                                             Misrecall stored information
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Perceptual discrimination failure
                                          
                                          
                                             •
                                             Similarity interference
                                          
                                          
                                             •
                                             Distraction
                                          
                                          
                                             •
                                             Lack of knowledge
                                          
                                          
                                             •
                                             Incorrect knowledge
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Interface
                                          
                                          
                                             •
                                             Training
                                          
                                          
                                             •
                                             Experience
                                          
                                       
                                    
                                 
                                 
                                    3.2Click “vessel exchange”
                                    S
                                    
                                       
                                          
                                             •
                                             Omits entire task step
                                          
                                          
                                             •
                                             Right action on wrong object
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Poor decision
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Perceptual discrimination failure
                                          
                                          
                                             •
                                             Similarity interference
                                          
                                          
                                             •
                                             Distraction
                                          
                                          
                                             •
                                             Stimulus overload
                                          
                                          
                                             •
                                             Infrequency bias
                                          
                                       
                                    
                                    
                                       
                                          
                                             •
                                             Interface
                                          
                                          
                                             •
                                             Training
                                          
                                          
                                             •
                                             Organization
                                          
                                          
                                             •
                                             Stress
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                  

@&#REFERENCES@&#

