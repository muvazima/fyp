@&#MAIN-TITLE@&#Towards optimal VLAD for human action recognition from still images

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Tackle the empty cavity issue by properly selecting reference points.


                        
                        
                           
                           Reveal codeword ambiguity by imbalance assignment and enhance the reliable codeword.


                        
                        
                           
                           Incorporate GMP into VLAD.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

VLAD

Empty cavity

Ambiguity

Generalized max pooling

Human activity recognition

@&#ABSTRACT@&#


               
               
                  Human action recognition from still image has recently drawn increasing attention in human behavior analysis and also poses great challenges due to the huge inter ambiguity and intra variability. Vector of locally aggregated descriptors (VLAD) has achieved state-of-the-art performance in many image classification tasks based on local features. The great success of VLAD is largely due to its high descriptive ability and computational efficiency. In this paper, towards optimal VLAD representations for human action recognition from still images, we improve VLAD by tackling three important issues including empty cavity, ambiguity and pooling strategies. The empty cavity limits the performance of VLAD and has long been overlooked. We investigate the empty cavity and provide an effective solution to deal with it, which improves the performance of VLAD; we enhance the codewords with middle level of assignments which are more reliable and can provide more useful information for realistic activity; we propose incorporating the generalized max pooling to replace sum pooling in VLAD, which is more reliable for the final representation. We have conducted extensive experiments on four widely-used benchmarks to validate the proposed method for human action recognition from still images. Our method produces competitive performance with state-of-the-art algorithms.
               
            

@&#INTRODUCTION@&#

Human action recognition from video can make the full advantage of structure and motion information [1–3]. Furthermore, together with temporal information, the system can be more robust to noisy background [4,5]. While the same task from still image poses even bigger challenges due to the limited useful information which deduces large intra-class variation and inter-class ambiguity.

Human action recognition from still images plays an important role in human behavior analysis. The bag-of-feature (BoF) [6,7] is one of the most effective framework in image and video representations based on local features. Recently, Fisher vector (FV) [8,9] and its non-probabilistic version, i.e., vector of locally aggregated descriptor (VLAD) [10,11] have been extensively used due to their high performances in different tasks including image retrieval, scene/object classification and human activity recognition [12–15].

BoF encodes local features by calculating the histogram of local feature descriptors on a pre-learned codebook, and the histogram can be viewed as an approximate of the distribution of local features [16]. The Fisher vector, which is derived from the Fisher kernel, describes a signal by gradient vectors calculated from its probability density function [9]. Normally, the probability density function is modeled by Gaussian mixture models (GMMs). In contrast to BoF using the hard assignment, FV with GMMs can be regarded as using soft assignment [17], which has shown better performance. Moreover, FV concerns not only on which codeword is activated but also on the gradient indicating the direction in which parameters should be adjusted to best fit the data. In contrary to FV, VLAD avoids the estimation of the complex GMM model which is extremely time-consuming and achieves competitive and even better performance than FV [9]. In this work, we will build our method on VLAD to take advantage of its high performance and computational efficiency.

BoF, FV and VLAD can be viewed in a unified framework which describes images with local features by two main components: codebook creation and feature encoding. Most researchers focus on these two aspects to generate good codebooks and achieve discriminative encodings. However, the empty cavity issue always happens in these methods and would compromise the performance significantly, which will be thoroughly investigated in this work.

As for codebook creation, apart from the traditional k-means clustering algorithm, hierarchical clustering [18] and spectral clustering [19] can also be applied, avoiding to determine the size of codebook beforehand. With recent prevalence of sparse coding [20], supervised[21–23] and unsupervised [24–26] codebook generation approaches on sparse coding are proposed. The main distinction among these approaches on sparse coding rests on the constraint conditions, such as locality-constrained linear coding (LLC) [27] limits the local descriptor only to be encoded by the codewords in its neighbor, and local coordinate coding (LCC) [25] adds the locality constraint from a different way.

With regard to assignment, extensive work including soft assignment [17], triangle assignment [28], localized soft assignment [29] has been conducted to avoid the limitation of hard assignment. Hard assignment assigns each local descriptor to its closest codeword. However, soft assignment describes a local descriptor by multiple codewords using a kernel function (e.g., Gaussian function) of the distance between local descriptor and codewords. Applying the distance kernel function involved multiple codewords may be the key reasons of the accuracy enhancement of probability density estimation [30] in soft assignment. Triangle assignment and localized soft assignment are kinds of tradeoff between hard and soft assignments. The former one only counts the distance to the codeword whose distance is closer than the mean of the distances to all codewords. Yet the latter one combines the idea of localization and the soft assignment, where only the distance to the codeword is meaningful if the local descriptor belongs to the k-nearest neighbors of this codeword.

However, no matter which approach is adopted to generate the codebook and how the local descriptors are assigned to codewords, the problem of empty cavity and imbalance of local descriptor assignments do exist. Although Ref. [31] discussed the negative effect of empty cavity in BoF model and Ref. [29] gave the analysis about ambiguity of codeword, the negative effects of empty cavity and the imbalance influence of assignment in VLAD have long been neglected.

Towards optimal VLAD, in this paper, we improve VLAD by dealing with three key issues in VLAD: empty cavity, ambiguity and pooling. We make the following three major contributions:


                     
                        
                           1)
                           By exploiting the negative effect of empty cavity theoretically and experimentally, we propose an effective method to tackle the empty cavity issue, which can significantly improve the performance.

By investigating the distribution of the number of assignment in each codeword in one image, and rebuilding the relations of the imbalance assignment and codeword ambiguity, we propose selecting the reliable codewords to enhance the weight of the pooled vector related to those codewords in VLAD.

We for the first time incorporate the generalized max pooling into the construction of VLAD, which shows better performance in ordinary VLAD using the sum pooling.

Human action recognition from still images has played increasing important role in human behavior analysis and it poses even more challenges than video-based action recognition. Before introducing our method, we would like to describe four challenging and widely-used image datasets that are used in this work. As shown in Fig. 1
                     , the most difficult factor in each dataset is listed. In PPMI, confusing examples of holding and playing instruments are the hardest factor to be distinguished, as shown in sub-figure (a). Similarly, for actions from the same class in Standford 40 Action, the poses and backgrounds are different such as brushing and cooking in sub-figure (b). Moreover, sub-figure (c) in Fig. 1 gives the examples of asmall object hard to be detected in UIUC sports, and similar pose and background examples cross-class in Sports 6 are listed in sub-figure (d).


                     
                        
                           •
                           PPMI [32] contains images of humans interacting with twelve different musical instruments. They are: bassoon, cello, clarinet, erhu, flute, French horn, guitar, harp, recorder, saxophone, trumpet and violin. For each instrument, there are images that contain a person playing the instrument (PPMI+), as well as images that contain a person holding the instrument without playing (PPMI−). Here we treat it as a classification task with 24 classes. We use 100 images in each class for training and the rest 100 images for testing.

Stanford 40 Action [33] contains images of 40 diverse daily human action, such as ‘brushing teeth’, ‘cleaning the floor’, ‘reading book’, and ‘throwing a frisbee’. All images are obtained from Google, Bing and Flickr. There are 9532 images in total with 180–300 images per action class. The images within each class have large variations in human pose, appearance and background clutter. We use 100 images in each class for training and all the rest images for testing as in Ref. [33].

UIUC sports [34] contains 8 sports categories collected from internet: bocce, croquet, polo, rowing, snowboarding, badminton, sailing and rock climbing. The number of images in each category varies from 137 (bocce) to 250 (rowing). The background of each image is highly cluttered and human poses largely diverse. Furthermore, as shown in Fig. 1, the foreground human poses are too small to be detected in some images. We follow the experimental setting in Ref. [35] by randomly selecting 70 images as training set and 60 images as testing set, respectively.

Sports 6 [36] contains images from six different sports as ‘tennis-forehand’, ‘tennis-serve’, ‘volleyball-smash’, ‘cricket-defensive-shot’, ‘cricket-bowling’ and ‘croquet-shot’ with 50 images per class. This dataset is with significant confusion due to similar pose in different action. For example, the poses in ‘volleyball-smash’ are with high similarity to those in ‘tennis-serve’ and the backgrounds from ‘tennis-forehand’ and ‘tennis-serve’ are exactly the same shown in Fig. 1. We use fixed 30 images in each class for training and the other 20 images for testing.

Empty cavity means during the assignment of local descriptors in one image, there are always some codewords with no local descriptor assigned to, leaving empty in the final representation. As a result, the obtained representation tends to be less discriminative and therefore compromises the recognition performance. The phenomenon of empty cavity does widely exist in both BoF and VLAD, which severely compromises the overall performance. To show the empty cavity problem, Fig. 2
                         gives the empty cavity rate for above four datasets, where the empty cavity rate means average number of empty cavity over training data divided by the size of codebook.

It can be seen that for some datasets, the empty cavity rate is extremely high. For example, the empty cavity rate in PPMI dataset is up to 19.36% with 512 of the codebook size, which means nearly 100 codewords with no local descriptor assigned to. Additionally, the ascending trend of the empty cavity rate with increasing of the codebook size is obvious in this figure. That means, if the codebook size is further increased as 1024 or 2048, the empty cavity phenomenon is much more serious.

Let {u
                           1,u
                           2,…,u
                           
                              K
                           } be the codebook learned by k-means. For one image X, each local descriptor x
                           
                              t
                            ∈ R
                           
                              D
                            is assigned to the nearest codeword as u
                           
                              i
                           
                           =
                           NN (x
                           
                              t
                           ). Moreover, the pooled vector
                           v
                           
                              i
                            for codeword i is computed by: 
                              
                                 (1)
                                 
                                    
                                       
                                          
                                             v
                                          
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                
                                                   x
                                                
                                             
                                             
                                                t
                                             
                                          
                                          :
                                          N
                                          N
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                      
                                                   
                                                   
                                                      t
                                                   
                                                
                                             
                                          
                                          =
                                          
                                             
                                                
                                                   u
                                                
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   x
                                                
                                             
                                             
                                                t
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   u
                                                
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           which is sum pooling strategy of the residual vectors, i.e., the subtraction of local descriptor x
                           
                              t
                            and its belonging codeword. Finally, K pooled vectors are concatenated as a single K
                           ×
                           D dimensional vector.

By deeply analyzing the pooled vector v
                           
                              i
                           , it can be seen that the distances of local descriptors to codewords are encoded. Thus the similarities relative to the codewords during matching are all incorporated which increases the accuracy of measuring the relationship between local descriptors. While for BoF, only the number of pairs of local descriptors assigned to the same codeword in two images is counted, with no consideration about the similarity of local descriptors. This is the main reason for the better performance of VLAD.

Traditional methods including the standard VLAD do not provide any way to handle empty cavity phenomenon, which means that the pooled vector
                           v
                           
                              i
                            is zero for empty cavity codeword. We will demonstrate the negative effect of this zero vector from the kernel aspect.

In both classification and retrieval tasks, the essential part is to compute the similarity between two images X and Y. Kernel is one way to fulfill this aim.

Let 
                              
                                 X
                              
                              =
                              {
                              
                                 
                                    
                                       
                                          
                                             
                                                x
                                             
                                             
                                                1
                                             
                                          
                                          ,
                                          …
                                          
                                             
                                                x
                                             
                                             
                                                D
                                             
                                          
                                       
                                       ︷
                                    
                                 
                                 
                                    
                                       
                                          
                                             X
                                          
                                       
                                       
                                          1
                                       
                                    
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    
                                       
                                          
                                             
                                                x
                                             
                                             
                                                D
                                                ×
                                                (
                                                K
                                                −
                                                1
                                                )
                                                +
                                                1
                                             
                                          
                                          ,
                                          …
                                          
                                             
                                                x
                                             
                                             
                                                D
                                                ×
                                                K
                                             
                                          
                                       
                                       ︷
                                    
                                 
                                 
                                    
                                       
                                          
                                             X
                                          
                                       
                                       
                                          K
                                       
                                    
                                 
                              
                              }
                            be the VLAD representations for image X and 
                              
                                 Y
                              
                              =
                              {
                              
                                 
                                    
                                       
                                          
                                             
                                                y
                                             
                                             
                                                1
                                             
                                          
                                          ,
                                          …
                                          
                                             
                                                y
                                             
                                             
                                                D
                                             
                                          
                                       
                                       ︷
                                    
                                 
                                 
                                    
                                       
                                          
                                             Y
                                          
                                       
                                       
                                          1
                                       
                                    
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    
                                       
                                          
                                             
                                                y
                                             
                                             
                                                D
                                                ×
                                                (
                                                K
                                                −
                                                1
                                                )
                                                +
                                                1
                                             
                                          
                                          ,
                                          …
                                          
                                             
                                                y
                                             
                                             
                                                D
                                                ×
                                                K
                                             
                                          
                                       
                                       ︷
                                    
                                 
                                 
                                    
                                       
                                          
                                             Y
                                          
                                       
                                       
                                          K
                                       
                                    
                                 
                              
                              }
                            is VLAD for image Y. Then we can kernelize the match between X and Y as: 
                              
                                 (2)
                                 
                                    K
                                    (
                                    
                                       X
                                    
                                    ,
                                    
                                    
                                       Y
                                    
                                    )
                                    =
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          K
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   X
                                                
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                          
                                             
                                                
                                                   Y
                                                
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           where K
                           
                              i
                            (X
                           
                              i
                           , Y
                           
                              i
                           ) presents thepart kernel in cluster i. The essence of Eq. (2) is to decompose the whole kernel function into several independent elements, and each element only focuses on the part generated by one cluster during VLAD.

Without loss of generality, we consider a linear kernel widely used for recognition, and therefore we have 
                              
                                 (3)
                                 
                                    K
                                    (
                                    
                                       X
                                    
                                    ,
                                    
                                    
                                       Y
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             X
                                          
                                       
                                       
                                          T
                                       
                                    
                                    
                                       Y
                                    
                                    =
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          
                                             X
                                          
                                       
                                       
                                          i
                                       
                                       
                                          T
                                       
                                    
                                    
                                       
                                          
                                             Y
                                          
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                           where X
                           
                              i
                            is part representation build on cluster i, namely, pooled vector above, and 
                              
                                 
                                    K
                                 
                                 
                                    i
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             X
                                          
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                    
                                       
                                          
                                             Y
                                          
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                              =
                              
                                 
                                    
                                       X
                                    
                                 
                                 
                                    i
                                 
                                 
                                    T
                                 
                              
                              
                                 
                                    
                                       Y
                                    
                                 
                                 
                                    i
                                 
                              
                           .

From Eq. (3), it is obvious that if X
                           
                              i
                            happens to be a zero vector caused by empty cavity in cluster i, and no matter what value Y
                           
                              i
                            is, the part kernel
                           K
                           
                              i
                            is the same. This kind of matching has misleading result under two conditions: 1) When the codeword is mutually missed in two images, i.e., both X
                           
                              i
                            and Y
                           
                              i
                            are zero vectors, the codeword that can give more information will receive higher weights in the similarity measurement. 2) When the codeword is missed in only one image, that is, either X
                           
                              i
                            or Y
                           
                              i
                            is a zero vector, the final similarity measurement should vary with non-zero vector left, rather than fixed zero.


                           Fig. 3
                            is part figure of the assignments for each codeword between two randomly selected images in the PPMI dataset, which shows the above two conditions. Firstly, it can be seen that the number of points from image 1 or image on the horizontal axis occupies a certain proportion, which means that the empty cavity is not an accidental phenomena. Secondly, the pairs in green ellipses express the condition of codeword jointly missed in two images, corresponding to zero assignment for those codewords. Additionally, the pairs in black rectangles present the condition that the empty cavity codeword index in images X and Y does not happen at the same location. For example, for the highest rectangle, the assignments of local descriptors are eight for image 2 and zero for image 1.

Another thing worth to notice is that the above two conditions of empty cavity do not happen occasionally.

In order to solve the empty cavity problem, a non-zero reference vector should be found for pooled vector v
                        
                           i
                         by intuition. To obtain a better insight into reference vector selection, Fig. 4
                         illustrates the procedure of VLAD and shows the meaning of a reference vector in our approach. In Fig. 4, the real blue arrows and black dashed arrows represent the residual vector as x
                        
                           t
                         −u
                        
                           i
                         in VLAD from two images. Moreover, the real red arrow and the dashed red arrows represent the pooled vectors from different images. For a certain codeword which local descriptors from both images are assigned to, as the dashed rectangles parts, the part kernel is to compute the inner product between point a and point b where the origin is the related codeword. However, there are also two other conditions for codewords as shown by dashed ellipses and dashed triangle in Fig. 4.

The dashed ellipse case is to show the mismatched empty cavity condition between two images (for example, for codeword i, where only two local descriptors in image 2 are assigned and no points in image 1 is in this cavity). The other case is for co-missing codeword j in both two images, which is shown by dashed triangle part. The standard VLAD is to neglect the effect of this codeword under these two conditions. No matter how many local descriptors in image 2 are assigned to the cluster i, the part kernel has no difference, which is zero. Similar, for the dashed triangle case, the part kernel is also zero. To fix this problem, we aim to find a point as the reference to keep the pooled vector nonzero. For empty cavity from distinct image, this reference point should keep unchanged. b
                        1, b
                        2 and b
                        3 are three strategies of reference point which is independent to different image itself. They are type I to type III cases to replace the pooled vector as follows. 
                           
                              1)
                              Type I: 
                                    
                                       (4)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         v
                                                      
                                                   
                                                   ^
                                                
                                             
                                             
                                                i
                                             
                                          
                                          =
                                          
                                             
                                                1
                                             
                                             
                                                K
                                             
                                          
                                          
                                             
                                                ∑
                                             
                                             
                                                k
                                                =
                                                1
                                             
                                             
                                                K
                                             
                                          
                                          
                                             
                                                
                                                   u
                                                
                                             
                                             
                                                k
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   u
                                                
                                             
                                             
                                                i
                                             
                                          
                                          .
                                       
                                    
                                 This is to treat the mean of all codewords (blue four-points star) as the reference point in Fig. 4. Then the pooled vector for image 1 is the vector starting from codeword i and ending to this blue four-points star. The main idea for this strategy is to find the point with highest probability as the reference point in R
                                 
                                    D
                                  space, since the average of all codewords is the real center of training data points.

Type II: 
                                    
                                       (5)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         v
                                                      
                                                   
                                                   ^
                                                
                                             
                                             
                                                i
                                             
                                          
                                          =
                                          
                                             
                                                1
                                             
                                             
                                                c
                                                a
                                                r
                                                d
                                                (
                                                S
                                                )
                                             
                                          
                                          
                                             
                                                ∑
                                             
                                             
                                                k
                                                ∈
                                                S
                                             
                                          
                                          
                                             
                                                
                                                   u
                                                
                                             
                                             
                                                k
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   u
                                                
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 where 
                                    S
                                  is the set that codeword is close to codeword i and at the same time, the number of points assigned to this codeword is smaller than a threshold. The nearby codewords with smaller assignments are similar to the codeword with no assignment. This is the case of the red arrow to five-points star in Fig. 4.

Type III: 
                                    
                                       (6)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         v
                                                      
                                                   
                                                   ^
                                                
                                             
                                             
                                                i
                                             
                                          
                                          =
                                          
                                             
                                                
                                                   u
                                                
                                             
                                             
                                                i
                                             
                                          
                                          .
                                       
                                    
                                 For this case, it just treats the codeword itself as the pooled vector if empty cavity happens.

Different from text retrieval systems, where the document vector space is discrete itself, the local descriptor space R
                     
                        D
                      is a continuous one. When we use limited visual words to represent the whole continuous space, there will be an ambiguity problem. The ambiguity between codewords will be influenced by the number of words in the codebook. On the one hand, a big size of codebook allows a rich selection of codewords, while the small vocabulary essentially leads to different image parts represented by the same vocabulary element with more serious ambiguity. On the other hand, big size of vocabulary will aggravate the sparse situation and increase computational complexity.

The ambiguity in codeword can be reflected in two-field. One is the misrepresentation situation, that is, the codeword could not represent the characteristic of local descriptors. The other case corresponds to the codewords with higher assignments, which is similar as the useless function word in document vector space. Although the function word as ‘is' or ‘are’ may happen in one document many times, it is still meaningless in semantic analysis for classification or recognition.

In this section, we aim to build the relations between the number of assignment and ambiguity, and then give the proper solutions to enhance the codeword with less ambiguity.


                        Fig. 5
                         presents the variation of local descriptor assignment number in the PPMI dataset, where left sub-figure and right sub-figure are from distinct categories. It can be seen that except several high peaks in both sub-figures, others are flat among images and different codeword indexes. It is also worth to notice that for different classes, the locations of peaks in Fig. 5 are different.

For the number of local descriptors assigned to different codewords, we divide the corresponding ranges into three parts: higher assignments as the peak or close to peak values in Fig. 5, lower assignments close to zero, and middle assignments which are neither higher nor lower parts.


                        Fig. 6
                         gives the location information of local descriptors with different number of assignments to one codeword. The blue circles in left sub-figure represent the locations of local descriptors in the cavity with highest assignments, and each red stars are the locations for those cavities with only one assignment. Moreover, stars of four different colors (pink, red, yellow and green) in right sub-figure in Fig. 6 show the locations of local descriptors in the cavity with middle level of assignments.

It is clear to see that the locations in the left sub-figure could not grasp the essence of the image. In fact, the blue circles in Fig. 6 express the burstness phenomenon, i.e., the property that a given visual element appears more times in an image than a statistically independent model would predict [37]. Since the visual word with burstness mostly provides the background information, it should be attenuated during later processing. These peak or near peak assignments correspond to the ambiguity situation similar to that function words in the document vector space.

However, the red stars carry similar indiscriminate information for recognition in the left sub-figure of Fig. 6, which is related to the misrepresentation of the ambiguity. It is interesting to find that this kind of one assignment appearance does not occasionally happen and may exist many times in one image (e.g., over 20 times in left sub-figure of Fig. 6). It also should be scaled down during recognition.

Moreover, most stars related to the middle level assignments in the right sub-figure of Fig. 6 can represent the key information which is crucial to the recognition performance. The information grasped by the middle level assignments should be further enhanced.

Burstness means that during assignment, some codewords are much more frequently selected than others. Inspired from text retrieval [38], most existing approaches use an inverse document frequency (idf) weighting scheme to alleviate the codeword effect which happens in all images. In fact, the traditional idf could not directly attenuate the burstness, especially when the size of codebook is large. In Fig. 5, it can be seen that the peaks do not happen at fixed locations along codeword index axis and image index axis, which means that the codeword with peak assignment in an image may have zero assignment in other images.

Moreover, Ref. [37] proposed the modified idf with the score normalization. The normalization factor is to sum the score over the descriptors in the same codeword in the image for intra-image burstness or to sum the score over all descriptors in all images for inter-image burstness. The shortcomings of this modified idf lie in two aspects: 1) The normalization factor is computed over all local descriptors in all images, which will increase the computational burden. 2) It is more suitable for a retrieval task rather than a recognition task, and it does not consider the situation of lower assignments problem.

For the unbalance assignment to each codeword, we propose to enhance the codewords with the middle level assignment in VLAD instead of penalizing the codewords with the higher or lower level assignments, since the middle level assignment has less ambiguity. In order to determine the range of middle level assignment, we assume the probability distribution function of the number of assignments as Gaussian distributions. Then the range can be determined by the mean and standard variance as follows: 
                           
                              (7)
                              
                                 
                                    range
                                 
                                 =
                                 
                                    
                                       max
                                       
                                          
                                             
                                                
                                                   c
                                                
                                                
                                                   1
                                                
                                             
                                             ,
                                             
                                             
                                                
                                                   N
                                                
                                                
                                                   μ
                                                
                                             
                                             −
                                             
                                                
                                                   k
                                                
                                                
                                                   1
                                                
                                             
                                             
                                                
                                                   N
                                                
                                                
                                                   σ
                                                
                                             
                                          
                                       
                                       ,
                                       
                                       min
                                       
                                          
                                             
                                                
                                                   c
                                                
                                                
                                                   2
                                                
                                             
                                             ,
                                             
                                             
                                                
                                                   N
                                                
                                                
                                                   μ
                                                
                                             
                                             +
                                             
                                                
                                                   k
                                                
                                                
                                                   2
                                                
                                             
                                             
                                                
                                                   N
                                                
                                                
                                                   σ
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where N
                        
                           μ
                         is the mean number of assignments in one image, and σ is corresponding standard variance. k
                        1, k
                        2
                        c
                        1 and c
                        2 are constants determined by cross validation in following experiments.

Then the pooled vector in VLAD is changed as: 
                           
                              (8)
                              
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                w
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                         
                                                         
                                                            t
                                                         
                                                      
                                                      :
                                                      N
                                                      N
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     x
                                                                  
                                                               
                                                               
                                                                  t
                                                               
                                                            
                                                         
                                                      
                                                      =
                                                      i
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                         
                                                         
                                                            t
                                                         
                                                      
                                                      −
                                                      
                                                         
                                                            
                                                               u
                                                            
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                   
                                                
                                                
                                                
                                                   if
                                                
                                                
                                                
                                                   Card
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     x
                                                                  
                                                               
                                                               
                                                                  t
                                                               
                                                            
                                                            :
                                                            N
                                                            N
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           x
                                                                        
                                                                     
                                                                     
                                                                        t
                                                                     
                                                                  
                                                               
                                                            
                                                            =
                                                            i
                                                         
                                                      
                                                   
                                                
                                                ∈
                                                
                                                   range
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                         
                                                         
                                                            t
                                                         
                                                      
                                                      :
                                                      N
                                                      N
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     x
                                                                  
                                                               
                                                               
                                                                  t
                                                               
                                                            
                                                         
                                                      
                                                      =
                                                      i
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                         
                                                         
                                                            t
                                                         
                                                      
                                                      −
                                                      
                                                         
                                                            
                                                               u
                                                            
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                   
                                                
                                                
                                                
                                                   if
                                                
                                                
                                                
                                                   Card
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     x
                                                                  
                                                               
                                                               
                                                                  t
                                                               
                                                            
                                                            :
                                                            N
                                                            N
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           x
                                                                        
                                                                     
                                                                     
                                                                        t
                                                                     
                                                                  
                                                               
                                                            
                                                            =
                                                            i
                                                         
                                                      
                                                   
                                                
                                                ∉
                                                
                                                   range
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where w is bigger than one, and we vary this weight from 1 to 2.4 with 0.2 step increment.

Pooling technique has played an important role in image representation based on local descriptors. Max pooling has shown its great advantages over other pooling strategies including average pooling and sum pooling. However, max pooling has not yet been investigated in VLAD due to non-trivial use in it. In this paper, we propose a new method to incorporate the newly proposed generalized max pooling strategy into VLAD.

It has been indicated in Ref. [39] that max pooling is particularly well-suited to the separation of features with high sparsity. However, the conventional max pooling cannot be directly incorporated into VLAD. We investigate and adopt the recently proposed generalized max pooling (GMP) [40] into VLAD, which shows improved performance for all the different tasks.

The generalized max pooling involves equalizing the similarity between local descriptors and the pooled representation, which is formulated as: 
                        
                           (9)
                           
                              
                                 
                                    φ
                                 
                                 
                                    t
                                 
                                 
                                    T
                                 
                              
                              
                                 
                                    φ
                                 
                                 
                                    g
                                    m
                                    p
                                 
                              
                              =
                              m
                           
                        
                     where φ
                     
                        t
                      is the local descriptor and φ
                     
                        gmp
                      is the vector after generalized max pooling. When applying this rule into VLAD and setting m
                     =1 since this constant has no influence for final classification, this equation is changed into: 
                        
                           (10)
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   x
                                                
                                             
                                             
                                                
                                                   i
                                                
                                             
                                             
                                                
                                                   t
                                                
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   u
                                                
                                             
                                             
                                                
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       T
                                    
                                 
                              
                              
                                 
                                    
                                       φ
                                    
                                 
                                 
                                    
                                       g
                                    
                                    
                                       m
                                    
                                    
                                       p
                                    
                                 
                              
                              =
                              
                                 1
                              
                              .
                           
                        
                     
                  

Expanding Eq. (10) into matrix form for cluster i, it can be rewritten as: 
                        
                           (11)
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                      
                                                   
                                                   
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                      
                                                         1
                                                      
                                                   
                                                
                                                −
                                                
                                                   
                                                      
                                                         u
                                                      
                                                   
                                                   
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                      
                                                   
                                                   
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                      
                                                         2
                                                      
                                                   
                                                
                                                −
                                                
                                                   
                                                      
                                                         u
                                                      
                                                   
                                                   
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                          
                                          ⋯
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                      
                                                   
                                                   
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               N
                                                            
                                                         
                                                         
                                                            
                                                               i
                                                            
                                                         
                                                         
                                                            
                                                               X
                                                            
                                                         
                                                      
                                                   
                                                
                                                −
                                                
                                                   
                                                      
                                                         u
                                                      
                                                   
                                                   
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       T
                                    
                                 
                              
                              
                              
                                 
                                    
                                       φ
                                    
                                 
                                 
                                    
                                       g
                                    
                                    
                                       m
                                    
                                    
                                       p
                                    
                                 
                              
                              =
                              
                                 
                                    
                                       R
                                    
                                 
                                 
                                    
                                       T
                                    
                                 
                              
                              
                                 
                                    
                                       φ
                                    
                                 
                                 
                                    
                                       g
                                    
                                    
                                       m
                                    
                                    
                                       p
                                    
                                 
                              
                              =
                              
                                 
                                    
                                       1
                                    
                                 
                                 
                                    
                                       
                                          
                                             N
                                          
                                       
                                       
                                          
                                             i
                                          
                                       
                                       
                                          
                                             X
                                          
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           
                              
                                 1
                              
                           
                           
                              
                                 
                                    N
                                 
                                 
                                    i
                                 
                                 
                                    X
                                 
                              
                           
                        
                      denotes the 
                        
                           
                              N
                           
                           
                              i
                           
                           
                              X
                           
                        
                      dimensional vector of all ones and R corresponds to the residual matrix. In practice, we can optimize the φ
                     
                        gmp
                      by following equation as: 
                        
                           (12)
                           
                              
                                 
                                    
                                       φ
                                    
                                 
                                 
                                    
                                       g
                                    
                                    
                                       m
                                    
                                    
                                       p
                                    
                                 
                              
                              =
                              arg
                              
                                 
                                    min
                                 
                                 
                                    
                                       φ
                                    
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   R
                                                
                                             
                                             
                                                
                                                   T
                                                
                                             
                                          
                                          
                                             
                                                
                                                   φ
                                                
                                             
                                             
                                                
                                                   g
                                                
                                                
                                                   m
                                                
                                                
                                                   p
                                                
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   1
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         N
                                                      
                                                   
                                                   
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                      
                                                         X
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       2
                                    
                                 
                              
                              +
                              
                                 λ
                              
                              
                                 
                                    
                                       
                                          
                                             φ
                                          
                                       
                                    
                                 
                                 
                                    
                                       2
                                    
                                 
                              
                              .
                           
                        
                     
                  

This is a ridge regression problem whose solution is 
                        
                           (13)
                           
                              
                                 
                                    
                                       φ
                                    
                                 
                                 
                                    
                                       g
                                    
                                    
                                       m
                                    
                                    
                                       p
                                    
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             R
                                          
                                          
                                             
                                                
                                                   R
                                                
                                             
                                             
                                                
                                                   T
                                                
                                             
                                          
                                          +
                                          
                                             λ
                                          
                                          
                                             I
                                          
                                       
                                    
                                 
                                 
                                    −
                                    
                                       1
                                    
                                 
                              
                              
                                 R
                              
                              
                                 
                                    
                                       1
                                    
                                 
                                 
                                    
                                       
                                          
                                             N
                                          
                                       
                                       
                                          
                                             i
                                          
                                       
                                       
                                          
                                             X
                                          
                                       
                                    
                                 
                              
                              .
                           
                        
                     
                  

The regularization parameter λ can be determined during experiments. When λ is large enough, it backs to sum pooling as 
                        
                           
                              φ
                           
                           
                              g
                              m
                              p
                           
                        
                        ≈
                        
                           R
                        
                        
                           
                              
                                 1
                              
                           
                           
                              
                                 
                                    N
                                 
                                 
                                    i
                                 
                                 
                                    X
                                 
                              
                           
                        
                        /
                        λ
                     .

@&#EXPERIMENTS AND RESULTS@&#

We have evaluated our method on the four challenging datasets extensively, provided in-depth analysis on the performance, and compared with existing algorithms on the datasets. The proposed method has significantly improved the original VLAD and largely outperformed state-of-the-art algorithms.

For local descriptors, we extract dense SIFT descriptors with 3×3 grids for the better performance of dense sampling on image classification than sparse interest points [41]. It has been shown that directly applying on raw SIFT leads to suboptimal performance. Therefore, we follow existing work [10] to conduct PCA on the raw SIFT to reduce the dimensionality to 100, which shows improved performance in previous work.

The codebook is learned by the k-means clustering algorithm with randomly selecting 20% training data from each class for computational efficiency. In most cases, the codebook size is selected as 512 unless specified.

For VLAD, L
                        2 normalization is conducted on the SIFT descriptor, pooled vectors and final aggregated vectors. It has been shown that L
                        2 normalization on SIFT is helpful for background suppression and accurate assignment in high dimensional space. Moreover, L
                        2 normalization on pooled vector can further alleviate the burstiness of visual words which heavily influences the final performance [37]. At last, L
                        2 normalization on final aggregated vector is applied for eliminating the magnitude influence among different images from distinct classes.

A linear SVM classifier [42] is adopted for final recognition and classification task to benchmark with the compared algorithms. The parameters k
                        1 and k
                        2 in Eq. (7) are varied from 1 with ± 0.2 steps. c
                        1 and c
                        2 are initially selected as {2,70} on the analysis of Fig. 5, and then are varied from the initial value with {±2,±5} steps respectively. The final value are set as 0.6, 1.8, 5 and 80 by cross validation in following experiments.

In this section we investigate the benefits of the proposed solutions to the empty cavity and ambiguity issues in codewords together with generalized max pooling both separately and jointly.

In Fig. 7
                           , we report the comparisons of different solutions to empty cavity on four datasets for (a) sum pooling and (b) GMP in VLAD and show the advantages of the proposed solutions over the standard VLAD.

Sub-figure (a) in Fig. 7 reflects the effectiveness of different solutions to empty cavity in standard VLAD. It can be observed that all types of solutions outperform the standard VLAD. The improvements are significant on UIUC sports and Sports 6 datasets. On PPMI and Stanford 40 Action datasets, although the enhancement is not as obvious as shown in top right sub-figure, there are still some effectiveness on overall performance. Secondly, it is noticed that the strategy with highest performance is different on these four datasets.

However, when GMP is applied to VLAD in sub-figure (b) of Fig. 7, the effectiveness of different solutions to empty cavity are not as obvious as those in sub-figure (a). As for the comparison of sum pooling and GMP, from the final performance, except the Sports 6 dataset, the performance on other three datasets is largely enhanced by GMP. The performance of GMP in Sports 6 dataset may be restricted by the limited training data. Additionally, comparing the first dark green bar in each sub-figure, the effect of GMP separately can be shown, where the improvements of GMP on Stanford 40 and UIUC are impressive.

We tune the weight from 1 to 2.4 with 0.2 step increment on four datasets. The tuning results are shown in Fig. 8
                            for sum pooling and Fig. 9
                            for GMP in VLAD. In fact, Fig. 8 gives the separate influence of ambiguity solution. The trend is flattened in the upper sub-figure, while there is some oscillating in the lower sub-figure in both Figs. 8 and 9. From these results, it is clear that the enhancement of the codewords with middle level assignment is effective by properly selecting the weights.

In order to compare with the state-of-the-art performance on these four datasets, in this section, both accuracy and mean average precision (MAP) are considered. According to the results in Fig. 9, we list the best performance of joint solution of proposed approach and the comparison to VLAD with sum pooling and GMP in Table 1
                           . We discuss the performance of joint solution of empty cavity and ambiguity together with GMP on the four datasets one by one.


                           
                              
                                 1)
                                 
                                    PPMI: No matter for accuracy or MAP, the improvement of joint solution over standard VLAD with sum pooling is more than 1%. While together with GMP pooling strategy, the enhancement is weakened, which is less than 0.5%. Additionally, GMP instead of sum pooling in standard VLAD can boost system performance on this dataset.


                                    Stanford 40 Action: Three observations can be made from the results on this dataset in Table 1. First, although the performance difference for joint solution is not so obvious as that on PPMI, there are still some improvements over VLAD regardless of pooling strategies. Second, effectiveness of GMP has no consistency in accuracy and MAP. Although it will improve the performance with respect to accuracy, there is a little decline for MAP. Last, it is worth to notice that the best MAP is achieved by combination of GMP and joint solution approach.


                                    UIUC sports: Among these four datasets, this is the most suitable one with the remarkable improvement of proposed scheme. The improvements of joint solution for both accuracy and MAP are more than 2.5% over VLAD with sum pooling strategy. On the contrary, the final performances descend about 0.5% in conjunction with GMP. The best result of accuracy is 88.12% and highest MAP achieves 88.94%.


                                    Sports 6: Since the limited training data in this dataset, GMP has no positive influence on this dataset. However, considering the empty cavity and ambiguity in codewords can enhance the performance compared with standard VLAD, and it can also compensate the degrade performance caused by GMP.

We compare our method with existing algorithms in Table 2
                        . It can be seen that our method produces a much better performance than most of the state-of-the-art algorithms. Our method has great superior performance on the PPMI and UIUC sports datasets and the competitive results on the Sports 6 and Stanford 40 Action datasets compared to other approaches.

@&#CONCLUSION@&#

We have presented an improved VLAD for human action recognition from still images based on local features. Towards optimal VLAD, we propose improving three key components in the construction of VLAD, which includes empty cavity, codeword ambiguity and generalized max pooling (GMP). We successfully tackled the empty cavity and codeword ambiguity problems and for the first time incorporated the generalized max pooling into VLAD. Our method significantly improved the performance of VLAD and achieved competitive and even better performance than that of state-of-the-art algorithms on four widely used benchmark datasets for human action recognition from still images.

@&#REFERENCES@&#

