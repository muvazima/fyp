@&#MAIN-TITLE@&#A computational framework for cancer response assessment based on oncological PET-CT scans

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Computational cancer evolution assessment from a pair of oncological PET-CT scans.


                        
                        
                           
                           Automatic PET tumor segmentation and decision making system proposal.


                        
                        
                           
                           Supervised learning framework with a novel multi modal feature set.


                        
                        
                           
                           Introduction to computer aided diagnosis tools in a nuclear medicine scenario.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Computer aided diagnosis

Nuclear medicine

Machine learning

Image processing

Quantitative analysis

@&#ABSTRACT@&#


               
               
                  In this work we present a comprehensive computational framework to help in the clinical assessment of cancer response from a pair of time consecutive oncological PET-CT scans. In this scenario, the design and implementation of a supervised machine learning system to predict and quantify cancer progression or response conditions by introducing a novel feature set that models the underlying clinical context is described. Performance results in 100 clinical cases (corresponding to 200 whole body PET-CT scans) in comparing expert-based visual analysis and classifier decision making show up to 70% accuracy within a completely automatic pipeline and 90% accuracy when providing the system with expert-guided PET tumor segmentation masks.
               
            

@&#INTRODUCTION@&#


                     18F-fluorodeoxyglucose (18F-FDG) positron emission tomography (PET) has become a standard imaging method for the staging, restaging, and monitoring of treatment response in a variety of tumors. By injecting the 18F-FDG (fluorodeoxyglucose) radiopharmaceutical to the patient, a metabolic activity volume, measured in SUV (standard uptake value [1]) units, is acquired. FDG-avid tumors such as lymphoma, sarcoma, breast cancer or ovarian cancer show higher than normal SUV values in non-physiological locations, leading to a more accurate diagnosis than MR (magnetic resonance) imaging or CT (computed tomography) in some oncological scenarios [2]. Current technology offers integrated PET-CT and more recently PET-MRI scanners, which provide co-registered PET and CT/MR scans of the patient [3].

This technology has proven especially useful in patient׳s global cancer response assessment [4,5], where a comparative analysis between two time consecutive whole body PET-CT scans can provide an accurate insight of the morphological and physiologic cancer evolution trends. Generally, nuclear medicine physicians assess a patient’s cancer progression or response (
                     Fig. 1) condition following a trained visual analysis of both scans.

By segmenting the tumor volume from the PET scans, changes in metabolic tumor volume (MTV) and its metabolic activity (typically modeled by its mean or maximum SUV values) have shown to be a valuable quantitative indicator of the cancer evolution stage, as described in Refs. [6,7]. However, these studies focus on a particular subset of cancer scenarios, where typically only one significant tumor lesion is analyzed, and changes in the tumor spread over time are not taken into consideration. Sampedro et al. recently showed in [8] that the cancer spread information should be taken into consideration in the quantitative analysis of the oncological state from PET scans.

Note that as obtaining an accurate expert-guided tumor segmentation of whole body PET scans is highly time-inefficient in the clinical day-to-day setting, sometimes a very rough approximation of the volume and activity of each tumor lesion is obtained by placing a user-variable radius sphere on top of the lesion and reading its diameter (commonly in mm) and the mean and maximum metabolic activity values (SUVmean, SUVmax) within the sphere’s volume.

In this work we introduce a computational framework for the analysis of the cancer time evolution based on two time consecutive PET-CT scans. The aim of this system is to aid in the decision making process regarding the cancer progression or response condition by providing supporting quantitative information from image analysis and machine learning techniques.

Despite being a highly challenging computational scenario (as shown in subsequent sections), nuclear medicine experts agree on the need of such a computational system that could provide objective and quantitative information to support the visual analysis, which is well-known to suffer from inter- and intra-observer variabilities [9].

To the best of our knowledge, the computational modeling of the described scenario has not been addressed by the scientific community. Although nuclear medicine software stations include several tools to carry out expert-guided PET segmentation and allow the direct superimposition of segmentation masks from one PET scan to another, they lack a comprehensive computational decision making and quantitative framework analogous to the one proposed in this work.

The rest of the paper is organized as follows. Section 2 describes the materials used for the validation of the proposed system. Section 3 presents an in-depth characterization of the methodology proposed for the implementation of the computational framework. Section 4 shows the results of the proposed system (using semi-automatic and automatic configurations) and discusses its possible applications in the clinical setting. Finally, Section 5 concludes the paper and points out some future work required to fully validate the proposed system in the clinical domain.

A total of 200 whole body FDG PET-CT scans were obtained from the Philips PET-CT Gemini TF machine located at the nuclear medicine department in the Hospital de Sant Pau (Barcelona, Spain). Each scan contains two co-registered volumes (PET and CT) in DICOM (Digital Imaging and Communications in Medicine) format. From the DICOM metadata, SUV values for PET voxels and HU (Hounsfield units [10]) values for CT voxels can be computed. A PET voxel corresponds to 64mm3 and a CT voxel to 2mm3. Volume dimensions are 144×144×Np for PET volumes and 512×512×Nc for CT volumes (Np and Nc being the number of slices for each volume). Np and Nc (ranging from 192 to 213 and 511 to 623, respectively) are related with a ratio of Nc/Np=2.66. However, the actual number of slices is dependent on the volume of interest selected by the acquisition technician, varying with the patient’s height and the anatomical limits of interest (typically either from neck to middle-thigh or from the top of the skull to the feet). The patient’s position during acquisition is also variable (mainly related to arm positioning).

These 200 scans correspond to 100 patients with Non-Hodgkin lymphoma or breast cancer (where the proposed framework would be of much clinical interest), each one having two time consecutive scans (T and T+1) with which oncological evolution is addressed. The time elapsed between scans is typically between 3 and 8 months, depending on external clinical factors such as the type of treatment received and other clinical and logistic variables.

The ground truth information regarding clinical condition for each case was given following a consensus of three independent nuclear medicine physicians (53 cancer progression and 47 cancer response conditions). The same consensus also agreed conceptually about the existence and location of tumor lesions in all PET scans. Subsequently, each physician segmented a random third of the scans, therefore providing the expert-guided tumoral segmentation masks of 200 PET scans. Note that in an analogous context, Sampedro et al. [11] showed good inter-observer segmentation overlap degree in segmenting oncological PET scans, and therefore this issue is not considered in this work.

The disease extent was variable across subjects, ranging from a single small tumor lesion to a highly spread metastatic tumor. However, the majority of the cases (approximately 80%) showed multiple tumor lesions (analogous to the scans in 
                     Figs. 1a and 2), where the decision making process becomes more difficult in clinical practice and automatic segmentation techniques would be valuable, as the semi-automatic approach would be highly time-consuming.

@&#METHODS@&#

In this section we first present the proposed computation model of the stated clinical scenario at a system level and then describe its implementation based on a supervised machine learning framework.

In general, there exist several clinical scenarios that may be present when analyzing the cancer evolution condition from a particular pair of time consecutive PET scans, which base cases are summarized in 
                        Fig. 3.

The most common cases are also the most intuitive, which are shown in Fig. 3a–c. As an example, the cancer progression shown in Fig. 1a can be modeled as a combination of the progression conditions in Fig. 3a, b and d, as in time T+1 the tumor lesions that were present at time T increased in size and metabolic activity and new lesions appeared throughout the subject’s body. Analogously, the cancer response condition shown in Fig. 1b can be associated with a combination of Fig. 3a and b conditions, as the tumor shrank in size and decreased in SUV value. Finally, examples of Fig. 3d and e conditions are shown in 
                        Fig. 4.

Once the clinical cases that are used to identify a cancer progression or response condition have been modeled, the clinical problem can be simplified to a pattern recognition problem where a binary decision has to be made, providing as input two time-consecutive PET scans from the same patient. 
                        Fig. 5 shows the block diagram of the proposed fully automatic computational system to address this problem.

Two modules are required to successfully identify the cancer evolution condition from a pair of PET-CT scans. First, the tumor segmentation masks need to be obtained by an Automatic Tumor Segmentation module which, as will be described in Section 3.2 represents a challenging computational problem and therefore can be identified as a performance bottleneck of the problem. Second, an automatic decision making module is responsible for recognizing the clinical patterns illustrated in Fig. 3 (or a combination of thereof) and output the predicted cancer evolution condition. Note that the incorporation of an additional module to quantify the “intensity or severity” (in terms yet to be defined) of a particular cancer progression or response would raise the potential clinical value of the system. However, the sole computational ability to accurately predict the type of cancer evolution represents a challenging enough problem to be addressed in this work.

Given the high complexity of the problem under consideration, the proposed system, albeit proving time-efficient and objective information, may produce a significant amount of incorrect results at any level (both within the ATS and ADM modules). Therefore, a semi-automatic approach where an expert may validate the process carried out by the computational system at any level may be a reasonable setting, as the accuracy of the overall framework would increase and the expert may receive clinically relevant quantitative information from the system modules.

In this subsection the implementation of the system blocks in Fig. 5 is presented. A key point to note is the high complexity of the whole system. On one hand, the ATS system should be able to discriminate between tumoral and physiological volumes within any whole body PET scan, which requires the correct identification of the heart, bladder, kidneys and brain among others, which can show substantial differences in morphology and intensity patterns between a healthy infant and an eldest person with an advanced oncological state. On the other hand, the ADM system can be faced with a huge variety of cancer evolution patterns, from which it is responsible for the identification of some of the conditions illustrated in Fig. 3. Furthermore, there is no general agreement on the magnitude of an SUV change to be considered as significant to model a progression or a response (Fig. 3b and c), that is, a small change in SUVmean between 1% and 10% may not be considered enough for discrimination. As the whole decision making process carried out by the medical experts relies on a combination of mainly visual analysis features, the implementation of a direct and analytical mathematical model would be too restrictive.

These properties of the computational setting lead to the adoption of a supervised machine learning based solution for both system blocks. An ATS system was already designed and implemented in Sampedro et al. [11] where the authors deal with all the specific challenges of the problem by building an ad-hoc PET voxel feature set that given enough training data can recognize most pathological and physiological patterns (from specific organ normal uptake to other phenomena such as muscular uptake, brown fat uptake or inflammation). In short, in [11] the authors proposed an ATS system derived from modeling a set of clinical facts that are related to the presence of tumor tissue in a computational PET voxel feature set and obtained an expert-guided training set of PET volumes which was used to train a supervised learning classifier capable of providing a tumor segmentation mask proposal from any given whole body PET scan. Note that this alterative offers conceptual advantages to the few other existing fully automatic whole body PET tumor segmentation techniques, which assume tumor homogeneity and “hot spot” structure [19]. In this work we will use this system as a black box module for the automatic extraction of the PET tumoral segmentation masks.

Therefore, the remaining portion of this subsection will present in depth the design and implementation of our ADM system. First, given that it will be based on a machine learning scheme, an ad-hoc feature set must be obtained from the ADM input data (both T and T+1 PET tumor segmentation masks). The main design goal of this feature set is to maximize its capacity to model appropriately the underlying scenario and therefore help the supervised learning algorithms to accurately detect the desired patterns and correctly discriminate between a cancer progression and response condition.

The chosen feature set is described next. On one hand, consider a base case with the presence of a single tumor lesion in both T and T+1 scans (Fig. 3a–c). In this scenario, changes in total tumor volume (ΔV
                        
                           T
                        ) and its SUVmean (ΔSUVmean) are probably the most relevant descriptors of the underlying phenomena. We have also considered the incorporation of the change in SUVmax (ΔSUVmax) as an appropriate descriptor, as it has proven to be a relevant indicator in the clinical literature [12] and at the computational level could account for the alteration of the ΔSUVmean indicator due to segmentation variations between both masks.

On the other hand, it is clear that additional descriptors are needed to model the cases illustrated in Fig. 3d and e. To do so, the multi-lesion scenario must be addressed. First, in order to do it within an image processing framework, the number of connected components (NCC) in the segmentation mask is used to model the number of tumor lesions present within the patient’s body at any time as described in [8]. Then, the change in NCC between time T and T+1 masks (ΔNCC) is introduced as a descriptor in the feature set. A particular limitation is that although this is probably the most sensible approach, the NCC parameter can be “noisy”, meaning that a particular single lesion may be segmented in different connected components depending on the segmentation method employed (especially if using threshold-based methods).

Note that although the ΔNCC parameter provides useful information for the decision making process regarding the global change of tumor lesions in time, it is still not enough to model the evolution cases illustrated in Fig. 3d and e (i.e., a negative or nil ΔNCC can still reflect a progression condition). To overcome this final limitation, two additional features are added to the final descriptor. The first is responsible of modeling the appearance of new tumoral lesions in time T+1 with respect to time T, that is the detection of the tumor volume that spread to a different anatomical location in time (V
                        
                           N
                        ). The computation of V
                        
                           N
                         is not direct. First, in order to spatially compare both segmentation masks in a robust manner, they must be coregistered in space, since the patient’s position and physical shape (severe loss weight is a common cancer symptom) may have considerably changed in both PET scans. In particular, we use the Normalized Mutual Information (NMI) technique to perform this operation (
                        Fig. 6).

Second, both coregistered masks are smoothed (and rebinarized) in order to compensate for small segmentation artifacts and to be able to robustly compare the voxel wise overlaps between specific connected components from both masks. Let the resulting masks be S
                        
                           T
                         and S
                        
                           T+1. Since they are both binary (logical) 3D matrices, the following element-wise operation can be computed:
                           
                              
                                 
                                    
                                       M
                                    
                                    
                                       N
                                    
                                 
                                 =
                                 
                                    
                                       S
                                    
                                    
                                       T
                                       +
                                       1
                                    
                                 
                                 ∩
                                 
                                 ¬
                                 
                                    
                                       S
                                    
                                    
                                       T
                                    
                                 
                              
                           
                        
                     

Now, the M
                        
                           N
                         can be used to detect the set of new tumoral lesions and compute its volume. However, a last processing step should be carried out in order to avoid taking into account the positive voxels in M
                        
                           N
                         that are “semantically wrong”, meaning that they are actually associated to a shared tumor lesion but appear in M
                        
                           N
                         due to segmentation differences, slight changes in lesion morphology across time or coregistering effects (Fig. 6 right). Therefore, they need to be excluded in the computation of V
                        
                           N
                        . To accomplish this, Algorithm 1 is applied.
                           
                              
                                 
                                 
                                    
                                       
                                          VN=0
                                    
                                    
                                       
                                          For each non-zero voxel v in M
                                          
                                             N
                                          :
                                    
                                    
                                       
                                          
                                          
                                          Compute the connected component of the S
                                          
                                             T+1 mask where v belongs. Let c
                                          
                                             T+1 be its corresponding binary mask.
                                    
                                    
                                       
                                          
                                          
                                          Compute the connected component of the ST mask that has the closest voxel (in (Euclidean distance) to v. Let c
                                          
                                             T
                                           be its corresponding binary mask.
                                    
                                    
                                       
                                          
                                          
                                          Compute the Jaccard overlap index between both connected components:
                                    
                                    
                                       
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   J
                                                   =
                                                   
                                                      
                                                         
                                                            
                                                               C
                                                            
                                                            
                                                               T
                                                               +
                                                               1
                                                            
                                                         
                                                         ∩
                                                         
                                                            
                                                               C
                                                            
                                                            
                                                               T
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               C
                                                            
                                                            
                                                               T
                                                               +
                                                               1
                                                            
                                                         
                                                         ∪
                                                         
                                                            
                                                               C
                                                            
                                                            
                                                               T
                                                            
                                                         
                                                      
                                                   
                                                   
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          
                                          If J<D (a predefined threshold):
                                    
                                    
                                       
                                          
                                          
                                          
                                          
                                          
                                          V
                                          
                                             N
                                          =V
                                          
                                             N
                                          +1
                                    
                                 
                              
                           
                        
                        
                           Algorithm I
                           Computation of V
                              
                                 N
                              .

The last feature to be included is intended to model the most subtle clinical evolution scenarios, illustrated in Fig. 3e and b. For that purpose, local increasing of tumor intensity at the lesion level in time T+1 with respect to time T are detected and quantified in the descriptor A
                        
                           N
                        . To accomplish that, we consider the intersection mask:
                           
                              
                                 I
                                 =
                                 
                                    
                                       S
                                    
                                    
                                       T
                                       +
                                       1
                                    
                                 
                                 ∩
                                 
                                 
                                    
                                       S
                                    
                                    
                                       T
                                    
                                 
                              
                           
                        
                     

Then, consider the masked PET volumes by I, P
                        
                           T
                         and P
                        
                           T+1, in SUV voxel units. We define the logical volume:
                           
                              
                                 H
                                 =
                                 
                                    
                                       P
                                    
                                    
                                       T
                                       +
                                       1
                                    
                                 
                                 
                                 >
                                 α
                                 
                                    
                                       P
                                    
                                    
                                       T
                                    
                                 
                                 
                              
                           
                        which accounts for the tumor voxels that shown a significantly higher (controlled by α) intensity in time T+1 with respect to T. Finally, A
                        
                           N
                         is defined as the number of non-zero entries in H.

To sum up, the feature set of the supervised learning-based ADM system is shown below:
                           
                              
                                 F
                                 =
                                 
                                    {
                                    
                                       Δ
                                       
                                          
                                             V
                                          
                                          
                                             T
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 
                                 
                                    
                                       Δ
                                       SUV
                                    
                                    
                                       mean
                                    
                                 
                                 ,
                                 
                                 
                                 
                                    
                                       Δ
                                       SUV
                                    
                                    
                                       max
                                    
                                 
                                 ,
                                 
                                 
                                 Δ
                                 NCC
                                 ,
                                 
                                 
                                 
                                    
                                       V
                                    
                                    
                                       N
                                    
                                 
                                 
                                    
                                       ,
                                       
                                       
                                       
                                          
                                             A
                                          
                                          
                                             N
                                          
                                       
                                    
                                    }
                                 
                              
                           
                        
                     

Note that if the system is used in semi-automatic mode, meaning that the medical expert are assessing and correcting the computations performed by the system modules, the presentation of the particular feature vector as well as all the intermediate volume masks (especially M
                        
                           N
                         and H), can provide relevant quantitative information to complement the expert’s visual analysis process.

Once our proposed feature set has been defined, the choice of the supervised learning algorithm of the ADM system is addressed. First, note that this particular learning problem is challenging, not only due to the high variability of the input and the subtle evolution scenarios to be detected, but also because of the small and subjective training set available (since PET scans are costly and the ground truth information is derived mainly from an expert’s visual analysis).

Therefore, a range of learning algorithms is applied to seek for the best supervised learning strategy within this scenario (following the no free lunch theorem [13]). The set of chosen learning algorithms is the following: Naïve Bayes classifier (given the high independence of the individual features), kNN, decision trees (to try to capture a clinically relevant decision rule) [16], neural networks [17], logistic regression (to obtain an output probability instead of binary decision) [14], and state of the art radial basis function SVM [15] and Discrete AdaBoost [14] approaches.

All the required hyperparameters (k from kNN, SVM RBF parameters, number of units/layers of the neural network, number of decision stumps for Adaboost, etc) where chosen using a nested leave-one-out cross validation scheme within the training data, and the external leave-one-out was used at the test phase.

@&#RESULTS AND DISCUSSION@&#

In this section we present the performance results of the proposed system and discuss its implications both at the clinical and technological levels. Two main system usage modes are distinguished, one where the system runs in a completely automatic manner having only as input the pair of time consecutive PET-CT scans of a given patient, and another where the PET tumor segmentation masks carried out by medical experts are also provided as input to the system.

In the first case, the ATS module plays a major role in the overall system performance. As it was mentioned, in this work we use the ATS system trained on 200 independent PET-CT scans described in [11] as a black box module to automatically segment the tumor volume from a give PET-CT scan. For each of the 100 cancer evolution cases described in Section 2, physicians provided the expert-guided PET tumor segmentation masks, allowing us to evaluate the ATS module segmentation performance.

As expected, the segmentation accuracy results at the voxel level are quite low: mean Jaccard overlap index of 0.18±0.21, mean sensitivity (ratio of correctly classified tumor voxels and the sum of correctly classified tumor voxels and incorrectly classified tumor voxels) of 0.23±0.28 and mean specificity (ratio of correctly classified non-tumor voxels and the sum of correctly classified non-tumor voxels and incorrectly classified non-tumor voxels) of 0.9998±0.0005, with a clear tendency to prioritize false negative over false positive decisions (which was intended by the authors as is sensible given the clinical context). However, a more relevant performance indicator of the ATS module is its relative (rather than absolute) discrimination power between the quantities of tumor present in different PET scans. In this respect, our ATS module shows a 71% Pearson correlation coefficient when comparing the total tumor volume of the expert-guided and automatic PET segmentation masks, which is clearly superior to other completely automatic segmentation strategies such as direct thresholding [8] (49%).

Once the system has access to both time T and T+1 PET tumor segmentation masks (either from the ATS module or provided by the medical experts), the ADM module executes. The dataset available to address its performance is built from the computation of the six features described in Section 3.2 for each of the 100 cancer evolution cases (where D and α where empirically set to 0.05 and 1.2, respectively). This 100×6 matrix, in conjunction with the ground truth cancer evolution condition (progression or response) for each case is then provided to the set of supervised learning algorithms within a nested leave-one-out cross-validation scheme described in Section 3.2. The tendency of the mean values of the learning algorithm parameters across the cross-validation phase were k=3 in kNN, 20 weak classifiers in AdaBoost, C=1 (box constraint) and σ=0.05 (scaling factor in the radial basis function kernel) in SVM, and three layers in NN.

Performance results are shown in 
                     Table 1. In this context, accuracy is defined by the fraction of correctly classified evolution cases, sensitivity as the ratio of correctly classified progression cases and the sum of correctly classified progression cases and incorrectly classified response cases, and specificity as the ratio of correctly classified response cases and the sum of correctly classified response cases and incorrectly classified progression cases.

As it can be observed, a substantial performance gap is observed between the automatic and semi-automatic approach, which is associated with the limitations of the ATS module to provide a correct tumor segmentation mask. As an example, consider the cancer progression shown in 
                     Fig. 7.

Clearly, the set of features computed from the automatic segmentation masks will be altered due to the errors made by the ATS block and, although these errors may have some patterns (typical errors include the wrong detection of the heart, bladder or kidneys), the learning algorithms may not be able to resolve them when obtaining the classification rule.

Finally, in order to address the convenience of the proposed ADM feature set, the relevance of each feature within the decision making process in the two learning algorithms that achieved the best performance results is shown in 
                     Fig. 8. Note that both results are perfectly consistent with the underlying clinical scenario, where the total volume change (ΔV
                     
                        T
                     ) is clearly the most relevant visual indicator and the change in the number of lesions (modeled by ΔNCC) is only relevant if some of the lesions are actually new in time T+1 (which is already modeled by V
                     
                        N
                     ).

These results contribute to both computed aided diagnosis and quantitative longitudinal analysis in the medical imaging field. The derivation of quantitative indicators from segmentation procedures to monitor clinically relevant information is common in a number of medical imaging modalities (see [20–22] for examples in MRI, CT, and cerebral FDG-PET). Computer aided diagnosis systems and the use of machine learning techniques in them are also emerging within the field [23,24]. In this work, by introducing the proposed framework, we extended the range of applications of these computational techniques in the assessment of cancer evolution in oncological PET-CT scans.

@&#CONCLUSIONS AND FUTURE WORK@&#

We described a common clinical diagnostic scenario in nuclear medicine imaging, the cancer evolution assessment (its progression or response stage) via a pair of time consecutive PET-CT scans, and proposed a computational framework to complement the expert’s visual analysis with relevant quantitative and subject-independent information within this diagnostic context.

Since modeling this particular clinical scenario following the high level expert’s knowledge and reasoning from a computational point of view represents a challenging problem, a supervised machine learning framework has been proposed. A trade-off is observed between the use of a completely automatic (and therefore time-efficient and subject independent) approach with a performance accuracy of 70% at detecting the correct cancer evolution condition (taking as a reference the expert’s visual analysis), and a semi-automatic approach where the system is provided with the PET tumoral segmentation masks carried out by the trained physicians, which show up to 90% performance. In any case, the set of numerical indicators used as feature set within the machine learning framework or the numerical outputs of the trained classifiers may provide the expert with relevant quantitative information, contributing to improve the overall diagnostic accuracy in the clinical setting.

Future work includes, at the clinical level, the incorporation and long term validation of the proposed system in the day-to-day medical practice as well as the introduction of a new quantification module to model the overall intensity of a particular cancer progression or response condition, which would become a very useful tool to help in oncological treatment response analysis and management. At the technological level, we plan on reducing the computation time of the system using GPU-based parallelization techniques as well as dealing with the multi-class problem of recognizing each of the evolution patterns instead of the general response/progression state by introducing successful multi-class frameworks such as error-correcting-output-codes [18].

None declared.

@&#ACKNOWLEDGMENTS@&#

The work of Frederic Sampedro is supported by the Spanish government FPU (Formación del Profesorado Universitario) doctoral grant (Grant No. AP2012-0400). The project has been partially supported by Spanish government project 
                  TIN2013-43478-P.

@&#REFERENCES@&#

