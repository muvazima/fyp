@&#MAIN-TITLE@&#Crime profiling for the Arabic language using computational linguistic techniques

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Text mining system for extraction of information related to crime from Arabic texts.


                        
                        
                           
                           Local grammar used to extract information and build dictionaries automatically.


                        
                        
                           
                           Visualisation of clustering enhances ability to analyse crime information in corpora.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Arabic language

Crime domain

Pattern recognition

Clustering

Information extraction

Syntactic analysis

@&#ABSTRACT@&#


               
               
                  Arabic is a widely spoken language but few mining tools have been developed to process Arabic text. This paper examines the crime domain in the Arabic language (unstructured text) using text mining techniques. The development and application of a Crime Profiling System (CPS) is presented. The system is able to extract meaningful information, in this case the type of crime, location and nationality, from Arabic language crime news reports. The system has two unique attributes; firstly, information extraction that depends on local grammar, and secondly, dictionaries that can be automatically generated. It is shown that the CPS improves the quality of the data through reduction where only meaningful information is retained. Moreover, the Self Organising Map (SOM) approach is adopted in order to perform the clustering of the crime reports, based on crime type. This clustering technique is improved because only refined data containing meaningful keywords extracted through the information extraction process are inputted into it, i.e. the data are cleansed by removing noise. The proposed system is validated through experiments using a corpus collated from different sources; it was not used during system development. Precision, recall and F-measure are used to evaluate the performance of the proposed information extraction approach. Also, comparisons are conducted with other systems. In order to evaluate the clustering performance, three parameters are used: data size, loading time and quantization error.
               
            

@&#INTRODUCTION@&#

Nowadays, the volume of data in electronic form is increasing rapidly, whether it is structured or unstructured data. According to McKnight (2005), between 85% and 90% of data is held in unstructured form. Therefore, text mining is necessary to manage and extract useful information from unstructured sets of data, such as web pages and emails, using text mining techniques. Hence, text mining has become an important and active research field. It is well known that text mining techniques have been mostly developed for the English language because most electronic data are in English. However, the recent advances in localisation and expansion of non-English electronic text make it more imperative than ever to develop techniques to process other languages, such as Arabic.

Moreover, in the Arabic language, like other languages, there is a multitude of specialist texts in areas such as the biomedical sciences, finance, politics and crime, some of which have not been investigated by researchers. This study focuses on the crime domain in the Arabic language for which no information system can be found in the literature. Therefore, there is a need to investigate the crime context using a text mining approach.

Furthermore, text mining research relies on the availability of a suitable corpus. However, there is no available crime corpus in the Arabic language, and it is one of the contributions of this study to create such a corpus. The content of this corpus has been collected from the crime sections of different Arabic newspapers published in a number of Arabic countries.

In our previous works, we developed a system to extract crime information from texts (Alruily, Ayesh, & Zedan, 2009). The approach is based on a dictionary that is created manually. Also, we were able to develop three dictionaries, for crime type, crime location and nationality, that were automatically built in order to be used in the information extraction (Alruily, Ayesh, & Zedan, 2010). Moreover, we developed the first system that was able to cluster Arabic crime news reports using the Self Organising Map (SOM) technique (Alruily, Ayesh, & Al-Marghilani, 2010). The current research represents a combination of our previous works (Alruily et al., 2010; Alruily et al., 2010). As a result, the system will be able to recognise phrases that contain information related to crime in a given document in order to extract the type of crime, crime location and nationality of persons involved in the event, and in order to generate a summary. Moreover, during the information extraction stage, dictionaries containing the crime type, crime location and nationality will be automatically created, which will also assist in the information extraction process. In this research, the extracted keywords (summary) will be utilised by a Self Organising Map (SOM) to perform the clustering and visualisation tasks. As well known, the extracted entities are keywords, which play an important role in many text mining tasks, such as clustering, summarisation and document retrieval, because they reveal the essential idea of the document or article (TeCho, Nattee, & Theeramunkong, 2008).

The rest of the paper is organised as follows. In Section 2, a background to the topic and a review of related work are presented. Section 3 provides an introduction to the Arabic language, focusing on the syntactic information that is related to this research. Section 4 presents the crime domain syntactic analysis. Section 5 provides an overview of the framework’s architecture. Section 6 presents the experimental results. In Section 7, the performance evaluation results are given. Finally, the conclusion of this work is presented in Section 8.

@&#BACKGROUND@&#

This section is on information extraction, and it includes descriptions of its early development and the approaches utilised. Also, related works are discussed. It is divided into three parts, as in the subsections below.

Generally, the information extraction approaches can be divided into the following categories:
                           
                              •
                              Handcrafted rules, known as linguistic approaches.

This approach is usually used for extracting information from specific domains. It is simple to build because its goal is only to fill out templates, i.e. it is not document understanding. Moreover, it can be trained on annotated or unannotated corpora. Although it can achieve reasonable levels of success, there are some disadvantages (Toral & Munoz, 2006; Riloff, 1993). For instance, it is time consuming because it is slow to build, and it is difficult to scale to new domains (Riloff, 1993). Toral and Munoz (2006) explained that this approach is applied using rules and gazetteers. The Gate system was developed at Sheffield University, and is a type of software that follows this approach. The task of this system is to extract named entities (Cowie & Lehnert, 1996).

Machine learning approaches.

Because of the difficulties that faced researchers when building hand crafted rules, a need for automatically learning extraction rules emerged. There are three types of machine learning approach: supervised, semi-supervised and unsupervised. According to Nadeau (2007), most developed systems have been designed based on handcrafted rule based systems or supervised learning based systems. In both approaches, a corpus must be studied and analysed by hand to gain sufficient pertinent knowledge to build the rules or to feed the machine learning algorithms. However, the supervised learning techniques, which include Hidden Markov Models (HMM), Maximum Entropy (ME), Support Vector Machines (SVM) and Conditional Random Fields (CRF) need a large annotated corpus for designing the systems. As a result, this disadvantage of supervised machine learning led to the emergence of semi-supervised and unsupervised machine learning (Nadeau, 2007). The semi-supervised (weakly supervised) is implemented with little supervision. The idea of this type of machine learning is to use a set of seeds to provide a system with a little external support to start learning how to extract. For example, finding the names of the diseases to extract can be done by providing the system with seeds, such as five disease names. First, the system seeks out the sentences that contain these seeds in order to understand the contexts in which they appear. Then, the system tries to find other disease names that exist in the same context (Sekine, 2004). Nadeau (2007) developed a semi-supervised Named Entity Recognition (NER) technique that learned to recognise 100 entity types with little supervision. With regards to unsupervised machine learning, clustering is considered the most typical approach where, for example, named entities can be gathered based on the similarity of context from clustered groups (Nadeau, 2007; Sekine, 2004).

Moreover, there is an approach called hybrid, which is a combination of the handcrafted recognition and machine learning approaches.

Most of the systems developed for Arabic in the literature focus on Named Entity Recognition (NER) (Attia, Toral, Tounsi, Monachini, & van Genabith, 2010, Abdul-Hamid & Darwish, 2010). The majority of systems developed for Arabic in this field rely on predefined proper name gazetteers. Maloney and Niv (1998) developed a system called TAGARAB in order to recognise information relating to names, dates, times and numerics within Arabic text. A combination of a pattern matching engine and morphological analysis as well as a words list is used to achieve the recognition task. The morphological analysis is used to assist the system in recognising the various morphological word-shapes and to provide ‘part of speech’ information for each token before entering the data into the pattern matching engine, i.e. a high precision morphological analysis is employed. The performance achieved for the aforementioned entities is presented in Table 1
                        . Good results were achieved but it is an expensive process as each token is examined. Also, Mesfar (2007) developed a system for the Arabic language to recognise proper names (person, location, and organisation names), dates and numerics through a combination of morphological analysis, syntactic grammar and rules. The role of the morphological analyser is to strip off affixes from inflected words to assist in the matching process. Also, this system relies on gazetteers that contain the names of persons, locations and organisations, together with trigger words that indicate entities of interest, such as a person’s title. As in the TAGARAB system, this is an expensive process because the whole text must enter the morphological analyser. Furthermore, huge knowledge resources are used in this system, which include many predefined gazetteers, such as a personal names list that contains 12,400 Arabic first names and a locations list that consists of 5038 entries, as well as a list of keywords that has 872 trigger words for indicating these entities. The evaluation results of this system are listed in Table 1. Likewise, Shaalan and Raza (2008) developed a system called Name Entity Recognition for Arabic (NERA) to extract 10 named entities from Arabic text; personal name, location, company, date, time, price, measurement, phone number, ISBN and file name. They use a rule-based approach that relies on various fixed predefined dictionaries, such as for personal names (263,598 complete names, 175,502 first names and 33,517 last names), locations (4900 names) and organisations (273,491 names of companies). Also, there is a dictionary containing trigger words (indicator words) for helping to identify entities, such as using job titles to indicate persons’ names. Moreover, a dictionary called Blacklist is used to reject unwanted entities in order to filter the result. It is noticeable that gazetteers are extensively used but building them is time consuming because Arabic resources (corpora, gazetteers) are generally not free and can be hard to access; they are also relatively few in number. Table 1 presents the performance results achieved for this system in terms of precision, recall and F-measure. Moreover, Al-Shalabi et al. (2009) presented an algorithm for extracting proper nouns from Arabic texts. They use a set of keywords and special verbs together with some specific rules. Firstly, they use predefined keywords to mark phrases that may contain proper nouns. Secondly, the proposed rules are applied to extract the proper nouns that directly follow the keywords, and then the extracted words are classified into one of these categories, based on the type of the keyword: people, locations, organisations, events and products. However, it should be noted that the system is not able to extract proper nouns that do not appear directly after the keywords or the special verbs. Although they reported that they could extract 86.1% of proper nouns in a text, they evaluated their developed system using only 20 documents, which is a very small set of data and is perhaps insufficient for determining the effectiveness of the system. Nevertheless, the performance evaluation is presented in Table 1. Elsebai, Meziane, and Belkredim (2009) adopted a rules-based approach that makes use of the outputs generated by the Buckwalter Arabic Morphological Analyser (BAMA) for developing the Persons Names Arabic Extraction System (PNAES). Their system uses a set of keywords (introductory verbs and words) to indicate the phrases that might contain personal names, i.e. there is no predefined personal names gazetteer. However, lists containing Arabic personal names that start with the definite article “al/the”, organisations and location names are used. The reason for using organisation and location names lists is to match the extracted word with words in the lists, and once the matching occurs, the word is discarded, otherwise it is classified as a personal name, i.e. it is similar to the Blacklist used in Shaalan and Raza (2008) to perform filtering. Although this system is able to deal with names that appear not necessarily next to a keyword, unlike the above system developed by Al-Shalabi et al. (2009), quite complicated rules are created to cover all the probabilities of a person’s name occurring in a text. Finally, the performance achieved for the personal names recognition task was good but it was tested only on one resource, and therefore, the efficiency of this system was not rigorously examined. Table 1 presents the performance results. Abuleil and Evens (2004) developed an events extraction and classification system for Arabic information retrieval systems. Their developed system is based on a predefined keyword lists and a parser to extract the events, the dates and related proper nouns. The lists include event words (such as assassinated), valid dates and proper nouns (people, organisations, locations). The system relies on ‘direct look-up’ to recognise the type of an event, i.e. if an event type is not in the list, it will not be identified. Additionally, Abuleil (2007) proposed an algorithm for scanning and understanding events (natural disasters, bombings and deaths) in Arabic text in order to extract information related to them, such as event locations, event types and dates. The system uses different lists that are described as event elements. For example, in order for an event to be marked in the text, special words (keywords), including nouns (e.g. earthquakes, hurricanes and massacres) and verbs, are used. A proper names list, consisting of personal, location and organisation names, is used to extract entities from the text. Moreover, some particles and nouns are used as link tools when they occur in the text next to proper nouns and noun phrases, linking the event and its location. Also, some particles and nouns are used as relationship tools between more than one event in the text when they occur before the keyword of the second event. The evaluation results of the system were good, as in Table 1; however, the system was only tested on a corpus compiled from one source. Furthermore, the above lists were manually built, based on reading the texts, which is a key disadvantage of this system. In order to confirm this drawback, the system was tested twice; the first experiment showed that some keywords were missing, and some particles and nouns had to be added to the lists. It was reported that after updating the lists, in the second experiment, the system was not able to extract 28 events due to 9 missing keywords. This problem is caused by a lack of any deep contextual analysis, which must be done on huge Arabic texts (within the data collection phase). Also, the authors mentioned that once all the elements (keywords, noun phrases, special nouns and particles, and proper nouns) appeared in the description of the event, the system worked well, i.e. the system fails to detect and understand the event if one of the previous elements is missing.


                        Piskorski, Tanev, Atkinson, Van, and Goot (2008) developed a multilingual news event extraction system at the Joint Research Centre of the European Commission. The system was able to extract violent and natural disaster events from online news. They use a pattern matching engine and a set of lexicons, which means that a rule-based approach is adopted. The event extraction grammar was originally designed to be applied on the English language but the technique has been extended to work on other languages, such as French, Italian and Arabic. With regards to the Arabic language, the Arabic news articles were translated into English using translation systems, and then they implemented the event extraction grammar. The evaluation results were not reported for this system.

Clearly, all the above systems use the rule-based approach. Also, common entities (personal names, locations, organisations, dates and numbers) have been investigated by these systems, except the two systems developed by Abuleil and Evens (2004) and by Abuleil (2007); they tried to recognise events in texts. Additionally, the described systems did not mention the data sparseness problem in the Arabic language, except Shaalan and Raza (2008). Furthermore, it seems that most dictionaries (gazetteers), especially the keywords lists in the aforementioned systems, were built based on authors’ observations or knowledge. In other words, there is no objective explanation or analysis phase conducted on the data being studied for identifying the keywords. However, this phase (data analysis phase) was discussed by Traboulsi et al. (2009) in order to identify patterns of personal names in Arabic texts. Three types of analysis (frequency, collocation and concordance) were conducted on huge corpora to identify the most important keywords, to discover the most frequent words collocating with keywords, and to obtain the concordance of the keywords. Consequently, the structures of the most frequent named entities were discovered, which led to constructing the local grammar for recognising personal names, i.e. the other structures that may contain personal names are discarded or neglected. As a result, the system might lose the ability to identify some named entities. Also, no performance evaluation was conducted for this system.

On the other hand, machine learning approaches have also been adopted in Arabic NER research. Benajiba, Rosso, and Ruiz (2007) designed the Arabic Named Entity Recognition System (ANERsys) based on Maximum Entropy (ME). Annotated corpora were used in this work as well as external resources such as dictionaries. Three different gazetteers were manually built: a location gazetteer consisting of 1950 names of continents, countries, cities, rivers and mountains, a person gazetteer containing 2309 names and an organisation gazetteer consisting of a list of 262 names of companies, football teams and other organisations. As mentioned earlier, it is time consuming to build gazetteers. Also, in order for the system to be tested, several experiments must be first performed to train it and to derive a set of features for assisting in the recognition process. Moreover, Benajiba et al. (2008) changed the probabilistic model by using Conditional Random Fields (CRF) instated of ME. Although they achieved promising results through their improvement of ANERsys, their system still relies on an annotated corpus and the same predefined manually built gazetteers that were used with ME. Preprocessing, such as stemming as well as a part of speech (POS) feature, is used. Also, they use a nationality feature for marking nationalities in the input text because nationalities are utilised in detecting the entity of personal names; they are used as precursors to recognising them. This feature relies on a dictionary of 334 different nationalities, which was manually built. Table 2
                         shows the evaluation results for the ANERsys using ME and CRF. Also, Benajiba, Diab, and Rosso (2009) compared three machine learning approaches: Support Vector Machines (SVM), ME and CRF. The latter was the best and it yielded an overall F-measure of 83. AbdelRahman, Elarnaoty, Magdy, and Fahmy (2010) integrated two machine learning techniques (bootstrapping semi-supervised pattern recognition and Conditional Random Fields (CRF)) for identifying 10 named entities: person, location, organisation, job, device, car, cell phone, currency, date, and time classes. However, the developed system, as with the above systems, relies on predefined gazetteers (person (3228), location (2183), organisation (403), job (70), device (253), car (223) and cell phone (184)) to assist in recognising the entities. Also, 16 different features are employed for implementing CRF as well as 232 different seeds. Table 2 lists the performance results of this system. Abdul-Hamid and Darwish (2010) created a system that is able to recognise named entities (personal, location and organisation names) for the Arabic language, based on a set of features without using morphological or syntactic analysis or gazetteers. For implementing this work, Conditional Random Fields (CRF) was used. This technique was trained on a large set of surface features in order to avoid using Arabic morphological and syntactic features. Table 2 presents the evaluation results.

It is noticeable that the above machine learning approaches need an annotated corpus for them to be implemented. According to Ku et al. (2008), they also need large training datasets. Moreover, they often rely on different predefined gazetteers. Table 3
                         presents a comparison of all the above systems in terms of method, type of corpus and whether or not they use gazetteers, POS and/or stemming.

With regards to the crime domain, there have been several efforts to develop information extraction systems for automatically extracting meaningful crime-related information. Data mining techniques have been used in this domain and a comprehensive survey of the effectiveness of the various methods for crime data analysis is provided by Thongtae and Srisuk (2008). However, these techniques are beyond the remit of this research.

In the text mining field, Chau, Xu, and Chen (2002) studied police narrative reports to extract five meaningful entities, namely, person, address, vehicle, narcotic drugs and personal property, in order to facilitate crime investigation. The developed system is comprised of hand-crafted lexicons, rule-based and machine learning. It begins with extracting noun phrases from documents based on linguistic rules. The extracted noun phrases are processed by a finite state machine to generate binary values. This process is achieved by conducting a matching process using predefined dictionaries for each word in the phrase and for the words that immediately precede and follow the noun phrase. Also, other sets of features (e.g. upper and lower case) are used in the process. The binary values are sent to the feedforward/backpropagation neural network component as input in order for it to predict the most likely entity type. The system was evaluated only on 36 documents collected from one source, as can be seen in Table 4
                        . Also, Chen et al. (2004) integrated the above system (entity extraction) with some data mining techniques, such as association and prediction methods, to develop a crime data mining system.


                        Ku et al. (2008) developed an information extraction system to extract crime-related information from different resources, such as police reports, newspaper articles and witness narrative reports written in the English language. Their system relies on rule-based and lexical look-up approaches. They manually built eighty-eight gazetteer lists. Also, they employed the Gate open-source framework, which includes several modules, such as tokenizer, sentence splitter, POS tagger, noun chunk, and JAPE rules for pattern matching.

Moreover, Riloff (1993) developed a program called AutoSlog for extracting information in the terrorism domain. The system relies on concept nodes in order to extract information about terrorist incidents. AutoSlog is comprised of 13 concept nodes, and each one is triggered by predefined keywords (terrorist action words), such as ‘bombed’ and ‘kidnapped’, and is activated within a specific linguistic grammar (e.g. in passive form) in order to extract information, such as targets, perpetrators and victims. Also, it relies on a corpus tagged by a POS tagger.

Also, in order to support crime investigation, other attempts have been made by governments and companies. The European Commission funded the AVENTINUS project, which can be described as a multilingual information extraction system for identifying these entities: persons, narcotics, location, organisations, transportation means, communication means and places, and dates. AVENTINUS is designed for multilingual drug enforcement authorities, improving multilingual communication and information processing (Schneider, 1998), and the rule-based approach is used for implementing the information extraction. Also, the Scene Of Crime Information System (SOCIS) was developed by a team from Sheffield and Surrey Universities in collaboration with four UK police forces (Surrey Police, Hampshire Constabulary, Kent County Constabulary and South Yorkshire Police) for crime scene photograph indexing and retrieval (Pastra, Saggion, & Wilks, 2003). Moreover, the system performs information extraction using a rule-based technique to extract all the names entities that might appear in a caption: address, age, conveyance-make, date, drug, gun type, identifier, location, measurement, money, offence, organisation, person and time. The system achieved 80% precision and 95% recall. Both systems (AVENTINUS and SOCIS) rely on predefined gazetteers. Finally, the Locard Company developed an evidence tracking system (commercial software), which offers management for all crime-related exhibits; their website (http://www.locard.co.uk) provides more information about this software.

It can be inferred that each one of the above systems is focused on extracting specific entities. Also, different approaches have been used to implement the entity extraction, for example, Chau et al. (2002) utilise machine learning, Ku et al. (2008) use rule-based and lexical look-up, and the system developed by Riloff (1993) is based on linguistic grammar.

The information extraction approach used in this work relies on some syntactic constructions. Accordingly, this section considers these constructions, which include the transitivity of the Arabic language and genitive case. Therefore, types of verb and noun case assignment are investigated. Prepositions that play a crucial role in the transitive construction and the genitive case are examined. The Arabic language is a Semitic language, and it is the native tongue in 22 Arab countries. Arabic consists of 29 letters that can be used to form words; other languages, such as Farsi and Urdu, also use Arabic characters (Al-shatnawi & Omar, 2008). Arabic words can be divided into three classes: noun (
                        
                     / esm), verb (
                        
                     / fe’al) and particle (
                        
                     / hrf) (Al-Shalabi et al., 2009; Khoja, 2001; Hadj, Al-Sughayeir, & Al-Ansari, 2009). However, when working with the Arabic language, some other important characteristics need to be taken into account (Benajiba et al., 2007):
                        
                           1.
                           A character may have up to three different forms, each form corresponding to the position of that character in the word (beginning, middle or end), such as the letter “
                                 
                              / Ayn” in Table 5
                              .

Arabic does not have capital letters; this characteristic represents a considerable obstacle to the Named Entity Recognition (NER) task because in other languages capital letters represent a very important feature.

Finally, it is a language with a very complex morphology because it is highly inflectional.

As previously mentioned, the structure of Arabic can comprise of three categories: noun, verb and particle, and they are explained in the following sections.

In this research the target is to identify the nouns that carry information about crime type (theft, murder, and rape), crime location (Dubai, Riyadh and Cairo), and nationality (Saudi, Yemeni and Indian). This category in Arabic includes any word that describes a thing, idea, person or location, and is not related to tense (time) (Al-Shalabi et al., 2009). There are certain signs that can help in identifying nouns in Arabic, Ibn Malik (Badralddin, 2000) explained that if one of them is present, it is possible to classify that word as a noun; these signs are as follows:
                           
                              •
                              If a word accepts being assigned the genitive case; the genitive case is achieved through a prepositional phrase or a ‘construct state’ (Idaafah structure) (Homeidi, 2003; Habash, Gabbard, Rambow, Kulick, & Marcus, 2007). Consider the following examples:
                                    
                                       1.
                                       
                                          
                                             
                                          / dhab altalb-u ila almadrst-i.

Went the student (nominative) to the school (genitive).

The student went to the school.


                                          
                                             
                                          / ktab-u almoalm-i.

Book (nominative) the teacher (genitive).

The teacher’s book.

Sentence 1 represents the first case (prepositional phrase), where the noun “
                                    
                                 / almadrst/ the school” (governee) follows the preposition “
                                    
                                 / ila/ to” (governor). As a result, the object of the preposition is assigned the genitive case. On the other hand, Sentence 2 represents the construct state (possessive case), which is comprised of two nouns.

If a word accepts a diacritic called nunation (the Arabic term is 
                                    
                                 / tanwyn) at its end (Jiyad, 2006), then that word must be a noun.

If a word is preceded by the vocative particle “
                                    
                                 / yea/ O”, such as 
                                    
                                 / O, Mohammad.

If a word is fused at its beginning with the definite article 
                                    
                                 / al/ the.

The Arabic language has two types of noun. The first type is the primitive noun (non-derived nouns); these are not derived, as in Table 6
                           . The second type is the derivative noun, these are derived from verbs or other nouns, as in Table 7
                           .

Arabic nouns are inflected for gender (masculine and feminine) and number (singular, dual and plural) (Ghali, 2007). Also, nouns are either definite, which start with the article “
                              
                           / al/ the” or indefinite, having no “
                              
                           / al/ the” article at the beginning. Moreover affixes and clitics, such as some prepositions, conjunctions and possessive pronouns, can be attached to them.

The notion of case assignment has been discussed by researchers under the theory of Government and Binding (GB), which was coined by Chomsky. The concept of ‘government’ is defined as, “a particular structural relationship which may hold between two nodes in a tree. It plays a crucial role in GB in the assignment of case and in containing the distribution of empty categories” (Homeidi, 2003). Habash et al. (2007) defined case assignment as, “a relationship between two words: one word (the case governor or assigner) assigns a case to the other word (the case assignee)”. In other words, the word has the power to affect the case of another word; this is called ‘the governor’, and which in Arabic is called “
                              
                           / alamil”. The affected word is called ‘the governee’ or “
                              
                           / almamul”. According to Habash et al. (2007), there are different types of syntactic governors that assign cases to their governees. Consequently, all nouns and adjectives can be one of the following cases: nominative (
                              
                           / mrfwo’a), accusative (
                              
                           / mnsob) or genitive (
                              
                           / mjrwr). Buckley (2004), explained these three cases in detail, clarifying the conditions for each case; 7 conditions for nominative, 25 for accusative and 2 for the genitive case.

This word type points out an event or action. Arabic verbs have two tenses: perfect and imperfect. Verbs are inflected in terms of number (singular, plural and dual), gender (masculine and feminine), person (1st, 2nd, 3rd), voice (active and passive) and mood (subjunctive, indicative, jussive and imperative) (Chen & Gey, 2002).

Moreover, Arabic verbs are divided into two types: transitive or intransitive (Khan, 2007). With respect to transitive verbs, a verb needs one object or more as well as the subject, in order for its meaning to be completed, e.g. “
                           
                        / qtft altfaht/ I picked up the apple” (Khan, 2007; Al-Jarf, 1990). On the other hand, a sentence that contains an intransitive verb has no object, e.g. “
                           
                        / mrd khaled/ khaled got sick”, i.e. this type of verb does need an object to complete its meaning. However, intransitive verbs can be transformed into transitive verbs either by changing the form of the word or by adding a preposition after the verb (Khan, 2007; Al-Jarf, 1990). For example, in “
                           
                        / thhbt ela dubai/ I went to Dubai”, the verb is converted to transitive by adding the preposition “
                           
                        / ila/ to” after the verb. Alahmadi (1986) described these types of verbs, such as “
                           
                        / qam/ did”, “
                           
                        / dhb/ went”, “
                           
                        / twrt/ involved”, “
                           
                        / athr/ found”, “
                           
                        / tksas/ specialised”, “
                           
                        / adyn/ convicted”, “
                           
                        / shraa/ commenced”, “
                           
                        / taard/ subjected”, “
                           
                        / aatraf/ confessed”, “
                           
                        / aqdm/ conducted” as ‘transitive verbs by preposition’. Most Arab linguists state that most intransitive verbs cannot refer to the object of the sentence but they can be strengthened by certain prepositions, which are called transitive prepositions, such as “
                           
                        / bi/ by”, “
                           
                        / li/ for”, “
                           
                        / min/ from”, “
                           
                        / ala/ on”, “
                           
                        / fi/ in”, “
                           
                        / ila/ to”, in order to refer to the object. As a result, these prepositions can be described as governors because they control the status of the verbs, i.e. they assign the transitive case to verbs. This research concentrates on ‘transitive verbs by preposition’ in terms of exploiting them to extract patterns of interest, as will be shown later.

This class includes prepositions, conjunctions, interrogative particles, exceptions, and interjections. This section is dedicated to prepositions (
                           
                        / Huruof Aljar). Prepositions are words that are used to connect other words in order to form useful sentences. Curme (1935) defined a preposition as “a word that indicates a relation between the noun, or pronoun it governs and another word, which may be a verb, an adjective, or another noun or pronoun”. Prepositions are defined in classical Arabic, by Fiteih (1983), as, “words that form with the noun phrases (or other linguistic entities) they govern exocentric constructions functioning as adjunct, prepositional object, predicate, post-modifier or as conjunctive of a relative pronoun”.

The Arabic language has more than fifteen prepositions, most of which are short. Most of them are formed from three letters, such as “
                           
                        / ala”, or from two, such as “
                           
                        / fi”, but they can be formed with only one Arabic letter and fused as a prefix with nouns, such as “
                           
                        / li” or “
                           
                        / bi” (Ryding, 2005). So, a preposition is either separate or attached. The separable prepositions are words that precede nouns, such as “
                           
                        / fi (preposition) almdrst (noun)/ in the school”. On the other hand, the inseparable prepositions are letters that are attached to the following noun, such as in “
                           
                        / dhb li-ylab/ went for playing”, where the preposition “
                           
                        / li/ for” is fused with “
                           
                        / ylab/ playing”. Table 8
                         lists the prepositions with their pronunciations and meanings in the English language (Ghali, 2007).

Sometimes, the prepositions are called genitive words because they add the verbs’ meanings to the nouns, and they are also called in Arabic “
                           
                        / aljr/ dragging” because they drag the meaning of a verb to a noun (Najjar, 1986). As a result, the task of the preposition in the language is to convey the meaning of a verb to the word that follows the preposition because some verbs are unable to reach nouns by themselves, such as intransitive verbs. For example; “
                           
                        / dhab alrjl ila dubai/ The man went to Dubai”. In this example, the verb has a subject “
                           
                        / alrjl/ the man” and a quasi sentence (the prepositional phrase) “
                           
                        / ila dubai/ to Dubai”, which is the complement of the sentence “
                           
                        / thhbt/ I went”; this helps in determining the meaning of the whole sentence. Also, in the example “I went by car”, the object of the preposition “car” reveals the manner of movement. As a result, it can be said that prepositions are tools used to indicate meanings that are latent or implicit in verbs (Najjar, 1986). In other words, the verb’s latent meaning is stimulated by the presence of a preposition (Alzablawy, 1988). Therefore, prepositions can work as links between some verbs and nouns, whether they are adjacent or not.

The structure of a prepositional phrase (pp) in Arabic, as in English, is composed of two parts: preposition and noun-phrase (Satterthwait, 1963). For example, “
                           
                        / alwalad fi albyt” means “the boy in the house”. In this example, the preposition is “
                           
                        / fi/ in”, and the noun is “
                           
                        / albyt/ house”. Also, Fiteih (1983) stated that a phrase that is governed by a preposition in a sentence is called a prepositional complement and together they form a prepositional phrase. According to Dukes, Atwell, and Sharaf (2010), in the Arabic language, a prepositional phrase must always be linked to a head node, either a verb or noun. Fig. 1
                         shows an example of a prepositional phrase in a verbal sentence. The prepositional phrase “
                           
                        / fi srqt/ in theft” is the object of the verb “
                           
                        / twrt/ involved”. The verbal sentence is comprised of three elements Verb, Subject and Object (VSO), as can be seen in Fig. 1.

On the other hand, there is the case of the nominal sentence (verbless sentences). This type of sentence contains two parts, the first of which is called the subject phrase (in Arabic “mubtada/ initial”), and the second part is a predication (or khabar/ reporter in Arabic); the predication can be a prepositional phrase (Ramsay & Mansour, 2008), as in Fig. 2
                        . In this example, the predication “
                           
                        / fi almdrst/ in the school” comes after “
                           
                        / altabl/ the student” in order to clarify it.

This section describes the development of the computational linguistic techniques for recognising and extracting crime-related information (crime type, location (scene) and nationality). The syntactic analysis for the Arabic crime domain is performed in order to identify the behaviour of the words used in the crime context. For implementing this intensive analysis, a huge corpus, which contains news reports on various crime incidents collected from different sources, is used. This corpus contains 502,609 tokens. LOLO, which is a system for extracting statistical information from Arabic or English corpora developed by Almas and Kurshid (2006), is used for conducting the analysis phase, i.e. frequency analysis, collocation analysis and concordance analysis, because it is able to manage, process and visualise collections of multilingual texts.

All languages can be divided into two types: general language and special language (or sub-language) (Almas & Kurshid, 2006); they can also be called open domain and restricted domain (Diekema, Yilmazel, & Liddy, 2004). With regard to specialist language, every context or sector has its own, and so each specific text domain has its own special vocabulary and idiosyncratic syntactic structures. Hirschman and Sager (1982) described sub-language as, “the particular language used in a body of texts dealing with a circumscribed subject area (often reports or articles on a technical specialty or science field) in which the authors of the documents share a common vocabulary and common habits of word usage”. Thus, recurrent expressions regularly appear in restricted language, and these repeated patterns are the key to processing textual documents in any specific domain (Hirschman & Sager, 1982). According to Diekema et al. (2004), systems that work on domain-specific texts have to use specific extraction methods.

Much like the English language, Arabic texts are comprised of two types of linguistic units. Firstly ‘closed class’ words such as prepositions, determiners and conjunctions, which indicate the natural language. Secondly, ‘open class’ words such as verbs, nouns and adjectives, which indicate the topic (Traboulsi, 2006). More open class words are used in specialist texts; these words are often distinct and occur more frequently. The use of open class words together with well-defined words may reveal sentences governed by local grammar, which is described as syntactic restriction (Traboulsi et al., 2009). Frequency analysis is able to present the most frequent words in the corpus being studied. Therefore, the first hundred open class words are selected after removing prepositions and conjunctions (closed class). As already mentioned, the open class words are rich in terms of the information that they carry because they reveal or indicate the topic of the document. It is found that some words within the first hundred most frequent words can be considered highly informative. These words are divided into three groups based on the type of information that they convey or indicate. Table 9
                         lists crime action words. Table 10
                         presents the word ‘nationality’, which is usually used to illustrate a person’s nationality in Arabic texts. Finally, Table 11
                         shows the words that are often used for stating locations.

The nature of the event can be obtained directly through certain words, such as “
                           
                        / qtl/ murder” and “
                           
                        / bi-srqt/ in theft” in Table 9. Moreover, the word “
                           
                        / aljensyt/ nationality” in Table 10 and the words relating to location, such as “
                           
                        / mntqt/ area”, “
                           
                        / madint/ city” and “
                           
                        / mohfdt/ province” in Table 11, are particularly useful for recognising and extracting nationality and crime location because these two entities often occur in the context of the previous words. The most significant result from this frequency analysis is that the way crime reports are written indicates that the type of crime most usually relies on using nouns instead of verbs, such as ‘theft’, which appeared twice in Table 9; in the first case “
                           
                        / bi-srqt/ in theft”, the preposition “
                           
                        / bi/ in” is attached to it, and in the second, “
                           
                        / alsrqt/ the theft”, the definite article “
                           
                        / al/ the” is attached to it. Moreover, place names are in noun form because they are proper nouns. With regards to nationality, in the Arabic language there is a type of adjective called ‘nisba’, which is used to denote pertinence, such as origin and nationality (Halpern, 2009). It is derived from a noun by adding “
                           
                        / iyy” in the masculine case or “
                           
                        / iyyt” in the feminine case, as a suffix to the noun (Condon et al., 2009; Halpern, 2009). This type of adjective also exists in English, such as ‘from Kuwait’, from which the adjective ‘Kuwaiti’ can be derived, and from ‘sun’ the adjective ‘sunny’ is derived. Therefore, these words can be considered as seeds for discovering the syntactic context of the event type, event location and nationality in order to identify the local grammar.

The following sections present the results of the collocation analysis and the concordance analysis for the words in Tables 9–11. The collocation analysis is performed to identify the most frequent collocation pairs of these words. This step is important because it can assist in determining the behaviour of these words within sentences. As a result, the most frequent words that collocate with the words in the above tables are discovered as well as their syntactic construction. Moreover, the concordance analysis is carried out in order to identify the structural patterns that contain crime type, crime location and nationality. Consequently, the dominant patterns used for stating the crime event, crime location and nationality are obtained and their local grammars can be built.

The collocation and concordance analyses of the crime action words in Table 9 were performed. It was found that these words often occur in the form of prepositional phrases and sometimes in the form of noun phrases. Because of the fact that a prepositional phrase is considered to be the complement of a head node (noun or verb), we investigated the head nodes that precede these prepositional phrases. It was found that the head nodes are transitive verbs, which are prevalent. Moreover, with regards to the noun phrases that contain a crime type, the transitive verbs are also head nodes for them. Therefore, the local grammar for extracting the type of crime shown in Fig. 3
                         is constructed based on transitive verbs. In other words, these verbs are used as keywords when they are followed by specific prepositions, i.e. syntactic constraint is applied.

As mentioned previously, the nationality type is one of the targets that need to be recognised and then extracted. It is has been found that the word “
                           
                        / aljensyt/ the nationality”, which is the word usually used to illustrate a person’s nationality, is within the list of 100 tokens. The frequency of “
                           
                        / aljensyt/ the nationality” (singular with the definite article “
                           
                        / al/ the”) occurs 676 times. Other forms of this word were found, and their frequencies were also counted. For example, the words “
                           
                        / jensyt/ nationality” (singular without definite article) and “
                           
                        / jensyat/ nationalities” (plural) occur 198 and 53 times, respectively. On the other hand, the word “
                           
                        / aljensyat/ the nationalities” (plural with definite article “
                           
                        / al/ the”) occurs 33 times.

The collocation and concordance analyses of all these words were performed. The results show that nationality is often represented by a word that immediately follows the above words with a syntactic constraint. The syntactic condition is that the words “
                           
                        / aljensyt/ the nationality”, “
                           
                        / jensyt/ nationality”, “
                           
                        / jensyat/ nationalities” and “
                           
                        / aljensyat/ the nationalities” must be assigned the genitive case by the preposition (governor) “
                           
                        / min/ from”. However, there is one exception: “
                           
                        / aljensyt/ the nationality”; when this word is not preceded by the preposition “
                           
                        / min/ from”, the word that occurs instead of the preposition is considered to indicate nationality. The following Fig. 4
                         depicts the nationality local grammar that was generated based on the syntactic analysis.

Likewise, collocation and concordance analyses for the words in Table 11 were performed in order to investigate their contexts and to obtain a fuller picture. The results show that place names (crime locations) are often represented by words that immediately follow them. This is similar to the English compositions ‘city of’, ‘province of’ and ‘region of’, all of which constitute a construct state in terms of grammar. Accordingly, these words are chosen as keywords in order to recognise and extract a crime location. Although these words are often assigned the genitive case as objects of specific prepositions within prepositional phrases, or as the second noun of specific construct heads in construct sates, there is no need to apply a syntactic constraint. Fig. 5
                         describes the location local grammar.

The core of this work is the exploitation of the local grammar in order to extract crime type, crime location and nationality. The system consists of four stages, as follows:
                        
                           •
                           Initial preprocessing stage.

Information extraction stage.

Intermediate preprocessing stage.

Clustering Stage.

The proposed architecture and its components are shown in Fig. 6
                     .

This stage is comprised of four components. Each component is described as follows:

Text mining research relies on the availability of a suitable corpus. As a result, many corpora have been created for specific purposes. For this research, the corpus has been collected from different Arabic newspapers published in different Arabic countries, such as Alriyadh, Aljazeera, Okaz and Sabq from Saudi Arabia, Elkhabar and Echoroukonline newspaper issues from Algeria, Addustour from Jordan, Ahram and Massai Ahram from Egypt, Alqabas and Alraimedia from Kuwait and Albayan and Alkhaleej from United Arab Emirates. The reason for compiling this corpus from different resources is to avoid the problem of bias, which could occur if the system is tested on documents that were collected from only one country.

An important step in the processing of textual documents, which takes place before information extraction and clustering, is tokenisation. It allows the unstructured text to be split into tokens, which assists the system in processing the text. As a result, each textual file is represented through one vector.

Because there are spelling variations in the Arabic language, and because some letters that perform the same function are written in different forms, it is necessary to employ a normalisation strategy. There are two normalisation strategies; the first is related to the letter “
                              
                           ”; this may appear in text as “
                              
                           ” (with hamza above), “
                              
                           ” (with hamza below) or “
                              
                           ” (with maad above), and these will be normalised to “
                              
                           ”. The second letter is “
                              
                           ”, which may appear as “
                              
                           ”, and this will be normalised to “
                              
                           ”. The reason behind this process is to make the corpus more consistent.

In most text mining systems, the functional words, such as prepositions and conjunctions, as well as punctuation marks, are usually removed during filtering. These stopwords, which are found extensively in the text, have no useful meaning. In this framework, there are two filtering processes: early filtering and advanced filtering. In the early filtering, all the stopwords, punctuation marks and numbers are removed with the exception of some prepositions, as shown in Table 12
                           ; these are retained for the process that follows. These stopwords, i.e. the prepositions, are considered in this research to be central to the operation of the system in that they play a significant role in recognising the type of crime and nationality.

Information extraction is the process where relevant information is extracted from a document. This is achieved by using the developed local grammar. Moreover, the information extraction process creates a higher quality of data for the clustering process. This system performs three different tasks in order to extract three different types of information, the specific process for each type of information is explained below.

As shown above, our study of the crime domain corpus has led us to identify the characteristics of the language used (crime language). The types of offence usually occur within the transitive grammatical structure.

For extracting crime types the system looks for words in a text that match the words in the verb list, and when a match occurs, the system will look for the first possible associated preposition that follows that verb, i.e. transitive construction is achieved. As a result, the system could avoid using annotated corpus by using prepositions to achieve the syntactic constraint. After that, the three words that immediately follow the preposition are extracted, and within these three words, the crime word should be present. Moreover, these extracted words are used to describe the document during the clustering stage that will follow. The reason that the three words after the preposition are extracted is to increase the probability of the crime word being identified. In Arabic, the crime action word sometimes does not appear immediately after the preposition, as already seen in the concordance result of “
                              
                           / twarat/ involve”, i.e. it is preceded by a noun. Another point to consider in this grammatical construct is that sometimes the preposition does not always immediately follow the verb. An example of this is illustrated in Fig. 7
                           , where the preposition, which is followed by the type of crime noun, appears nine words after the verb. If these nine words were removed, effectively abbreviating the sentence, the meaning can still be inferred from the head node (verb) and the preposition and noun (prepositional phrase), therefore, removing the nine words shown in the example does not detract from the key meaning of the sentence.

As well known, the Arabic language is an agglutinant language, and thus we must consider some agglutination cases. In Arabic, the preposition can either be separate from the noun, or fused together with the noun to form a single word, as previously illustrated. Some verbs, such as “
                              
                           / taard/ subjected” and “
                              
                           / aatrf/ confessed” (in the transitive construction form) come with the prepositions “
                              
                           / li/ to” and “
                              
                           / bi/ by”, which must be attached to the following noun. Accordingly, this stage is also designed to deal with the case of clitics.

Moreover, the Arabic language is rich in terms of morphology, whereby a word can be broken down into its base form and affixes, and usually it is the base, or root, of the word that is kept in dictionaries for extraction purposes. Examples in Arabic include verbs that have suffixes or prefixes attached to denote gender or plurality, and some of these affixes change the word into a noun. However, the proposed framework maintains a list of verbs in the past tense instead of the base or root form; this is because most news reports about crime are written in the past tense. The system uses N-gram to recognise many of the inflected forms of the verb by identifying the keyword, i.e. the past tense of the verb, from which the inflected forms are derived. These verbs can also produce nouns and such nouns in Arabic, derived from the verb, are often followed by the same preposition that the verb takes. Examples of the keywords (past tense of verbs) and their associated inflected forms are shown in Table 13
                           . The advantages of this approach are, firstly, not all of the words in the document need to be stemmed, as with other text mining systems, and secondly, it reduces the amount of required keywords in the list. Also, there is no need for an annotated corpus, in other words, the framework has no linguistic components, such as POS taggers. Instead, lists of intransitive verbs and their prepositions are provided to the system in order to extract the desired patterns.

As mentioned earlier in Section 4.4, the word “
                              
                           / jnsyt/ nationality” in the form of singular or plural (“
                              
                           / jnsyat/ nationalities”) is usually used to illustrate a person’s nationality in crime news reports, and its linguistic context is usually the genitive construction. As a result, for extracting nationality patterns from Arabic crime texts, the nationality local grammar previously presented is utilised to perform this task. The system looks for words in a text that match the words in the nationality keyword list, and when a match occurs, the system checks the syntactic construction of the word to determine whether or not it is in a genitive construction. Therefore, the system will check whether the word is preceded by the preposition “
                              
                           / min/ from” in order to achieve the syntactic constraint. If the condition is achieved, then the word that immediately follows the keyword is extracted as a nationality. However, there is one exception; the word “
                              
                           / aljensyah/ the nationality” in singular form with the article “
                              
                           / al/ the” attached. When there is no preposition before it, the word that occurs instead of the preposition is identified as the nationality.

In order for the system to extract the place names, the location local grammar is used without syntactic constraint. Therefore, once matching occurs between any trigger word in the location keywords list and a word in the contents of the file, the word that follows the keyword is extracted and classified as a location name. Using this linguistic technique serves to overcome the lack of any capital letter feature; this is not available in the Arabic language, so it cannot be used as a clue for extracting proper names, as in the English language.

The following are the two outcomes of the information extraction stage:
                              
                                 1.
                                 Summarisation.

A summary is a condensed copy of the original document, containing only the essential information. The idea of summarisation is to reduce the length of the document, retaining only key information and the overall meaning.

It is from these summaries that the clustering will be produced according to the type of crime. These summaries are considered to have meaningful information about the documents required for the clustering process. Because the extracted data are considered to be of high quality, the quality of the clustering result will also be of a high standard.

Generating dictionaries .

It is an important aspect of this work that the dictionary that is generated for the system is done so automatically, i.e. there is no manual building of the dictionary. It is necessary, first of all, to explain the function of the dictionary in the crime profiling system, followed by an explanation of how the dictionary is generated automatically. As explained in the previous section, the summaries are created to be used in the clustering process that follows. When these summaries are created, in some cases, the system may return an empty result, for example the system may not identify a location from the report, however, a word denoting the location may exist in the text but will not be identified as its context does not fall under the system’s rule, i.e. location local grammar. In order to overcome this failing the system will check the document for a word that, for example, denotes location by matching with the generated dictionary. There are three dictionaries, one for each information type, i.e. crime type, location and nationality.

As mentioned above, the three dictionaries are generated automatically, which is a distinctive feature of our system. For the location and nationality categories, the words that are in the summary are automatically sent to their respective dictionaries; these words include both relevant and irrelevant words. The way that the dictionary determines whether or not specific words should be used in the matching, i.e. that they are relevant words, is by checking the frequency of each word; it is expected that relevant words, e.g. Dubai, will occur more frequently, and when a word reaches a certain frequency threshold, it will be used in matching. In order to generate the dictionary for type of crime, the system only extracts from the summary the word that immediately follows the preposition. Fig. 8
                                     presents the whole process of the early filtering stage with the information extraction stage. It shows the transformation phases for the text being processed through each process.

The intermediate preprocessing stage is comprised of four processes, which are post filtering, stemming, generating words indices, and document representation. The main goal of this stage is to prepare the extracted data from the summarising stage in adequate form in order to be processed in the clustering stage, where they can be visualised.

Software called Multilingual Morphological Analysis (MMA), developed by Al-Marghilani (2008), is used to implement the processes of this stage.

The post-filtering process of the intermediate preprocessing stage is designed to remove the prepositions (see Table 12) that were retained for the information extraction stage but were not removed in the early filtering process. Therefore, the size of the data is reduced.

Once all crime reports are summarised and the dictionaries are automatically constructed, the summarised files and dictionaries are ready to be stemmed. As already explained, in order to obtain the root of a word, all suffixes, prefixes and/or infixes are removed. Table 14
                            shows three cases with the word “
                              
                           / srq/ steal” in Arabic, and how the system deals with them.

Thus, this process removes all the affixes from the words, reducing them to their stems. Stemming in the proposed system is required because it makes it easier for the system to allocate numbers for generating the words’ indices, which are used in the clustering process. Furthermore, it assists in applying the frequency analysis process in an efficacious manner.

The clustering process only has the ability to process numerical data, not text, and therefore it is necessary to allocate to each word a specific number. Once the summarised files have been stemmed, each file is assigned a set of numbers, whereby each number corresponds to just one word within the file. Table 15
                            presents a sample of the crime action words with their unique numbers.

Because the methods of classification or clustering are not able to directly process unstructured data, a document representation method is required to assist these techniques, so that textual documents can be handled effectively (Jing, Huang, & She, 2002; Amine, Elberrichi, Simonet, & Malki, 2008). Therefore, texts need to be transformed into an appropriate form, one that computers can process. There are many methods for representing free texts, such as Vector Space Model (VSM), Bag of Phrase, N-gram, and ontology based representation (Amine et al., 2008). However, in the preprocessing phase, a typical document clustering process uses VSM (Freeman, Yin, & Allinson, 2002). Accordingly, the clustering technique in this work uses VSM for document representation.

For the clustering process, the Self Organising Map (SOM) technique has been chosen to cluster the documents that were generated by the information extraction process, based on their similarity. The SOM technique is popular and widely used for clustering and visualising high dimensional data spaces. According to Eyassu and Gamback (2005), SOM has many different structures but the most popular architecture is composed of two layers of processing units; the input layer and the output layer. These two layers are fully interconnected. The idea behind SOM is that it performs mapping for similar input vectors to similar areas on the output grid.

@&#EXPERIMENTS@&#

In the following experiments, 401 crime reports collected from various online news sources and comprising 57,595 tokens are used to test the performance of the Crime Profiling system (CPS) for extracting the aforementioned entities in order to generate summaries for each report and to automatically build dictionaries. Also, another 401 crime news reports (71,882 tokens) are used to perform the clustering in order to show how the proposed information extraction approach guides the Self Organising Map (SOM) to gain improvement in clustering quality. Furthermore, based on extracting crime-related information from 80 reports randomly selected from the above corpus, graphs and tables are generated for providing statistical information about the crime status in the locations.

In this experiment, the CPS crime type local grammar was assessed for its ability to recognise and extract the crime type from each report, and to generate a summary for each report. The system was able to extract 758 tokens. The number of entities that were correctly recognised is 398 out of 496. The extracted patterns were then sent to the crime type dictionary file. The dictionary was filtered to remove certain stopwords, and its size was accordingly reduced to 676 tokens. Furthermore, a frequency analysis was carried out after the filtering process, and this led to reducing the number of tokens to 380. In order to obtain the words only in their base form, their affixes were removed, and consequently, the size of the dictionary became 228 words. Fig. 9
                         shows a comparison of the content of the dictionary before and after the stemming process. It can be seen that the frequencies of the various forms of the word “
                           
                        / srqt/ theft” were collated into one frequency, occurring 122 times. Also, it can be noticed that the most frequent words either before or after the stemming are crime action words. Moreover, four crime action words (“
                           
                        / gsb/ rape”, “
                           
                        / ktf/ snatch”, “
                           
                        / raj/ smuggle” and “
                           
                        / kdr/ drug”) rose to within the list of the 16 most frequent crime words following the stemming process.

The crime type dictionary was tested to see if it could identify any crime types that the crime type local grammar had failed to extract. The experiment shows that using the crime type dictionary assisted in recognising more crime types, and therefore, the number of entities correctly identified increased to 481 (from 398 entities). Consequently, only 15 types of crime were not extracted. However, the types of crime that were wrongly extracted also increased to 416 (from 360).

The experiment here is dedicated to testing the CPS location local grammar in order to assess its ability to extract crime location from the given reports to generate summaries and to automatically build the location dictionary. The system was able to correctly recognise 368 out of 475 location entities. The CPS location local grammar was initially able to extract a collection of tokens, which form the location dictionary. The frequency analysis process was also applied here. Consequently, the location dictionary contains only 131 different location names. As already mentioned, the dictionary is used when the CPS location local grammar fails to extract the crime location. In the following experiment, the location dictionary was tested to extract crime locations that had not been identified directly by the location local grammar. It is found that the assistance of the location dictionary has led to increasing the number of location entities that were correctly identified to 447 (from 368). As a result, 28 crime locations were not recognised either by the local grammar or the dictionary. Also, using the dictionary increased the incorrectly recognised entities to 19 (from 8).

The CPS nationality local grammar was tested in this experiment in order to assess its ability to extract nationality entities from the same datasets used in the above experiments. The CPS was able to extract 88 entities; 80 correct out of 210 entities. As a result, 8 entities were wrongly identified.

The system was able to generate the nationality dictionary; here, the number of tokens extracted was 88. The processes of removing affixes and a frequency analysis were then applied. As a result, the number of words that form this dictionary is only 21. Likewise, the nationality dictionary was tested to extract the missing entities that had not been identified by the nationality local grammar. In this experiment, the number of nationality entities that were correctly identified after using the nationality dictionary improved to 185 (from 80). Also, the total number of nationality entities that were incorrectly recognised increased to 24.

Two experiments were carried out on 401 documents in order to show how the information extraction process guides the Self Organising Map (SOM) toward delivering acceptably accurate results. The corpus contains 71,882 tokens. The SOM was trained on the same documents, obtaining good results; the best learning rate, radius and iteration are 0.5, 30 and 1000, respectively. The size of the map is 6×6.
                           
                              •
                              Clustering with utilising the CPS information extraction stage.

As explained earlier, the information extraction process was employed to extract the types and locations of the crimes as well as the nationalities, and then a summary for each file as well as three dictionaries were generated. In this experiment, we focus on the type of crime. The extracted crime type patterns from each document are used by the SOM to perform the clustering, instead of processing the whole of each document’s content. Accordingly, after extracting the type of crime, the new size of the corpus is now 4043tokens (13KB), which is much smaller than the original size of 40KB (71,882 tokens). Fig. 10
                                  shows a sample of the document clustering results based on type of crime, using the extracted patterns obtained from the previous processes.

Clustering without using the CPS information extraction stage.

For assessing this work in terms of the effectiveness of the clustering, another experiment was carried out on the same corpus, but this one did not rely on the information extraction process. The whole content of each file was stemmed and used for the clustering process through the SOM. A sample of the results of this experiment can be seen in Fig. 11
                                 .

An additional benefit of this work is that this current system can be easily adapted to provide crime profiling for regions. In other words, it can be used to present a general picture about the security status of any area, based on local news reports. The system can offer statistical information about the highest and/or lowest type of crime. Fig. 12
                         shows that the crime of theft is the most common crime occurring in the Arab region; it is reported 31 times in our corpus (80 crime news reports). In addition, extracting the crime location can assist in identifying how safe a particular area is, and through combining such statistics, this system is able to provide information about the number of crimes occurring in a specific location.

@&#EVALUATION@&#

This section is dedicated to evaluating the performance of the Crime Profiling System (CPS), i.e. it assesses the efficacy of the crime type, location and nationality local grammars used in this research as well as evaluating the effect of utilising dictionaries on the performance of the CPS. The system is evaluated using precision, recall and F-measure. Additionally, the efficacy of the information extraction approach with respect to the performance of the Self Organising Map (SOM) is evaluated through three parameters: data size, loading time and quantization error.

The performance evaluation of information extraction (IE) systems is carried out by comparing the answer file (result) that was automatically generated by the system against the same texts that were manually produced by humans (gold standard file) (Cowie & Lehnert, 1996). Accordingly, each of the 401 crime news reports was read to identify each type of crime, crime location and nationality. We manually annotated all the aforementioned information in the crime reports and then we counted the total number of relevant entities that should be extracted by our system in order to evaluate it in terms of precision (P) and recall (R). Although several evaluation metrics can be found in the literature, the most popular and official metrics (mentioned in a series of Message Understanding Conferences MUS3, MUC4 and MUC6) are P and R (NØklestad, 2009; Gaizauskas & Wilks, 1998). For more details about evaluation metrics see Gaizauskas and Wilks (1998) and Chinchor (1992).


                        Gaizauskas and Wilks (1998) defined recall as “a measure of the fraction of the required information that has been correctly extracted” and precision as “a measure of the fraction of the extracted information that is correct”. These two metrics are combined using F-measure, which is the weighted harmonic mean of precision and recall, because there is usually a trade-off between precision and recall (van Rijsbergen, 1979; Shaalan & Raza, 2007). The precision, recall and F-measure are formulated as follows:
                           
                              
                                 Precision
                                 =
                                 
                                    
                                       number
                                       
                                       of
                                       
                                       correctly
                                       
                                       recognised
                                       
                                       entities
                                    
                                    
                                       total
                                       
                                       number
                                       
                                       of
                                       
                                       recognised
                                       
                                       entities
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 Recall
                                 =
                                 
                                    
                                       number
                                       
                                       of
                                       
                                       correctly
                                       
                                       recognised
                                       
                                       entities
                                    
                                    
                                       total
                                       
                                       number
                                       
                                       of
                                       
                                       correct
                                       
                                       entities
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 F
                                 -
                                 measure
                                 =
                                 
                                    
                                       2
                                       
                                       ×
                                       
                                       recall
                                       
                                       ×
                                       
                                       precision
                                    
                                    
                                       recall
                                       
                                       +
                                       
                                       precision
                                    
                                 
                              
                           
                        The performance results obtained after evaluating the CPS in terms precision, recall and F-measure for the crime type, location and nationality extraction processes are presented in the following sections.

The system was able, directly (using the crime type local grammar) and through using the crime type dictionary, to extract a total number of 897 entities. The number of entities that were correctly recognised is 481 out of a total number of 496 relevant entities. The performance results achieved for the crime type extraction process through using the crime type local grammar and the crime type dictionary are presented in Table 16
                           .

The results derived from the 401 crime news reports show that using the crime type dictionary has enabled the CPS to perform better (F-measure 69%). These results indicate that the CPS was able to build a reliable crime type dictionary. This means that the performance of the crime local grammar seems satisfactory, either for building the dictionary or correctly recognising the type of crime (by obtaining a recall score of 80%). The remaining unidentified entities result from the crime action words being outside the local grammar, i.e. they were used for describing the incident in the form of a verb. As can be seen, the assistance of the dictionary has led to improving the recall result to 97%. However, the precision value seems low, because the sentences’ boundaries were neglected. In other words, removing commas and full stops caused some confusion for the system while the text was being processed. As a result, when the system was searching for the preposition that should follow the verb (although not necessarily in all cases) in order to achieve the syntactic constraint (for the transitive construction), it proceeded into the following sentence in order to find that preposition, and this led to incorrect pattern extraction. Fig. 13
                            shows an example explaining this case.

The verb “
                              
                           / twrt/ involved” in the first sentence is in the intransitive construction because its companion preposition “
                              
                           / fi/ in” does not follow it. Therefore, the system should ignore this verb because it is not a transitive verb, but because the full stop between the two sentences is removed, the system carries on searching for the preposition “
                              
                           / fi/ in”. As a result, the system selects the preposition “
                              
                           / fi/ in” located in the second sentence. This preposition, performing a special role within the second sentence, has no relationship with the first sentence. Hence, a false transitive verb is used to extract a type of crime, which leads to an incorrectly extracted pattern. As a consequence, commas and full stops should be retained, and the text should be split into sentences using the sentence splitter process.

The evaluation results vis-á-vis extracting crime type with or without the crime type dictionary have been compared with the system developed by Abuleil (2007), which, to our knowledge, is the only system that has been developed for the Arabic language to extract events from within text (although not specific to any particular domain). It was chosen for comparison with the CPS because there is no system available that has been specifically developed for the Arabic crime domain. However, to overcome this problem, we compared our CPS with the system created for the English crime domain by Ku et al. (2008); both systems were discussed in Section 2. Fig. 14
                            shows the comparison between the three systems in terms of their performance in extracting events.

Although the performance comparison in terms of F-measure shows that the systems developed by Abuleil (2007) and Ku et al. (2008) obtained results better than our system (CPS), both systems use external predefined event gazetteers, which leads to obtaining high precision scores. On the other hand, the CPS does not utilise any external event list, rather it makes use of the automatically built crime type dictionary. However, the CPS outperforms the others in terms of the recall score.

As already seen in the previous section, the CPS was able to correctly recognise 447 entities out of 475. Table 17
                            shows the evaluation results for the location extraction process, using the location local grammar and the location dictionary.

The precision and recall results show that the location local grammar was able to extract 77% of the location entities, with a 98% precision rate. On the other hand, using the location dictionary has led to increasing in the recall result to 94%, with a precision rate of 96%. It can be noticed that, after utilising the location dictionary, the precision is slightly decreased; this is because of a lack of semantics. It is found that the location dictionary contains personal name entities; these were extracted and classified as locations by the location local grammar because they are also location names. As a result, incorrect recognition occurs when the dictionary, which neglects semantics, is used by the CPS. Moreover, sometimes crime locations cannot be discovered by the location local grammar, and therefore the CPS fails to capture them. For example, in the sentence “
                              
                           / aljrimt hdthat fi dubai/ the crime happened in Dubai”, the city name “Dubai” cannot be identified because it does not follow these words: “
                              
                           / mantqt/ area”, “
                              
                           / bi-mantqt/ in area”, “
                              
                           / mohafdat/ province” or “
                              
                           / mdynt/ city”. However, the location dictionary overcomes this problem, and it increased the recall value to 94% (from 77%). Consequently, the F-measure score increased to 95% (from 86%).

The CPS performance in terms of extracting location entities with and without utilising the location dictionary (i.e. the latter only using the location local grammar) was compared with systems that utilise a rule-based approach (TAGARAB Maloney & Niv, 1998; Mesfar (2007); NERA Shaalan & Raza, 2008 and Ku et al. (2008)), as in Fig. 15
                           .

Also, Fig. 16
                            provides a comparison between the CPS (with and without using the location dictionary) with other systems that were developed based on machine learning approaches (ANERsys using Maximum Entropy (ME) Benajiba et al. (2007); ANERsys using Conditional Random Field (CRF) Benajiba et al. (2008) and AbdelRahman et al. (2010)).

The comparison accuracies of the CPS against these other systems show that, with the assistance of its dictionaries, the CPS is the second best system (after NERA Shaalan & Raza, 2008) in terms of recall. However, a predefined location dictionary containing 4900 names was utilised in NERA (Shaalan & Raza, 2008). Moreover, the CPS is approximately equal to the top system AbdelRahman et al. (2010) in terms of precision, although AbdelRahman et al. (2010) also used a predefined location dictionary (2183 names). However, the CPS achieved the best performance result (F-measure 95%).

As already seen, the nationality local grammar and nationality dictionary together were able to correctly recognise 185 entities out of 210, with 24 entities wrongly extracted. The results of the performance evaluation for the CPS with using the nationality local grammar and nationality dictionary are listed in Table 18
                           .

The results for precision and recall obtained by applying only the nationality local grammar in recognising nationality entities in the above dataset are 91% and 38%, respectively. Although a high precision value is obtained, the rate for recall is too low, i.e. many nationality entities were not identified. This means that certain entities appear to be outside the nationality local grammar. In some newspapers, it is found that a nationality word (e.g. Saudi, Indian or British) is coupled with the word “
                              
                           / wafd/ expatriate”, e.g. “
                              
                           / wafd hndy/ Indian expatriate” instead of using the word “
                              
                           / aljnsyt/ nationality”, e.g. “
                              
                           / hndy aljnsyt/ Indian nationality”. This has led to obtaining a low recall score. However, the dictionary plays a crucial role here, and the results of utilising the nationality dictionary show that the recall rate is increased to 88%. Therefore, the average F-measure value increased to 88% (from 54%). There is, to the author’s knowledge, no system developed for extracting this type of entity in Arabic text.

Accordingly, Table 19
                            lists the overall performance results for the CPS. As can be seen, the performance of the CPS is improved through utilising the dictionaries in terms of precision, recall and F-measure.

The Self Organising Map (SOM) was used for the clustering and visualisation tasks, and for assessing the effectiveness of the proposed approach on the SOM outputs (i.e. in terms of its clustering performance). As already seen, the SOM was able to cluster 401 texts and to visualise them. The evaluation phase here is performed based on three parameters, as follows:
                           
                              •
                              Data size

Loading time

Quantization error

With regards to the size of data, the significant point here is that using the CPS led to a huge reduction in the quantity of data fed into the SOM. Although our system reduced the size of the corpus from 71,882 tokens (40KB) to only 4043 tokens (13KB), the most important data (that the SOM used for the clustering task) were not affected. Thus, these 4043 tokens can be considered as effectively representing the original 71,882 tokens. The clustering experiment (utilising the CPS information extraction stage) was assessed by comparing it with a clustering of the same documents but without the CPS information extraction stage, i.e. where the SOM processed the whole of each document’s content. The loading time after using the CPS was 1.51s, and in the other experiment (without the CPS) the loading time was 3.99s, i.e. the loading time was reduced by more than a half with the CPS.

The average distance between each data vector and its quantization error in the experiment that was supported by the CPS was between 0.462 and 0.47, but in the second experiment (without the information extraction process), the quantization error was between 0.59 and 0.594 (see Fig. 17
                           ). Therefore, the performance of the SOM in the experiment that relied on the CPS information extraction represents an improved technique in terms of the quality of clustering; the information extraction process has a strongly positive effect on the performance of the SOM.

@&#CONCLUSION@&#

This paper has proposed a Crime Profiling System (CPS) for extracting meaningful information, i.e. crime type, crime location and nationality, by utilising syntactic construction. Moreover, it has been shown that the system is able to extract this information from an unannotated corpus to generate summarisations, to automatically construct dictionaries and to cluster Arabic crime texts (employing the Self Organising Map (SOM) technique). Also, the developed system can assist in crime analysis in terms of providing other useful information, e.g. general and specific crime trends (frequencies of crime within a particular area) to law enforcement bodies or the general public.

The performance of the CPS for extracting the aforementioned crime-related information using both the local grammars and dictionaries were evaluated using standard precision, recall and F-measure. Moreover, the use of the automatically constructed dictionaries helped in improving the performance of the CPS. The overall results obtained were: for precision 71%, recall 94% and F-measure 81%. Also, to evaluate the effectiveness of employing the CPS information extraction approach on the Self Organising Map (SOM) clustering technique, three parameters (size of data, loading time and quantization error) were used. This evaluation, performed through comparative experiments, was conducted between the SOM clustering technique with and the SOM clustering technique without using the CPS information extraction stage. The results show that the SOM is improved because only refined data containing meaningful keywords (extracted through the information extraction process) are inputted into it. As a result, a huge reduction in the quantity of data fed into the SOM is obtained, consequently, saving memory, data loading time and the execution time needed to perform the clustering. Therefore, the computation of the SOM is accelerated. Finally, the quantization error is reduced, which leads to high quality clustering.

In terms of contribution to the field, the approach developed herein, for extracting crime information, does not rely on an annotated corpus, and speech taggers are not used in this task, rather, computational linguistic techniques, based on transitive and genitive constructions, are used, i.e. the local grammars for crime type, location and nationality are applied to texts in order to extract the aforementioned information. The prepositions that are often described as stop-words (non-useful words, and often neglected and removed) play a crucial role in obtaining the desired transitive and genitive constructions. Furthermore, traditional systems usually rely on manually built dictionaries, which is time consuming. The CPS is able to automatically generate crime type, location and nationality dictionaries from unlabelled data to assist in extracting patterns from texts. Also, the system produces higher quality extracted data, which in turn improves the quality of the clustering results. Also, a special corpus has been compiled for the Arabic crime domain.

Finally, future work will investigate improving the CPS further in order to extract more information about crime, such as type of perpetrator (e.g. group or individual) and the instruments used in crime incidents, such as weapons and vehicles. However, it seems clear that the same approach would be used to extract the above information, i.e. the transitive construction. In the frequency analysis, the words “
                        
                     / qbd/ arrested” and “
                        
                     / athr/ found” appeared, and they may assist in extracting the crime perpetrator and instrument entities if they are followed by the preposition“
                        
                     / ala/ on”. Experiments will be performed to examine these two verbs.

@&#REFERENCES@&#

