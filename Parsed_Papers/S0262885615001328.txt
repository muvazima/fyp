@&#MAIN-TITLE@&#Invariant texture classification using a spatial filter bank in multi-resolution analysis

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel idea has been presented for invariant texture classification.


                        
                        
                           
                           The idea uses a combination of wavelet analysis and spatial filter bank method.


                        
                        
                           
                           The method has been tested on a variety of texture databases.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Texture classification

Multi-resolution analysis

Wavelet transform

Spatial filter bank

@&#ABSTRACT@&#


               
               
                  This paper proposed a new method based on spatial filter banks and discrete wavelet transform (DWT) for invariant texture classification. The method used a multi-resolution analysis method like DWT and applied the proposed filter bank on different resolutions. Then, a simple fusion of features on different resolutions was used for invariant texture analysis. A comprehensive study was done to examine the effectiveness of the proposed method. Different datasets with different properties were used in this paper such as Brodatz, Outex, and KTH-TIPS for the evaluation. Local binary pattern (LBP) methods have been one of the powerful methods in recent years for invariant texture classification. A comparative study was performed with some state-of-the-art LBP methods. This comparison indicated promising results for the proposed approach as compared with the LBP methods.
               
            

@&#INTRODUCTION@&#

Textures are one of the most important features in image processing and machine vision. Remote sensing, medical image segmentation, etc. are different areas where texture analysis is frequently used. Many authors proposed different methods for texture analysis. Texture classification, segmentation, and synthesis are three main categories of texture analysis. In [1] texture classification is divided into four main groups, including statistical methods, filter-based techniques, model-based schemes, and structural approaches.

Statistical features are normally extracted from the first order and the second order statistics of texture. Moreover, second order statistical methods are divided into three categories, namely the gray-level difference methods, the spatial gray-level difference techniques using analysis of co-occurrence matrix (GLCM) proposed in [2], and the gray-level run schemes. Wouwer et al. [3] proposed wavelet co-occurrence signatures, then they extracted the GLCM features from different channels of wavelet decomposition. Local binary patterns (LBPs) proposed by Ojala et al. [4] is one of the most important statistical features that has attracted much attention in recent years. In this method, at first, a neighborhood around each pixel is defined, then it exploits the relationship between neighborhood pixels. Many extensions of this approach have lately been proposed, which have good robustness against rotation, illumination, and noise [5,6].

Using spatial filter banks for texture analysis has been proposed by different authors. Leung and Malik [7] proposed a spatial filter bank, including 48 different filters for illumination and posed invariant texture classification. Later on, Varma and Zisserman [8,9] used a different filter bank for texture classification. They proposed a special filter bank, namely the maximum response sets (MR8) in [8] for rotation and scale invariant texture classification. Their filter bank consisted of 38 filters, which contained 2 isotropic filters, such as Gaussian and Laplacian of Gaussian, and also two anisotropic filters at 3 different scales and 6 different orientations. These methods intrinsically had a problem that lost frequency information in texture analysis. Ghita et al. [10] presented a good comparison between the LBP and different filter bank methods on different datasets with a variation of texture sizes and orientations.

Spatial-frequency filters like discrete wavelet transform (DWT), complex wavelet transform (CWT), and Gabor wavelets are powerful tools for texture analysis. Chang and Kuo [11] proposed a tree structure wavelet transform on the DWT. In this method, until the energy of each decomposed image is low as compared with other channels, the DWT is applied. Next, l
                     1-norm energy was extracted from different channels, then it was used for texture classification. Kashyap and Khotanzad [12] proposed a model-based method for rotation invariant texture classification. This model was among the first methods in this field. Translation invariant wavelet transform and Radon transform were used in [13] to extract rotation invariant features for texture classification. The Radon transform was used to convert rotation to translation, then their proposed features were extracted using a translation invariant wavelet transform. Classification process was performed by K-nearest neighbor classifier (KNN) with different Ks. In [14] the same authors proposed another idea that initially tried to estimate the rotation angle of the texture using the Radon transform, and after that image could be converted into the principal direction of 0°.

Furthermore, DWT was also used for extraction of invariant texture features. Porter and Canagarajah [15] proposed combining channel scheme on DWT to achieve features that were robust to the rotation of textures. These features were almost independent of directionality of textures. They proposed 4 different wavelet features that were extracted from l
                     1-norm of combination of LH and HL channels in three levels of decomposition and also LL channel in the last level. Mantalkar et al. [16] extended this method and proposed different combining schemes, then they extracted 14 different features such as standard deviation and energy features. Log-polar transform was used in [17] for extracting rotation and scale invariant features.

Kingsbury [18] proposed a dual-tree complex wavelet transform (DT-CWT) approach which was made of the Gabor filters with complex coefficients, hence it had two advantages like directional selectivity and shift invariant, as compared with DWT. Hill et al. [19] extended the idea of combining channels in [15] to the DT-CWT with 6 different channels in each level of decomposition. Xie et al. [20] presented a multi-channel filter bank based on the Gabor filters for rotation and scale invariant texture classification. Contextual information is a very important feature in image processing. Random field models like Gibbs, Markov, and Gaussian random field can properly model spatial relationship between pixels [21,22].

The conventional wavelet based methods are effective and powerful tools for texture analysis; however, they have basic shortcomings for invariant texture classification. The 2D-DWT suffers from intrinsic problem like lack of directionality, which highly complicates the processing of geometric image features, including ridges and edges. Also, they have moderate ability to deal with illumination invariant texture classification. Moreover, the spatial filter banks have a good ability to deal with the geometric image features, although they mostly have high running time because of using a lot of anisotropic filters at different orientations. Spatial filter bank methods have good results for invariant texture classification such as rotation and illumination [8]. Furthermore, spatial filter banks just consider images at one scale; therefore, they lose the frequency and spectral information of textures.

For these reasons, this paper tries to incorporate appropriate properties of the spatial filter bank methods into the DWT in order to alleviate their shortcomings. This paper proposes a combination of wavelet and spatial filter bank for invariant texture classification. A spatial filter bank is proposed, and then it is applied on different resolutions of texture. The DWT is used for multi-resolution analysis. Wavelet features extracted from the DWT are sensitive to rotation. Moreover, the rate of these changes is higher in anisotropic textures as compared with isotropic textures. By rotating the texture image, the detailed images are strongly changed. Therefore, the proposed feature vector only used LL channels for feature extraction and ignored the detailed channels. Finally, a fusion of features in different resolutions is applied for texture classification. A minimum Mahalanobis distance classifier is used for classification section. In addition, different wavelet bases are used and their results are reported.

Wavelet transform is a mathematical function that produces a multi-resolution representation of images. Wavelet transform attracts much attention in many areas like image compression, segmentation, and texture classification. Wavelet transform can produce a spatial-frequency analysis of signals. 1D continuous wavelet transform (CWT) is represented by Eq. (1).
                        
                           (1)
                           
                              C
                              W
                              
                                 a
                                 b
                              
                              =
                              
                                 1
                                 
                                    a
                                 
                              
                              
                                 ∫
                              
                              s
                              
                                 t
                              
                              φ
                              
                                 
                                    
                                       t
                                       −
                                       b
                                    
                                    a
                                 
                              
                              
                              d
                              t
                           
                        
                     where s(t) is signal and φ is wavelet function and also a and b are scale and translation parameters, respectively. The wavelet function φ
                     
                        a,b
                     (t) can be created by a mother wavelet function φ (t) using the following form that is a dilated and shifted version of the mother wavelet function.
                        
                           (2)
                           
                              
                                 φ
                                 
                                    a
                                    ,
                                    b
                                 
                              
                              
                                 t
                              
                              =
                              
                                 1
                                 
                                    a
                                 
                              
                              φ
                              
                                 
                                    
                                       t
                                       −
                                       b
                                    
                                    a
                                 
                              
                              .
                           
                        
                     
                  

The base functions of a DWT are captured via sampling from CWT, which can be expressed in Eq. (3).
                        
                           (3)
                           
                              
                                 φ
                                 
                                    n
                                    ,
                                    m
                                 
                              
                              
                                 t
                              
                              =
                              
                              =
                              
                                 2
                                 
                                    
                                       −
                                       n
                                    
                                    
                                       2
                                       
                                    
                                 
                              
                              φ
                              
                                 
                                    
                                       2
                                       
                                          −
                                          n
                                       
                                    
                                    t
                                    −
                                    m
                                 
                              
                              
                              ,
                              
                              m
                              ,
                              n
                              
                              ∈
                              N
                              .
                           
                        
                     
                  

Some of the mother wavelets, such as the Mexican Hat, Symlet, Biorthogonal, and Daubechies are proposed with different properties. Moreover, The CWT is not so efficient for implementation; therefore, DWT has been proposed because it has a simple and easy implementation. This paper used wavelet bases, such as the Symlet, Biorthogonal, and Daubechies for multi-resolution analysis. Based on the results, changing wavelet bases have a small effect on final results, although the Symlet 4-tap obtains higher results than the other methods in most of datasets.

A 2D-DWT is computed by the 1D-DWT. The filters are applied along rows and columns of the image. This property in the implementation of wavelet transform is a main reason which makes it sensitive to rotation. After applying wavelet transform, each image is divided into four sub-images, including low horizontal and low vertical frequency (LL), high horizontal and low vertical frequency (HL), low horizontal and high vertical frequency (LH), and high horizontal and high vertical frequency (HH) that consisted of low resolution image, horizontal detail, vertical detail, and diagonal detail, respectively. This procedure can be applied for several times on a low resolution image.

Furthermore, some authors proposed wavelet features that can be extracted from the different sub-bands. These features can directly be extracted from wavelet coefficients or, after some statistical process, on wavelet coefficients. First order and second order statistical features like co-occurrence matrix are one of the main groups of features that are extracted from wavelet coefficients. Some of statistical properties that have been suggested as a feature are l
                     1-norm energy [11], l
                     2-norm energy, standard deviation, and entropy [16] as first order statistical features, which can be extracted from histogram detail image. Moreover, contrast, cluster shade, and cluster prominence as second order statistical features can be extracted from the co-occurrence matrix [2,3].

In this paper, we extract three different first order statistical features such as l
                     1-norm, standard deviation, and entropy as a feature. However, these features are extracted after applying filter banks on spatial and wavelet domain which is similar to the second order features because they can also indirectly extract from both of the resolutions.

@&#PROPOSED METHOD@&#

This paper proposes a feature vector that tries to combine the abilities of the spatial filter banks and spatial-frequency analysis to confront with different challenges like rotation, illumination, scale, and pose changes of textures. Therefore, different methods with more complex filter banks, as compared with the proposed filter bank and different spatial-frequency analyses, which can be more appropriate than the DWT and also different fusion methods can be used to improve the abilities of the proposed method.

We proposed a filter bank and applied it on 2 different resolutions of textures. This filter bank was employed in two stages: first, on resolution 1 or texture samples and second, on resolution 2 or the LL channel in the first level of decomposition. Different resolutions can be used; however, using different resolutions increases computational complexity and also they do not produce much information. Fig. 1
                         shows the two different resolutions, which are used for feature extraction.Absolute value of the wavelet coefficient was used for resolution 2, as shown in Eq. (5):
                           
                              (4)
                              
                                 R
                                 e
                                 s
                                 
                                 1
                                 =
                                 S
                                 I
                              
                           
                        
                        
                           
                              (5)
                              
                                 R
                                 e
                                 s
                                 
                                 2
                                 =
                                 
                                    
                                       L
                                       L
                                       1
                                       
                                          
                                             D
                                             W
                                             T
                                             
                                                
                                                   S
                                                   I
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where Res 1 and Res 2 are two different patches captured from two resolutions of sample image and also SI shows the sample image obtained from the original image. Moreover, LL1(DWT(SI)) shows the approximation image in the first level of decomposition captured from applying 2D-DWT on sample image SI. The proposed filter bank consists of four different filters, which three of them are isotropic filters such as Gaussian (GAU) and Laplacian of Gaussian (LOG) applied on two different scales, which are named LOG1 and LOG2, respectively. Three different features like wavelet sub-band features are extracted from the filter responses. These features consisted of l
                        1-norm, standard deviation, and entropy. We use another filter such as local standard deviation (LSD), and then aforementioned features are extracted from the result of this filter, too. These features have been shown in Eqs. (6) to (9). Fig. 2
                         shows 3 different spatial filter banks in this paper.
                           
                              (6)
                              
                                 
                                    F
                                    
                                       
                                          l
                                          1
                                       
                                       −
                                       norm
                                    
                                 
                                 =
                                 
                                    1
                                    
                                       M
                                       N
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       M
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       N
                                    
                                 
                                 
                                    
                                       X
                                       
                                          i
                                          j
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    F
                                    
                                       s
                                       d
                                    
                                 
                                 =
                                 
                                    
                                       
                                          1
                                          
                                             M
                                             N
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             M
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             N
                                          
                                       
                                       
                                          
                                             
                                                
                                                   X
                                                   
                                                      i
                                                      j
                                                   
                                                   −
                                                   
                                                   μ
                                                
                                             
                                          
                                          2
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    F
                                    entropy
                                 
                                 =
                                 −
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       M
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       N
                                    
                                 
                                 X
                                 
                                    i
                                    j
                                 
                                 log
                                 
                                    
                                       X
                                       
                                          i
                                          j
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (9)
                              
                                 μ
                                 =
                                 
                                    1
                                    
                                       M
                                       N
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       M
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       N
                                    
                                 
                                 
                                    
                                       X
                                       
                                          i
                                          j
                                       
                                    
                                 
                              
                           
                        where M and N are width and height of sample and X contains the coefficients of sample image in both of the resolutions. μ also indicates the average of the wavelet coefficients. These features are extracted from spatial and wavelet domain of each texture sample. Final features are calculated by averaging of two similar features that are generated from two similar filters in wavelet and spatial domains like Eqs. (11) to (13). Then, two features are separately extracted that determine the threshold value for extracting strong edges in two different resolutions of image. These two features have been extracted from the LOG1 filter. Actually, the LOG filter is a zero-crossing filter; therefore, both features of 13 and 14 are numerical because they show the threshold value for the zero-crossings, based on jump across zero. A total of 14 different features are extracted from the wavelet and spatial domains of each texture sample. Eqs. (10) to (15) show the proposed feature vector.
                           
                              (10)
                              
                                 filte
                                 
                                    r
                                    1
                                 
                                 =
                                 LOG
                                 1
                                 ,
                                 
                                 filte
                                 
                                    r
                                    2
                                 
                                 =
                                 LOG
                                 2
                                 ,
                                 
                                 filte
                                 
                                    r
                                    3
                                 
                                 =
                                 G
                                 A
                                 U
                                 ,
                                 
                                 filte
                                 
                                    r
                                    4
                                 
                                 =
                                 L
                                 S
                                 D
                              
                           
                        
                        
                           
                              (11)
                              
                                 filte
                                 
                                    r
                                    
                                       i
                                       j
                                    
                                 
                                 =
                                 
                                 filte
                                 
                                    r
                                    i
                                 
                                 
                                    
                                       R
                                       e
                                       
                                          s
                                          j
                                       
                                    
                                 
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 
                                 4
                                 
                                 and
                                 
                                 j
                                 =
                                 1
                                 ,
                                 2
                              
                           
                        
                        
                           
                              (12)
                              
                                 F
                                 
                                    1
                                    …
                                    4
                                 
                                 
                                 =
                                 0.5
                                 
                                 ×
                                 
                                    
                                       
                                          F
                                          
                                             l
                                             1
                                             −
                                             norm
                                          
                                       
                                       
                                          
                                             filte
                                             
                                                r
                                                
                                                   i
                                                   1
                                                
                                             
                                          
                                       
                                       +
                                       
                                          F
                                          
                                             l
                                             1
                                             −
                                             norm
                                          
                                       
                                       
                                          
                                             filte
                                             
                                                r
                                                
                                                   i
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 4
                              
                           
                        
                        
                           
                              (13)
                              
                                 F
                                 
                                    5
                                    ..
                                    8
                                 
                                 =
                                 0.5
                                 
                                 ×
                                 
                                    
                                       
                                          F
                                          
                                             s
                                             t
                                             d
                                          
                                       
                                       
                                          
                                             filte
                                             
                                                r
                                                
                                                   i
                                                   1
                                                
                                             
                                          
                                       
                                       +
                                       
                                       
                                          F
                                          
                                             s
                                             t
                                             d
                                          
                                       
                                       
                                          
                                             filte
                                             
                                                r
                                                
                                                   i
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 4
                              
                           
                        
                        
                           
                              (14)
                              
                                 F
                                 
                                    9
                                    ..
                                    12
                                 
                                 =
                                 0.5
                                 
                                 ×
                                 
                                    
                                       
                                          F
                                          entropy
                                       
                                       
                                          
                                             filte
                                             
                                                r
                                                
                                                   i
                                                   1
                                                
                                             
                                          
                                       
                                       +
                                       
                                       
                                          F
                                          entropy
                                       
                                       
                                          
                                             filte
                                             
                                                r
                                                
                                                   i
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 4
                              
                           
                        
                        
                           
                              (15)
                              
                                 F
                                 
                                    
                                       13
                                       ,
                                       
                                       14
                                    
                                 
                                 =
                                 
                                    
                                       thresh
                                       
                                          
                                             filte
                                             
                                                r
                                                
                                                   1
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 j
                                 
                                 =
                                 
                                 1
                                 ,
                                 
                                 2
                              
                           
                        where thresh in Eq. (15) indicates the threshold value for zero-crossing and filter
                        
                           i
                        (Res
                        
                           j
                        ) shows the result of applying filter i on resolution j. A pseudocode for the proposed method is shown in Fig. 3
                        .

@&#EXPERIMENTAL RESULTS@&#

The proposed method was applied on different datasets such as the Brodatz, Outex, and KTH-TIPS, that some samples of which have been shown in Figs. 4, 5, and 6
                        
                        
                        . These datasets have different properties such as different number of classes, rotations, illuminations, and poses.


                        Experiments 1, 2, 3, and 4 were done on texture samples of size 128×128 at rotation angle of 0° and were tested on different angles that existed in the dataset. In all experiments, the Mahalanobis minimum distance classifier was used. In the training phase, each texture image of size 128×128 was divided into imaginary regions, then the proposed feature vector was extracted from each region. For all the images in the training set this procedure was performed. A Gaussian model was fitted on all the samples for each texture class, then a covariance matrix and means vector were calculated. In the test phase, each texture image was divided into imaginary regions and then the proposed feature vector from each region was extracted. Subsequently, a mean vector was computed for each texture image of size 128×128, then this vector was applied in the classification phase.

Furthermore, to create regions, we used non-overlapping and overlapping regions. For example, with imaginary regions of size 32×32, each image can be divided into 16 non-overlapping regions or 36 overlapping regions, although in the test phase, each image was divided into 16 non-overlapping regions only to decrease the runtime. We applied the proposed method on different wavelet bases like the Symlet, Biorthogonal, and Daubechies. The texture intensity in all datasets like the Outex_TC10 was normalized to a mean of 128 and standard deviation of 20 before feature extraction.

We provided different experiments for evaluation of the proposed method. First, our method was applied on the entire Brodatz and also Outex_TC13 datasets with original orientation of 0° with 112 and 68 different classes, respectively. This experiment showed that the proposed feature vector had a good ability to classify texture datasets with large number of classes. Then, to illustrate the impact of the proposed method on rotation invariant texture classification, we applied it on two different datasets such as the well-known Outex_TC10 and Brodatz datasets that are included 24 and 25 different classes with 9 and 16 rotation angles, respectively. We also applied the proposed method on the Outex_TC12 dataset with different illuminations and rotations. Moreover, the proposed method was applied on the KTH-TIPS dataset with different scales, poses, and illuminations.

In this experiment, two different datasets with large number of classes were used for evaluating the abilities of the proposed method for texture classification. Outex_TC_00013 (TC13) with 68 different classes and also the entire Brodatz dataset with 112 classes were used for this experiment. In the first part, we used Outex_TC13 dataset that each texture image has the size of 128×128pixels under illumination condition “inca”. This dataset included 1360 (68×20) texture images that 680 (68×10) texture images were randomly selected for the training set and 680 (68×10) for the test set. Each image in Outex_TC13 was converted to gray scale before feature extraction.

Other experiments were also done on the Brodatz dataset in that the images had the size of 640×640pixels. For creating the training and test set, we divided each image into 25 non-overlapping texture images of size 128×128, then 12 different textures were randomly used for training and 13 different textures were employed for test. Therefore, the training set included 1344 (112×12) and the test set consisted of 1456 (112×13) different texture images. This partitioning was performed 40 times and the average classification accuracy was reported.

We extracted the well-known features like e1
                           =(l
                           1-norm, standard deviation) or e2
                           =(l
                           1-norm, entropy) or e3
                           =(l
                           1-norm, entropy, standard deviation) from tree-structured wavelet transform [11]. In total, we extracted 20 different wavelet features using (e1, e2) and 30 features using (e3) for each texture sample. The Mahalanobis minimum distance classifier was used for classification.

The result has been shown in Table 1
                            with two different wavelet bases and with samples of size 64×64. According to the result, the proposed method had better classification accuracy as compared with the conventional wavelet features for texture classification. The proposed method was also performed on different sample sizes like 32×32 and achieved the maximum classification accuracy of 90.14% and 88.11% for the Outex_TC13 and the entire Brodatz datasets, respectively. All experiments were conducted with 16 samples from each image for the training phase.


                           Table 2
                            shows the classification accuracy for the different state-of-the-art LBP methods on the Outex_TC13 dataset. Based on the result, the proposed method and the different wavelet energy features performed well as compared with the LBP methods.

Dataset 1 consisted of the well-known Outex dataset with 24 classes at 9 different rotation angles (0°, 5°, 10°, 15°, 30°, 45°, 60°, 75°, and 90°). Images from this dataset, like Outex_TC_00010 (TC10), were of size 538×716pixels with 24-bit RGB and 100dpi resolution under illuminant “inca”. The training set was created by twenty non-overlapping regions from each image at a rotation angle of 0°. Therefore, a total of 480 (20×24) texture images were used in the training phase. Moreover, test set consisted of 3840 (24×8×20) images at different rotation angles. Moreover, the Mahalanobis minimum distance classifier was used for classifying each image in the test set. Different experiments were performed using different wavelet bases, sample sizes, and number of samples.


                        Table 3
                         shows the classification accuracy using four different wavelet bases, number of samples, and sample sizes. The classification accuracies implied that the proposed method had promising results for this dataset. The maximum classification accuracy is 98.56% that is higher than the other state-of-the-art method like [13] with maximum classification accuracy of 97.4% with 26 features. Ojala et al. [4] achieved the classification accuracy of 97.9% with the large number of features. They applied the Porter and Canagarajah method [15] on this dataset and obtained 80.4% classification accuracy with regions of size 128×128.


                        Table 4
                         shows classification accuracy for the different state-of-the-art LBP methods on the Outex_TC10 dataset. Based on the results, the proposed method has had higher accuracy than the LBP methods.

The number of errors at 8 different rotation angles has been reported in Fig. 7
                        . This figure represents the number of errors for the Symlet 4-tap with 16 training samples of size 32×32 with the average classification accuracy of 98.09%. In this experiment, maximum error belonged to the rotation angle of 90° with about 3.5% error rate.


                        Fig. 8
                         shows the Boxplot belonging to the proposed method for 4 different wavelet bases and 5 different sample sizes on 24 different classes in the Outex_TC10 dataset.

In this experiment, more challenging dataset with greater number of classes and rotation angles, as compared with the Outex dataset was used. This dataset consisted of 25 different texture classes of size 512×512 from the Brodatz dataset which had been used in [13,17]. Some sample images from this dataset have been shown in Fig. 5. For producing the training and test set, each image was divided into 16 non-overlapping regions of size 128×128 at rotation angle 0°. Test set was created by rotating the images at different angles from 0° to 160° with 10° intervals, then 4 samples were captured with size of 128×128, such that sub-images from different orientations had a minimum overlap similar to [13]. Therefore, the training set consisted of 400 (25×16) texture images with a size of 128×128pixels and the test set consisted of 1600 (25×16×4) different images.


                        Table 5
                         shows classification accuracy for different conditions. The maximum classification accuracy, which was achieved for the proposed method, was 96.5% with 14 features, as compared with the other methods such as [17] with maximum classification accuracy of 93.8% with 64 features and [13] with maximum of 97.9% with 26 features.


                        Table 6
                         shows the classification accuracy for the different state-of-the-art LBP methods and the other important methods on the Brodatz dataset. The result indicated that the proposed method had higher classification accuracy than the other methods. The Gabor–Wavelet features had better classification accuracy than the LBP methods on the Brodatz dataset, although these features had too bad results on the Outex dataset.

The number of errors at 16 different rotation angles has been reported in Fig. 9
                        . This figure shows the number of errors for the Symlet 4-tap with 16 training samples of size 32×32. The average classification accuracy was 93.12% for this case. In this experiment maximum error belonged to the rotation angle of 130° with about 12 percent error rate.


                        Fig. 10
                         shows the Boxplot belonging to the proposed method for 4 different wavelet bases and 5 different sample sizes on 25 different classes in the Brodatz dataset.

The feature vector here was proposed for rotation invariant wavelet classification. For more comparison in this section, we applied the proposed method for rotation and illumination invariant texture classification. The Outex_TC_00012 (TC12) with two illuminants of “tl84” and “horizon” was used to test the robustness of the proposed method against illumination and rotation changes. Training set was similar to dataset 1 on illuminant “inca” at rotation angle 0° with 480 (24×20) texture images. Test set consisted of 4320 (24×9×20) textures under illuminant “tl84” and “horizon” at 9 different rotation angles. Before feature extraction, intensity of each texture was normalized to zero mean and unit standard deviation. This normalization made the features robust to linear changes of illumination. Ojala et al. [4] showed that the wavelet-based features moderately tolerated illumination changes. Table 7
                         shows the result of the proposed method on the Outex_TC12 for illumination and rotation invariant texture classification.

Ojala et al. [4] reported a classification accuracy of 71.2% for illuminant “tl84” and 72.4% for illuminant “horizon” by the features proposed by Porter and Canagarajah [15]. The result shows that the classification accuracy of the proposed method approximately increases 17% as compared with the conventional invariant wavelet features. Table 8
                         shows the classification accuracy for the different LBP methods. The result indicates that the LBP methods have better classification accuracy than the proposed method.


                        Table 9
                         shows classification rate for some important methods on the Outex dataset. The result shows that the proposed method has best result on Outex_TC10 and has better results as compared with other important multi-resolution based methods and also some extensions of LBPs on Outex_TC12.

The KTH-TIPS dataset is one of the well-known datasets in texture classification. This dataset is more challenging than the other datasets like the Brodatz and Outex. This dataset consists of 10 different classes at 9 different scales, three different illuminations, and 3 different poses. This experiment was performed on first 5 different scales, including 45 textures of size 200×200, which had no background. Twenty-three textures were randomly selected for the training set and also the other 22 textures were used for the test. Texture images were converted to gray scale before feature extraction. The experiment was repeated 50 times, then average result was reported.

Result has been shown in Table 10
                         with the Symlet 4-tap wavelet base and image samples of size 64×64. Table 10 also shows the classification accuracies for the different LBP methods and the other well-known methods. The result indicated that the proposed method had higher classification accuracy, in comparison with the state-of-the-art LBP methods and also the other important methods.

@&#DISCUSSION@&#

The proposed method was applied on the seven different datasets with different properties. In addition, the results were compared with the different kinds of methods. They showed that the proposed method mainly achieved better results than the other methods in all the datasets, expect for Outex_TC12 “Tl80” and Outex_TC12 “horizon” as compared with the two kinds of LBPs, VZ-joint, and VZ-MR8, although it outperformed the other multi-resolution and frequency based methods.

We compared our method with the 4 different LBP methods on all the datasets. The results showed that the proposed method outperformed the LBP methods on the 5 datasets. This mainly happened because the LBP methods just considered spatial relationship between neighborhood pixels; therefore, they lost vital spectral and frequency information. Also, the abilities of the spatial filter banks and wavelet analysis in description of textures, and a good combination of these abilities could help the proposed method to have better results.

To wrap up, we believe that our method is preferable than LBPs because of two main reasons.

The LBP methods suffered from huge computational cost, especially for high order neighborhoods. For example, LBPV
                     24, 3
                     
                        u2
                     GM
                     
                        ES
                      extracted a feature vector with the length of 13,251. However, the proposed method suggested a feature vector of length 14, used DWT, which was so fast in implementation, and also applied a filter bank that just consisted of 4 filters.
                        
                           1-
                           The proposed method, according to its simplicity, opens opportunities for other researcher to work on different parts. For instance, working on filter bank section, like using frequency filter banks and applying more complex spatial filter banks, employing the other multi-resolution analysis such as CWT and the other extensions of DWT and also working on fusion section.

@&#CONCLUSIONS@&#

The conventional DWT features were sensitive to rotation of textures and had moderate ability to deal with illumination invariant texture analysis. In this paper, we proposed feature vector at different resolutions and ignored the detailed images. The findings of the study showed that the proposed method had promising result as compared with the other state-of-the-art methods. Our method used 14 different features for invariant texture classification. This number of features was very small in comparison with the other state-of-the-art method, like LBPs. The proposed method was applied on different datasets with different properties such as rotation, illumination, pose, and scale and it could obtain promising results.

@&#REFERENCES@&#

