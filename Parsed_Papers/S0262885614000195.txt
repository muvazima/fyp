@&#MAIN-TITLE@&#Adaptive on-line similarity measure for direct visual tracking

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present an adaptive metric for measuring the similarity of a target for the purpose of visual tracking.


                        
                        
                           
                           This metric assigns a robust weight to each matching error based on the error type.


                        
                        
                           
                           A histogram-based classifier is learned on-line to determine the error type.


                        
                        
                           
                           The proposed robust metric dynamically adapts to the actual appearance changes by tuning its parameters.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Adaptive metric

Similarity measure

Visual tracking

Template matching

@&#ABSTRACT@&#


               
               
                  This paper presents an on-line adaptive metric to estimate the similarity between the target representation model and new image received at every time instant. The similarity measure, also known as observation likelihood, plays a crucial role in the accuracy and robustness of visual tracking. In this work, an L2-norm is adaptively weighted at every matching step to calculate the similarity between the target model and image descriptors. A histogram-based classifier is learned on-line to categorize the matching errors into three classes namely i) image noise, ii) significant appearance changes, and iii) outliers. A robust weight is assigned to each matching error based on the class label. Therefore, the proposed similarity measure is able to reject outliers and adapt to the target model by discriminating the appearance changes from the undesired outliers. The experimental results show the superiority of the proposed method with respect to accuracy and robustness in the presence of severe and long-term occlusion and image noise in comparison with commonly used robust regressors.
               
            

@&#INTRODUCTION@&#

Visual tracking is a fundamental and essential part of many computer vision, robotic, and video analytic applications including Automatic visual surveillance [10], Behavior analysis [23], Motion capture and animation [20], Vehicle navigation and tracking [1], Traffic monitoring [3], Intelligent preventive safety systems [9], and Industrial robotics. In its simplest form, visual tracking is defined as the problem of locating three-dimensional (3D) target objects (such as a human or car) in a two-dimensional (2D) image plane as they move around a scene [24]. Besides other main parts such as target representation model and localization algorithm, the efficiency and reliability of a tracker are also highly affected by the used similarity measure method. The main goal of a similarity measure is to estimate the distance from the target representation model and the received data or image. Usually a predefined metric such as Euclidean distance is employed to measure the distance. However, these static metrics cannot accurately and robustly estimate the similarity level over time under challenging situations such as long-term occlusion and significant appearance changes.

A primary similarity measure used for the template matching problem is the Euclidean distance between the object template and the candidate sub-image. Assume that T is the object template, I is the received image frame, and W(X;
                     P) is the warping function which maps every pixel X
                     ={x,
                     y} in the image plane to a pixel X′=
                     W(X;
                     P) in the template based on the transformation parameters P
                     ={p
                     1,…
                     p
                     
                        k
                     }. At every tracking time instant t, the goal of a template-based tracker is to find the best transformation parameter P
                     
                        t
                      in a way that the distance between the template T
                     
                        t
                      and the candidate sub-image I
                     
                        t
                      is minimized. [16] used the sum of square difference (SSD) to measure this distance:
                        
                           (1)
                           
                              
                                 
                                    P
                                    t
                                 
                                 =
                                 arg
                                 
                                    min
                                    P
                                 
                                 
                                    
                                       ∑
                                       X
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      T
                                                      t
                                                   
                                                   
                                                      X
                                                   
                                                   −
                                                   
                                                      I
                                                      t
                                                   
                                                   
                                                      
                                                         W
                                                         
                                                            X
                                                            P
                                                         
                                                      
                                                   
                                                
                                             
                                             2
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  

As illustrated in Eq. (1), the SSD measure can be used in conjunction with a gradient based optimization to estimate the transformation parameter. A least square algorithm is proposed in Ref. [16] to optimize Eq. (1). In general, L2-norm of errors is not robust against outliers, severe appearance variations, illumination changes, and occlusion. As a remedy, a robust error function, ρ(e) is used to estimate the error e between the template and the candidate sub-image. Using a robust estimator instead of L2-norm, we obtain:
                        
                           (2)
                           
                              
                                 
                                    P
                                    t
                                 
                                 =
                                 arg
                                 
                                    min
                                    P
                                 
                                 
                                    
                                       ∑
                                       X
                                    
                                    
                                       ρ
                                       
                                          
                                             
                                                T
                                                t
                                             
                                             
                                                X
                                             
                                             −
                                             
                                                I
                                                t
                                             
                                             
                                                
                                                   W
                                                   
                                                      X
                                                      P
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  

Any function which satisfies the following criteria can be considered as a robust estimator [18]:
                        
                           1.
                           
                              
                                 
                                    
                                       
                                          ∀
                                          e
                                          ∈
                                          ℜ
                                          →
                                          ρ
                                          
                                             e
                                          
                                          >
                                          0
                                       
                                    
                                 
                              
                           


                              
                                 
                                    
                                       
                                          
                                             e
                                             1
                                          
                                          >
                                          
                                             e
                                             2
                                          
                                          >
                                          0
                                          →
                                          ρ
                                          
                                             
                                                e
                                                1
                                             
                                          
                                          >
                                          ρ
                                          
                                             
                                                e
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           


                              
                                 
                                    
                                       
                                          
                                             e
                                             1
                                          
                                          <
                                          
                                             e
                                             2
                                          
                                          <
                                          0
                                          →
                                          ρ
                                          
                                             
                                                e
                                                1
                                             
                                          
                                          <
                                          ρ
                                          
                                             
                                                e
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           


                              ρ(e) is piece-wise differentiable.

A wide variety of robust error functions have been used in the literature. The Geman-McClure function is commonly used for the task of visual tracking [2,21].
                        
                           (3)
                           
                              
                                 ρ
                                 
                                    e
                                 
                                 =
                                 
                                    
                                       e
                                       2
                                    
                                    
                                       
                                          e
                                          2
                                       
                                       +
                                       
                                          σ
                                          2
                                       
                                    
                                 
                              
                           
                        
                     
                  

Another robust estimator used for tracking [8] is the Huber function.
                        
                           (4)
                           
                              
                                 ρ
                                 
                                    e
                                 
                                 =
                                 {
                                 
                                    
                                       
                                          
                                             1
                                             2
                                          
                                          
                                             e
                                             2
                                          
                                       
                                       
                                          if
                                          |
                                          e
                                          |
                                          ≤
                                          σ
                                       
                                    
                                    
                                       
                                          σ
                                          |
                                          e
                                          |
                                          −
                                          
                                             1
                                             2
                                          
                                          
                                             σ
                                             2
                                          
                                       
                                       
                                          otherwise
                                       
                                    
                                 
                              
                           
                        
                     where in Eqs. (3) and (4), σ is a scale parameter.

It has been shown that these functions can improve the robustness of a visual tracker against outliers and occlusion [2]. In general, a robust estimator assigns a weight to each error value based on the magnitude of the error. The weight is less when the error is large. Despite the theoretical benefits, there are two practical problems which may significantly damage the efficiency and robustness of these functions. First the robust estimator is application dependent and has to be picked by a designer for different cases. This can be an acceptable limitation for some application, but it is not feasible under general conditions. Also, depending on the distribution of the error a proper scale vector (σ) has to be selected. Moreover, robust regression methods cannot distinguish between outliers and actual significant target appearance changes.

Besides the sum of square differences and robust estimators, other metrics such as cross cumulative residual entropy (CCRE) [22], mutual information (MI) [6], the Bhattacharyya coefficient 
                     [5], a convolution of spatial and feature space kernel functions [7], and sum of conditional variance (SCV) [19] have been proposed to measure the similarity of the target model and the received images. However, these methods are developed based on static and predefined measures which cannot sufficiently deal with challenging situations in a visual tracking scenario. One challenge is that the most similar candidate sub-image to the target model may not be the best match using a predefined similarity measure. The mentioned problem mainly rises when the target appearance changes over time or it is partially occluded by either itself or other background objects. Another phenomenon which can cause a tracker to fail is the existence of similar background objects known as distracters in a close proximity to the target object. Several works have been introduced to improve the accuracy of trackers in such situations. For instance, Li et al. [15] presented a pyramid-base scale adaptation method for mean-shift tracking. This tracker generates similarity functions at different scales and uses a coarse-to-fine search to avoid trapping in local minimum. Also, Karavasilis et al. [14] used the Gaussian Mixture Model (GMM) as the target representation model and the Differential Earth Mover's Distance (DEMD) as similarity measure for the task of tracking. This method combines DEMD-based tracker and Kalman filer algorithm to handle occlusion. Nevertheless, still the applicability of these predefined similarity measures is limited to specific cases.

Adaptive similarity measures, on the other hand, can be used to find the best match of the target model over time robustly. Collins et al. [4] proposed a dynamic feature selection method for estimating the similarity of the target model and the candidate image. In this method, the total number of features is fixed and the goal is to adaptively rank these features and use a subset of high ranked ones for matching. Although the method proposed in Ref. [4] can select discriminative features properly in some cases, the color features used in this method are not suitable in various applications, and also it is not always feasible to employ a more discriminative feature vector instead of color features due to the used exhaustive search for ranking the features. Recently Jiang et al. [13] proposed a classifier which is learned on-line from the tracking information to find the best match of the target model over time. In this method, an adaptive Mahalanobis distance is used to weight each feature in the classification process. According to the experimental results, this adaptive metric performed well in the existence of distracters. However, this method may fail in case of occlusion because of several reasons. First, this method uses proximity based approach to generate positive and negative samples at every time instant. However, in case of occlusion (specifically long term occlusion which has been emphasized in our work) image regions in very close vicinity of the target may not be true positive samples. Therefore, learning from false positive samples may cause the tracker to drift from the target. In addition, there is no specific mechanism in this method for handling occlusion and outliers. Although the method proposed in Ref. [13] is adaptive against target appearance and illumination changes, there is not enough evidence from the experimental results to verify its robustness and accuracy in case of occlusion.

Our proposed adaptive similarity measure differs from the works in the literature in several ways. First, unlike metrics presented in [4] where a subset of the feature vector is adaptively selected for matching, in our method the distance between the target and the image is modeled on-line by an adaptive hybrid model. Also, our method is more robust against severe and long-term occlusion than other relevant methods such as in [13]. Thus, our proposed adaptive metric is designed to reject outliers whereas it deals with appearance changes. Finally, our method requires less predefined parameters in comparison with other methods such as robust regression estimation [18] where a scale vector plays a crucial role in the robustness of the regressors.

In Section 2, first the proposed similarity measure is defined, and then an on-line algorithm to train a histogram-based classifier is described in detail. Next in Section 3, the proposed adaptive metric is used in a template matching problem. The results obtained by our metric is compared with several robust regressors as well as manually labeled ground truth data in Section 4. Lastly, in Section 5 some conclusions and potential future works are discussed.

From the definition, the goal of a similarity measure is to estimate the distance from a target model and an image. In the proposed adaptive similarity measure, the Euclidean distance of the target model and the image is considered as the matching error. However, unlike a typical SSD method, a histogram-based classifier is learned on-line using the matching error history. Later, this classifier is used to assign a robust weight to each matching error based on its type.

Let A
                     ={a
                     1,…,
                     a
                     
                        m
                     } and B
                     ={b
                     1,…,
                     b
                     
                        n
                     } be the features describing the target model and the image, respectively.
                        1
                     
                     
                        1
                        In this work, the image pixel values are considered as features. However, the proposed method can be suitably integrated with a feature-based tracker.
                      Assuming that the feature space is metric, the number of features of the target and the image are the same (i.e.,m
                     =
                     n), and features have injective relation (i.e., a
                     
                        j
                     
                     =
                     b
                     
                        k
                     
                     ⇒
                     j
                     =
                     k), we can find the Euclidean error E
                     ={e
                     1,…,
                     e
                     
                        n
                     } in the feature space as:
                        
                           (5)
                           
                              
                                 
                                    e
                                    j
                                 
                                 =
                                 
                                    a
                                    j
                                 
                                 −
                                 
                                    b
                                    j
                                 
                                 .
                              
                           
                        
                     
                  

Inspiring from the work proposed in Ref. [12], we categorize the matching error E based on their history into three classes:
                        
                           
                              E
                              
                                 i
                              
                           
                           image noise and/or illumination variations,

target appearance changes, and

outliers and occlusion.

The first source of error, E
                     
                        i
                     , is mainly caused by either small illumination variations or some image noise which is inevitable in image capturing and computer vision. Usually the distribution of this type of error can be modeled by a zero-mean Gaussian function as E
                     
                        i
                     
                     ∼
                     N(0,
                     σ
                     
                        i
                     ). In this work, instead of a Gaussian function a symmetrical range is learned from the previous matching errors. Other source of errors (i.e., E
                     
                        a
                      and E
                     
                        o
                     ), on the other hand, cannot be easily discriminated from each other. The actual appearance changes may cause significant matching errors which are usually considered as outliers or occlusion by the conventional robust estimators [18]. A proper similarity measure has to reject outliers while it is adapting to the errors because of actual changes in target appearance and pose. Since in a tracking scenario, the target appearance usually changes smoothly
                        2
                     
                     
                        2
                        In visual tracking, the input images are captured with a high frame per second rate e.g., 15 and also the target is usually a real-world object such as a human face; therefore, it is very unlikely that the target appearance significantly changes between two consecutive images.
                      over time, we model the distribution of E
                     
                        a
                      by two adaptive ranges which are learned on-line from previous errors, and the outliers are identified if a matching error does not occur because of either E
                     
                        i
                      or E
                     
                        a
                      source. We consider outliers as abnormal matching errors which cannot be easily modeled or predicted. In the following, the algorithm to model error types E
                     
                        i
                      and E
                     
                        a
                      is presented in detail.

Assume that each matching error e
                        
                           j
                         is quantized into Q bins where b(e
                        
                           j
                        )∈[1,
                        Q] is the bin index in the quantized space. Using k previous matching errors of feature j, we can estimate the number of times that e
                        
                           j
                         occurred in the bin index q as h
                        
                           j,q
                        
                        =∑
                        
                           l
                           =
                           k
                           −
                           t
                           +1
                        
                           t
                        
                        
                        δ[b(e
                        
                           j
                        
                        
                           l
                        )−
                        q] where δ is the Kronecker delta function. In this work, image features
                           3
                        
                        
                           3
                           In this work, gray-scale values are used as features. However, the proposed similarity measure can be integrated with different image features.
                         are first normalized into the range of zero and one, i.e., ∀
                        j;
                        a
                        
                           j
                        ,
                        b
                        
                           j
                        
                        ∈[0,1], and accordingly, the matching errors are in the range of negative one and one i.e., ∀
                        j;
                        e
                        
                           j
                        
                        ∈[−1,1]. As a result, 
                           
                              
                                 q
                                 ¯
                              
                              =
                              Q
                              /
                              2
                           
                         is the bin index corresponding to the smallest errors i.e., 
                           
                              b
                              
                                 
                                    e
                                    j
                                 
                              
                              =
                              
                                 q
                                 ¯
                              
                              →
                              
                                 
                                    e
                                    j
                                 
                              
                              <
                              1
                              /
                              Q
                           
                        .

We propose an iterative algorithm to estimate the ranges of error types E
                        
                           i
                         and E
                        
                           a
                        . In this algorithm, the center and radius of each range are estimated in the quantized feature space. For error type E
                        
                           i
                        , the center point 
                           
                              μ
                              
                                 E
                                 i
                              
                           
                         is fixed and set to 
                           
                              q
                              ¯
                           
                        , and the radius 
                           
                              ϵ
                              
                                 E
                                 i
                              
                           
                         is iteratively estimated based on the following algorithm. Note that the subscript j is eliminated from the equations in Algorithms 1 and 2 for clarity.
                           Algorithm 1
                           Error type E
                              
                                 i
                               range estimation
                                 
                                    
                                 
                              
                           

As shown in Algorithm 1, small matching error (E
                        
                           i
                        ) distribution is modeled by an adaptive error range where its center point is intuitively set to the histogram bin corresponding to the smallest matching error 
                           
                              
                                 q
                                 ¯
                              
                           
                        . The radius 
                           
                              
                                 ϵ
                                 
                                    E
                                    i
                                 
                              
                           
                         is found by iteratively expanding the range starting from the center point. The expansion is terminated where the number of newly considered matching errors due to the recent range expansion is not significant in comparison with the previous expansions. In this work, a non-linear ratio of the number of matching errors and the range size (Algorithm 1 line 6) is experimentally selected to terminate the range expansion. Thus, using a cubic form of the number of errors, we can find a proper wide range which suitably models the aforementioned error type.

Unlike E
                        
                           i
                        , error type E
                        
                           a
                         is modeled by two adaptive ranges which are estimated using Algorithm 2. Also, the center of these ranges is set to the histogram bin corresponding to the first and second highest number of errors respectively. Similar to E
                        
                           i
                        , the range boundaries are found based on a non-linear ratio of the number of matching errors in the history and the range size.

This algorithm is repeated two times to obtain both ranges E
                        
                           a
                        
                        1 and E
                        
                           a
                        
                        2. These ranges adaptively model a quantized multi-modal distribution of matching errors which is changing over time. Also shown in Algorithm 2 line 16, the number of errors that occurred in selected bins is set to zero after estimating the center and radius of each range, therefore, the error type ranges are not overlapped. In the next section, the error ranges (i.e., E
                        
                           i
                         and E
                        
                           a
                        ) are used to calculate the weight of each matching error w(e
                        
                           j
                        ).
                           Algorithm 2
                           Error type E
                              
                                 a
                               range estimation
                                 
                                    
                                 
                              
                           

At every matching step, the matching error of each feature e
                        
                           j
                         is compared with the error type ranges and accordingly a weight w(e
                        
                           j
                        ) is obtained.
                           
                              (6)
                              
                                 
                                    w
                                    
                                       
                                          e
                                          t
                                       
                                    
                                    =
                                    {
                                    
                                       
                                          
                                             2
                                          
                                          
                                             if
                                             
                                             
                                                
                                                   e
                                                   j
                                                
                                             
                                             <
                                             ηQ
                                             
                                                ϵ
                                                
                                                   E
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          
                                             1
                                          
                                          
                                             if
                                             
                                             
                                                
                                                   
                                                      e
                                                      j
                                                   
                                                   −
                                                   
                                                      μ
                                                      
                                                         E
                                                         a
                                                         1
                                                      
                                                   
                                                
                                             
                                             <
                                             ηQ
                                             
                                                ϵ
                                                
                                                   E
                                                   a
                                                   1
                                                
                                             
                                          
                                       
                                       
                                          
                                             1
                                          
                                          
                                             if
                                             
                                             
                                                
                                                   
                                                      e
                                                      j
                                                   
                                                   −
                                                   
                                                      μ
                                                      
                                                         E
                                                         a
                                                         2
                                                      
                                                   
                                                
                                             
                                             <
                                             ηQ
                                             
                                                ϵ
                                                
                                                   E
                                                   a
                                                   2
                                                
                                             
                                          
                                       
                                       
                                          
                                             0
                                          
                                          
                                             otherwise
                                          
                                       
                                    
                                 
                              
                           
                        where η
                        =2/m
                        ×∑
                        
                           j
                        
                        δ(w(e
                        
                           j
                        )) is two times of the previous outliers' percentage and m is the number of features.

As illustrated in Eq. (6), the adaptive weight w(e
                        
                           t
                        ) is robust against outliers and occlusion. Moreover, the errors caused by small illumination variations receive a higher weight in comparison with those of the appearance changes to improve the accuracy of the method.

Obtaining the weights of matching errors, we can calculate the similarity distance S by taking the weighted squared error per-pixel and sum over them as:
                           
                              (7)
                              
                                 
                                    S
                                    
                                       A
                                       B
                                    
                                    =
                                    
                                       
                                          ∑
                                          j
                                       
                                       
                                          w
                                          
                                             
                                                e
                                                j
                                             
                                          
                                          ×
                                          
                                             
                                                
                                                   e
                                                   j
                                                
                                                2
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

In the following sections, the proposed similarity measure is formulated along with a typical template-based tracking.

In this section, the proposed similarity measure is applied on a typical template-based tracker. In this method, the object is represented by a dynamic template which is updating every k frame using the new received images. As opposed to the conventional template trackers, in this method a condensation-like sampling algorithm [11] is used to locate and track the target at every image frame. In the following subsections, the tracking algorithm followed by the representation model is described in detail.

In the conventional template-based tracking method, the target object is simply represented by its sub-image region obtained from the first image I
                        1, i.e., T
                        1(X)=
                        I
                        1(W(X;
                        P));
                        X
                        ∈
                        R
                        1 where W(X;
                        P) is the warping function and R
                        1 is the object region at time step t
                        =1. The function W(X;
                        P) maps the image pixel at location X
                        ={x,
                        y} from the candidate sub-image into the reference model using an affine transformation consisting of six variables P
                        ={t
                        
                           x
                        ,
                        t
                        
                           y
                        ,
                        θ,
                        s,
                        α,
                        ϕ} which are x and y translations, rotation angle, scale, aspect ratio, and skew direction, respectively. There are different methods to update the template over time. One option is to not change the template [16] i.e., T
                        
                           t
                        
                        =
                        T
                        1 which performs poor in case of appearance and illumination changes, the second method is to update the template every frame i.e., T
                        
                           t
                        
                        =
                        T
                        
                           t
                           −1 called naive update [17]. This approach is also not stable because of drift problem.
                           4
                        
                        
                           4
                           It is the problem of updating the target model using unrelated information such as background pixels [17].
                         In this work, the template is updated every k frames based on a forgetting factor λ. Therefore, the model can simply represent the target appearance changes while it is robust against the drift problem.


                        
                           
                              (8)
                              
                                 
                                    
                                       T
                                       t
                                    
                                    
                                       X
                                    
                                    =
                                    
                                       
                                          λ
                                          
                                             
                                                t
                                                −
                                                k
                                             
                                          
                                       
                                       
                                          λ
                                          
                                             
                                                t
                                                −
                                                k
                                             
                                          
                                          +
                                          k
                                       
                                    
                                    
                                       T
                                       
                                          t
                                          −
                                          k
                                          −
                                          1
                                       
                                    
                                    
                                       X
                                    
                                    +
                                    
                                       k
                                       
                                          λ
                                          
                                             
                                                t
                                                −
                                                k
                                             
                                          
                                          +
                                          k
                                       
                                    
                                    
                                       I
                                       ¯
                                    
                                    
                                       X
                                    
                                 
                              
                           
                        where λ and k are empirically set to 0.97 and 5 respectively for all experiments, and 
                           
                              I
                              ¯
                           
                         is the average value of k most recent object image. Also, for the first k image frames, the first image is used as the template.
                           
                              (9)
                              
                                 
                                    
                                       I
                                       ¯
                                    
                                    
                                       X
                                    
                                    =
                                    
                                       1
                                       k
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             t
                                             −
                                             k
                                          
                                          t
                                       
                                       
                                          
                                             I
                                             j
                                          
                                          
                                             
                                                W
                                                
                                                   X
                                                   
                                                      P
                                                      j
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Visual tracking can be viewed as a sequential inference task in a Markov model with hidden state variables P
                        
                           t
                         describing the object motion parameters at time step t. Given an image sequence I
                        ={I
                        1,…,
                        I
                        
                           t
                        } and reference models (in this case object templates) T
                        ={T
                        1,…,
                        T
                        
                           t
                        }, the hidden state variables can be estimated based on Bayes' theorem as follows:
                           
                              (10)
                              
                                 
                                    p
                                    
                                       
                                          
                                             P
                                             t
                                          
                                          |
                                          
                                             I
                                             t
                                          
                                          ;
                                          
                                             T
                                             t
                                          
                                       
                                    
                                    ∝
                                    p
                                    
                                       
                                          
                                             I
                                             t
                                          
                                          |
                                          
                                             P
                                             t
                                          
                                          ;
                                          
                                             T
                                             t
                                          
                                       
                                    
                                    
                                       ∫
                                       
                                          p
                                          
                                             
                                                
                                                   P
                                                   t
                                                
                                                |
                                                
                                                   P
                                                   
                                                      t
                                                      −
                                                      1
                                                   
                                                
                                             
                                          
                                          
                                          p
                                          
                                             
                                                
                                                   P
                                                   
                                                      t
                                                      −
                                                      1
                                                   
                                                
                                                |
                                                
                                                   I
                                                   
                                                      t
                                                      −
                                                      1
                                                   
                                                
                                                ;
                                                
                                                   T
                                                   
                                                      t
                                                      −
                                                      1
                                                   
                                                
                                             
                                          
                                          d
                                          
                                             P
                                             
                                                t
                                                −
                                                1
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where p(P
                        
                           t
                        |I
                        
                           t
                        ;
                        T
                        
                           t
                        ), p(I
                        
                           t
                        |P
                        
                           t
                        ;
                        T
                        
                           t
                        ), p(P
                        
                           t
                        |P
                        
                           t
                           −1), and p(P
                        
                           t
                           −1|I
                        
                           t
                           −1;
                        T
                        
                           t
                           −1) are the posterior probability, observation likelihood, dynamical or motion model between two states, and prior probability respectively.

The proposed adaptive similarity measure is used to define the observation likelihood.


                        
                           
                              (11)
                              
                                 
                                    p
                                    
                                       
                                          
                                             I
                                             t
                                          
                                          |
                                          
                                             P
                                             t
                                          
                                          ;
                                          
                                             T
                                             t
                                          
                                       
                                    
                                    =
                                    exp
                                    
                                       
                                          −
                                          
                                             
                                                S
                                                
                                                   
                                                      T
                                                      t
                                                   
                                                   
                                                      
                                                         
                                                            I
                                                            ˜
                                                         
                                                         t
                                                      
                                                   
                                                
                                             
                                             
                                                σ
                                                c
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where in this work, the condensation algorithm variance σ
                        
                           c
                         is set to 0.2, and 
                           
                              
                                 
                                    
                                       I
                                       ˜
                                    
                                    t
                                 
                              
                              
                                 X
                              
                              =
                              
                                 I
                                 t
                              
                              
                                 
                                    W
                                    
                                       X
                                       
                                          P
                                          t
                                       
                                    
                                 
                              
                           
                         is the transformed candidate image.

Modeling the observation likelihood using the proposed similarity measure in the previous subsection, we aim to approximate the posterior distribution p(P
                        
                           t
                        |I
                        
                           t
                        ;
                        T
                        
                           t
                        ) defined in Eq. (10) using a condensation-like sampling algorithm [11].

Assume that the prior distribution p(P
                        
                           t
                           −1|I
                        
                           t
                           −1;
                        T
                        
                           t
                           −1) is approximated by N samples (or particles) with corresponding weights ({P
                        
                           n
                        
                        
                           t
                           −1,
                        π
                        
                           n
                        
                        
                           t
                           −1}
                           n
                           =1
                        
                           N
                        ). The first step is to randomly choose N samples (with replacement) from the set {P
                        
                           n
                        
                        
                           t
                           −1} based on the probability {π
                        
                           n
                        
                        
                           t
                           −1}. As a result, those samples with high weight may be selected several times. In the next step, known as diffusion, each sample undergoes a Brownian motion using a Gaussian distribution usually with a diagonal covariance matrix. The weights {π
                        
                           n
                        
                        
                           t
                        } of the new sample set {P
                        
                           n
                        
                        
                           t
                        } are obtained as:
                           
                              (12)
                              
                                 
                                    
                                       π
                                       n
                                       t
                                    
                                    =
                                    p
                                    
                                       
                                          
                                             I
                                             t
                                          
                                          |
                                          
                                             P
                                             n
                                             t
                                          
                                          ;
                                          
                                             T
                                             t
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

As a result, the best transformation parameter P
                        
                           t
                         is thus the particle corresponding to the maximum sampling weight.
                           
                              (13)
                              
                                 
                                    
                                       P
                                       t
                                    
                                    =
                                    
                                       
                                          arg
                                          
                                          max
                                       
                                       
                                          P
                                          n
                                          t
                                       
                                    
                                    
                                       
                                          p
                                          
                                             
                                                
                                                   P
                                                   n
                                                   t
                                                
                                                |
                                                
                                                   I
                                                   t
                                                
                                                ;
                                                
                                                   T
                                                   t
                                                
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          arg
                                          
                                          max
                                       
                                       
                                          P
                                          n
                                          t
                                       
                                    
                                    
                                       
                                          π
                                          n
                                          t
                                       
                                    
                                 
                              
                           
                        
                     

In the next section, the accuracy and robustness of the proposed method are evaluated using several challenging videos.

@&#EXPERIMENTAL RESULTS@&#

In this section, the performance of the proposed adaptive similarity measure is validated using several experiments. In addition to the ground truth data, our experimental results have been compared with L2-norm measure and four other robust regression methods (i.e., σ
                     ={0.3,0.4,0.5,0.6}). For fair comparison, the same target representation model and tracking algorithm described in Section 3 are used with different similarity measures, in addition, the tracking parameters are kept the same in all experiments, and also different scaling vectors are used for the robust regression method to obtain the best result. Also, before time instant k the L2-norm is used for measuring the target appearance similarity with the candidate sub-image. Choosing a very small value for k (i.e., in our experiments k
                     =5), we assume that the L2-norm can accurately estimate the target similarity for time instant smaller than k. Comparing with other similarity measures, not only our method is more robust against severe occlusion and outliers, but also it can handle non-uniform illumination and appearance changes.

In the following subsections, the experimental results using five challenging gray-scale image sequences are illustrated. These videos and the related ground truth data are publicly available and considered as a benchmark in the literature.

In this section, the tracking result obtained by the proposed similarity measure is qualitatively compared with L2-norm, four other regressors, and ground truth data. In Figs. 1, 2, 3, 4, 5, 6, and 7
                        
                        
                        
                        
                        
                        
                        , the target bounding box obtained by the proposed adaptive measure (AM — red solid box), L2-norm (L2 — green dashed box), robust regression with scale parameter σ
                        =0.3 (R3 — pink dashed box), robust regression with scale parameter σ
                        =0.4 (R4 — cyan dashed box), robust regression with scale parameter σ
                        =0.5 (R5 — yellow dashed box), robust regression with scale parameter σ
                        =0.6 (R6 — black dashed box), and the ground truth data (white dotted-dashed box) is illustrated. The object template corresponding to each method is shown at the bottom of each image. The mask image of outliers and appearance changes obtained by the proposed method is also shown at the top-right portion of the image. In the mask image, the black and gray pixels are outliers and appearance changes respectively. Also, figures are best seen in color.

The first video
                              5
                           
                           
                              5
                              Taken from http://vision.ucsd.edu/~bbabenko/project_miltrack.shtml.
                            consists of 326 image frames with the resolution of 320×240, and the target object is a dollar paper moving in a simple background. This video is selected for our experiments because it contains self-occlusion and similar objects, known as distracters.

As illustrated in Fig. 1, at early stages (e.g., frames #48 and #56) the dollar is bended and a part of it is self-occluded, after that the target is moved in a close proximity of a similar object (e.g., frames #131 and #251). In this experiment, our method performed well against self occlusion and distracter and L2-norm failed to handle outliers. Also, all regression methods could accurately track the target in the entire video.

The second sequence
                              6
                           
                           
                              6
                              Taken from http://vision.ucsd.edu/~bbabenko/project_miltrack.shtml.
                            is a long video containing 884 gray-scale images with the resolution of 352×348 where a human face is occluded several times by a book. This video is considered as one of the challenging benchmarks for occlusion handling task due to the long-term and significant occlusion.

In the faceocc sequence, shown in Fig. 2, the target is a human face in a simple background, however around 80% of images contain occlusion and at some frames only a very small part of the target is visible (e.g., frames #571, #711, and #832). Due to the severe and long-term occlusions that existed in this sequence, the target model shown in the bottom of each image is most of the time corrupted with irrelevant pixels and cannot correctly represent the human face. However, the proposed method was able to accurately and robustly track the target. The second best is R5 whose accuracy is far from our method while others largely drift from the target at frame around #581.

In the next image sequence
                              7
                           
                           
                              7
                              Taken from http://vision.ucsd.edu/~bbabenko/project_miltrack.shtml.
                            also the target object is a human face and it consists of 812 gray-scale image frames with the resolution of 321×295. The different between faceocc2 and faceocc sequences is that in the latter the face is almost stationary with a simple background, however in the former, the face is moving in a cluttered background with similar pixel values. In addition, this face appearance and orientation change a lot over time while the target is significantly occluded by different objects. Thus, the template matching error is not only because of occlusion but also due to the appearance changes.

As illustrated in Fig. 3, the human face is occluded by a book several times specifically when the target is rotated (e.g., frames #416 and #491). The coexistence of occlusion and appearance variation also happened at frames #575 and #700. According to the experimental results, our similarity measure outperformed other methods. The regression estimations with σ
                           =[0.4,0.5] (R4 and R5) could also track the target up to the end of the sequence, but their accuracy was not as good as the proposed method.

This video
                              8
                           
                           
                              8
                              Taken from http://www.cs.toronto.edu/~dross/ivt/.
                            contains 462 gray-scale images with the resolution of 321×295. The target object is a face which is moved in a cluttered background. The appearance, scale, and orientation of the face are changed during the tracking, and also the target is self occluded at some image frames. The new challenge in this sequence comparing with the previous ones is that in this experiment, the target encounters different lighting conditions as well as appearance variations. Although the simple template tracking method used in this work cannot robustly represent the object appearance, the proposed similarity measure is capable of increasing the tracking robustness and accuracy against illumination, scale, appearance variation as well as outliers and occlusions.

Based in Fig. 4, the target object, human face, is moved in a room under different lighting situations e.g., frames #30 and #86. The half of the target is then occluded by itself at around frame #161. From this time step, all other methods started drifting from the target due to the significant appearance changes and occlusion, but our method could manage to reject outliers and matching errors because of large appearance changes and track the face. However, because of poor performance of template tracking methods in case of non-rigidity and simultaneous appearance and illumination changes, the proposed similarity measure failed to match the template model with the target from frame around #276.

The last sequence
                              9
                           
                           
                              9
                              Taken from http://www.cs.toronto.edu/~dross/ivt/.
                            is a video containing 501 gray-scale images with the resolution of 321×295. The target object is a face which is moved in a cluttered background. The appearance, scale, and orientation of the face are changed during the tracking, and also the target is self occluded at some image frames. The new challenge in this sequence comparing with the previous ones is that in this experiment, the target encounters different lighting conditions as well as appearance variations. Although a typical template tracking method used in this work cannot robustly represent the object appearance, the proposed similarity measure can increase the tracking robustness and accuracy against illumination, scale, appearance variations as well as outliers and occlusions.

As illustrated in Fig. 5, the target appearance is partially changed because of different lightings e.g., frames #38, #78, #123, and #155. Some parts of the target are also occluded due to the out of plane rotation of the face in around frame #178. All trackers except our method failed to accurately track the object from frame around #78. However, the proposed similarity measure could find the correct match to the target template in the coexistence of severe illumination changes and outliers up to frame #229. It is expected that employing our method along with an illumination invariant representation model can improve the robustness and accuracy of the tracker against significant illumination changes and partial occlusions. Moreover, the target model can be updated considering the outliers detected by the proposed method. This way, irrelevant information such as background pixels and occluding objects will not damage the target model over time.

In the following subsection, the ground truth data is used to provide a quantitative evaluation of the results obtaining by all methods.

In addition to the qualitative analysis illustrated in the previous section, the tracking results of all methods are compared with the ground truth data to show the accuracy and robustness of each method. In Figs. 1, 2, 3, 4, 5, 6, and 7, the RMS (root mean square) error and the average RMS error of the target bounding box obtained by each method and the ground truth data are shown. Similar to the above figures, the results corresponding to our method, L2-norm, robust regression with σ
                        =0.3, σ
                        =0.4, σ
                        =0.5, and σ
                        =0.6 are specified by a red solid line, a green dashed line, a pink dashed line, a cyan dashed line, a yellow dashed line, and a black dashed line respectively.


                        Figs. 6 and 7 show the RMS error and the average RMS error of each method in comparison with the ground truth data. Illustrated in these figures, all methods expect that R3 (robust regression with σ
                        =0.3) and L2 could accurately track the target in sequence dollar. R3 (pink dashed) and L2 (green dashed) methods drift form the target at around frame #50 because of partial occlusion. In sequence faceocc, only our method could track the target up to the end of the sequence. The second best method is R6 which failed to correctly locate the target at frame around #480, other ones lost the face earlier at frame around #520.

Similarly in sequence faceocc2, the proposed method outperformed other methods. In this experiment, R4 and R5 could also track the target up to the end with less accuracy in comparison with our method. The forth sequence, david, involves significant appearance and illumination variations which generally cause a typical template tracker to fail. However, the proposed method could robustly track the target up to frame around #280 while R3 and L2 failed at early stages (i.e., at frames #25 and #90 respectively), also, R4, R5, and R6 started to drift from the target at frame around #160 due to simultaneous illumination changes and out of plane rotation. In the last sequence trellis70, similar to the previous one, there are frequent illumination, appearance, scale, and orientation changes which make it difficult to obtain a precise template of the target object. In this sequence, although none of the methods could accurately track the target in the entire image frames, the proposed similarity measure was capable of rejecting outliers whereas adapting the appearance and illumination changes at the same time in several challenging situations e.g., in Fig. 5 from frames #145 to #207. All other methods failed to track the target at frame around #145.

@&#DISCUSSIONS@&#

From the quantitative results illustrated in Figs. 6 and 7, the proposed method outperformed all other similarity measures in most times and could adaptively identify and reject outliers in our experiments. The reasons for high accuracy and robustness of the proposed method include using a hybrid model for estimating the matching error distribution, and next, an on-line classification and auto-tuning mechanism used for parameter training. In addition, all parameters of our similarity measure are kept the same in all sequences, as a result, our method is fairly robust to the choice of parameters and can work accurately and robustly without any modification. In general, among different scaling factors, the robust regression with σ
                        =0.5 could generally handle outliers more accurately in comparison with other ones. However, in sequence faceocc R6 obtained less RMS error than R5. Also as expected, L2-norm performed poorly in all experiments due to its weakness in rejecting outliers and occlusion.

Although the proposed method can accurately find the best match to the target model against outliers, its RMS error is not small at some frames. One reason for this phenomenon is that the manually generated ground truth data are approximately precise and subject to the human error, it is expected that the RMS error of the proposed method in comparison with a more accurate ground truth data can be less in several cases. For instance, in Fig. 4 in frames #161, #176, #181, and #201 the tracking results obtained by our method (solid red box) are obviously more accurate that those of the ground truth data (dotted dashed white box). Moreover, a typical template cannot robustly represent a non-rigid target whose appearance and illumination are significantly changed over time. Therefore, the proposed similarity measure is able to improve the accuracy and robustness of an advanced visual tracking method in case of severe outliers and long-term occlusions.

@&#CONCLUSIONS@&#

This paper presented a robust similarity measure which can adaptively learn the matching error type using on-line classification. The proposed method is capable of categorizing the error into three classes: 1) small variations of the target illumination and appearance, 2) significant changes in target appearance, and 3) abnormal errors because of outliers and occlusion. According to the error types an image mask is generated to assign a weight to each matching error. The normalized weighted errors are then used to find the best match to the target model. As an advantage to other comparable methods, our similarity measure is able to adapt its regression parameters over time.

The accuracy and robustness of our method have been compared with several commonly used robust regression methods. The proposed method is able to find the best match to the target template in different challenging situations including partial illumination variations, significant appearance changes, and long-term occlusion. It is observed that our proposed method excels all the regressors including R5 (regression method with scaling factor 0.5) which has performed better than the others.

A new direction of this work is to use the outliers masking by the proposed similarity measure to update the target representation model. Note that a matched sub-image with proportionally high percentage of occlusion may not be a proper information for updating the target model. In addition, using our method, we can approximate the start and end of an occlusion which is useful for generating a temporary representation model.

@&#REFERENCES@&#

