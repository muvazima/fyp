@&#MAIN-TITLE@&#Modeling, forecasting and trading the EUR exchange rates with hybrid rolling genetic algorithms—Support vector regression forecast combinations

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We introduce a hybrid Rolling Genetic Algorithm-Support Vector Regression (RG-SVR) model.


                        
                        
                           
                           In the RG-SVR, a GA is applied for optimal parameter selection and feature subset combination.


                        
                        
                           
                           We introduce a GA fitness function for financial forecasting purposes.


                        
                        
                           
                           The RG-SVR model is benchmarked against GA-SVR and simple SVR algorithms.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Genetic algorithms

Support vector regression

Feature subset

Forecast combinations

Exchange rates

@&#ABSTRACT@&#


               
               
                  The motivation of this paper is to introduce a hybrid Rolling Genetic Algorithm-Support Vector Regression (RG-SVR) model for optimal parameter selection and feature subset combination. The algorithm is applied to the task of forecasting and trading the EUR/USD, EUR/GBP and EUR/JPY exchange rates. The proposed methodology genetically searches over a feature space (pool of individual forecasts) and then combines the optimal feature subsets (SVR forecast combinations) for each exchange rate. This is achieved by applying a fitness function specialized for financial purposes and adopting a sliding window approach. The individual forecasts are derived from several linear and non-linear models. RG-SVR is benchmarked against genetically and non-genetically optimized SVRs and SVMs models that are dominating the relevant literature, along with the robust ARBF-PSO neural network. The statistical and trading performance of all models is investigated during the period of 1999–2012. As it turns out, RG-SVR presents the best performance in terms of statistical accuracy and trading efficiency for all the exchange rates under study. This superiority confirms the success of the implemented fitness function and training procedure, while it validates the benefits of the proposed algorithm.
               
            

@&#INTRODUCTION@&#

Forecasting financial time series appears to be a challenging task for the scientific community because of its non-linear and non-stationary structural nature. On one hand, traditional statistical methods fail to capture this complexity, while on the other hand non-linear techniques present promising empirical evidence. However, their practical limitations and the expertise required to optimize their parameters are creating skepticism on their utility.

This study introduces a hybrid Rolling Genetic Algorithm-Support Vector Regression (RG-SVR) algorithm for optimal parameter selection and features subset combination when applied to the task of forecasting and trading the EUR/USD, EUR/GBP and EUR/JPY exchange rates. The proposed model genetically searches over a feature space (pool of individual forecasts) and then combines the optimal feature subsets for each exchange rate. A novel fitness function specialized for financial purposes is used to simultaneously minimize the error of the obtained forecasts, increase the profitability of the final forecast combination and reduce the complexity of the algorithm. This is crucial in financial applications, where statistical accuracy is not always synonymous with the financial profitability of the deriving forecasts. The reduced complexity of the algorithm decreases the computational cost of the proposed methodology and makes it ideal for trading applications, where time efficiency is important. At the same time, it acts as a protection against overfitting and impeded generalization abilities. The model employs a sliding window training approach and is capable of capturing the time-varying relationship that dominates the financial trading series.

RG-SVR is benchmarked against seven models. Their statistical and trading performance is compared during the period of 1999–2012. The rationale behind the selection of the benchmarks is twofold. Firstly, Support Vector Machine (SVM) and Support Vector Regression (SVR) architectures with genetically and non-genetically optimized parameters are dominant in the relevant literature. The six most popular and promising variants are identified and included in the study (see Section 4.3). Secondly, it would be unfair to compare the hybrid RG-SVR only with hybrids of the same methodology class, especially when other proposed methodologies have shown statistical and trading superiority in a similar task. One such example is the hybrid Neural Network (NN) that combines Adaptive Radial Basis Functions with Particle Swarm Optimization (ARBF-PSO), as introduced recently by Sermpinis, Theofilatos, Karathanasopoulos, Georgopoulos, and Dunis (2013). The authors apply ARBF-PSO to the task of forecasting and trading the same exchange rates that the present study is investigating. Their results show that ARBF-PSO is outperforming several linear and non-linear models, while its structural complexity is not high. Consequently, ARBF-PSO's selection as a benchmark to this application is required and justified.

To the best of our knowledge, the proposed RG-SVR methodology has not been presented in the literature. Similar hybrid applications exist but they are either limited in classification problems (Dunis, Likothanassis, Karathanasopoulos, Sermpinis & Theofilatos, 2013; Huang & Wang, 2006; Min, Lee, & Han, 2006; Wu, Tzeng, Goo, & Fang, 2007) or the Genetic Algorithm (GA) does not extend to optimal feature subset selection (Chen & Wang, 2007; Pai, Lin, Hong, & Chen, 2006; Yuan, 2012). In addition to this fact, the RG-SVR hybrid is the first GA hybrid SVR algorithm that deploys v-SVR models, minimizes the number of support vectors and applies a specialized loss function and a sliding window training approach. A detail comparison of the algorithm and the previous two algorithms is presented in the next section. The genetically optimized SVM model (GA-SVM) proposed by Huang and Wang (2006), Min et al. (2006), Wu et al. (2007) and Dunis et al. (2013) and the genetically optimized ε-SVR model (GA-ε-SVR) proposed by Chen and Wang (2007) , Pai et al. (2006) and Yuan (2012) will act as benchmarks to the RG-SVR algorithm. Compared to non-adaptive algorithms presented in the literature, the proposed model does not require from the practitioner to follow any time-consuming optimization approach (such as cross validation or grid search) and is free from the data snooping bias (all parameters of RG-SVR are optimized in a single optimization procedure).

From the results of this analysis, it emerges that RG-SVR presents the best performance in terms of statistical accuracy and trading efficiency for all exchange rates under study. RG-SVR's trading performance and forecasting superiority not only confirms the success of the implemented fitness function, but also validates that applying GAs in this hybrid model to optimize the SVR parameters is more efficient compared to the optimization approaches (cross validation and grid search algorithms), that dominate the relevant literature.

The rest of the paper is organized as follows. Section 2 is a literature review of previous relevant research on SVMs and SVRs in forecasting. A detailed description of the study's dataset, the EUR/USD, EUR/GBP and EUR/JPY European Central Bank (ECB) fixing series, is presented in Section 3. Section 4 includes the complete description of the hybrid RG-SVR model, while the statistical and trading performance of the implemented models is presented in Sections 5 and 6 respectively. The concluding remarks are provided in Section 7. The essential theoretical background for the complete understanding of the proposed methodology is given in Appendices A–D, along with the technical characteristics of the models used in this study.

@&#LITERATURE REVIEW@&#

SVMs were originally developed for solving classification problems in pattern recognition frameworks. The introduction of Vapnik's (1995) insensitive loss function has extended their use in non-linear regression estimation problems. SVRs’ main advantage is that they provide global and unique solutions and do not suffer from multiple local minima, while they present a remarkable ability of balancing model accuracy and model complexity (Kwon & Moon, 2007; Suykens, De Brabanter, Lukas, & Vandewalle, 2002).

The literature of SVM and SVR applications is voluminous, especially when they are applied in financial tasks. This study aims to delve deeper into their hybrid structures that are already very popular (Lo, 2000). Lee, Lin, and Wahba (2004) propose the multi-category SVM as an extension of the traditional binary SVM and apply it in two different case studies with promising results. They note that their proposed methodology can be a useful addition to the class of nonparametric multi-category classification methods. Liu and Shen (2006) advance the previous mentioned approach by presenting the multi-category ψ-learning methodology. The main advantage of their method is that the convex SVM loss function is replaced by a non-convex ψ-loss function, which leads to smaller number of support vectors and sparser solution. Martens, Baesens, van Gestel, and Vanthienen (2007) introduce two extraction techniques for SVMs and prove their utility in a series of tests. Hsu, Hsieh, Chih, and Hsu (2009) integrate SVR in a two-stage architecture for stock price prediction and present empirical evidence that shows that its forecasting performance can be significantly enhanced compared to a single SVR model. Lu, Lee, and Chiu (2009) and Yeh, Huang, and Lee (2011) propose also hybrid SVR methodologies for forecasting the TAIEX index and conclude that that they perform better than simple SVR approaches and other autoregressive models. Wu and Liu (2007) introduce the robust truncated hinge loss SVM and claim that their method can overcome drawbacks of traditional SVM models, such as the outliers’ sensitivity in the training sample and the large number of support vectors. Huang, Chuang, Wu, and Lai (2010) forecast the EUR/USD, GBP/USD, NZD/USD, AUD/USD, JPY/USD and RUB/USD exchange rates with a hybrid chaos-based SVR algorithm. In their application, they confirm the forecasting superiority of their proposed technique compared to chaos-based NNs and several traditional non-linear models. Lin and Pai (2010) introduce a fuzzy SVR model for forecasting indices of business cycles, Kim and Sohn (2010) forecast the credit score of small and medium enterprises with SVM, while Wu and Akbarov (2011) apply successfully weighted SVRs to the task of forecasting warranty claims. Moreover, Jiang and He (2012) propose a hybrid SVR that incorporates the Grey relational grade weighting function. When applied to financial time series forecasting, the local Grey SVR outperforms locally weighted counterparts in terms of computational speed and prediction accuracy. A hybrid architecture for computer products’ sales forecasting is also introduced by Lu (2014) based on SVR and multivariate adaptive regression splines.

Most recently, Yao, Crook, and Andreeva (2015) use SVRs in the credit risk modeling framework. Specifically, the authors evaluate the predictive ability of SVR over recovery rates of defaulted corporate instruments between 1985 and 2012. The results show the superiority of the SVR techniques in forecasting these rates compared to other commonly used methods, such as linear regression, fractional response regression and the two-stage methodology. Finally, Geng, Bose, and Chen (2015) present a forecast competition of methodologies, such as NNs, SVM, decision trees and majority voting classifiers, to the task of predicting financial distress of listed Chinese companies. The empirical evidence of that study shows that NNs outperform the SVM, but they acknowledge that this is contradicting previous literature.

Similar applications to the proposed hybrid approach of this study can be found in the literature. For example Min et al. (2006) and Wu et al. (2007) use hybrid GA-SVM models in order to forecast the bankruptcy risk. In both applications, the GAs optimize the parameters of the SVM and select the financial ratios that most affect bankruptcy. Dunis et al. (2013) developed a GA-SVM algorithm and applied it to the task of trading the daily and weekly returns of the FTSE 100 and ASE 20 indices. This approach deals with financial forecasting as a classification problem and has limited applicability. In financial forecasting, though, it is crucial to obtain forecasts that predict not only the sign but also the size of the examined financial indices.


                     Pai et al. (2006) apply epsilon SVR with genetically optimized parameters (GA-ε-SVR) in forecasting exchange rates, while Chen and Wang (2007) forecast the tourist demand in China with a similar model. Yuan (2012) suggests that a GA-ε-SVR model is more efficient than traditional SVR and NN models, when applied to the task of forecasting sales volume. All these GA-ε-SVR applications do not deploy the GA to locate the optimal feature subset but restrict it in optimizing the parameters of the ε-SVR models and select the model's inputs empirically.

As described in the previous paragraph, a variety of hybrid methodologies combining SVM/SVR models with GAs have been proposed during the last decade. However, the proposed RG-SVR theoretically outperforms them as it deploys v-SVR models, which are more suitable to this problem than SVMs and ε-SVR. SVMs are constrained to classification problems and their applicability is limited. The ε-SVR models require the desired accuracy of the approximation to be specified beforehand and are extremely sensitive to the selection of the ε parameter compared to the sensitivity of v-SVRs to the v parameter (Schölkopf, Smola, Williamson, & Bartlett, 2000). Moreover, RG-SVR optimizes on parallel the SVR's parameters and the input's subset, it deploys a sliding window approach to capture the dynamic nature of the examined financial time series, it employs a specialized loss function and tries to minimize the number of support vectors of the final model in order to improve its generalization abilities. The advantages of RG-SVR over other published methodologies are outlined in Table 1.
                  

The ECB publishes a daily fixing for selected EUR exchange rates. These reference mid-rates are based on a daily concentration procedure between central banks within and outside the European System of Central Banks, which normally takes place at 2.15 p.m. ECT time. The reference exchange rates are published both by electronic market information providers and on the ECB's website shortly after the concentration procedure has been completed.

Although only a reference rate, many financial institutions are ready to trade at the EUR fixing and it is therefore possible to leave orders with a bank for business to be transacted at this level. Thus, the ECB daily fixings of the EUR exchange rate are tradable levels and using them is a more realistic alternative to, say, London closing prices. This superiority for financial institutions to transact at the EUR fixing is a well-known fact by FX markets' participants. Financial institutions and institutional investors, and therefore through them, High Net Worth individuals (HNW) would definitely be able to transact at the ECB fixing through their investments with hedge funds, private banks, etc. Smaller private investors could also benefit from reasonably attractive cost conditions (see www.interactivebrokers.com), although they would not be able to transact at the ECB fixing as considered in this study.

In this paper, the ECB daily fixings of EUR/USD, EUR/GBP and EUR/JPY exchange rates are examined over the period of 01/02/1999–30/04/2012. The range of these observations (3395 trading days) is used in four consecutive forecasting exercises. In order to train the RG-SVR, it is necessary to divide the in-sample dataset to training and test subsets (see Section 4.1). The test dataset constitutes approximately the 38 percent
                        1
                     
                     
                        1
                        This choice is based on in-sample experimentation. We obtain similar trading and statistical in-sample RG-SVR performance for test datasets that constitute the 29 percent up to the 47 percent of the in-sample dataset.
                      of the in-sample dataset in each forecasting exercise. The total dataset and the length of each forecasting exercise are presented in Table 2
                     .

The graph in Fig. 1
                      shows the total dataset for the three exchange rates under study.

The three observed time series are non-normal (Jarque–Bera statistics, 1980 confirm their non-normality at the 99 percent confidence interval) containing slight skewness and high kurtosis. They are also non-stationary and hence they are transformed them into three daily series of rate returns
                        2
                     
                     
                        2
                        Confirmation of their stationary property is obtained at the 1 percent significance level by both the Augmented Dickey Fuller (ADF) and Phillips–Perron (PP) test statistics.
                      using the following formula:

                        
                           (1)
                           
                              
                                 
                                    R
                                    t
                                 
                                 =
                                 ln
                                 
                                    (
                                    
                                       
                                          P
                                          t
                                       
                                       
                                          P
                                          
                                             t
                                             −
                                             1
                                          
                                       
                                    
                                    )
                                 
                              
                           
                        
                     where Rt
                      is the rate of return and Pt
                      is the price level at time t.

The summary statistics of the EUR/USD, EUR/GBP and EUR/JPY return series reveal that the slight skewness and high kurtosis remain. In addition, the Jarque–Bera statistic confirms again their non-normality at the 99 percent confidence interval.

The aim of these forecasting exercises is to forecast and trade the one day ahead EUR/USD, EUR/GBP and EUR/JPY exchange rate return (E(R
                     t)). As a first step, we estimate the three return series with several linear and non-linear models. Then, these estimations are used as potential inputs to the RG-SVR algorithm.

The hybrid Rolling Genetic-Support Vector Regression (RG-SVR) model for optimal parameter selection and feature subset combination is presented in this section. Initially, the generic architecture of the proposed methodology is described. Then the feature space, in which the model will search for the optimal subsets and combinations, is identified along with the models that are going to be used as benchmarks. The SVR models are more suitable than the classical SVMs ones for this daily forecasting task, as they provide an exact prediction instead of a binary output. The exact prediction enables the effective application of confirmation filters to improve its performance and provide a hint about the strength of the trading signal.

The proposed model genetically searches over the feature space (pool of individual forecasts) and then combines the optimal feature subsets (SVR forecast combinations) for each exchange rate. In order to achieve this, a simple GA is used where each chromosome comprises feature genes that encode the feature subsets and parameter genes that encode the choice of parameters.

The lack of information about the noise's nature and parameters of the training datasets makes the a priori ε-margin setting of ε-SVR a difficult task. In order to overcome this and decrease the computational demands of the methodology, the RBF v-SVR approach is applied in this hybrid RG-SVR model (see Appendix A.2). The impact of ε parameter in ε-SVR is much more crucial than the impact of ν in ν-SVR. The ν-SVR method is limiting with an upper and lower bound of the number of SVs. In this way, the optimization becomes more stable and the algorithm needs less iterations to find an optimal v value, which would provide accurate results and good generalization properties. The use of the RBF kernel is justified by its extensive use in the literature and its superiority to other types of kernels, when used in financial time series forecasting (Ince & Trafalis, 2008; Lu et al., 2009).

A RBF kernel is in general specified as:

                           
                              (2)
                              
                                 
                                    K
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       ,
                                       x
                                       )
                                    
                                    =
                                    exp
                                    
                                       (
                                       −
                                       γ
                                       
                                          
                                             ∥
                                             
                                                
                                                   x
                                                   i
                                                
                                                −
                                                x
                                             
                                             ∥
                                          
                                          2
                                       
                                       )
                                    
                                    ,
                                    
                                    γ
                                    >
                                    0
                                 
                              
                           
                        where γ represents the variance of the kernel function. Consequently, the parameters which should be optimized by the GA are C, v and γ.

Binary representation is used to model the chromosome. Every feature gene is represented with a binary digit. If that gene takes value 1 (0), then this indicates that the relevant feature should be used (not be used) as input. For the parameter genes 50 bits are used, which are specified as follows:

                           
                              •
                              10 bits to represent the integer part of parameter C of SVRs (range [0–1024])

10 bits to represent the decimal part of parameter C of SVRs (∼0.001 precision)

10 bits to represent the integer part of the γ parameter of RBF functions (range [0–1024])

10 bits to represent the decimal part of the γ parameter of RBF functions (∼0.001 precision)

10 bits to represent the ν parameter of v-SVR (range [0–1] with ∼0.001 precision)

During the initialization step all genes are randomly set with values 0 or 1 with equal probabilities for both of them.

The GA uses the one-point crossover and the mutation operators. The one-point crossover creates two offspring from every two parents. The parents and a crossover point cx
                         are selected at random. The two offsprings are made by both concatenating the genes that precede cx
                         in the first parent with those that follow (and include) cx
                         in the second parent. The probability for selecting an individual as a parent for the crossover operator is called crossover probability and in this application is set to 0.90. Having a high crossover probability enables the model to keep some population for the next generation, hoping to create better new chromosomes from good parts of the old chromosomes. The offspring produced by the crossover operator replaces their parents in the population. The one-point crossover is used for simplicity and to allow bigger blogs of genes to be exchanged during the crossovers. The uniform crossover is usually better only when small mutation probabilities (for example 0.001) are used in order to keep the diversity of chromosomes in the population. In this study, the applied mutation probability is quite big (0.1) and thus the one-point crossover is adequate. The crossover probability is not set to 1 to leave a space for very good solutions of a population to pass through the next generation's population without being altered. The other variation operator which is deployed is the mutation one. The mutation operator places random values in randomly selected genes with a certain probability named as mutation probability. This operator is very important for avoiding local optima and exploring a larger surface of the search space. This probability is set to 0.1 in order to prevent the algorithm from performing a random search.

For the selection step of the GA, the roulette wheel selection process is used (Holland, 1975). In roulette wheel selection chromosomes are selected according to their fitness. The better the chromosomes are, the more chances to be selected they have. In the proposed approach, elitism is used to raise the evolutionary pressure in better solutions and to accelerate the evolution. In that way, it is assured that the best solution is copied without changes to the new population, so the best solution found can survive at the end of every generation.

More specifically, the selection step is used to resemble the survivor of the fittest principle. In that way better solutions have higher probabilities to provide offspring in the next generation. After the application of selection probability, an intermediate population of solutions is formed with each one of them being identical to the solutions from the previous population. Then, crossover and mutation probabilities are applied to provide offsprings combining solutions from this intermediate population. Before the application of the crossover operator, the solutions from this intermediate population are used to make pairs of solutions. Some of these pairs are combined using the crossover operator (with the crossover probability) to produce new solutions, while the others are passed to the next intermediate population without being altered. The offspring is derived from the crossover operator and the pairs of solutions that are not recombined form the next intermediate population. After this process, the intermediate population is altered using the mutation operator. In the case of this study, binary mutation is used. If a gene is selected for mutation, then a new random binary value (0 or 1) is constructed and replaces the previous value in this gene. In order to re-assure that no good solutions are missed, elitism is applied. In that way, the best solution is allowed to pass to the next population. The crossover probability is not set to 1 to leave space also for some solutions that are close to optimal not to be recombined, but just change only with the mutation operator (which is applied only in a few genes of the whole chromosome).

For example, assuming population size is N and elitism of one member, the use of the selection operator leads to an intermediate population of N solutions, which form N/2 pairs. Some of these pairs are selected for crossover using the crossover probability and others are passing on for mutation, as they are. When the mutation process is completed, a complete set of final N solutions is derived. The next iteration starts once one solution is randomly replaced with the best solution of the previous generation.

The RG-SVR hybrid performs a rolling window forecasting exercise. The window size is always ten days (two trading weeks). Else the parameters and the inputs of the algorithm are re-estimated every two weeks. This fact allows the model to capture any structural brake in the dataset. Trading series are exhibiting a highly non-linear nature and are affected by a wide range of factors. Mathematical models with fixed parameters are impossible to model them perfectly. Fund managers and professionals apply a range of different models and re-estimate their parameters frequently. The nature of the financial markets and their erratic behavior make impossible a perfect ex ante configuration of the re-estimation period. Frequent re-estimations dramatically increase the computational time and might induce noise in the estimations.
                           3
                        
                        
                           3
                           Financial markets are vulnerable to behavioral (Froot, Schafrstein, & Stein, 1992) and exogenous factors such as political decisions (Fisman, 2001). These factors are impossible to capture with mathematical models and include noise to time series estimations.
                         Infrequent re-estimations will lead to a low trading performance. The ten trading days sliding window approach is selected based on a trade-off between the in-sample trading performances of RG-SVR and the computational time needed for its estimation with the used dataset.

The population of chromosomes is initialized in the training sub-period. The optimal selection of chromosomes is achieved, when their forecasts maximized the proposed loss function (see
Eq. (3)) in the test-sub period. Then, the optimized parameters and selected predictors of the best solution are used to train the SVR and produce the final optimized forecast for the next observation. This procedure is repeated in each of the four forecasting exercises. For each of these forecasting exercises, the algorithm stores its optimized C, γ and v parameters and set of optimal predictors.

In order to achieve the optimal selection of the feature subsets (individual forecasts) a three-objective fitness function is applied to the hybrid approach. Firstly the annualized return of the SVR forecast combinations should be maximized and secondly the Root Mean Square Error (RMSE) of the output should be minimized in the test sub-period. The presence of the SVs term in the proposed fitness function enables RG-SVR to extract the minimum prediction models which present good prediction accuracy improving its generalization abilities. Based on the above, the fitness function takes the form of
Eq. (3):

                           
                              (3)
                              
                                 
                                    Fitness
                                    =
                                    
                                    annualized
                                    
                                    return
                                    
                                    −
                                    10
                                    ×
                                    RMSE
                                    −
                                    0.001
                                    ×
                                    (
                                    SVs
                                    /
                                    Ntr
                                    )
                                 
                              
                           
                        where Ntr is the size of the training sample.

The aim is to maximize
Eq. (3). The proposed fitness function aims to bring a balance between trading profitability (first factor of
Eq. (3)) and statistical accuracy (second factor) while retaining the complexity of the algorithm to a minimum. This is very important in trading applications as statistical accuracy does not always imply financial profitability. Additionally, complex models in trading applications are not always applicable due to the increased computation time and the dangers of over fitting and lack of generalization. There are two computational time metrics related to the task of building forecasting models and applying them to new data. The first one is the Computational Time required for Training a model (CTT). The second is the Computational Time for the Application of the trained model to new data (CTA).
                           4
                        
                        
                           4
                           For our experiments the CTT time using a simple personal computer with Intel I7 processor ranged from 3 to 4 hours while the CTA time for 10 trading days was a less than 1 second.
                         The training phase is an offline procedure. Thus, the CTT is of minor importance. It is not required to be applied in real time and can be applied in scheduled times (for example once per week). The critical phase in terms of time complexity is the application to new data phase, as this is probably applied in real time. In the proposed approach, the term SVs/Ntr is introduced in the fitness function. This term does not affect the CTT, but dramatically decreases the CTA by using simplest models. This approach has also a significant effect in the model's generalization properties reducing the danger of overfitting. The RMSE is multiplied by 10 to make the first two factors in Eq. (3) more or less equal in levels. SVs is the number of Support Vectors of the trained SVR model. This number is first divided to Ntr to normalize its values from 0 to 1 and then it is further divided with 1000 to decrease its impact to the final fitness function. Reducing model complexity is a secondary task compared to forecasting accuracy and the trading profitability.

The size of the initial population is set to 40 chromosomes while the maximum number of generations is set to 200. The algorithm though terminates when the number of generations is 60 on average. This number must be reached in combination with a termination method that stops the evolution, when the population is deemed as converged. The population is deemed as converged when the average fitness across the current population is less than 5 percent away from the best fitness of the current population. More specifically, when it is less than 5 percent the diversity of the population is very low and evolving it for more generations is unlikely to produce different and better individuals than the existing ones or the ones already examined by the algorithm in previous generations.

The summary of the GA's characteristics is presented in Table 3
                        .

The flowchart of the proposed methodology is depicted in detail in Fig. 2
                        .

The forecasting ability of the proposed methodology is evaluated over a feature space that is synthesized by individual linear and non-linear forecasts of each exchange rate over the periods outlined in Table 2. More specifically, the pool is consisted by a Random Walk (RW), a series of Autoregressive (AR), Moving Average (MA), Autoregressive Moving Average (ARMA) linear models and five non-linear algorithms, namely a Nearest Neighbors Algorithm (k-NN) a Multi-Layer Perceptron (MLP), a Recurrent Neural Network (RNN), a Higher Order Neural Network (HONN) and a Psi-Sigma Neural Network (PSN). A summary of the used linear models is presented in Table 4
                        , while the applied non-linear models are explained in Appendix B.

These models create a pool of 259 individual forecasts in total for each forecasting exercise. The algorithm genetically searches the above feature space and selects the optimal feature subsets. In all cases, GA selects as inputs for the proposed model among the MLP, HONN, RNN, PSN and k-NN predictors. In other words, the model discards as inputs the linear predictors in favor of the non-linear models (see Table 5
                        ). This was expected to some extent due to the non-linear nature of the financial time series.

In the final stage of the algorithm, RG-SVR applies a GA to select the SVR parameters. As mentioned before this process is repeated every ten trading days, in order the proposed model captures any possible structural break in the series under study. Although the selection of the inputs seems to be consisted in the majority of the runs for the three exchange rates, this is not the case for the SVR parameters as the C, v and γ parameters vary between 4.10 and 9.20, 0.35 and 0.84 and 8.12 and 19.77 respectively. These variations of the parameters through the different sliding window iterations enable it to adjust to the continuously changing real financial model. It is worth noting that the proposed RG-SVR methodology is fully adaptive. The practitioner does not need to experiment with the parameters of the algorithm in order to optimize the forecasts. RG-SVR structure and its parameters are generated in a single optimization procedure, which prevents the data snooping effect.

The statistical and trading efficiency of the hybrid model is initially evaluated by benchmarking it with traditional genetic and non-genetically optimized SVRs and SVMs. For the non-genetically optimized SVRs, the selection of the inputs and the SVR parameters is optimized with the statistical methods that dominate the relevant literature (grid search or 5-cross validation over the in-sample period).
                           5
                        
                        
                           5
                           The inputs in all cases of forecasting exercise 1 are selected based on 5-fold cross validation, while in forecasting exercises 2 and 3 with grid search. For the forecasting exercise 4, the inputs selected for EUR/USD and EUR/GBP are based on grid search algorithms, while the ones for EUR/JPY are derived based on 5-fold cross validation.
                         The genetically optimized SVR and SVM benchmarks are constructed following the instructions of those introducing them in the literature (see Table 1). A summary of the SVR-SVM benchmarks is presented below:

                           
                              •
                              An ε-SVR model that implements a 5-fold cross-validation and a simple data-driven calculation on the in-sample dataset to calculate parameters ε, γ and C respectively (ε-SVR1).

A v-SVR model that calculates its parameters v, γ and C as the ε-SVR1 (v-SVR1).

An ε-SVR and v-SVR model that all parameters are selected based on a grid-search algorithm in the in-sample dataset (ε-SVR2 and v-SVR2).

A GA-SVM model as proposed by Min et al. (2006), Wu et al. (2007) and Dunis et al. (2013).

A GA-εSVR model as proposed by Chen and Wang (2007) , Pai et al. (2006) and Yuan (2012).

For more on the ε-SVR1 and v-SVR1 approaches see Duan, Keerthi, and Poo (2003) and Cherkassky and Ma (2004), while for the ε-SVR2 and v-SVR2, see Schölkopf and Smola (2002).

In addition, Sermpinis et al. (2013) introduce the ARBF-PSO method in a forecasting and trading competition of several linear and non-linear models. Their hybrid NN structure proves to be superior in statistical and trading terms over the more traditional MLP, RNN and PSN. The authors investigate the ECB daily fixings of EUR/USD, EUR/GBP and EUR/JPY over the period of 04/01/1999–29/04/2011 (3158 trading days). Their statistical and trading results are not directly comparable with the ones of this study, since their in-sample and out-of-sample periods are overlapping with those of Table 2. Nonetheless, ARBF-PSO's performance is impressive over a dataset of the same three exchange rates that this study is investigating. Thus, ARBF-PSO seems a perfect benchmark for RG-SVR. More details on the ARBF-PSO can be found in Appendix B.

The selected inputs for all the benchmark models are presented in Table 5.

It is notable that all models apply non-linear predictors as inputs. The five non-linear predictors seem to contain all the information that the 254 linear ones contain. This was expected to some extent by the non-linear behavior of financial trading series such as the ones under study.

As it is standard in the literature, in order to evaluate statistically the obtained forecasts, the Root Mean Squared Error (RMSE) is computed. The mathematical formula of this statistic is presented in Appendix C. The lower the output, the better the forecasting accuracy of the model concerned. The Pesaran–Timmermann (PT) test (1992) examines whether the directional movements of the real and forecast values are in step with one another. In other words, it checks how well rises and falls in the forecasted value follow the actual rises and falls of the time series. The null hypothesis is that the model under study has no power on forecasting the relevant exchange rate. The in-sample statistical performance of the models for the four forecasting exercises and the EUR/USD, EUR/GBP and EUR/JPY exchange rates is presented in Table 6
                     .

The results of Table 6 show that RG-SVR presents the best in-sample statistical performance for every exchange rate under study. The PT-statistics rejects the null hypothesis of no forecasting power at the 1 percent confidence interval for all models and series under study. It is also notable that the v-SVR models (v-SVR1 and v-SVR2) have lower RMSEs of the statistical measures than the ε-SVR ones (ε-SVR1 and ε-SVR2). Additionally the genetically optimized models clearly outperform their statistical optimized SVR benchmarks. ARBF-PSO presents occasionally better RMSE values than some SVR-SVM benchmarks, but it always underperforms in comparison to RG-SVR. Table 7
                      summarizes the statistical performance of the models under study in the out-of-sample period. As a reference, the overall performance of the best predictor is presented in the first column (see Appendix D).

From Table 7 it is suggested that RG-SVR retains its forecasting superiority in the out-of-sample period. The genetically optimized SVRs and SVMs outperform their statistically optimized benchmarks while the PT-statistics indicate that all models continue to forecast accurately the directional change of the three exchange rates. In the out-of-sample statistical evaluation, ARBF-PSO still remains less efficient than the proposed model, although in most cases it outperforms the traditional SVR in terms of RMSE. Tables 5 and 6 show that the grid-search optimized SVRs outperform their counterparts optimized with 5-cross validation while the v-SVR models produce more accurate forecasts than the ε-SVR algorithms. The results support the arguments of Yuan (2012) on the performance of the GA-εSVR over the traditional SVR models.

In order to further verify the statistical superiority of the proposed algorithm, the Diebold–Mariano (DM) (1995) statistic for predictive accuracy is computed, while the MSE is considered as the loss function. The test is applied in the four consecutive out-of-sample periods. Table 8
                     
                      below presents the DM statistic comparing the RG-SVR with its benchmarks.

From the above table it is obvious that the null hypothesis of equal predictive accuracy is rejected for all comparisons and for both loss functions at the 1 percent confidence interval (absolute values higher than the critical value of 2.33). Moreover, the statistical superiority of RG-SVR forecasts is confirmed as the realizations of the DM statistic are negative.
                        6
                     
                     
                        6
                        In this study, we apply the DM test to couples of forecasts (RG-SVR vs. another forecasting model). A negative realization of the DM test statistic indicates that the first forecast (RG-SVR) is more accurate than the second forecast. The lower the negative value, the more accurate are the RG-SVR forecasts.
                     
                  


                     Section 5 evaluates the forecasts through a series of statistical accuracy measures and tests. However, statistical accuracy is not always synonymous with financial profitability. In financial applications, the practitioner's utmost interest is to produce models that can be translated to profitable trades. It is therefore crucial to further examine the proposed model and evaluate its utility through a trading strategy. The trading strategy applied is to go or stay ‘long’ when the forecast return is above zero and go or stay ‘short’ when the forecast return is below zero. The ‘long’ and ‘short’ EUR/USD, EUR/GBP or EUR/JPY position is defined as buying and selling Euros at the current price respectively. Therefore, the trigger for taking a position is the sign of the daily obtained forecast.

In order to calculate transaction costs, trading positions are needed. Transaction costs for a tradable amount, say USD 5–10 million, are about 1 pip per trade (one way) between market makers. But since the EUR/USD, EUR/GBP and EUR/JPY time series are considered as a series of middle rates, the transaction cost is one spread per round trip. For this dataset a cost of 1 pip is equivalent to an average cost of 0.0074 percent, 0.0117 percent and 0.0091 percent per position for the EUR/USD, the EUR/GBP and the EUR/JPY respectively. The annualized return after transaction costs is simply the annualized return minus the relevant annualized transaction cost. The annualized transaction cost is the annualized number of transactions multiplied with their relevant cost. Table 9 presents the summary of the in-sample trading performance of the models for each exchange rate under study, while Appendix C includes the specification of the trading performance measures used in this paper.

From the results of Table 9, RG-SVR demonstrates superior trading performance in terms of annualized return and information ratio for all exchange rates and in-sample periods. The maximum drawdowns of the benchmark models are worse than the RG-SVR ones. Their trading performance in the out-of-sample period is presented in Table 10
                     .

RG-SVR continues to outperform all other SVR forecast combination models in terms of trading efficiency. GA-εSVR is found to be the second best model in forecasting exercises 1 and 2. RG-SVR presents on average 3 percent higher annualized returns and 0.33 higher information ratios compared to GA-εSVR in these first two simulations. In the other two exercises the results of ARBF-PSO are better than GA-εSVR, which is ranked third. RG-SVR again achieves higher profits and information ratios than ARBF-PSO on an average of 1.46 percent and 0.15 respectively. Thus, the proposed methodology clearly outperforms its benchmarks in terms of statistical accuracy and financial profitability. The non-genetically optimized SVR methodologies remain less efficient in trading terms compared to their counterparts. But it is interesting to outline the profitability divergence between the different SVR models. For instance, between v-SVR2 and ε-SVR1 there is an average difference of 4.55 percent in annualized returns after transaction costs for the three exchange rates. Smaller differences are also evident in the other SVR approaches. The SVR's trading performance appears very sensitive to the parameters optimization process.

The motivation of this paper is to introduce a RG-SVR model for optimal parameter selection and feature subset combination, when applied to the task of forecasting and trading the EUR/USD, EUR/GBP and EUR/JPY exchange rates. The proposed model genetically searches over a pool of individual forecasts, identifies the optimal feature subsets and finally provides a robust single SVR forecast combination for each exchange rate. This is achieved by applying a fitness function specialized for financial purposes and adopting a sliding window approach. RG-SVR is benchmarked not only against genetically and non-genetically optimized SVRs, but also a robust hybrid NN, the ARBF-PSO.

RG-SVR presents the best performance in terms of statistical accuracy and trading efficiency for all the exchange rates under study. RG-SVR's superiority not only confirms the success of the implemented fitness function, but also validates the benefits of applying GAs to v-SVR models. The results also support prior evidence of ARBF-PSO's efficiency in forecasting and trading the three exchange rates. ARBF-PSO outperforms in all simulations for the traditional SVRs. RG-SVR is far more profitable, though. The large differences in the trading performance of the models under study indicate the sensitivity of SVRs to their parameters optimization processes. In summary, the empirical evidence provided by this study should go some way toward convincing statisticians and practitioners to experiment beyond the bounds of traditional SVR optimization techniques.

@&#ACKNOWLEDGMENT@&#

We thank an anonymous referee for his/her valuable comments.

In this section follows a short theoretical background on SVR, GAs and the issues of parameter and feature subset selection.

If we consider the training data {(x
                        1,y
                        1), (x
                        2,y
                        2)…, (xn, yn
                        )}, where 
                           
                              
                                 x
                                 i
                              
                              ∈
                              X
                              ⊆
                              R
                              ,
                              
                              
                                 y
                                 i
                              
                              ∈
                              Y
                              ⊆
                              R
                              ,
                              
                              i
                              =
                              1
                              …
                              n
                           
                         and n the total number of training samples, then the SVR function can be specified as:

                           
                              (A.1)
                              
                                 
                                    f
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    
                                       w
                                       T
                                    
                                    φ
                                    
                                       (
                                       x
                                       )
                                    
                                    +
                                    b
                                 
                              
                           
                        where w and b are the regression parameter vectors of the function and φ(x) is the non-linear function that maps the input data vector x into a feature space where the training data exhibit linearity (see Fig. A.1c). The ε-sensitive loss function Lε
                         is defined as:

                           
                              (A.2)
                              
                                 
                                    
                                       L
                                       
                                          ɛ
                                       
                                    
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   0
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      if
                                                      
                                                      |
                                                   
                                                   
                                                      y
                                                      i
                                                   
                                                   −
                                                   f
                                                   
                                                      (
                                                      
                                                         x
                                                         i
                                                      
                                                      )
                                                   
                                                   
                                                      |
                                                      ≤
                                                      
                                                         ɛ
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      |
                                                   
                                                   
                                                      y
                                                      i
                                                   
                                                   −
                                                   f
                                                   
                                                      (
                                                      
                                                         x
                                                         i
                                                      
                                                      )
                                                   
                                                   
                                                      |
                                                      −
                                                      
                                                         ɛ
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   if
                                                   
                                                   other
                                                
                                             
                                          
                                       
                                       ,
                                    
                                    
                                       ɛ
                                    
                                    ≥
                                    0
                                 
                              
                           
                        
                     


                        Eq. (A.2) identifies the predicted values that have at most ε deviations from the actual obtained values yi
                        . The ε parameter defines the ‘tube’, while the two slack variables, ξi
                         and 
                           
                              ξ
                              i
                              *
                           
                        , show the distance of yi
                         and 
                           
                              y
                              i
                              *
                           
                         from the upper and lower bound of the ‘tube’ respectively (see Fig. A.1a and b).

The goal is to solve the following argument:

                           
                              (A.3)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             Minimize
                                             
                                             C
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                (
                                                
                                                   ξ
                                                   i
                                                
                                                +
                                                
                                                   ξ
                                                   i
                                                   *
                                                
                                                )
                                             
                                             +
                                             
                                                1
                                                2
                                             
                                             
                                                
                                                   
                                                      ∥
                                                      w
                                                      ∥
                                                   
                                                
                                                2
                                             
                                             
                                             
                                                subject
                                                to
                                             
                                             
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            
                                                               ξ
                                                               i
                                                            
                                                            ≥
                                                            0
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               ξ
                                                               i
                                                               *
                                                            
                                                            ≥
                                                            0
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            C
                                                            >
                                                            0
                                                         
                                                      
                                                   
                                                
                                                
                                                
                                                
                                                
                                                }
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             and
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            
                                                               y
                                                               i
                                                            
                                                            −
                                                            
                                                               w
                                                               T
                                                            
                                                            φ
                                                            
                                                               (
                                                               
                                                                  x
                                                                  i
                                                               
                                                               )
                                                            
                                                            −
                                                            b
                                                            ≤
                                                            +
                                                            
                                                               ɛ
                                                            
                                                            +
                                                            
                                                               ξ
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               w
                                                               T
                                                            
                                                            φ
                                                            
                                                               (
                                                               
                                                                  x
                                                                  i
                                                               
                                                               )
                                                            
                                                            +
                                                            b
                                                            −
                                                            
                                                               y
                                                               i
                                                            
                                                            ≤
                                                            +
                                                            
                                                               ɛ
                                                            
                                                            +
                                                            
                                                               ξ
                                                               i
                                                               *
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                
                                                
                                                
                                                }
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The above quadratic optimization problem is transformed in a dual problem and its solution is based on the introduction of two Lagrange multipliers 
                           
                              
                                 a
                                 i
                              
                              ,
                              
                                 a
                                 i
                                 *
                              
                           
                         and mapping with a kernel function K(xi, x):

                           
                              (A.4)
                              
                                 
                                    f
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          ι
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       
                                          (
                                          
                                             a
                                             i
                                          
                                          −
                                          
                                             a
                                             i
                                             *
                                          
                                          )
                                       
                                       K
                                       
                                          (
                                          
                                             x
                                             i
                                          
                                          ,
                                          x
                                          )
                                       
                                    
                                    +
                                    b
                                    ,
                                    
                                    
                                    where
                                    
                                    0
                                    ≤
                                    
                                       a
                                       i
                                    
                                    ,
                                    
                                       a
                                       i
                                       *
                                    
                                    ≤
                                    C
                                 
                              
                           
                        
                     

SVs are called all the xi
                         that contribute to
Eq. (A.4), thus they lie outside the ε-tube, whereas non-SVs lie within the ε-tube. Increasing ε leads to less SVs’ selection, whereas decreasing it results to more ‘flat’ estimates. The norm term ‖w‖2 characterizes the complexity (flatness) of the model and the term 
                           
                              {
                              
                                 .
                                 
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       (
                                       
                                          ξ
                                          i
                                       
                                       +
                                       
                                          
                                             ξ
                                             i
                                             *
                                          
                                          
                                             )
                                          
                                       
                                    
                                 
                                 
                                    }
                                 
                              
                           
                         is the training error, as specified by the slack variables. Consequently the introduction of the parameter C satisfies the need to trade model complexity for training error and vice versa (Cherkassky & Ma, 2004).

The v-SVR algorithm encompasses the ε parameter in the optimization process and controls it with a new parameter v ∈ (0, 1) (Basak, Pal, & Patranabis, 2007). In v-SVR the optimization problem transforms to:

                           
                              (A.5)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             Minimize
                                             
                                             C
                                             
                                                (
                                                
                                                   v
                                                   
                                                      ɛ
                                                   
                                                   +
                                                   
                                                      1
                                                      n
                                                   
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      n
                                                   
                                                   
                                                      (
                                                      
                                                         ξ
                                                         i
                                                      
                                                      +
                                                      
                                                         ξ
                                                         i
                                                         *
                                                      
                                                      )
                                                   
                                                
                                                )
                                             
                                             +
                                             
                                                1
                                                2
                                             
                                             
                                                
                                                   ∥
                                                   w
                                                   ∥
                                                
                                                2
                                             
                                             
                                             
                                                subject
                                                
                                                to
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            
                                                               ξ
                                                               i
                                                            
                                                            ≥
                                                            0
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               ξ
                                                               i
                                                               *
                                                            
                                                            ≥
                                                            0
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            C
                                                            >
                                                            0
                                                         
                                                      
                                                   
                                                
                                                
                                                
                                                
                                                
                                                }
                                             
                                             and
                                             
                                             
                                                {
                                                
                                                   
                                                      
                                                         
                                                            
                                                               y
                                                               i
                                                            
                                                            −
                                                            
                                                               w
                                                               T
                                                            
                                                            φ
                                                            
                                                               (
                                                               
                                                                  x
                                                                  i
                                                               
                                                               )
                                                            
                                                            −
                                                            b
                                                            ≤
                                                            +
                                                            
                                                               ɛ
                                                            
                                                            +
                                                            
                                                               ξ
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               w
                                                               T
                                                            
                                                            φ
                                                            
                                                               (
                                                               
                                                                  x
                                                                  i
                                                               
                                                               )
                                                            
                                                            +
                                                            b
                                                            −
                                                            
                                                               y
                                                               i
                                                            
                                                            ≤
                                                            +
                                                            
                                                               ɛ
                                                            
                                                            +
                                                            
                                                               ξ
                                                               i
                                                               *
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                
                                                
                                                
                                                }
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The methodology remains the same as in ε-SVR and the solution takes a similar form:

                           
                              (A.6)
                              
                                 
                                    f
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          ι
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       
                                          (
                                          
                                             a
                                             i
                                          
                                          −
                                          
                                             a
                                             i
                                             *
                                          
                                          )
                                       
                                       K
                                       
                                          (
                                          
                                             x
                                             i
                                          
                                          ,
                                          x
                                          )
                                       
                                    
                                    +
                                    b
                                    ,
                                    
                                    
                                    where
                                    
                                    0
                                    ≤
                                    
                                       a
                                       i
                                    
                                    ,
                                    
                                       a
                                       i
                                       *
                                    
                                    ≤
                                    
                                       C
                                       n
                                    
                                 
                              
                           
                        
                     

Based on the ‘v-trick’, as presented by Schölkopf, Bartlett, Smola, and Williamson (1999), increasing ε leads to the proportional increase of the first term of 
                           
                              {
                              
                                 v
                                 
                                    ɛ
                                 
                                 +
                                 
                                    1
                                    n
                                 
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 
                                    (
                                    
                                       ξ
                                       i
                                    
                                    +
                                    
                                       
                                          ξ
                                          i
                                          *
                                       
                                       
                                          )
                                       
                                    
                                 
                              
                              }
                           
                        , while its second term decreases proportionally to the fraction of points outside the ε-tube. So v can be considered as the upper bound on the fraction of errors. On the other hand, decreasing ε leads again to a proportional change of the first term, but also the second term's change is proportional to the fraction of SVs. That means that ε will shrink as long as the fraction of SVs is smaller than v, therefore v is also the lower bound in the fraction of SVs.

Although SVR has emerged as a highly effective technique for solving non-linear regression problems, designing such a model can be impeded by the complexity and sensitivity of selecting its parameters. This procedure can be summarized in the following steps:

                           
                              1.
                              Selection of the kernel function

Selection of the regularization parameter C
                              

Selection of parameters of the kernel function

Selection of the tube size of the ε-sensitive loss function

This selection can be even more complicated and computationally demanding, since individual optimization of the parameters of the above steps is not sufficient. Thus, SVR's performance depends on all parameters being set optimally. Numerous approaches for this optimization have been presented in literature. For example in the ε-SVR, parameter ε can be set simply as a non-negative constant for convenience (ε = 0 or equal to a very small value) (Trafalis & Ince, 2000). This parameter can also be calculated by maximizing the statistical efficiency of a location parameter estimator (Smola, Murata, Schölkopf, & Muller, 1998). Many researchers turn to the v-SVR approach because it is easier to control parameter ε with parameter v (Basak et al., 2007; Schölkopf et al., 1999). Cherkassky and Ma (2004) apply RBF kernels in v-SVR and propose a data-driven choice of parameter C, based on the range of the output values of the training data. But the most popular approach is to use the cross-validation technique (Cao, Chua, & Guan, 2003; Duan et al., 2003) or grid-search algorithms over the dataset (Schölkopf & Smola, 2002).

Feature selection is an optimization problem that refers to the search over a space of possible feature subsets in order to find those that are optimal with respect to specific criteria. Such a problem requires a search strategy that picks the feature subsets and an evaluation method that tests their goodness of fit. Many searching strategies have been proposed in literature, but those who seem to attract more attention are the randomized searches, where probabilistic steps are applied (Sun, Bebis, & Miller, 2004). GAs are commonly used in such cases (Siedlecki & Sklansky, 1989). GAs, formerly introduced by Holland (1975), are search algorithms inspired by the principle of natural selection. They are useful and efficient if the search space is big and complicated or there is not any available mathematical analysis of the problem. A population of candidate solutions, called chromosomes, is optimized via a number of evolutionary cycles and genetic operations, such as crossovers or mutations. Chromosomes consist of genes, which are the optimizing parameters. At each iteration (generation), a fitness function is used to evaluate each chromosome, measuring the quality of the corresponding solution, and the fittest chromosomes are selected to survive. This evolutionary process is continued until some termination criteria are met. In general, GAs can deal with large search spaces and do not get trapped in local optimal solutions like some other search algorithms.

This appendix section provides a brief description of the k-NN and the NN algorithms applied in this study.

Nearest Neighbors is a non-linear and non-parametric forecasting method based on the work of Fix and Hodges (1951). It is based on the idea that pieces of time series in the past have patterns which might have resemblance to pieces in the future. Similar patterns of behavior are located in terms of nearest neighbors using a distance called the Euclidean distance and these patterns are used to predict behavior in the immediate future. It only uses local information to forecast and makes no attempt to fit a model to the whole time series at once. The user defines parameters such as the number of neighbors K, the length of the nearest neighbor's pattern m and the weighting of final prices in a neighbor α'. When α' is greater than 1, a greater emphasis is given to similarity between the more recent observations. Huck and Guégan (2005) suggest that a good approximation for choosing the parameters K and m is dependent on the size of the information set. They choose m from the interval:

                           
                              (B.1)
                              
                                 
                                    m
                                    =
                                    [
                                    R
                                    (
                                    ln
                                    (
                                    T
                                    )
                                    )
                                    ,
                                    
                                    R
                                    (
                                    ln
                                    (
                                    T
                                    )
                                    +
                                    2
                                    )
                                    ]
                                 
                              
                           
                        where R is the rounding function rounding to the immediate lower figure and T the size of the in-sample dataset. They also suggested that K should be approximately twice the value of m. Thus, for our dataset m lies between 7 and 9 and K lies between 14 and 18.
                           7
                        
                        
                           7
                           Based on Table 2, the in-sample datasets of forecasting exercises 1, 2, 3 and 4 are 1342, 1341, 1343 and 1341 trading days respectively. The difference between these datasets is very small in terms of observations. Therefore, the rounded range of m (and consequently K) remains the same for all four exercises.
                         Based on the above guidelines and Dunis and Nathani (2007) who apply k-NN in financial series, we experiment in the in-sample dataset. The set of parameters selected are those that provide the highest trading performance in the in-sample period.

The simpler and most popular NN architecture is the Multi-Layer Perceptron (MLP). A standard MLP has at least three layers. The first layer is called the input layer (the number of its nodes corresponds to the number of explanatory variables). The last layer is called the output layer (the number of its nodes corresponds to the number of response variables). An intermediary layer of nodes, the hidden layer, separates the input from the output layer. Its number of nodes defines the amount of complexity the model is capable of fitting. In addition, the input and hidden layer contain an extra node called the bias node. This node has a fixed value of one and has the same function as the intercept in traditional regression models. Normally, each node of one layer has connections to all the other nodes of the next layer.

The network processes information as follows: the input nodes contain the value of the explanatory variables. Since each node connection represents a weight factor, the information reaches a single hidden layer node as the weighted sum of its inputs. Each node of the hidden layer passes the information through a non-linear activation function and passes it on to the output layer if the calculated value is above a threshold. The training of the network (which is the adjustment of its weights in the way that the network maps the input value of the training data to the corresponding output value) starts with randomly chosen weights and proceeds by applying a learning algorithm called back-propagation of errors (Shapiro, 2000).
                           8
                        
                        
                           8
                           Backpropagation networks are the most common multi-layer networks and are the most commonly used type in financial time series forecasting (Kaastra & Boyd, 1996).
                         The maximum number of the allowed back-propagation iterations is optimized by maximizing a fitness function in the test dataset (see Table 2) through a trial and error procedure. More specifically, the learning algorithm tries to find those weights which minimize an error function (normally the sum of all squared differences between target and actual values). Since networks with sufficient hidden nodes are able to learn the training data (as well as their outliers and their noise) by heart, it is crucial to stop the training procedure at the right time to prevent overfitting (this is called ‘early stopping’). This is achieved by dividing the dataset into 3 subsets respectively called the training and test sets used for simulating the data currently available to fit and tune the model and the validation set used for simulating future values. The network parameters are then estimated by fitting the training data using the backpropagation of errors. The iteration length is optimized by maximizing the forecasting accuracy for the test dataset. Then the predictive value of the model is evaluated applying it to the validation dataset (out-of-sample dataset).

In addition to the classical MLP network, a Recurrent Neural Network is also applied. For an exact specification of recurrent networks, see Elman (1990). A simple recurrent network has an activation feedback which embodies short-term memory. In other words, the RNN architecture can provide more accurate outputs because the inputs are (potentially) taken from all previous values. Although RNNs require substantially more computational time (see Tenti, 1996), they can yield better results in comparison with simple MLPs due to the additional memory inputs. The third NN model included in the feature space is the Higher Order Neural Network (HONN). HONNs are able to simulate higher frequency, higher order non-linear data, and consequently provide superior simulations. For more information on HONNs see Dunis, Laws, and Sermpinis, (2010, 2011). Psi Sigma Networks (PSNs) are considered as a class of feed-forward fully connected HONNs. First introduced by Ghosh and Shin (1991), the PSN creation was motivated by the need to create a network combining the fast learning property of single layer networks with the powerful mapping capability of HONNs, while avoiding the combinatorial increase in the required number of weights. The order of the network in the context of PSN is represented by the number of hidden nodes. In a PSN the weights from the hidden to the output layer are fixed to 1 and only the weights from the input to the hidden layer are adjusted, something that greatly reduces the training time. More details on the PSN model can be found in Ghosh and Shin (1991).

As benchmark to the RG-SVR, this study applies an ARBF-PSO NN model. Its complexity, architecture and characteristics differ from the previous mentioned NNs. Compared to them, in the ARBF-PSO the parameters are optimized through a Particle Swarm Optimization
                           9
                        
                        
                           9
                           The PSO algorithm is a population based heuristic search algorithm based on the social behavior of birds within a flock. In PSO, individuals which are referred to as particles are placed initially randomly within the hyper dimensional search space. Changes to the position of particles within the search space are based on the social–psychological tendency of individuals to emulate the success of other individuals.
                         algorithm. This protects the ARBF-PSO from the dangers of over-fitting and data snooping. However, the practitioner still needs to select the network's inputs (in contrast with RG-SVR which is fully adaptive) through a trial and error approach in the in-sample dataset. For a complete description of the ARBF-PSO see Sermpinis et al. (2013). In order the forecasting completion to be fair, the ARBF-PSO has the same pool of inputs as the RG-SVR.

There is no
                         formal theory behind the selection of the NN inputs and their characteristics, such as number of hidden neurons, learning rate, momentum and iterations. For that reason, we conduct NN experiments and a sensitivity analysis on a pool of autoregressive and autoregressive-moving average terms of the series in the in-sample dataset.
                           10
                        
                        
                           10
                           We also explored as inputs autoregressive and autoregressive-moving-average terms of other exchange rates (e.g. the USD/JPY and GBP/JPY exchange rates), commodities prices (e.g. Gold Bullion and Brent Oil) and stock market prices (e.g. FTSE100, DJIA, NASDAQ and S&P500). However, the set of inputs presented in Table 2 provide the highest trading performance in the in-sample period.
                         For example for the number of iterations, our experimentation started from 5.000 iterations and stopped at the 200.000 iterations, increasing in each experiment the number of iterations by 5.000. This is a very common approach in the literature (Tenti, 1996; Zhang, Patuwo, & Hu, 1998). Based on these experiments and the sensitivity analysis, the sets of variables selected are those that provide the higher trading performance for each network in the in-sample period. For example, in the first forecasting exercise the different sets of inputs of the four NNs for the three series under study are presented in Table B.1.


                        Table B.2 shows the design and training characteristics of all the above NN architectures for the first forecasting exercise.

The statistical and trading performance measures are calculated as shown in Tables C.1
                      and C.2
                      respectively.


                     Table D.1
                      presents the best predictors for the four forecasting exercises and three exchange rates in the in-sample. The overall performance of the best predictor in trading terms is used as benchmark to the SVR and SVM forecast combinations. For example, the best predictor for the EUR/USD exchange rate is composed by the RNN out-of-sample forecasts for the first, second and fourth forecasting exercise and the PSN out-of-sample forecasts for the third forecasting exercise.

@&#REFERENCES@&#

