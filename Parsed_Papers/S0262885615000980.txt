@&#MAIN-TITLE@&#A framework for dynamic restructuring of semantic video analysis systems based on learning attention control

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A framework is proposed to restructure an initial video analysis system dynamically.


                        
                        
                           
                           The initial structure of the system must be hierarchical containing some units.


                        
                        
                           
                           The framework imposes a learning-based dynamic feature selection method on each unit.


                        
                        
                           
                           The system learns how to direct attention to the informative units to achieve a goal.


                        
                        
                           
                           The framework is tested for goal and card event detection in broadcast soccer videos.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Attention control

Broadcast soccer video

Event detection

Q-learning

Semantic video analysis

@&#ABSTRACT@&#


               
               
                  Current semantic video analysis systems are usually hierarchical and consist of some levels to overcome semantic gaps between low-level features and high-level concepts. In these systems, some features, descriptors, objects or concepts are extracted in each level and therefore, total computational complexity of such systems is huge. In this paper, we present a new general framework to impose attention control on a video analysis system using Q-learning. Thus, our proposed framework restructures a given system dynamically to direct attention to the blocks extracting the most informative features/concepts and reduces computational complexity of the system. In other words, the proposed framework directs flow of processing actively using a learning attention control method. The proposed framework is evaluated for event detection in broadcast soccer videos using limited numbers of training samples. Experiments show that the proposed framework is able to learn how to direct attention to informative features/concepts and restructure the initial structure of the system dynamically to reach the final goal with less computational complexity.
               
            

@&#INTRODUCTION@&#

Nowadays, a wide variety of digital videos such as movies, news and sport videos are available on the web, cell-phones, hard disks, and non-volatile memories. On the other hand, almost all users need semi- or fully automated tools for video analysis and management in different applications such as video indexing and retrieval, video summarization and event detection. Therefore, scientists and companies are trying to develop efficient methods and tools for content and semantic video analysis. In this section, we review semantic video analysis systems developed in recent years. Methods used in these systems are divided into two main approaches: (1) methods based on content analysis; and (2) methods based on semantic analysis. The second approach is more important and investigated in this paper but first, we review the first approach briefly. Then, the methods included in the second approach are reviewed and categorized into two categories: (1) heuristic based; and (2) learning based.

Many of the previous systems for video analysis are content based. In other words, they are usually based on low-level audio-visual information. For example, the systems proposed for shot boundary detection (low-level video segmentation) [1], key-frame extraction [2,3], object/scene detection [4], object tracking [5,6], content-based video compression [7], video genre classification [8], video summarization based on affective video content [9], and event detection in general videos [10] are based on low-level processing. Such systems use heuristic (rule based) or statistical (learning based) models applied on low-level features (e.g., color, texture and motion) and sometimes mid-level features. Limitations of content-based methods that only use low-level features have been in the semantic gap between low-level features and high-level concepts [11]. Thus, the main drawback of these methods stems from that fact that low-level features cannot describe high-level semantics properly.

Semantic video analysis can be performed in a hierarchical structure to fill the semantic gap between low-level and high-level features. To this end, intermediate descriptors (e.g., people, objects, and trajectory of moving objects) can be used to bridge the semantic gap. The current methods for semantic video analysis are divided into two main categories. The first category contains heuristic semantic video analysis systems. Many of semantic video analysis systems fall into this category. A lot of researches such as copy detection [12], event detection in sport videos [13–17] and anchor detection in news videos [18] use this approach for video analysis. In such systems, prior knowledge about the problem imposed by an expert is the key of the solution. Although these systems are usually more accurate and faster than learning-based systems (the second category), they depend on the expert's knowledge and thus are not fully automatic. Nowadays, users usually prefer to have fully-automatic systems; therefore, such systems are not of much interest.

The second category contains the methods based on learning approaches. Although learning based approaches are very useful in many fields of pattern recognition and computer vision, there are still many challenges in using learning methods in semantic video analysis systems. One of these challenges is that concepts or events in a video may occur in very wide forms according to different conditions. Therefore, it is not easy to learn the system by a limited number of training samples. On the other hand, many of the interesting high-level concepts are rare. Therefore, learning approaches have to be trained by a limited number of training samples that do not present a given concept properly in the feature space. These systems usually suffer from low accuracy when there are not enough training samples. Such systems usually use a few heuristics to overcome these challenges. Additionally, these systems are usually proposed for specific situations (e.g., for a given type of sport); otherwise, they will not be able to analyze the video efficiently. Some of the recent event detection systems for sport videos such as [19–23] fall into this category. Almost all of these methods partition the video to high-level segments called play-break or highlights. Then, they extract low and/or mid-level features from each segment. Then, according to some training samples, a fully- or semi-supervised learning mechanism construct semantic relationships between low, mid and high-level features. Finally, the resulting system is fully- or almost fully-automatic. In contrast to the above mentioned systems, the system proposed in [24] uses a knowledge adaptation method to compensate shortcomings of the training samples by some other samples partially related to the target concepts. First, this system refines (adapts) the concepts (knowledge) extracted from the auxiliary samples using the knowledge extracted from the training samples. Then, the results of event detection based on the training samples and the refined auxiliary samples are fused to obtain the final decision.

Many of the previous works, especially learning-based semantic video analysis systems have high computational complexity. One of the main approaches to reduce computational complexity and achieve an efficient system is feature selection. There are two basic categories for feature selection methods [25]: filter and wrapper. Filter methods rank the features based on a predefined criterion without any learning, thus, we call them unsupervised. In the wrapper methods, the feature selection criterion is the performance of the decision maker. Therefore, the wrapper methods are considered supervised. Recently, many supervised feature selection methods like evolutionary-incremental methods [26] were presented. However, limited number of training samples limits the use of supervised feature selection methods in semantic video analysis. In order to overcome this issue, a semi-supervised feature selection method was proposed for semantic video analysis [27]. This method exploits a lot of unlabeled video samples to find discriminative features for classification of a few labeled video samples. In fact, this method is a specific feature selection method which combines filter and wrapper methods.

In this paper, we propose a framework for semantic video analysis systems based on learning attention control to reduce computational complexity. Indeed, the proposed method is based on a new dynamic feature selection method. The proposed framework is applied to a hierarchical system containing some Processing Units (PU). PU's are building blocks of hierarchical semantic video analysis systems that extract low-, mid- or high-level features. Our framework imposes a local attention control on each PU and therefore, restructures the system dynamically according to the inputs and goal of the system. Then, the hierarchical structure of the system propagates local attention to the overall structure in a top-down manner. Attention control at each PU directs the decision making process to generate more efficient results with lower computational complexity. Rest of the paper is organized as follows. In Section 2, a brief review of attention control methods is presented. Details of the proposed framework are presented in Section 3. In Section 4, we apply our proposed framework to broadcast soccer videos for event detection. Experimental results are reported in Section 5. Conclusions and future works are explained in Section 6.

Attention control or selective attention is a multi-disciplinary concept which is considered in computer science, neurobiology, and psychology. Frintrop et al. [28] defined selective attention as “the mechanism in the brain that determines which part of the multitude of sensory data is currently of most interest”. In other words, attention control is a process to concentrate on some parts of input data selectively and ignore others. The cocktail party effect is the most common example for attention control: among a room full of unwanted voices and sounds, a person can concentrate and understand the voice of his/her friend.

One of the main applications of attention control is in computer vision which is known as visual attention control. Computational complexity of problems related to the interpretation of image and video is usually very high; thus, computer vision has become an interesting application of attention control. In computer science, the most important aspect of attention control is the computational models for attention control. Therefore, in this section, we concentrate on this aspect and review some famous and common computational models of visual attention control.

There are many computational models for visual attention control [28,29]. The oldest model, called Feature Integration Theory (FIT), was proposed by Treisman and Gelade [30]. Although FIT is the basic and oldest model for visual attention, the most famous and common models are Wolfe's guided search [31] and Itti's model [32]. These models are based on low-level features (e.g., color and edges) and usually used for rapid object detection in a natural scene.

Although attention control can be categorized and reviewed from multiple viewpoints [28,29], we consider two of the most relevant viewpoints in this section. The first viewpoint is based on the driving factor of attention: stimulus-driven and goal-driven [28,29]. Stimulus-driven or bottom-up attention control is derived from raw data of sensors (e.g., pixels of an image). The result of stimulus-driven attention control is detection of the regions of interest that are called salient points. On the other hand, goal-driven attention control has a top-down mechanism which is derived from a certain goal. In other words, stimulus-driven attention control is a blind search to detect salient points, but goal-driven attention control searches the image to find a certain goal using a prior knowledge.

Itti et al. [32] proposed a famous stimulus-driven attention control model to build a 2D saliency map. Itti's model is a general and complete version of the FIT model. This saliency map shows the importance level of each region of the image. Thus, the saliency map guides us to a limited number of regions that may contain a specific object.

Borji et al. [33] changed this model to build a goal-driven mechanism of attention for object detection. This method has a hierarchical structure in three layers. In the first layer, a biased version of Itti's model is used for salient point detection. Then, a domain specific attention control mechanism is applied. This layer is based on mid-level features, usually object-based. Finally, decision making is the last layer which is performed based on the attended information in the second layer. In the last layer, decision making is based on a specific type of decision tree called U-tree which is learned by a reinforcement learning approach.

Another viewpoint of attention control is based on the space of attention [34]: perceptual space and decision space. Attention in perceptual space deals with raw data gathered from sensors or features extracted from them. In this case, attention mechanism has to select some important sensors (features) to attend. Many current attention models such as the models presented in [32,33,35] use attention mechanism in perceptual space. In high level semantic video analysis, we have a few researchers that use attention control in perceptual space. For example, in [36], a method is presented about fast highlight detection and scoring in soccer videos using on-demand feature extraction. This method uses a bottom-up perceptual attention control in feature space to speed-up the procedure of summarization.

Attention in decision space deals with output of decision makers (e.g., label of classes). In other words, attention control in the decision space is similar to an ensemble (fusion) of decision makers that combines some decision makers and discards others dynamically to make a final decision.

In [37], a method called Active Decision Fusion Learning (ADFL) is proposed. This method assumes that each decision maker provides an output in the form of a probability distribution. ADFL tries to direct attention in the decision space and learn an active (dynamic) sequential selection of the decision makers in order to make the final decision. In other words, in a dynamic cascade classifier ensemble, ADFL learns the best sequence of decision makers in a cascade structure according to the on-line output of the decision makers. Markov Decision Process (MDP) is used to model a sequence of decision makers. Also, a continuous reinforcement learning method is used for learning of such MDP problem.

After ADFL, Mirian et al. [38] proposed a framework called Mixture-of-Experts Task and Attention Learning (METAL) for attention mechanism in a continuous decision space. This framework is similar to ADFL but handles a dynamic ensemble of decision makers when each decision maker has a partial knowledge of the task. The proposed framework learns the task and the attention mechanism simultaneously. In other words, METAL learns the task while it learns how to attend the decision makers.

Usually, the previous works presented in visual attention control are used in robot vision for image interpretation and object detection. There are a few researches about application of attention control in video analysis. Almost all of these researches are about detection of affective parts of video and determination of type and intensity of their effectiveness. However, early researches such as [39,40] are based on attention control on low-level features including sound energy, color and motion intensity; recent researches such as [41,42] present hierarchical and complicated methods to apply attention control into semantic video analysis. Recent methods try to fill the semantic gap using mid-level features. In fact, hierarchical analysis lets the system to extract content (mid-level features) from low-level features and then extract a high-level concept such as effectiveness from the analyzed content.

According to our best knowledge, all of the previous methods of semantic video analysis using attention control are in the perception space. In contrast, our proposed method is a top-down attention control model both in perception and decision spaces. At the first look, overall prospect of our proposed method is similar to a perception-based model when a PU wants to select the best inputs actively. On the other hand, a PU tries to select a dynamic sequence of inputs while it makes decision at each step of this sequence. Thus, it has an attention control in the decision space. In fact, selection of a dynamic sequence of inputs during performance improvement of decision making causes a hybrid attention model both in the perception and decision spaces.

The proposed framework tries to impose attention control on a pre-designed hierarchical system using machine learning methods to reduce computational complexity. In other words, our proposed framework restructures a given system actively to direct attention to the PU's extracting the most informative features/concepts. This framework is introduced in a hierarchical structure that consists of some PU's in different levels. Attention control of the framework is embedded locally at each PU using a simple mechanism. In fact, the proposed framework imposes attention control mechanism on each PU and thus, each PU is able to select its inputs according to a dynamic sequence. On the other hand, the structure of the system is hierarchical and output of some PU's at lower levels is used as input of some higher PU's. Therefore, the final system works based on an overall attention control. The structure of the framework and the mechanism of attention control are presented in this section.

The structure of the system, that we apply our proposed framework to, is hierarchical. Thus, the system may include multiple processing levels and there are some PU's at each level. A PU at each level may use the outputs of some or all of the PU's in the lower levels. The designer of system defines the number of levels and the PU's at each level. Additionally, the designer defines the connections between PU's. However, the designer can put a PU in the ith level if the inputs of this PU are not produced by any PU at the jth level (j
                        ≥
                        i) and the output of this PU is not connected to any PU at the jth level (j
                        ≤
                        i). Also, the designer can use a fully connected structure where a PU at the ith level is connected to all compatible PU's located at the lower levels (1st, 2nd,…, (i-1)th). It is important that the designer connects only compatible PU's because the output of some PU's at the lower levels may be incompatible (from viewpoint of format) with the input of some PU's at the higher levels. For example, if the output of a PU at the lower level is a matrix, it is incompatible with a PU at the higher level that uses a scalar as the input.

The proposed framework may remove connections between PU's permanently or dynamically during learning of attention control but does not create new connections between PU's. Therefore, the designer has to define all possible and compatible connections between PU's at the initial state if he does not have prior knowledge about proper structure of the system. The connections defined by the designer will be used as the initial connections between PU's in the learning phase.

From viewpoint of graph theory, the structure of the system in the proposed framework is a specific directed acyclic graph in which all directions are bottom-up. In other words, there is no connection from a PU at a higher level to a PU at a lower level. Because many of the current approaches for semantic video analysis systems are hierarchical (level-by-level), this constraint of our proposed framework is consistent with almost all current semantic video analysis systems. Fig. 1
                         shows the structure of an example video analysis system that is in the form of our proposed framework. This system has 4 levels and the initial connections between PU's are shown by arrows.

We define an adjacency matrix for the initial structure (connections) denoted by AM
                        
                           init
                        . AM
                        
                           init
                         describes the initial structure of the system completely. According to the defined constraint on the proposed structure, AM
                        
                           init
                         is always a lower (left) triangular matrix. For the structure depicted in Fig. 1, AM
                        
                           init
                         is a 9×9 matrix as shown in Table 1
                        .

In the proposed framework, we can define a cost for each PU denoted by PUC. Cost of a given PU determines the expected cost of that PU to process input and generate output. In this paper, we define the cost of a given PU (PU
                        
                           g
                        ) as the expected total processing time of PU
                        
                           g
                         denoted by PUT
                        
                           g
                        
                        
                           total
                        . If self-processing time of PU
                        
                           g
                         is denoted by PUT
                        
                           g
                        , PUT
                        
                           g
                        
                        
                           total
                         is calculated by:
                           
                              
                                 P
                                 U
                                 
                                    C
                                    g
                                 
                                 =
                                 P
                                 U
                                 
                                    T
                                    g
                                    total
                                 
                                 =
                                 P
                                 U
                                 
                                    T
                                    g
                                 
                                 +
                                 
                                    
                                       ∑
                                       
                                          i
                                          |
                                          P
                                          
                                             U
                                             i
                                          
                                          
                                          is
                                          
                                          called
                                          
                                          b
                                          y
                                          
                                          P
                                          
                                             U
                                             g
                                          
                                       
                                    
                                 
                                 P
                                 U
                                 
                                    T
                                    i
                                    total
                                 
                                 .
                              
                           
                        
                     


                        PUT
                        
                           i
                        
                        
                           total
                         is the total expected processing time of PU
                        
                           i
                         where the output of PU
                        
                           i
                         is used by PU
                        
                           g
                         as an input. In other words, the total expected processing time of PU
                        
                           g
                         is equal to self-processing time of PU
                        
                           g
                         plus sum of the total expected processing time of all PU's that are called by PU
                        
                           g
                        . In this case, because PU
                        
                           g
                         may call different PU
                        
                           i
                         in different conditions dynamically, PUT
                        
                           g
                        
                        
                           total
                         varies according to how many and what PU's are called by PU
                        
                           g
                        . Consequently, we assume that PUT
                        
                           g
                        
                        
                           total
                        
                        =
                        PUT
                        
                           g
                         if PU
                        
                           g
                         is at the first level.

It is important that PUT
                        
                           g
                         is the expected self-processing time of PU
                        
                           g
                         to process some inputs, make a decision and generate output. In other words, PUT
                        
                           g
                         does not contain the processing time of providing inputs for PU
                        
                           g
                        . The processing time related to providing inputs for PU
                        
                           g
                         is equal to sum of PUT
                        
                           i
                        
                        
                           total
                         while PU
                        
                           i
                        's provide input for PU
                        
                           g
                        .

In the proposed framework, we impose a top-down attention control on a hierarchical system using machine learning approaches. Attention control controls the connections between lower PU's and higher PU's. In other words, each PU may use some PU's at lower levels and discard others dynamically.

To achieve this goal, we build the final system by a bottom-up hierarchical approach. The PU's at the first level do not need learning attention control. In fact, these PU's extract low-level audio-visual features from video without any attention mechanism. Thus, at first, learning attention control is applied to all PU's at the second level. Similarly, learning attention control is applied to PU's at higher levels, level-by-level (see Algorithm 1 for details). In this algorithm, we assume that the system has lev levels and the number of PU's at level i is N
                        
                           PU
                        
                        
                           i
                        .
                           Algorithm 1
                           The proposed algorithm in a macro view to impose top-down learning attention control.


                              
                                 
                                    
                                 
                              
                           


                        Algorithm 1 builds a hierarchical structure of attention control. This algorithm uses an algorithm called DynamicFeatureSelection. DynamicFeatureSelection shows a micro view of our proposed framework that uses a learning attention control mechanism for dynamic feature selection at a PU. It gets PU
                        
                           g
                         and ConSet and calculates GTDFS
                        
                           g
                        . GTDFS
                        
                           g
                         is the Guidance Table for Dynamic Feature Selection (GTDFS) that is usable for dynamic feature (input) selection at PU
                        
                           g
                        . Hierarchical utilization of learning attention control mechanism at each PU results in a system with top-down learning attention control mechanism.

We assume that a given PU named PU
                        
                           g
                         is initially connected to a set of PU's denoted by ConSet
                        ={PU
                        
                           i
                        } where AM
                        
                           init
                        (i,
                        g)=1. It means that PU
                        
                           g
                         may use the output of one or some of {PU
                        
                           i
                        } as input. In other words, we have a feature set {f
                        
                           i
                        } where f
                        
                           i
                         is the output of PU
                        
                           i
                        . Additionally, we assume that f
                        
                           i
                         is a number or symbol from a finite and countable set. In some cases, output of PU
                        
                           i
                         may be a vector. In this case, we assume that the output vector of PU
                        
                           i
                         is a unique bag of features and we still denote it by f
                        
                           i
                        . Therefore, all elements of this bag of features are always considered together as a unique feature.

Now, attention control mechanism has to select the best sequence of features from {f
                        
                           i
                        } actively to maximize the expected value of a utility function. In ordinary feature selection methods, the order of the final selected features is not important and the selected features are the same for all situations. Despite ordinary feature selection methods that find a static set of best features, our proposed method finds the best sequence of features dynamically.

In our proposed method, we select features step by step. At each step, all of the selected features are used as a bag of features for decision making. Then, according to the output of the decision maker (PU), another feature may be selected or feature selection may stop. When feature selection stops, the output of the decision maker using all selected features is the final output of PU.

We use the Q-learning method to perform dynamic feature selection at every PU. Q-learning is a famous method for reinforcement learning [43]. As depicted in Fig. 2
                        , reinforcement learning approaches are usually used for the training of an agent to take actions in either a static or dynamic environment. The agent takes an action at the current state, and then the state of the agent changes to a new state. Additionally, the agent receives a feedback called reward from the environment for the taken action. Reward is generated by a critic that we can consider it as a utility function.

Reinforcement learning approaches help the agent to learn how to maximize the expected reward (utility). Q-learning is a model-free method that learns an action-value function in a Markov Decision Process (MDP) environment. The action-value function determines the expected utility of taking a given action in a given state and following a fixed policy thereafter.

In our problem, the agent is a PU and the environment is the training set at a given feature space. States, actions and rewards (utility function) are defined below.

For complete modeling of our dynamic feature selection problem by an MDP, we have to define states, actions and a utility function. We assume that there are n features in {f
                           
                              i
                           }. Additionally, we assume that the output of PU
                           
                              g
                            is an element of a finite and countable set. Therefore, we denoted that different output values/symbols of PU
                           
                              g
                            by oj
                           . {o
                           
                              j
                           } is the set of different possible values/symbols that may be used as the output of PU
                           
                              g
                            and we assume that it has m elements.

Next, we define each state as a pair of (f
                           
                              i
                           ,
                           o
                           
                              j
                           ). Additionally, we define another state called initial state denoted by ∅
                              s
                           . Initial state shows that no feature is selected yet. (f
                           
                              i
                           ,
                           o
                           
                              j
                           ) shows that the latest selected feature at the previous state is fi
                            and the output of the decision maker using all of the previously selected features is oj
                           . According to the above explanations, the set of states is denoted by St where the total number of states is n
                           ×
                           m
                           +1.
                              
                                 
                                    St
                                    =
                                    
                                       
                                          
                                             ∅
                                             s
                                          
                                          ,
                                          
                                          
                                             
                                                
                                                   f
                                                   1
                                                
                                                ,
                                                
                                                
                                                   o
                                                   1
                                                
                                             
                                          
                                          ,
                                          
                                          
                                             
                                                
                                                   f
                                                   1
                                                
                                                ,
                                                
                                                
                                                   o
                                                   2
                                                
                                             
                                          
                                          ,
                                          
                                          …
                                          ,
                                          
                                          
                                             
                                                
                                                   f
                                                   i
                                                
                                                ,
                                                
                                                
                                                   o
                                                   j
                                                
                                             
                                          
                                          ,
                                          
                                          …
                                          
                                             
                                                
                                                   f
                                                   n
                                                
                                                ,
                                                
                                                
                                                   o
                                                   m
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Then, we define an action as selection of a feature from {f
                           
                              i
                           }. At the current state (f
                           
                              i
                           ,
                           o
                           
                              j
                           ), we select another feature fk
                            (take an action), make a decision using fk
                            and the previously selected features to generate ol
                            as the output. Thus, (f
                           
                              k
                           ,
                           o
                           
                              l
                           ) is the next state. Consequently, we define another action denoted by ∅
                              a
                            to terminate the feature selection process. When ∅
                              a
                            is selected as an action, feature selection ends. According to the above explanations, we define the action set Ac with the following n
                           +1 elements:
                              
                                 
                                    A
                                    c
                                    =
                                    
                                       
                                          
                                             ∅
                                             a
                                          
                                          ,
                                          
                                          
                                             f
                                             1
                                          
                                          ,
                                          
                                          …
                                          ,
                                          
                                          
                                             f
                                             i
                                          
                                          ,
                                          
                                          …
                                          ,
                                          
                                          
                                             f
                                             n
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        

According to the above definition of states and actions, Q-table will be a (n
                           ×
                           m
                           +1)×(n
                           +1) matrix as shown in Table 2
                           . Q-table is the memory of the agent about preference of each action in each state.

Q-learning essentially learns the first order Markov models. A first order Markov model has just a memory of size one for each state–action pair. In other words, a two dimensional matrix may save the total memory of the agent about all state–action pairs. In this case, the current state depends only on the previous state. For higher order Markov models, a higher dimensional matrix can serve as the memory of the agent.

In our problem, selection of a feature at a given state does not only depend on the previous state (indeed the previous feature), but also depends on the complete set of features selected from the initial state to the current state. Although our problem to select the best set of features is not essentially a first order Markov model, we use a two dimensional matrix as the Q-table. In this case, we have to change the ordinary Q-learning method to adapt it to our problem.

Our proposed definition for states and actions is the first step towards changing a non-first order Markov model to a first order Markov model, but it is not sufficient. In order to adapt Q-learning, we use an auxiliary variable called LSF (List of Selected Features). LSF helps us to remember the history of the states visited and the actions taken at the previous states.

Additionally, we change the definition of the utility function to a new definition. In our proposed method, the utility function determines the expected utility of selecting a given feature in a given state, adding the selected feature to current LSF, and makes decision based on the new LSF. In other words, the utility of the current state is calculated based on the utility of decision making using the current sequence of selected features (LSF). Utility of LSF may be defined as a combination of error rate and total processing time of decision making based on LSF. For example, we may define utility (Ut) as:
                              
                                 
                                    U
                                    t
                                    =
                                    −
                                    
                                       
                                          E
                                          
                                             r
                                             
                                                P
                                                
                                                   U
                                                   g
                                                
                                             
                                          
                                          +
                                          P
                                          U
                                          
                                             T
                                             g
                                             total
                                          
                                       
                                    
                                 
                              
                           where 
                              E
                              
                                 r
                                 
                                    P
                                    
                                       U
                                       g
                                    
                                 
                              
                            and PUT
                           
                              g
                           
                           
                              total
                            are the error rate and the expected total processing time of PU
                           
                              g
                            respectively, when PU
                           
                              g
                            uses the features listed in LSF for decision making.

According to the above problem definition, the problem is modeled by a first order Markov model with n
                           ×
                           m
                           +1 states, n
                           +1 actions, an auxiliary data structure LSF, and a new utility function.

According to the above explanations, our problem is not a first order MDP problem; thus, we change the ordinary Q-learning method to be adapted with our high order MDP problem. This method is similar to the ordinary Q-learning method presented in [43], but there are two main differences between the ordinary Q-learning and our proposed learning method.

The first difference is in the update mechanism of Q-table. In the ordinary Q-learning method, with the assumption of the first order MDP problem, Q-table is updated at every step of episode. In this case, in every steps of episode, agent gets a reward from the critic for its action taken in the current state and it updates Q-table. But in our proposed method, Q-table is updated at the end of episode, when we get the utility (reward) of the dynamic feature selection. In this case, we update all the Q-values of state–action pairs of the episode simultaneously at the end of episode.

The second difference is in the mathematical formulation of updating Q-table. In the ordinary Q-learning method, Q-table is updated in every steps of episode by [43]:
                              
                                 
                                    Q
                                    
                                       
                                          
                                             s
                                             j
                                          
                                          ,
                                          
                                          
                                             a
                                             j
                                          
                                       
                                    
                                    =
                                    Q
                                    
                                       
                                          
                                             s
                                             j
                                          
                                          ,
                                          
                                          
                                             a
                                             j
                                          
                                       
                                    
                                    +
                                    α
                                    
                                       
                                          
                                             r
                                             j
                                          
                                          +
                                          γ
                                          
                                          m
                                          a
                                          
                                             x
                                             k
                                          
                                          
                                             
                                                Q
                                                
                                                   
                                                      
                                                         s
                                                         
                                                            j
                                                            +
                                                            1
                                                         
                                                      
                                                      ,
                                                      
                                                      
                                                         a
                                                         k
                                                      
                                                   
                                                
                                             
                                          
                                          −
                                          Q
                                          
                                             
                                                
                                                   s
                                                   j
                                                
                                                ,
                                                
                                                
                                                   a
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where rj
                            is the reward observed after performing aj
                            in sj
                           , going to the next state s
                           
                              j
                              +1 and following the optimal policy thereafter (selecting the best action in s
                           
                              j
                              +1). α and γ, the learning rate and discount factor, are the parameters of learning algorithm.

The max operator in the above equation lets us follow the optimal policy in the next state. In this case, we have a standard first order MDP problem. Thus, we can follow an optimal policy in each step of an episode, without attention to the policy of the previous state. Also, we can calculate rj
                            (the reward of each step related to performing aj
                            in sj
                           ) of each step separately. However, our problem is not a real first order MDP and we cannot calculate rj
                            separately for each step of episode. Our problem is a high order MDP that we follow an ε
                           −greedy policy in all the states of an episode during the learning phase. Thus, we do not use the max operator and calculate the total utility (reward) of episode, from the initial state ∅
                           
                              s
                            to the final state in which ∅
                           
                              a
                            is selected as an action in that state. The total utility of episode is denoted by Ut and applied to the Q-values of all the state–action pairs of episode. Additionally, we use ε
                           −greedy policy for action-value updates during an episode. In other words, after performing a
                           
                              j
                            in s
                           
                              j
                            and going to the next state s
                           
                              j
                              +1, we are not following the optimal policy thereafter. Thus, mathematical formulation for updating Q-table in our proposed learning method is as:
                              
                                 
                                    
                                       
                                          
                                             
                                                ∀
                                                
                                                   
                                                      
                                                         s
                                                         j
                                                      
                                                      ,
                                                      
                                                      
                                                         a
                                                         j
                                                      
                                                   
                                                
                                                that
                                                
                                                visited
                                                
                                                in
                                                
                                                the
                                                
                                                episode
                                                :
                                             
                                          
                                          
                                             
                                                Q
                                                
                                                   
                                                      
                                                         s
                                                         j
                                                      
                                                      ,
                                                      
                                                      
                                                         a
                                                         j
                                                      
                                                   
                                                
                                                =
                                                Q
                                                
                                                   
                                                      
                                                         s
                                                         j
                                                      
                                                      ,
                                                      
                                                      
                                                         a
                                                         j
                                                      
                                                   
                                                
                                                +
                                                α
                                                
                                                   
                                                      U
                                                      t
                                                      +
                                                      γ
                                                      Q
                                                      
                                                         
                                                            
                                                               s
                                                               
                                                                  j
                                                                  +
                                                                  1
                                                               
                                                            
                                                            ,
                                                            
                                                            
                                                               a
                                                               
                                                                  j
                                                                  +
                                                                  1
                                                               
                                                            
                                                         
                                                      
                                                      −
                                                      Q
                                                      
                                                         
                                                            
                                                               s
                                                               j
                                                            
                                                            ,
                                                            
                                                            
                                                               a
                                                               j
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        

Above explanations are about the main differences of our proposed learning algorithm with the ordinary Q-learning method. In the following, we describe details of our proposed method.

Our proposed method for learning the attention control in the dynamic feature selection algorithm is shown in Algorithm 2. Like an ordinary Q-learning method, we use a (m
                           ×
                           n
                           +1)×(n
                           +1) matrix as Q-table for learning. The Q-table is initialized by small random values. The Q-learning method updates the Q-table by an iterative algorithm. A learning iteration is a procedure that starts from the initial state (∅
                           
                              s
                           ) and ends at the state in which ∅
                           
                              a
                            is selected as an action in that state. In other words, LSF is always empty at the start of the learning episode and it contains the final set of selected features when the learning episode ends. At each learning episode, some elements of the Q-table are updated simultaneously when state transitions ends.

In the proposed method, Q-learning updates the Q-table iteratively using the training samples. In fact, after a state transition in the proposed combined space, the decision maker (PU) is evaluated on the training samples using the selected features (LSF). The training samples are shuffled randomly and each training sample is used in a learning episode. We can use the training samples a few times if necessary (e.g., in the case of lack of enough training samples).

There are two action selection policies in Q-learning: a policy for learning and a policy for living (when learning finishes and agent is ready for test). Action selection policy during learning is ε
                           −greedy, but when learning phase finishes, the action selection policy is greedy (selection of the best action in each state). During learning, we let the agent to explore the space and evaluate different actions (even non-best actions) at a given state by ε
                           −greedy. But when learning finishes, we assume that the Q-table contains almost exact preference of each action at a given state; therefore, we select the best action surely by greedy policy [43].

The ε
                           −greedy method selects the best action with probability of 1−
                           ε and selects a random action with probability of ε where 0≤
                           ε
                           ≤1 [43]. When ε approaches 1, the selection strategy becomes fully random, and when ε approaches 0, the selection strategy is fully greedy. In this paper, we use a common value for ε, i.e., ε
                           =0.1.

When learning finishes, we save the best action of every state in the GTDFS
                           
                              g
                           . In fact, GTDFS
                           
                              g
                            guide us to select the best feature at each state dynamically for PU
                           
                              g
                           . Finally, we remove the rows of GTDFS
                           
                              g
                            that not accessible from the initial state ∅
                           
                              s
                           . It means that some rows of GTDFS
                           
                              g
                            related to the PU's which are not called by PU
                           
                              g
                            at all are removed. Thus, the final GTDFS
                           
                              g
                            is a well-defined and abstract table for dynamic feature selection.
                              Algorithm 2
                              The proposed algorithm for learning of dynamic feature selection in a PU.


                                 
                                    
                                       
                                    
                                 
                              

We present an example to illustrate our proposed method further. Assume α
                           =0.1, γ
                           =0.1, and a 5×3 Q-table as shown in Table 3
                            for PU
                           
                              g
                           . Assume that in an episode, we start with ∅
                           
                              s
                            and then select f
                           1 as the first feature while the output of decision maker with f
                           1 is o
                           1. Then, we select f
                           2 as the second feature while the output of decision maker with {f
                           1,
                           f
                           2} is o
                           2. In this step, we select ∅
                           
                              a
                            and the procedure of feature selection and decision making is finished. Here, we assume that Ut
                           =−0.2.

According to the proposed algorithm, the highlighted elements of Q-table in Table 3 will be updated as below:
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   row
                                                   :
                                                   1
                                                   ,
                                                   
                                                   column
                                                   :
                                                   2
                                                
                                             
                                             :
                                             
                                             0.116
                                             =
                                             0.15
                                             +
                                             0.1
                                             
                                                
                                                   −
                                                   0.2
                                                   +
                                                   0.1
                                                   ×
                                                   0.1
                                                   −
                                                   0.15
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   row
                                                   :
                                                   2
                                                   ,
                                                   
                                                   column
                                                   :
                                                   3
                                                
                                             
                                             :
                                             
                                             0.078
                                             =
                                             0.1
                                             +
                                             0.1
                                             
                                                
                                                   −
                                                   0.2
                                                   +
                                                   0.1
                                                   ×
                                                   0.8
                                                   −
                                                   0.1
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   row
                                                   :
                                                   5
                                                   ,
                                                   
                                                   column
                                                   :
                                                   1
                                                
                                             
                                             :
                                             
                                             0.71
                                             =
                                             0.8
                                             +
                                             0.1
                                             
                                                
                                                   −
                                                   0.2
                                                   +
                                                   0.1
                                                   −
                                                   0.8
                                                
                                             
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        

Accordingly, the updated Q-table is shown in Table 4
                           .

In this section, we apply our proposed framework in an event detection system for broadcast soccer videos. The proposed event detection system tries to detect goal and card event using cinematic features.

At first, we design the initial structure of the system. In Fig. 3
                     , the initial structure of the system, all usable PU's and their possible connections are depicted. It seems that the initial structure of the system is almost fully connected; however, there are some non-plausible connections in the structure. In fact, we do not connect the PU's that are not consistent from the view point of input/output format. Thus, the initial structure does not need high prior knowledge about connections of PU's; our proposed method will find the best connections.

In Fig. 3, simple lines show loose connections that may be eliminated after learning but double lines show tight connections. Tight connections are always connected and will not be affected by attention control learning. Tight connections are based on prior knowledge about the system. In the remaining of this section, we explain details of each PU and their connections.

All PU's at the first level perform a low-level processing on video frames. These PU's extract common low-level features in soccer video analysis systems. Table 5
                         shows brief explanations of these PU's that extract basic features. By the way, we normalize the output of all mentioned PU's to the range [0,1].

Essence of some PU's including RGB, HSV, RGBH and HSVH is simple and clear, but the PU's named SHSVH, 3DRGB and EgH need more explanations.

In SHSVH, first the image is segmented into 3×3 segments like the method proposed in [16], and then 16-bin histogram of H, S and V components of each segment is calculated. The final output of SHSVH is obtained using a concatenation of the histograms of all segments. Therefore, the size of the output of SHSVH is 48×9.

In 3DRGB, we calculate 3D RGB histogram of the image. Despite the ordinary RGB histogram, constructed by a concatenation of the histograms of the R, G, and B channels, a 3D RGB histogram is constructed by quantizing the 3D RGB color space and counting the number of pixels in each voxel of the resulting volumetric RGB space [36]. Here, we quantize R, G, and B channels to 4 levels. Therefore, 4×4×4=64 color voxels are generated and the length of the 3D RGB histogram is 64. Computational complexity of the 3D RGB histogram is more than the ordinary RGB histogram but the 3D RGB histogram may describe color information more precisely than the RGB histogram.


                        EgH presents a low-level texture feature of the frames using histograms of edges in different scales and orientations. To this end, we use 2D Discrete Wavelet Transform (DWT) in 4 levels. At each level, 2D DWT generates detail coefficient matrices for horizontal, vertical and diagonal edges. Therefore, we achieve wavelet coefficients in 4 scales and 3 orientations. Each coefficient matrix is binarized by the Otsu method [44] and then, the number of edges (white pixels) is counted. Finally, we concatenate the numbers of edges in different scales and orientations to achieve a feature vector with 12 elements.

PU's at the second level are SBD (Shot Boundary Detection), SrD (Shirt Detection) and VTR (View Type Recognition). In this section, we explain details of algorithms employed by these PU's.

Shot boundary detection is a low-level processing that partitions the video into low-level parts. There are many methods for shot boundary detection [45,46]. The main idea for shot boundary detection is based on calculating difference of features in consecutive frames. We use this idea in SBD. In other words, we get feature vectors of two consecutive frames and calculate a difference measure of the feature vectors. Then, two adaptive thresholds are applied to the calculated difference measure to detect shot boundary and classify shot transition (i.e., cut or gradual).

We assume that F
                           
                              i
                            and F
                           
                              i
                              +1 are two feature vectors related to frames i and i
                           +1, respectively. We calculate DM
                           
                              i
                            as the difference measure by:
                              
                                 
                                    D
                                    
                                       M
                                       i
                                    
                                    =
                                    
                                       
                                          ∑
                                          j
                                       
                                    
                                    
                                       
                                          
                                             F
                                             i
                                          
                                          
                                             j
                                          
                                          −
                                          
                                             F
                                             
                                                i
                                                +
                                                1
                                             
                                          
                                          
                                             j
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        

Now, we decide if a transition has occurred between frames i and i
                           +1 and if so, what the type of the shot transition is: cut or gradual. We use a thresholding method to do this:
                              
                                 
                                    
                                       O
                                       
                                          S
                                          B
                                          D
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                0
                                                
                                                
                                                   
                                                      no
                                                      
                                                      transition
                                                   
                                                
                                             
                                             
                                                if
                                                
                                                D
                                                
                                                   M
                                                   i
                                                
                                                ≤
                                                
                                                   T
                                                   1
                                                   
                                                      S
                                                      B
                                                      D
                                                   
                                                
                                             
                                          
                                          
                                             
                                                1
                                                
                                                
                                                   gradual
                                                
                                             
                                             
                                                if
                                                
                                                
                                                   T
                                                   1
                                                   
                                                      S
                                                      B
                                                      D
                                                   
                                                
                                                ≤
                                                D
                                                
                                                   M
                                                   i
                                                
                                                ≤
                                                
                                                   T
                                                   2
                                                   
                                                      S
                                                      B
                                                      D
                                                   
                                                
                                             
                                          
                                          
                                             
                                                2
                                                
                                                
                                                   cut
                                                
                                             
                                             
                                                if
                                                
                                                
                                                   T
                                                   2
                                                   
                                                      S
                                                      B
                                                      D
                                                   
                                                
                                                ≤
                                                D
                                                
                                                   M
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where O
                           
                              SBD
                            is the output of SBD. In the above equation, T
                           1
                           
                              SBD
                            and T
                           2
                           
                              SBD
                            are two adaptive thresholds which are calculated based on the training samples. In fact, shot transitions are detected by T
                           1
                           
                              SBD
                            and the detected shot transitions are classified by T
                           2
                           
                              SBD
                           . T
                           1
                           
                              SBD
                            is adaptively determined to achieve a 100% recall for shot transition detection while T
                           2
                           
                              SBD
                            is adaptively determined to achieve a high accuracy rate for shot transition classification.

The proposed method for shirt detection is based on detection of the color of the players' or referee's jersey. Our proposed method is similar to the method presented in [16]. In this method, we need some meta-data imported by the user. This meta-data is about the color of the shirt of the players of teams A and B, goal keepers of teams A and B and referee. User imports five patches of shirt images of size 40×40pixels for each of the mentioned people. In this case, there are 8000 sample pixels for each color. Then, the pixels for each color are modeled in the HSV color space by two thresholds on each color axis.

The input of SrD is a frame of video stream and the outputs of SrD are 5 binary images. In this PU, after color modeling, an input image (frame) in the HSV color space is investigated to detect 5 colors. Therefore, 5 output binary images depict all objects of a specific color (shirt) in the input image.

For noise reduction, the closing morphological operation is applied to each binary image and small objects are removed. A small object is an object whose size is smaller than 0.001 of the size of the image. Then, the sum of the areas of the remaining objects in each binary image is calculated as the output. Therefore, the output of SrD is a vector of length 5. Finally, we normalize the output vector using the image size.

Although SrD is placed at the second level, there is no attention mechanism inside this PU. SrD uses only the image in the HSV color space.

Soccer video shots are usually divided into four types: (1) far view (or long-shot), (2) medium view (or medium-shot), (3) close-up, and (4) out-field [47]. In our proposed method, the view type of a frame is recognized by a k-nearest neighbor approach. Thus, the input of VTR is a frame and the output is a scalar from {1,2,3,4} that determines the view type of the input frame. At first, we extract proper features from the training samples according to the current LSF. Then, the k-nearest neighbor algorithm with k=3 is used for view type recognition. Mahalanobis distance is used as the distance measure in the k-nearest neighbor method. Mahalanobis distance is the generalized form of Euclidean distance that uses the variance of the training data along the principle components to compute the distance between two points. Mahalanobis distance is an adaptive version of Euclidean distance and is data-driven.

There is only a PU at the third level called LgD (Logo Detection). In broadcast soccer video, replays provide another chance to review the important events for viewers. The replays may be utilized for event detection and summarization of broadcast soccer videos. Usually, logo detection is used to separate replays from other parts of video, because replays are usually sandwiched by two logo transition at the start and the end of the occurrence.

In our proposed method, logo detection is performed by a CART (Classification and Regression Tree) [48]. At first, we extract proper features from the training samples (frames) according to the current LSF. Some extracted features (e.g., RGB histograms) are in the form of feature vectors. Therefore, we expand all feature vectors and concatenate them to create the total feature vector as the input vector. CART is used to learn logo detection based on the input feature vector. The output of LgD is 1 if logo is detected in the input frame, otherwise the output is 0. This method is very fast and efficient for logo detection.

PU's at the forth level are HLD (Highlight Detection), GKD (Goal Keeper Detection), PyD (Player Detection) and RfD (Referee Detection). In fact, PU's at the forth level analyze the content of video to detect objects or people. In this section, we explain details of algorithms employed by these PU's.


                           HLD segments a high-level video based on the detection of the replay segments. HLD is based on heuristics; thus, dynamic feature selection method is not used in it. In fact, the input of HLD is a full length video and the output is a few features about highlight segments. HLD is called iteratively for the video to find all highlights.

The proposed method for highlight detection is based on replay segmentation. According to the output of LgD, all of the frames between two consecutive logo appearances are detected as replay. After replay segmentation, some shots before each replay are selected and combined with the corresponding replay to compose a highlight. The number of selected shots before a replay is related to the length of the corresponding replay. The total length of a highlight is denoted by L
                           
                              HL
                            and computed as:
                              
                                 
                                    
                                       L
                                       
                                          H
                                          L
                                       
                                    
                                    =
                                    
                                       L
                                       Play
                                    
                                    +
                                    
                                       L
                                       Replay
                                    
                                 
                              
                           where L
                           
                              Play
                            is the total length of the selected shots (in seconds) before the replay and L
                           
                              Replay
                            is the total length of the replay (in seconds). L
                           
                              Play
                           
                           
                              min
                            is the minimum length of L
                           
                              Play
                            determined by:
                              
                                 
                                    
                                       L
                                       Play
                                       min
                                    
                                    =
                                    max
                                    
                                       
                                          10
                                          ,
                                          
                                          
                                             L
                                             Replay
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        

According to the above equation, L
                           
                              Play
                           
                           
                              min
                            is always equal or greater than 10s. Now, the minimum value of L
                           
                              Play
                            is computed but the final value of L
                           
                              Play
                            depends on the view type of the first shot of this segment. The first shot of the play part must be a far view shot, because each important event in the soccer video starts with a far view shot that shows the scene of the event. According to Fig. 4
                           , the start point of each highlight is the same as the start point of the first far view shot while the length of the play part is equal or greater than L
                           
                              Play
                           
                           
                              min
                           .

The output of HLD is composed by: (1) the start point and the total length of the highlight, (2) the start point and the length of the replay, and (3) the start point of all shots in the highlight.

Our goal in GKD, PyD and RfD is detection of goal keepers, players and referee in a close-up view respectively. In the proposed methods for GKD, PyD and RfD, we use a CART decision tree to achieve our goal. The input of these PU's is an image (frame) and their output is a scalar that determines the existence of a close-up of goal keepers, players, or referee.

The main purpose of GKD is detection of goal keeper of teams A or B in a close-up view. Output of GKD is 0 when no goal keeper is detected in the image. When the goal keeper of teams A or B is detected, GKD generates 1 or 2 respectively as the output.

In PyD, similar to GKD, we detect players of teams A and B. Therefore, the output of PyD is 0, 1 and 2 for no detection, detection of a player of teams A and detection of a player of team B, respectively.

In RfD, our purpose is detection of the referee. Therefore, the output of RfD is 0 when the referee is not detected and it is 1 when the referee is detected.

PU's at the fifth level are GED (Goal Event Detection) and CED (Card Event Detection). These PU's extract events from highlights based on the outputs of the lower PU's. Both of GED and CED use the same method for event detection but we do not combine them in a single PU because our proposed dynamic feature selection may select different feature sequence for each one. In the following, we explain our proposed method for goal and card event detection.

In the proposed method for goal event detection, first, the video is partitioned to high-level segments by HLD: highlights and non-highlights. Therefore, the first input of GED is the output of HLD. Then, we only consider highlights for goal event detection (ignore other parts of video) and therefore, we always use the output of HLD in GED. Then, the outputs of LgD, GKD, PyD, RfD and VTR from each shot of highlights are used conditionally according to the proposed dynamic feature selection method.

The proposed method for goal event detection is based on the CART decision tree. The proposed CART uses the following post-processed features obtained by the connected PU's at the lower levels:
                              
                                 •
                                 Length of replay.

Total length of shots containing close-up of goal keeper.

Total length of shots containing close-up of players.

Total length of shots containing close-up of referee.

Total length of close-up shots before replay.

Total length of out-filed shots before replay.

For recognition of the view type of a shot or detection of the referee, it is not necessary to process all frames of the shot. In GED, view type recognition and referee detection are performed based on 5 frames selected from 10%, 30%, 50%, 70%, and 90% of a shot duration. After view type recognition or referee detection for the selected frames, the majority vote method is used to decide about the view type of the shot or existence of the referee.

According to the above explanations, GED investigates each highlight independently to detect a goal event. The output of GED is 1 when it detects a goal event in a highlight and 0 otherwise.

The proposed method for card event detection is similar to our proposed method for goal event detection. In other words, the first input of CED is the output of HLD. Then, we consider highlights to extract some high level features for card event detection. In CED, we detect card events (yellow/red) in a given highlight using a CART, but we cannot determine which player receives the card and what color the card is. Therefore, the output of CED is 1 when it detects a card event in a highlight and 0 otherwise.

@&#EXPERIMENTAL RESULTS@&#

Experiments are performed on some broadcast soccer videos related to FIFA World Cup 2010, South Africa. The selected matches are listed in Table 6
                     . The total duration of this dataset is about 10h. All videos are HDTV (1280×720) and the video frame rate is 25fps (frame per second). For reduction of computational complexity, we resized video frames to 640×360.

For shot boundary detection evaluation, we use 50 different clips containing 500 shots with total length of 76min. The final GTDFS for shot boundary detection is depicted in Table 7
                        . This table shows that at the initial state, RGBH (RGB histogram) is preferable. If the result of shot boundary detection using RGBH is 0 (no shot transition), the procedure ends and an output is generated. Otherwise, in the case that the output of shot boundary detection using RGBH is 1 or 2 (cut or gradual transitions), another feature called HSVH (HSV histogram) may be used.

Experimental evaluation of the shot boundary detection approach is performed on 50 different clips containing 500 shots with a total length of 81min. The results show that the proposed method detects 47 shots wrongly and misses only 6 shots. Many of the false shot detections are related to a sudden appearance of a player, referee or assistant referee, or sudden movement of camera along a medium or close-up view. For example, some sample frames of a false detected shot is depicted in Fig. 5
                        .

Additionally, all of the missing shots are gradual shots between a far view to a medium view, or vice versa. The proposed method achieves an accuracy of 93.3% in shot detection.

According to our ground truth data, the occurrence rate of different view types is not equal in a broadcast soccer video. Far view (long-shot) is the most common view in the soccer videos and its probability of occurrence is about 65%. Occurrence probability of other view types is about 18%, 6%, and 11% for medium view (medium-shot), close-up, and out-field, respectively. For view type recognition, we use 300 samples (frames) divided into 4 classes for training. According to the above explanations, the number of training samples for each class is proportional to its occurrence probability: 195 samples for far view, 54 samples for medium view, 18 samples for close-up, and 33 samples for out-field. The final GTDFS for view type recognition is listed in Table 8
                        . This table shows that at the initial state, SHSVH (segmented HSV histogram) is preferable. If the result of view type recognition using SHSVH is 1, 3 or 4 (far view, close-up or out-field), the procedure ends and an output is generated. Otherwise, in the case that the output of the view type recognition using SHSVH is 2 (medium view), EgH (edge histogram in different scales and orientations based on 2D DWT) is used.

Evaluation of the view type recognition approach is performed on another set containing 130, 36, 12 and 22 samples for far view, medium view, close-up, and out-field, respectively. The confusion matrix for the proposed view type recognition is shown in Table 9
                        . According to this table, the total accuracy of view type recognition is 86.5%. Fig. 6
                         shows some of the misclassified frames.

To train the LgD, 186 frames that contain logo image are selected manually as positive samples and 2000 frames are selected randomly as negative samples from Video #1. The resulting GTDFS for logo detection is depicted in Table 10
                        . This table shows that at the initial state, SBD (shot boundary detection) is preferable. If the result of view type recognition using SBD is 0 (no shot), the procedure ends and no logo is reported. Otherwise, in the case that output of SBD is 1 or 2 (cut or gradual), HSVH is used to detect logo. In contrast to many current systems that use heuristic features and methods for logo detection, our proposed method learns about logo appearance during a cut or gradual transition and the best feature for logo detection (HSVH). According to the experiments, FPR and FNR of the logo detection algorithm are 0.7% and 2.7%, respectively.

Although GKD, PyD and RfD are three independent PU's in our proposed framework, their detection procedure is similar. On the other hand, these PU's may use the output of SrD (shirt detection), while SrD is based on the color information of the shirts worn by goal keepers, players and referee. Therefore, for each match, the colors of the shirts are different and accordingly, the resulting GTDFS for GKD, PyD and RfD is different. In our experiments, the first half of each match is used for training and the second half is used for evaluation. Tables 11 and 12
                        
                         summarize the results obtained on different matches.

According to the results shown in this section, the GTDFS obtained by the learning mechanism is different for the detection of different individuals. Additionally, the proposed method is usually week to detect goal keepers, players or referees who wore achromatic shirts (gray, black or white) or green shirts. Detection of green shirts is inaccurate, because soccer filed is green and detection of a green object among a green background based on only color information is inaccurate. Additionally, detection of achromatic objects is inaccurate, because there are many objects with white, black and gray colors in the image, especially in parts of the image that show out-filed and spectators. On the other hand, the proposed method for modeling gray objects in the HSV color space is not robust. Therefore, for the people who wear green shirt or achromatic shirts, our proposed method uses VTR and SrD simultaneously to increase detection accuracy.

Our proposed method for goal event detection (GED) and card event detection (CED) is based on highlight detection. In other words, after a high-level video partitioning and dividing the video into some highlights, we explore the detected highlights to detect goal and card events. On the other hand, goal and card events are both rare events; there are only 22 goal events and 19 card events in our data set. Therefore, we use the leave-one-out method for learning these events. Note that our proposed method for card event detection is unable to discriminate yellow and red event card. The resulting GTDFS's for goal and card event detection are shown in Tables 13 and 14
                        
                        , respectively. These GTDFS's are obtained by applying simple majority vote method on the GTDFS's generated by each iteration of the leave-one-out experiments.


                        Table 15
                         shows the evaluation results of the proposed method for goal and card event detection. Precision and recall rate of our method for goal event detection is 90.5% and 86.4% and for card event detection is 21.5% and 73.7%, respectively.

@&#DISCUSSION@&#

The final structure of the proposed system for goal and card event detection in broadcast soccer video is depicted in Fig. 7
                        . In this figure, gray dotted lines and ellipses show eliminated connections and PU's after learning. Other connections and PU's have remained.

Although construction of the proposed framework is bottom-up, the final system works top-down. In other words, according to the final purpose (goal or card event detection), some PU's at lower levels are called in the process.

According to the reported results, goal event detection is easier and more accurate than card event detection. The main reason is that goal event is the most important event in a soccer match. Thus, when a goal event occurs, cameraman shows a lot of close-up and out-field shots that show the cheering of the players and spectators. Also, a long replay is shown after a goal event. These salient cues help us to detect goal events successfully. Nevertheless, our proposed method has miss detection and false detection for goal event detection. For example, the highlight wrongly detected as a goal event in Video #10 is part of the 2nd half of the match between Greece and Argentina. In this highlight, the goal keeper is injured. This part of the video is detected as a goal event because the cameraman shows the event in a long replay with many close-up shots. Some informative frames related to this event are shown in Fig. 8
                        .

In Video #7, there is a goal event that is missed by the proposed method. The length of replay related to this goal event is about 21s and it is the shortest length replay of a goal event in our dataset. Thus, our proposed method does not detect this goal event. Some sample frames related to this missed goal event are shown in Fig. 9
                        .

Card event has a medium importance and the main cue of this event is detection of referee. In some foul events, referee may be shown by cameraman while a card event (yellow or red card) has not occurred. Therefore, our proposed method has some false detection. On the other hand, usually the main referee and his assistants wear shirts with the same color. Thus, the proposed method for referee detection does not discriminate main and assistant referees. Accordingly, some offside events are detected as card events when cameraman shows an assistant referee. These reasons cause a relatively high false detection rate for card event detection. On the other hand, the main reason for missing some card events is that the cameraman does not show any replay of those events. Therefore, our method does not detect any highlight for those events and the events are missed.

In Video #11, there are a lot of falsely detected card events. One of these false detections is related to a normal foul event where the referee is shown in a close-up shot during this event. Sample frames of this falsely detected card event are shown in Fig. 10
                        .

Based on experiments done on a personal computer with an AMD 4200+ Dual Core CPU and 4GB DDR2 memory, the expected processing time of each PU is estimated. Many PU's of the system are called actively; therefore, we report the expected total processing time for all PU's.

We use two terms to investigate the expected processing time and computational complexity of each PU: (1) Average Processing Time Per Call (APTPC); and (2) Average Number of Calls Per Second (ANCPS).

APTPC determines how long it takes on average for a PU to work on raw video data until it generates an output. In fact, APTPC is equal to the expected total processing time of a PU which is denoted by PUT
                        
                           g
                        
                        
                           total
                         in the previous sections. ANCPS determines how many times a PU is called during one second on average to achieve a given purpose (goal/card event detection). ANCPS is calculated for GED and CED separately in cps (calls per second). Table 16
                         shows APTPC and ANCPS of each PU. Note that video acquisition time is not considered in this table. Based on our experiments, reading of video files from a hard disk takes about 15ms per frame.

As shown in Table 16, some PU's are not called at all but we report the expected total processing time for them. These PU's usually do not provide suitable inputs for higher PU's and their expected total processing time is relatively long. Additionally, some PU's like RfD are called only for card event detection; thus, ANCPS of such PU's for goal event detection is zero.

We expect that the ANCPS of PU's at lower-levels is greater than the ANCPS of PU's at higher-levels. This is because PU's at higher levels take longer times to execute. To minimize the expected total processing time, the system minimizes the number of calls to PU's at higher levels.

Investigating computational complexity of video analysis systems based on execution speed is more common than their processing time. According to our experiments, expected execution speed of GED and CED are 33fps and 30.7fps, respectively. In this case, we assume that a video stream is acquired in a parallel processing scheme. If we assume that video acquisition and video analysis are performed sequentially, and video acquisition time is 15ms per frame, expected execution speeds of GED and CED are 22.5fps and 21.1fps, respectively. GED and CED are located at the highest level and they may work in parallel. But, we did not implement the system by parallel programming. Consequently, the total expected execution speed of the proposed soccer video analysis system for goal and card event detection is 10.9fps. In this case, GED and CED are performed independently and thus, they may call common PU's (e.g., SBD) for a given part of video repetitively.

We compare the final system developed by our proposed framework (S
                        
                           AC
                        ) with the initial system (S
                        
                           I
                        ) and the previous systems presented in [16] (S
                        
                           E
                        ), [49] (S
                        
                           K
                        ), and [50] (S
                        
                           Z
                        ). S
                        
                           I
                         is the system depicted in Fig. 3 with static connections. Also, we consider the methods presented in [16,49,50] for goal event detection. The aforementioned systems do not detect card events. Thus, we consider accuracy and computational complexity of five systems (S
                        
                           E
                        , S
                        
                           K
                        , S
                        
                           Z
                        , S
                        
                           I
                         and S
                        
                           AC
                        ) for goal event detection and two systems (S
                        
                           I
                         and S
                        
                           AC
                        ) for card event detection. In the comparison experiments, we use an additional measure named F-measure to obtain fair comparative results. F-measure is defined by:
                           
                              
                                 F
                                 =
                                 2
                                 .
                                 
                                    
                                       Precesion
                                       .
                                       Recall
                                    
                                    
                                       Precesion
                                       +
                                       Recall
                                    
                                 
                                 .
                              
                           
                        
                     

The comparison results are shown in Table 17
                        . Note that S
                        
                           AC
                         runs faster than other systems and detects both goal and card events. In contrary, S
                        
                           I
                         has the lowest accuracy and the most computational complexity. S
                        
                           I
                         has an almost fully connected structure where some connections may be redundant or irrelevant. Redundant connections between PU's provide undesired noise-like features for higher PU's in S
                        
                           I
                        . Consequently, S
                        
                           I
                         consumes much time to run but does not detect events correctly.

Among S
                        
                           E
                        , S
                        
                           K
                         and S
                        
                           Z
                        , S
                        
                           E
                         runs faster than the others but achieves the lowest precision and recall rate. In fact, S
                        
                           E
                        , S
                        
                           K
                         and S
                        
                           Z
                         are based on heuristics but S
                        
                           E
                         uses a few and simple rules for goal event detection. In contrary, S
                        
                           K
                         and S
                        
                           Z
                         use more complicated heuristics and rules for goal event detection. Consequently, S
                        
                           E
                         has the highest execution speed and the lowest accuracy among the heuristic methods.

The results show that S
                        
                           I
                         is slightly better than S
                        
                           E
                         for goal event detection but it takes too much time to run. Also, although precision and recall rate of other systems including S
                        
                           K
                        , S
                        
                           Z
                         and S
                        
                           AC
                         for goal event detection is different, there is no meaningful difference between F-measure of these system. For card event detection, just S
                        
                           I
                         and S
                        
                           AC
                         are comparable. S
                        
                           AC
                         outperforms S
                        
                           I
                         both in terms of accuracy and execution time.

Finally, S
                        
                           AC
                         is the best system from the viewpoint of accuracy and computational complexity for goal and card event detection. It fills the semantic gaps similar to heuristic systems while reducing computational complexity of the system by an attention control mechanism. In fact, our proposed framework for dynamic restructuring of S
                        
                           AC
                         with initial heuristics reduces the semantic gaps and computational complexity simultaneously.

In this paper, a new framework is proposed for development of semantic video analysis systems. The proposed framework directs attention to informative PU's while reducing overall computational complexity of the system. It applies a top-down learning attention control mechanism in a hierarchically structured video analysis system. Learning attention control mechanism directs flow of processing actively in the system to fill the semantic gaps between low-level features and high-level concepts efficiently.

At a given PU, to select best dynamic sequence of inputs, attention is directed based on the performance of the PU. Performance (accuracy and computational complexity simultaneously) of a PU is optimized by the attention control mechanism. In the final structure, to use the system, the PU's at highest level control the total attention of the system to call some PU's dynamically and ignore other PU's. Because the outputs of PU's at lower levels are fed to the PU's at higher level as input, existence of attention control mechanism in a PU at higher-level is propagated to the PU's at lower-level.

The proposed framework uses less heuristic and prior knowledge than other methods to construct a plausible and efficient structure for a semantic video analysis system. In the comparison experiments, although the system designed and operated by our proposed framework outperforms other systems, but its accuracy is limited, especially for card event detection. The previous methods compensate weakness of their methods in filling the semantic gap by heuristics and prior knowledge but in the proposed method, we use learning methods with a few heuristics to construct a system that extracts knowledge from training samples. Dependency of our framework to the learning methods results in higher sensitivity to the training samples. On the other hand, our interested events are rare and accordingly, there are a few training samples. Many machine learning methods fail in such situations. Providing enough training samples is expected to increase accuracy of our system.

One of the advantages of our proposed framework is that it finds the best structure of the system automatically from an initial, fully connected system. This means that if there is no information about the optimal system, all compatible PU's can be connected to define an initial system. The proposed framework removes worthless connections permanently and uses the remaining connection dynamically.

The PU's at the highest level (in our system, GED and CED) work independently and thus, they may call lower PU's repetitively. In this case, a common PU like shot boundary detection may be called two or more times for a given part of video. These repetitive calls decrease the execution speed of the system. To overcome this limitation, we may use an auxiliary table to avoid unnecessary repetitive computations. This method needs additional memory storage.

In the proposed method, we combine different inputs actively based on a predefined method of decision making at a given PU. In the continuation of this research, we may evaluate different methods of classifier fusion (classifier ensemble) to make a decision at a PU. For example, we may use multiple classifiers for selected inputs and combine their outputs to make the final decision.

A semantic video analysis system is very complicated and thus we may extract additional features and cues to fill the semantic gaps between low-level features and high-level concepts efficiently. In this paper, we used a few cinematic features to evaluate the proposed framework in its application to broadcast soccer videos. For future works, we may use other audio-visual features in this application. Detection of features like referee whistling and goal mouth may increase the accuracy of goal and card event detection.

Although the proposed framework is tested on soccer video analysis, this framework may be suitable for other applications of semantic video analysis like event detection in other sport videos, surveillance videos and action/activity recognition. In such applications, initial structure of the system and types of PU may be defined according to the final purpose. We will apply the proposed framework on the other applications of semantic video analysis in the future.

@&#REFERENCES@&#

